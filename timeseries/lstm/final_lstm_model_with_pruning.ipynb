{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-29T16:00:39.513891Z",
     "start_time": "2025-05-29T15:57:47.189064Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_pruning as tp\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_BASE_NAME = \"lstm_nasa\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# NASA Dataset preprocessing functions from your provided code\n",
    "column_names = ['unit_number', 'time_in_cycles'] + [f'op_setting_{i}' for i in range(1, 4)] + [f'sensor_{i}' for i in\n",
    "                                                                                               range(1, 24)]\n",
    "\n",
    "\n",
    "def load_dataframe(file_path: str) -> pd.DataFrame | None:\n",
    "    \"\"\"Loads a single CMaps data file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=' ', header=None, names=column_names)\n",
    "        # Drop the last two columns if they are all NaNs (often artifacts of space delimiter)\n",
    "        df.dropna(axis=1, how='all', inplace=True)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def clean_data(df: pd.DataFrame) -> list:\n",
    "    \"\"\"Identifies columns to remove based on low std dev.\"\"\"\n",
    "    if df is None:\n",
    "        return []\n",
    "    # Columns with std dev < 0.02 (potential candidates for removal)\n",
    "    cols_to_check = [col for col in df.columns if 'sensor' in col or 'op_setting' in col]\n",
    "    low_std_cols = [col for col in cols_to_check if df[col].std() < 0.02]\n",
    "    print(f\"Columns with std < 0.02 (potential removal): {low_std_cols}\")\n",
    "    return low_std_cols\n",
    "\n",
    "\n",
    "def add_rul(df: pd.DataFrame) -> pd.DataFrame | None:\n",
    "    \"\"\"Calculates and adds the Remaining Useful Life (RUL) column.\"\"\"\n",
    "    if df is None:\n",
    "        return None\n",
    "    max_cycles = df.groupby('unit_number')['time_in_cycles'].max().reset_index()\n",
    "    max_cycles.columns = ['unit_number', 'max_cycle']\n",
    "    df = df.merge(max_cycles, on='unit_number', how='left')\n",
    "    df['RUL'] = df['max_cycle'] - df['time_in_cycles']\n",
    "    df.drop(columns=['max_cycle'], inplace=True)\n",
    "    # Clip RUL (optional, common practice to limit max RUL)\n",
    "    df['RUL'] = df['RUL'].clip(upper=125)\n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize_data(df: pd.DataFrame,\n",
    "                   columns_to_normalize: List[str], scaler: MinMaxScaler = None) -> Tuple[pd.DataFrame, MinMaxScaler] | \\\n",
    "                                                                                    Tuple[None, None]:\n",
    "    \"\"\"Normalizes specified columns using MinMaxScaler.\"\"\"\n",
    "    if df is None:\n",
    "        return None, None\n",
    "    data_to_scale = df[columns_to_normalize]\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler()\n",
    "        df[columns_to_normalize] = scaler.fit_transform(data_to_scale)\n",
    "    else:\n",
    "        # Ensure only columns present in the scaler are transformed\n",
    "        valid_cols = [col for col in columns_to_normalize if col in scaler.feature_names_in_]\n",
    "        if len(valid_cols) < len(columns_to_normalize):\n",
    "            print(\"Warning: Some columns not found in the provided scaler. Skipping them.\")\n",
    "        if valid_cols:\n",
    "            df[valid_cols] = scaler.transform(df[valid_cols])\n",
    "    return df, scaler\n",
    "\n",
    "\n",
    "def prepare_cmapss_data(data_dir: str, train_file: str, test_file: str, test_rul_file: str) -> Tuple[\n",
    "    pd.DataFrame, pd.DataFrame, pd.DataFrame, MinMaxScaler, List[str]]:\n",
    "    \"\"\"Loads, cleans, preprocesses train/test data and RUL.\"\"\"\n",
    "    print(\"--- Preparing Training Data ---\")\n",
    "    train_df = load_dataframe(os.path.join(data_dir, train_file))\n",
    "    train_df = add_rul(train_df)\n",
    "\n",
    "    print(\"\\n--- Preparing Test Data ---\")\n",
    "    test_df = load_dataframe(os.path.join(data_dir, test_file))\n",
    "    test_rul_df = pd.read_csv(os.path.join(data_dir, test_rul_file), header=None, names=['RUL'])\n",
    "\n",
    "    # Clean Data - identify columns based on TRAINING data variance\n",
    "    cols_to_remove = clean_data(train_df)\n",
    "    feature_cols = [col for col in train_df.columns if\n",
    "                    col not in ['unit_number', 'time_in_cycles', 'RUL'] + cols_to_remove]\n",
    "    print(f\"\\nUsing Features: {feature_cols}\")\n",
    "\n",
    "    # Drop removed columns from both train and test\n",
    "    train_df.drop(columns=cols_to_remove, inplace=True, errors='ignore')\n",
    "    test_df.drop(columns=cols_to_remove, inplace=True, errors='ignore')\n",
    "\n",
    "    # Normalize features based on TRAINING data\n",
    "    print(\"\\n--- Normalizing Data ---\")\n",
    "    train_df_norm, scaler = normalize_data(train_df.copy(), feature_cols, scaler=None)\n",
    "    test_df_norm, _ = normalize_data(test_df.copy(), feature_cols, scaler=scaler)\n",
    "\n",
    "    return train_df_norm, test_df_norm, test_rul_df, scaler, feature_cols\n",
    "\n",
    "\n",
    "# Custom Dataset for NASA time series (LSTM version - no flattening)\n",
    "class NASALSTMDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, feature_cols: List[str], window_size: int = 50,\n",
    "                 stride: int = 1, is_test: bool = False, test_rul_df: pd.DataFrame = None):\n",
    "        self.df = df\n",
    "        self.feature_cols = feature_cols\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.is_test = is_test\n",
    "        self.test_rul_df = test_rul_df\n",
    "        self.samples = []\n",
    "        self.targets = []\n",
    "\n",
    "        self._prepare_samples()\n",
    "\n",
    "    def _prepare_samples(self):\n",
    "        \"\"\"Create windowed samples from the dataframe\"\"\"\n",
    "        units = self.df['unit_number'].unique()\n",
    "\n",
    "        for unit in units:\n",
    "            unit_df = self.df[self.df['unit_number'] == unit].sort_values('time_in_cycles')\n",
    "\n",
    "            # For test data, we only need the last window for each unit\n",
    "            if self.is_test:\n",
    "                if len(unit_df) >= self.window_size:\n",
    "                    window_data = unit_df[self.feature_cols].iloc[-self.window_size:].values\n",
    "                    self.samples.append(window_data)\n",
    "                    # Get RUL from test_rul_df\n",
    "                    if self.test_rul_df is not None:\n",
    "                        self.targets.append(self.test_rul_df.iloc[unit - 1]['RUL'])\n",
    "                else:\n",
    "                    # Pad if necessary\n",
    "                    window_data = unit_df[self.feature_cols].values\n",
    "                    padded = np.zeros((self.window_size, len(self.feature_cols)))\n",
    "                    padded[-len(window_data):] = window_data\n",
    "                    self.samples.append(padded)\n",
    "                    if self.test_rul_df is not None:\n",
    "                        self.targets.append(self.test_rul_df.iloc[unit - 1]['RUL'])\n",
    "            else:\n",
    "                # For training data, create multiple windows\n",
    "                for i in range(0, len(unit_df) - self.window_size + 1, self.stride):\n",
    "                    window_data = unit_df[self.feature_cols].iloc[i:i + self.window_size].values\n",
    "                    rul = unit_df['RUL'].iloc[i + self.window_size - 1]\n",
    "                    self.samples.append(window_data)\n",
    "                    self.targets.append(rul)\n",
    "\n",
    "        self.samples = np.array(self.samples, dtype=np.float32)\n",
    "        self.targets = np.array(self.targets, dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # For LSTM, keep the sequence shape (window_size, num_features)\n",
    "        sample = self.samples[idx]\n",
    "        target = self.targets[idx]\n",
    "        return torch.FloatTensor(sample), torch.FloatTensor([target])\n",
    "\n",
    "\n",
    "# LSTM Model Definition\n",
    "class NASALSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=100, num_layers=2, fc_hidden_sizes=[64, 32], dropout_rate=0.2):\n",
    "        super(NASALSTM, self).__init__()\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        # Fully connected layers after LSTM\n",
    "        fc_layers = []\n",
    "        prev_size = hidden_size\n",
    "\n",
    "        for fc_hidden_size in fc_hidden_sizes:\n",
    "            fc_layers.extend([\n",
    "                nn.Linear(prev_size, fc_hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = fc_hidden_size\n",
    "\n",
    "        # Output layer for regression\n",
    "        fc_layers.append(nn.Linear(prev_size, 1))\n",
    "\n",
    "        self.fc = nn.Sequential(*fc_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM forward pass\n",
    "        # x shape: (batch_size, sequence_length, input_size)\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "\n",
    "        # Use the last hidden state\n",
    "        # lstm_out shape: (batch_size, sequence_length, hidden_size)\n",
    "        # We take the last time step\n",
    "        last_hidden = lstm_out[:, -1, :]\n",
    "\n",
    "        # Pass through fully connected layers\n",
    "        output = self.fc(last_hidden)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def get_data_loaders(data_dir='./data/NASA', batch_size=128, window_size=50, val_split=0.2, seed=42):\n",
    "    \"\"\"Load NASA C-MAPSS dataset with train/val/test splits\"\"\"\n",
    "    print(f\"Loading NASA C-MAPSS dataset from: {data_dir}\")\n",
    "\n",
    "    # Prepare data\n",
    "    train_df, test_df, test_rul_df, scaler, feature_cols = prepare_cmapss_data(\n",
    "        data_dir,\n",
    "        'train_FD001.txt',\n",
    "        'test_FD001.txt',\n",
    "        'RUL_FD001.txt'\n",
    "    )\n",
    "\n",
    "    # Create full training dataset\n",
    "    full_train_dataset = NASALSTMDataset(train_df, feature_cols, window_size=window_size)\n",
    "\n",
    "    # Split into train and validation\n",
    "    val_size = int(len(full_train_dataset) * val_split)\n",
    "    train_size = len(full_train_dataset) - val_size\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_train_dataset, [train_size, val_size], generator=generator\n",
    "    )\n",
    "\n",
    "    # Create test dataset\n",
    "    test_dataset = NASALSTMDataset(test_df, feature_cols, window_size=window_size,\n",
    "                                   is_test=True, test_rul_df=test_rul_df)\n",
    "\n",
    "    # Create data loaders\n",
    "    num_workers = min(4, os.cpu_count() or 2)\n",
    "    pin_memory = True if DEVICE.type == 'cuda' else False\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=num_workers, pin_memory=pin_memory)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                            num_workers=num_workers, pin_memory=pin_memory)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                             num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    print(f\"DataLoaders created - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "    print(f\"Input shape: ({window_size}, {len(feature_cols)}) (sequence_length, num_features)\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader, len(feature_cols)\n",
    "\n",
    "\n",
    "def get_lstm_model(input_size, hidden_size=100, num_layers=2, fc_hidden_sizes=[64, 32], dropout_rate=0.2):\n",
    "    \"\"\"Get LSTM model for NASA dataset\"\"\"\n",
    "    model = NASALSTM(input_size, hidden_size, num_layers, fc_hidden_sizes, dropout_rate)\n",
    "    print(f\"‚úÖ Created LSTM with architecture:\")\n",
    "    print(f\"   LSTM: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}\")\n",
    "    print(f\"   FC: {hidden_size} -> {' -> '.join(map(str, fc_hidden_sizes))} -> 1\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_ignored_layers(model):\n",
    "    \"\"\"Get layers to ignore during pruning (typically final layer)\"\"\"\n",
    "    ignored_layers = []\n",
    "\n",
    "    # Ignore the LSTM layer since torch_pruning only supports single-layer LSTMs\n",
    "    ignored_layers.append(model.lstm)\n",
    "\n",
    "    # Get the last linear layer in the fc sequential model\n",
    "    for module in model.fc:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            last_linear = module\n",
    "    ignored_layers.append(last_linear)\n",
    "\n",
    "    return ignored_layers\n",
    "\n",
    "\n",
    "def calculate_macs_params(model, example_input):\n",
    "    \"\"\"Calculate MACs and parameters using torch_pruning\"\"\"\n",
    "    model.eval()\n",
    "    target_device = example_input.device\n",
    "    model_on_device = model.to(target_device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        macs, params = tp.utils.count_ops_and_params(model_on_device, example_input)\n",
    "\n",
    "    return macs, params\n",
    "\n",
    "\n",
    "def save_model(model, save_path, example_input_cpu=None):\n",
    "    \"\"\"Save model state dict and optionally ONNX\"\"\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"‚úÖ Model saved to {save_path}\")\n",
    "\n",
    "    if example_input_cpu is not None:\n",
    "        onnx_path = save_path.replace('.pth', '.onnx')\n",
    "        try:\n",
    "            model_cpu = model.to('cpu')\n",
    "            torch.onnx.export(\n",
    "                model_cpu, example_input_cpu, onnx_path,\n",
    "                export_params=True, opset_version=13,\n",
    "                input_names=['input'], output_names=['output'],\n",
    "                dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "            )\n",
    "            print(f\"‚úÖ ONNX model saved to {onnx_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: ONNX export failed: {e}\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, example_input, criterion, device):\n",
    "    \"\"\"Evaluate model and return comprehensive metrics\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Calculate efficiency metrics\n",
    "    macs, params = calculate_macs_params(model, example_input.to(device))\n",
    "    model_size_mb = params * 4 / (1024 * 1024)  # Assuming float32\n",
    "\n",
    "    # Calculate MSE and MAE\n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            all_predictions.extend(output.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    mse = np.mean((all_predictions - all_targets) ** 2)\n",
    "    mae = np.mean(np.abs(all_predictions - all_targets))\n",
    "\n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'loss': total_loss / len(data_loader.dataset),\n",
    "        'macs': macs,\n",
    "        'params': params,\n",
    "        'size_mb': model_size_mb\n",
    "    }\n",
    "\n",
    "\n",
    "def prune_model(model, strategy_config, sparsity_ratio, example_input, ignored_layers=None):\n",
    "    \"\"\"Apply structured pruning to model\"\"\"\n",
    "    if sparsity_ratio == 0.0:\n",
    "        print(\"No pruning needed (sparsity = 0.0)\")\n",
    "        return model\n",
    "\n",
    "    model.eval()\n",
    "    pruned_model = copy.deepcopy(model)\n",
    "    pruned_model.to(example_input.device)\n",
    "\n",
    "    # Calculate initial MACs\n",
    "    initial_macs, _ = calculate_macs_params(pruned_model, example_input)\n",
    "    print(f\"Initial MACs: {initial_macs / 1e6:.2f}M\")\n",
    "\n",
    "    ignored_layers = ignored_layers or []\n",
    "\n",
    "    # Create pruner based on strategy\n",
    "    # Only prune Linear layers, not LSTM\n",
    "    pruner = strategy_config['pruner'](\n",
    "        pruned_model,\n",
    "        example_input,\n",
    "        importance=strategy_config['importance'],\n",
    "        iterative_steps=5,  # Use 5 iterative steps\n",
    "        ch_sparsity=sparsity_ratio,\n",
    "        root_module_types=[nn.Linear],  # Only prune Linear layers\n",
    "        ignored_layers=ignored_layers\n",
    "    )\n",
    "\n",
    "    print(f\"Applying {strategy_config['importance'].__class__.__name__} pruning at {sparsity_ratio:.1%} sparsity...\")\n",
    "    print(\"Note: Only pruning FC layers, LSTM layers are preserved\")\n",
    "\n",
    "    # Apply pruning\n",
    "    pruner.step()\n",
    "\n",
    "    # Calculate final MACs\n",
    "    final_macs, _ = calculate_macs_params(pruned_model, example_input)\n",
    "    reduction = (initial_macs - final_macs) / initial_macs * 100 if initial_macs > 0 else 0\n",
    "    print(f\"Final MACs: {final_macs / 1e6:.2f}M (Reduction: {reduction:.1f}%)\")\n",
    "\n",
    "    return pruned_model\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs,\n",
    "                val_loader=None, patience=7, log_prefix=\"\"):\n",
    "    \"\"\"Train model with early stopping\"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_mse': [],\n",
    "        'val_loss': [],\n",
    "        'val_mse': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_predictions = []\n",
    "        train_targets = []\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_predictions.extend(output.detach().cpu().numpy())\n",
    "            train_targets.extend(target.cpu().numpy())\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_mse = np.mean((np.array(train_predictions) - np.array(train_targets)) ** 2)\n",
    "\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_mse'].append(train_mse)\n",
    "\n",
    "        log_msg = f\"Epoch {epoch + 1}/{num_epochs} ({log_prefix}): Train Loss: {avg_train_loss:.4f}, Train MSE: {train_mse:.2f}\"\n",
    "\n",
    "        # Validation phase\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_predictions = []\n",
    "            val_targets = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for data, target in val_loader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                    val_predictions.extend(output.cpu().numpy())\n",
    "                    val_targets.extend(target.cpu().numpy())\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_mse = np.mean((np.array(val_predictions) - np.array(val_targets)) ** 2)\n",
    "\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            history['val_mse'].append(val_mse)\n",
    "\n",
    "            log_msg += f\", Val Loss: {avg_val_loss:.4f}, Val MSE: {val_mse:.2f}\"\n",
    "\n",
    "            # Early stopping check\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                epochs_no_improve = 0\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "                log_msg += \" (Best)\"\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"{log_msg}\")\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "                break\n",
    "        else:\n",
    "            history['val_loss'].append(None)\n",
    "            history['val_mse'].append(None)\n",
    "\n",
    "        print(log_msg)\n",
    "\n",
    "    # Load best model state if available\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"Loaded best model state\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def save_results_to_files(all_results, output_dir):\n",
    "    \"\"\"Save experimental results to JSON and CSV files\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save complete results as JSON\n",
    "    results_json_path = os.path.join(output_dir, 'complete_results.json')\n",
    "    with open(results_json_path, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2, default=str)\n",
    "    print(f\"‚úÖ Complete results saved to {results_json_path}\")\n",
    "\n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for strategy, strategy_results in all_results.items():\n",
    "        for sparsity, metrics in strategy_results.items():\n",
    "            row = {\n",
    "                'strategy': strategy,\n",
    "                'sparsity_ratio': sparsity,\n",
    "                'mse': metrics['mse'],\n",
    "                'mae': metrics['mae'],\n",
    "                'loss': metrics['loss'],\n",
    "                'macs_millions': metrics['macs'] / 1e6,\n",
    "                'params_millions': metrics['params'] / 1e6,\n",
    "                'size_mb': metrics['size_mb']\n",
    "            }\n",
    "            summary_data.append(row)\n",
    "\n",
    "    # Save summary as CSV\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_csv_path = os.path.join(output_dir, 'summary_results.csv')\n",
    "    summary_df.to_csv(summary_csv_path, index=False)\n",
    "    print(f\"‚úÖ Summary results saved to {summary_csv_path}\")\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "def create_results_plots(summary_df, output_dir):\n",
    "    \"\"\"Create visualization plots for regression metrics\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    strategies = summary_df['strategy'].unique()\n",
    "    sparsity_levels = sorted(summary_df['sparsity_ratio'].unique())\n",
    "\n",
    "    # Plot 1: MSE vs Sparsity\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for strategy in strategies:\n",
    "        strategy_data = summary_df[summary_df['strategy'] == strategy].sort_values('sparsity_ratio')\n",
    "        plt.plot(strategy_data['sparsity_ratio'] * 100, strategy_data['mse'],\n",
    "                 'o-', linewidth=2, markersize=8, label=strategy)\n",
    "\n",
    "    plt.xlabel('Sparsity (%)', fontsize=12)\n",
    "    plt.ylabel('MSE', fontsize=12)\n",
    "    plt.title('NASA LSTM: MSE vs Sparsity (FC Layers Only)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.gca().invert_yaxis()  # Lower MSE is better\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(output_dir, 'mse_vs_sparsity.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"‚úÖ MSE plot saved to {plot_path}\")\n",
    "\n",
    "    # Plot 2: Efficiency frontier (MSE vs MACs)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for strategy in strategies:\n",
    "        strategy_data = summary_df[summary_df['strategy'] == strategy].sort_values('sparsity_ratio')\n",
    "        plt.scatter(strategy_data['macs_millions'], strategy_data['mse'],\n",
    "                    s=100, label=strategy, alpha=0.8)\n",
    "        plt.plot(strategy_data['macs_millions'], strategy_data['mse'],\n",
    "                 '--', alpha=0.6)\n",
    "\n",
    "    plt.xlabel('MACs (Millions)', fontsize=12)\n",
    "    plt.ylabel('MSE', fontsize=12)\n",
    "    plt.title('NASA LSTM: Efficiency Frontier (MSE vs MACs)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.gca().invert_yaxis()  # Lower MSE is better\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(output_dir, 'efficiency_frontier.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"‚úÖ Efficiency frontier plot saved to {plot_path}\")\n",
    "\n",
    "\n",
    "def print_results_table(summary_df):\n",
    "    \"\"\"Print formatted results table\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXPERIMENTAL RESULTS SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Baseline results\n",
    "    baseline_results = summary_df[summary_df['sparsity_ratio'] == 0.0].iloc[0]\n",
    "    print(f\"\\nBaseline Performance:\")\n",
    "    print(f\"  MSE: {baseline_results['mse']:.2f}\")\n",
    "    print(f\"  MAE: {baseline_results['mae']:.2f}\")\n",
    "    print(f\"  MACs: {baseline_results['macs_millions']:.2f}M\")\n",
    "    print(f\"  Parameters: {baseline_results['params_millions']:.2f}M\")\n",
    "    print(f\"  Model Size: {baseline_results['size_mb']:.2f}MB\")\n",
    "\n",
    "    # Strategy comparison at 50% sparsity\n",
    "    print(f\"\\nStrategy Comparison at 50% Sparsity (FC Layers Only):\")\n",
    "    sparsity_50_data = summary_df[summary_df['sparsity_ratio'] == 0.5]\n",
    "    for _, row in sparsity_50_data.iterrows():\n",
    "        degradation = row['mse'] - baseline_results['mse']\n",
    "        relative_increase = (degradation / baseline_results['mse']) * 100\n",
    "        print(\n",
    "            f\"  {row['strategy']:>12}: MSE={row['mse']:>6.2f} ({degradation:>+5.2f}, {relative_increase:>+5.1f}% increase)\")\n",
    "\n",
    "    # Complete results table\n",
    "    print(f\"\\nComplete Results Table:\")\n",
    "    print(\"-\" * 90)\n",
    "    print(f\"{'Strategy':<12} {'Sparsity':<8} {'MSE':<8} {'MAE':<8} {'MACs(M)':<8} {'Params(M)':<9} {'Size(MB)':<8}\")\n",
    "    print(\"-\" * 90)\n",
    "\n",
    "    for _, row in summary_df.sort_values(['strategy', 'sparsity_ratio']).iterrows():\n",
    "        print(f\"{row['strategy']:<12} {row['sparsity_ratio'] * 100:>6.0f}% \"\n",
    "              f\"{row['mse']:>7.2f} {row['mae']:>7.2f} {row['macs_millions']:>7.2f} \"\n",
    "              f\"{row['params_millions']:>8.2f} {row['size_mb']:>7.2f}\")\n",
    "\n",
    "    print(\"\\nNote: Sparsity is applied only to FC layers. LSTM layers remain unchanged.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main experimental workflow\"\"\"\n",
    "    print(\"Starting NASA LSTM Pruning Experiments\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'strategies': {\n",
    "            'MagnitudeL2': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.MagnitudeImportance(p=2)\n",
    "            },\n",
    "            'Random': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.RandomImportance()\n",
    "            },\n",
    "            # Note: BNScale is not applicable to LSTM as it doesn't have BatchNorm layers\n",
    "        },\n",
    "        'pruning_ratios': [0.0, 0.2, 0.5, 0.7],\n",
    "        'hidden_size': 100,\n",
    "        'num_layers': 2,\n",
    "        'fc_hidden_sizes': [64, 32],\n",
    "        'dropout_rate': 0.2,\n",
    "        'window_size': 50,\n",
    "        'batch_size': 128,\n",
    "        'learning_rate': 0.0001,\n",
    "        'epochs': 50,  # More epochs for time series\n",
    "        'patience': 10,\n",
    "        'output_dir': './results_lstm_nasa',\n",
    "        'models_dir': './models_lstm_nasa',\n",
    "        'data_dir': './data/CMaps'\n",
    "    }\n",
    "\n",
    "    # Create output directories\n",
    "    os.makedirs(config['output_dir'], exist_ok=True)\n",
    "    os.makedirs(config['models_dir'], exist_ok=True)\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading NASA C-MAPSS dataset...\")\n",
    "    train_loader, val_loader, test_loader, input_size = get_data_loaders(\n",
    "        data_dir=config['data_dir'],\n",
    "        batch_size=config['batch_size'],\n",
    "        window_size=config['window_size']\n",
    "    )\n",
    "\n",
    "    # Prepare inputs and criterion\n",
    "    example_input_cpu = torch.randn(1, config['window_size'], input_size)\n",
    "    example_input_device = example_input_cpu.to(DEVICE)\n",
    "    criterion = nn.MSELoss()  # Regression loss\n",
    "\n",
    "    # Get baseline model and train it\n",
    "    print(\"\\nCreating and training baseline model...\")\n",
    "    model = get_lstm_model(\n",
    "        input_size,\n",
    "        config['hidden_size'],\n",
    "        config['num_layers'],\n",
    "        config['fc_hidden_sizes'],\n",
    "        config['dropout_rate']\n",
    "    )\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Train baseline model\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    trained_model, training_history = train_model(\n",
    "        model, train_loader, criterion, optimizer, DEVICE,\n",
    "        config['epochs'], val_loader, config['patience'], \"Baseline Training\"\n",
    "    )\n",
    "\n",
    "    # Save baseline model\n",
    "    baseline_model_path = os.path.join(config['models_dir'], 'baseline_model.pth')\n",
    "    save_model(trained_model, baseline_model_path, example_input_cpu)\n",
    "\n",
    "    # Evaluate baseline\n",
    "    print(\"\\nEvaluating baseline model...\")\n",
    "    baseline_metrics = evaluate_model(trained_model, test_loader, example_input_device, criterion, DEVICE)\n",
    "    print(f\"Baseline Results: MSE={baseline_metrics['mse']:.2f}, \"\n",
    "          f\"MAE={baseline_metrics['mae']:.2f}, \"\n",
    "          f\"MACs={baseline_metrics['macs'] / 1e6:.2f}M, \"\n",
    "          f\"Params={baseline_metrics['params'] / 1e6:.2f}M\")\n",
    "\n",
    "    # Initialize results storage\n",
    "    all_results = {}\n",
    "    for strategy_name in config['strategies'].keys():\n",
    "        all_results[strategy_name] = {0.0: baseline_metrics}\n",
    "\n",
    "    # Get ignored layers\n",
    "    ignored_layers = get_ignored_layers(trained_model)\n",
    "\n",
    "    # Run pruning experiments\n",
    "    print(\"\\nStarting pruning experiments...\")\n",
    "    print(\"Note: Due to torch_pruning limitations, only FC layers will be pruned (LSTM layers preserved)\")\n",
    "\n",
    "    for strategy_name, strategy_config in config['strategies'].items():\n",
    "        print(f\"\\n--- Strategy: {strategy_name} ---\")\n",
    "\n",
    "        for sparsity_ratio in config['pruning_ratios']:\n",
    "            if sparsity_ratio == 0.0:\n",
    "                continue  # Skip baseline (already done)\n",
    "\n",
    "            print(f\"\\nProcessing {strategy_name} at {sparsity_ratio:.1%} sparsity...\")\n",
    "\n",
    "            # Load fresh copy of trained baseline\n",
    "            model_copy = get_lstm_model(\n",
    "                input_size,\n",
    "                config['hidden_size'],\n",
    "                config['num_layers'],\n",
    "                config['fc_hidden_sizes'],\n",
    "                config['dropout_rate']\n",
    "            )\n",
    "            model_copy.load_state_dict(torch.load(baseline_model_path, map_location=DEVICE))\n",
    "            model_copy.to(DEVICE)\n",
    "\n",
    "            # Apply pruning\n",
    "            pruned_model = prune_model(\n",
    "                model_copy, strategy_config, sparsity_ratio,\n",
    "                example_input_device, ignored_layers\n",
    "            )\n",
    "\n",
    "            # Fine-tune pruned model\n",
    "            print(\"Fine-tuning pruned model...\")\n",
    "            optimizer_ft = optim.Adam(pruned_model.parameters(), lr=config['learning_rate'])\n",
    "            fine_tuned_model, ft_history = train_model(\n",
    "                pruned_model, train_loader, criterion, optimizer_ft, DEVICE,\n",
    "                config['epochs'], val_loader, config['patience'],\n",
    "                f\"{strategy_name}-{sparsity_ratio:.1%}\"\n",
    "            )\n",
    "\n",
    "            # Evaluate fine-tuned model\n",
    "            final_metrics = evaluate_model(fine_tuned_model, test_loader, example_input_device, criterion, DEVICE)\n",
    "            all_results[strategy_name][sparsity_ratio] = final_metrics\n",
    "\n",
    "            print(f\"Results: MSE={final_metrics['mse']:.2f}, \"\n",
    "                  f\"MAE={final_metrics['mae']:.2f}, \"\n",
    "                  f\"MACs={final_metrics['macs'] / 1e6:.2f}M\")\n",
    "\n",
    "            # Save fine-tuned model\n",
    "            model_filename = f\"{strategy_name.lower()}_sparsity_{sparsity_ratio:.1f}.pth\"\n",
    "            model_path = os.path.join(config['models_dir'], model_filename)\n",
    "            save_model(fine_tuned_model, model_path, example_input_cpu)\n",
    "\n",
    "    # Save and analyze results\n",
    "    print(\"\\nSaving results...\")\n",
    "    summary_df = save_results_to_files(all_results, config['output_dir'])\n",
    "\n",
    "    # Create plots\n",
    "    print(\"Creating plots...\")\n",
    "    create_results_plots(summary_df, config['output_dir'])\n",
    "\n",
    "    # Print summary\n",
    "    print_results_table(summary_df)\n",
    "\n",
    "    print(f\"\\nüéâ All experiments completed!\")\n",
    "    print(f\"üìÅ Results saved to: {os.path.abspath(config['output_dir'])}\")\n",
    "    print(f\"üìÅ Models saved to: {os.path.abspath(config['models_dir'])}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting NASA LSTM Pruning Experiments\n",
      "============================================================\n",
      "Loading NASA C-MAPSS dataset...\n",
      "Loading NASA C-MAPSS dataset from: ./data/CMaps\n",
      "--- Preparing Training Data ---\n",
      "\n",
      "--- Preparing Test Data ---\n",
      "Columns with std < 0.02 (potential removal): ['op_setting_1', 'op_setting_2', 'op_setting_3', 'sensor_1', 'sensor_5', 'sensor_6', 'sensor_10', 'sensor_16', 'sensor_18', 'sensor_19']\n",
      "\n",
      "Using Features: ['sensor_2', 'sensor_3', 'sensor_4', 'sensor_7', 'sensor_8', 'sensor_9', 'sensor_11', 'sensor_12', 'sensor_13', 'sensor_14', 'sensor_15', 'sensor_17', 'sensor_20', 'sensor_21']\n",
      "\n",
      "--- Normalizing Data ---\n",
      "DataLoaders created - Train: 12585, Val: 3146, Test: 100\n",
      "Input shape: (50, 14) (sequence_length, num_features)\n",
      "\n",
      "Creating and training baseline model...\n",
      "‚úÖ Created LSTM with architecture:\n",
      "   LSTM: input_size=14, hidden_size=100, num_layers=2\n",
      "   FC: 100 -> 64 -> 32 -> 1\n",
      "Epoch 1/50 (Baseline Training): Train Loss: 7289.6763, Train MSE: 7284.74, Val Loss: 7291.1691, Val MSE: 7283.26 (Best)\n",
      "Epoch 2/50 (Baseline Training): Train Loss: 6636.5603, Train MSE: 6636.81, Val Loss: 6310.7606, Val MSE: 6303.61 (Best)\n",
      "Epoch 3/50 (Baseline Training): Train Loss: 5502.8650, Train MSE: 5502.09, Val Loss: 4962.4899, Val MSE: 4956.54 (Best)\n",
      "Epoch 4/50 (Baseline Training): Train Loss: 4113.0171, Train MSE: 4121.38, Val Loss: 3516.5620, Val MSE: 3512.22 (Best)\n",
      "Epoch 5/50 (Baseline Training): Train Loss: 2889.8132, Train MSE: 2891.49, Val Loss: 2422.1856, Val MSE: 2419.59 (Best)\n",
      "Epoch 6/50 (Baseline Training): Train Loss: 2175.1376, Train MSE: 2175.46, Val Loss: 1900.2451, Val MSE: 1899.06 (Best)\n",
      "Epoch 7/50 (Baseline Training): Train Loss: 1925.7819, Train MSE: 1924.33, Val Loss: 1748.8457, Val MSE: 1748.43 (Best)\n",
      "Epoch 8/50 (Baseline Training): Train Loss: 1884.6628, Train MSE: 1885.20, Val Loss: 1717.2237, Val MSE: 1717.10 (Best)\n",
      "Epoch 9/50 (Baseline Training): Train Loss: 1902.9723, Train MSE: 1905.04, Val Loss: 1711.2000, Val MSE: 1711.15 (Best)\n",
      "Epoch 10/50 (Baseline Training): Train Loss: 1882.0876, Train MSE: 1880.81, Val Loss: 1708.0460, Val MSE: 1708.04 (Best)\n",
      "Epoch 11/50 (Baseline Training): Train Loss: 1880.0700, Train MSE: 1881.16, Val Loss: 1707.9252, Val MSE: 1707.92 (Best)\n",
      "Epoch 12/50 (Baseline Training): Train Loss: 1879.5067, Train MSE: 1879.33, Val Loss: 1707.0430, Val MSE: 1707.05 (Best)\n",
      "Epoch 13/50 (Baseline Training): Train Loss: 1884.9724, Train MSE: 1884.49, Val Loss: 1708.6285, Val MSE: 1708.62\n",
      "Epoch 14/50 (Baseline Training): Train Loss: 1890.9981, Train MSE: 1892.07, Val Loss: 1709.0438, Val MSE: 1709.02\n",
      "Epoch 15/50 (Baseline Training): Train Loss: 1897.8330, Train MSE: 1894.93, Val Loss: 1711.0776, Val MSE: 1711.03\n",
      "Epoch 16/50 (Baseline Training): Train Loss: 1883.8997, Train MSE: 1880.60, Val Loss: 1709.4486, Val MSE: 1709.42\n",
      "Epoch 17/50 (Baseline Training): Train Loss: 1871.2305, Train MSE: 1872.32, Val Loss: 1707.9598, Val MSE: 1707.96\n",
      "Epoch 18/50 (Baseline Training): Train Loss: 1885.9420, Train MSE: 1888.21, Val Loss: 1709.5983, Val MSE: 1709.57\n",
      "Epoch 19/50 (Baseline Training): Train Loss: 1883.3154, Train MSE: 1882.52, Val Loss: 1707.0906, Val MSE: 1707.10\n",
      "Epoch 20/50 (Baseline Training): Train Loss: 1875.4519, Train MSE: 1876.36, Val Loss: 1708.6061, Val MSE: 1708.59\n",
      "Epoch 21/50 (Baseline Training): Train Loss: 1893.1942, Train MSE: 1894.89, Val Loss: 1711.0210, Val MSE: 1710.97\n",
      "Epoch 22/50 (Baseline Training): Train Loss: 1889.4967, Train MSE: 1890.65, Val Loss: 1713.8333, Val MSE: 1713.75\n",
      "Early stopping triggered after 22 epochs\n",
      "Loaded best model state\n",
      "‚úÖ Model saved to ./models_lstm_nasa/baseline_model.pth\n",
      "‚úÖ ONNX model saved to ./models_lstm_nasa/baseline_model.onnx\n",
      "\n",
      "Evaluating baseline model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/onnx/symbolic_opset9.py:4277: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n",
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
      "  result = _VF.lstm(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Results: MSE=1732.94, MAE=37.11, MACs=90.45M, Params=0.14M\n",
      "\n",
      "Starting pruning experiments...\n",
      "Note: Due to torch_pruning limitations, only FC layers will be pruned (LSTM layers preserved)\n",
      "\n",
      "--- Strategy: MagnitudeL2 ---\n",
      "\n",
      "Processing MagnitudeL2 at 20.0% sparsity...\n",
      "‚úÖ Created LSTM with architecture:\n",
      "   LSTM: input_size=14, hidden_size=100, num_layers=2\n",
      "   FC: 100 -> 64 -> 32 -> 1\n",
      "Initial MACs: 90.45M\n",
      "Applying MagnitudeImportance pruning at 20.0% sparsity...\n",
      "Note: Only pruning FC layers, LSTM layers are preserved\n",
      "Final MACs: 90.45M (Reduction: 0.0%)\n",
      "Fine-tuning pruned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch_pruning/pruner/algorithms/base_pruner.py:87: UserWarning: ch_sparsity is deprecated in v1.3.0. Please use pruning_ratio.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 (MagnitudeL2-20.0%): Train Loss: 1872.6471, Train MSE: 1871.57, Val Loss: 1701.9805, Val MSE: 1701.87 (Best)\n",
      "Epoch 2/50 (MagnitudeL2-20.0%): Train Loss: 1372.8507, Train MSE: 1374.59, Val Loss: 761.2462, Val MSE: 761.53 (Best)\n",
      "Epoch 3/50 (MagnitudeL2-20.0%): Train Loss: 741.4669, Train MSE: 740.29, Val Loss: 444.1741, Val MSE: 444.02 (Best)\n",
      "Epoch 4/50 (MagnitudeL2-20.0%): Train Loss: 535.4898, Train MSE: 534.78, Val Loss: 305.9468, Val MSE: 306.07 (Best)\n",
      "Epoch 5/50 (MagnitudeL2-20.0%): Train Loss: 488.9269, Train MSE: 487.97, Val Loss: 239.0258, Val MSE: 238.43 (Best)\n",
      "Epoch 6/50 (MagnitudeL2-20.0%): Train Loss: 464.4396, Train MSE: 465.17, Val Loss: 230.5734, Val MSE: 230.47 (Best)\n",
      "Epoch 7/50 (MagnitudeL2-20.0%): Train Loss: 448.9106, Train MSE: 448.79, Val Loss: 232.7106, Val MSE: 233.01\n",
      "Epoch 8/50 (MagnitudeL2-20.0%): Train Loss: 439.2762, Train MSE: 438.83, Val Loss: 220.8464, Val MSE: 220.06 (Best)\n",
      "Epoch 9/50 (MagnitudeL2-20.0%): Train Loss: 415.5915, Train MSE: 415.60, Val Loss: 214.5647, Val MSE: 214.93 (Best)\n",
      "Epoch 10/50 (MagnitudeL2-20.0%): Train Loss: 420.2029, Train MSE: 420.82, Val Loss: 205.6759, Val MSE: 205.62 (Best)\n",
      "Epoch 11/50 (MagnitudeL2-20.0%): Train Loss: 430.3440, Train MSE: 430.85, Val Loss: 201.2753, Val MSE: 201.47 (Best)\n",
      "Epoch 12/50 (MagnitudeL2-20.0%): Train Loss: 407.0820, Train MSE: 408.45, Val Loss: 206.1801, Val MSE: 206.07\n",
      "Epoch 13/50 (MagnitudeL2-20.0%): Train Loss: 409.6793, Train MSE: 409.61, Val Loss: 190.0795, Val MSE: 189.98 (Best)\n",
      "Epoch 14/50 (MagnitudeL2-20.0%): Train Loss: 406.5145, Train MSE: 406.74, Val Loss: 197.3995, Val MSE: 197.07\n",
      "Epoch 15/50 (MagnitudeL2-20.0%): Train Loss: 400.5317, Train MSE: 401.10, Val Loss: 199.2478, Val MSE: 198.91\n",
      "Epoch 16/50 (MagnitudeL2-20.0%): Train Loss: 401.7305, Train MSE: 401.11, Val Loss: 180.1963, Val MSE: 179.88 (Best)\n",
      "Epoch 17/50 (MagnitudeL2-20.0%): Train Loss: 397.0669, Train MSE: 396.24, Val Loss: 178.9668, Val MSE: 178.61 (Best)\n",
      "Epoch 18/50 (MagnitudeL2-20.0%): Train Loss: 387.3547, Train MSE: 388.31, Val Loss: 182.5217, Val MSE: 181.88\n",
      "Epoch 19/50 (MagnitudeL2-20.0%): Train Loss: 390.3998, Train MSE: 390.03, Val Loss: 189.2812, Val MSE: 189.17\n",
      "Epoch 20/50 (MagnitudeL2-20.0%): Train Loss: 391.4922, Train MSE: 391.71, Val Loss: 171.1791, Val MSE: 171.15 (Best)\n",
      "Epoch 21/50 (MagnitudeL2-20.0%): Train Loss: 378.2130, Train MSE: 378.99, Val Loss: 169.3065, Val MSE: 169.13 (Best)\n",
      "Epoch 22/50 (MagnitudeL2-20.0%): Train Loss: 393.3594, Train MSE: 393.33, Val Loss: 173.3882, Val MSE: 173.06\n",
      "Epoch 23/50 (MagnitudeL2-20.0%): Train Loss: 376.8817, Train MSE: 377.15, Val Loss: 171.8646, Val MSE: 171.44\n",
      "Epoch 24/50 (MagnitudeL2-20.0%): Train Loss: 374.5217, Train MSE: 374.30, Val Loss: 184.1115, Val MSE: 184.61\n",
      "Epoch 25/50 (MagnitudeL2-20.0%): Train Loss: 375.9136, Train MSE: 376.00, Val Loss: 166.3572, Val MSE: 166.53 (Best)\n",
      "Epoch 26/50 (MagnitudeL2-20.0%): Train Loss: 368.9725, Train MSE: 369.46, Val Loss: 162.2815, Val MSE: 162.11 (Best)\n",
      "Epoch 27/50 (MagnitudeL2-20.0%): Train Loss: 380.2001, Train MSE: 380.10, Val Loss: 181.1877, Val MSE: 181.12\n",
      "Epoch 28/50 (MagnitudeL2-20.0%): Train Loss: 363.4744, Train MSE: 362.96, Val Loss: 196.0535, Val MSE: 194.89\n",
      "Epoch 29/50 (MagnitudeL2-20.0%): Train Loss: 361.1964, Train MSE: 360.84, Val Loss: 183.1419, Val MSE: 182.53\n",
      "Epoch 30/50 (MagnitudeL2-20.0%): Train Loss: 359.0299, Train MSE: 359.60, Val Loss: 163.2037, Val MSE: 163.11\n",
      "Epoch 31/50 (MagnitudeL2-20.0%): Train Loss: 369.5784, Train MSE: 369.95, Val Loss: 165.7388, Val MSE: 165.86\n",
      "Epoch 32/50 (MagnitudeL2-20.0%): Train Loss: 369.6018, Train MSE: 370.07, Val Loss: 158.0436, Val MSE: 158.03 (Best)\n",
      "Epoch 33/50 (MagnitudeL2-20.0%): Train Loss: 365.9550, Train MSE: 366.13, Val Loss: 171.8275, Val MSE: 171.64\n",
      "Epoch 34/50 (MagnitudeL2-20.0%): Train Loss: 355.4941, Train MSE: 356.12, Val Loss: 179.2340, Val MSE: 178.51\n",
      "Epoch 35/50 (MagnitudeL2-20.0%): Train Loss: 356.6582, Train MSE: 356.10, Val Loss: 229.5783, Val MSE: 228.67\n",
      "Epoch 36/50 (MagnitudeL2-20.0%): Train Loss: 356.0212, Train MSE: 355.89, Val Loss: 158.1493, Val MSE: 157.75\n",
      "Epoch 37/50 (MagnitudeL2-20.0%): Train Loss: 349.8548, Train MSE: 349.87, Val Loss: 149.3959, Val MSE: 149.23 (Best)\n",
      "Epoch 38/50 (MagnitudeL2-20.0%): Train Loss: 347.3558, Train MSE: 347.76, Val Loss: 162.3540, Val MSE: 162.58\n",
      "Epoch 39/50 (MagnitudeL2-20.0%): Train Loss: 354.9522, Train MSE: 354.95, Val Loss: 149.3603, Val MSE: 149.53 (Best)\n",
      "Epoch 40/50 (MagnitudeL2-20.0%): Train Loss: 343.4100, Train MSE: 343.77, Val Loss: 162.7860, Val MSE: 163.32\n",
      "Epoch 41/50 (MagnitudeL2-20.0%): Train Loss: 344.1732, Train MSE: 344.66, Val Loss: 153.2952, Val MSE: 152.96\n",
      "Epoch 42/50 (MagnitudeL2-20.0%): Train Loss: 338.0736, Train MSE: 337.94, Val Loss: 160.0678, Val MSE: 159.73\n",
      "Epoch 43/50 (MagnitudeL2-20.0%): Train Loss: 353.4815, Train MSE: 353.07, Val Loss: 216.6097, Val MSE: 215.53\n",
      "Epoch 44/50 (MagnitudeL2-20.0%): Train Loss: 344.3468, Train MSE: 344.15, Val Loss: 143.9937, Val MSE: 143.96 (Best)\n",
      "Epoch 45/50 (MagnitudeL2-20.0%): Train Loss: 352.4513, Train MSE: 351.92, Val Loss: 146.3354, Val MSE: 146.18\n",
      "Epoch 46/50 (MagnitudeL2-20.0%): Train Loss: 345.5739, Train MSE: 345.88, Val Loss: 172.8116, Val MSE: 172.42\n",
      "Epoch 47/50 (MagnitudeL2-20.0%): Train Loss: 341.2206, Train MSE: 341.50, Val Loss: 148.2673, Val MSE: 148.04\n",
      "Epoch 48/50 (MagnitudeL2-20.0%): Train Loss: 335.9129, Train MSE: 336.20, Val Loss: 162.7901, Val MSE: 162.95\n",
      "Epoch 49/50 (MagnitudeL2-20.0%): Train Loss: 343.6522, Train MSE: 344.42, Val Loss: 173.7405, Val MSE: 172.89\n",
      "Epoch 50/50 (MagnitudeL2-20.0%): Train Loss: 339.6176, Train MSE: 339.52, Val Loss: 140.5233, Val MSE: 140.30 (Best)\n",
      "Loaded best model state\n",
      "Results: MSE=215.62, MAE=10.73, MACs=90.45M\n",
      "‚úÖ Model saved to ./models_lstm_nasa/magnitudel2_sparsity_0.2.pth\n",
      "‚úÖ ONNX model saved to ./models_lstm_nasa/magnitudel2_sparsity_0.2.onnx\n",
      "\n",
      "Processing MagnitudeL2 at 50.0% sparsity...\n",
      "‚úÖ Created LSTM with architecture:\n",
      "   LSTM: input_size=14, hidden_size=100, num_layers=2\n",
      "   FC: 100 -> 64 -> 32 -> 1\n",
      "Initial MACs: 90.45M\n",
      "Applying MagnitudeImportance pruning at 50.0% sparsity...\n",
      "Note: Only pruning FC layers, LSTM layers are preserved\n",
      "Final MACs: 90.45M (Reduction: 0.0%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/50 (MagnitudeL2-50.0%): Train Loss: 1882.2304, Train MSE: 1882.08, Val Loss: 1698.4252, Val MSE: 1698.28 (Best)\n",
      "Epoch 2/50 (MagnitudeL2-50.0%): Train Loss: 1375.0270, Train MSE: 1378.75, Val Loss: 714.7982, Val MSE: 713.13 (Best)\n",
      "Epoch 3/50 (MagnitudeL2-50.0%): Train Loss: 712.1458, Train MSE: 711.91, Val Loss: 373.4653, Val MSE: 372.69 (Best)\n",
      "Epoch 4/50 (MagnitudeL2-50.0%): Train Loss: 559.2936, Train MSE: 557.89, Val Loss: 286.6189, Val MSE: 286.36 (Best)\n",
      "Epoch 5/50 (MagnitudeL2-50.0%): Train Loss: 488.7120, Train MSE: 488.13, Val Loss: 300.6466, Val MSE: 301.09\n",
      "Epoch 6/50 (MagnitudeL2-50.0%): Train Loss: 494.1467, Train MSE: 496.34, Val Loss: 281.8518, Val MSE: 282.16 (Best)\n",
      "Epoch 7/50 (MagnitudeL2-50.0%): Train Loss: 480.7994, Train MSE: 481.50, Val Loss: 242.3935, Val MSE: 242.58 (Best)\n",
      "Epoch 8/50 (MagnitudeL2-50.0%): Train Loss: 471.9062, Train MSE: 470.35, Val Loss: 236.9456, Val MSE: 237.20 (Best)\n",
      "Epoch 9/50 (MagnitudeL2-50.0%): Train Loss: 441.7119, Train MSE: 441.78, Val Loss: 213.9570, Val MSE: 213.98 (Best)\n",
      "Epoch 10/50 (MagnitudeL2-50.0%): Train Loss: 433.7994, Train MSE: 435.07, Val Loss: 216.1750, Val MSE: 215.98\n",
      "Epoch 11/50 (MagnitudeL2-50.0%): Train Loss: 418.4597, Train MSE: 418.32, Val Loss: 206.7212, Val MSE: 206.65 (Best)\n",
      "Epoch 12/50 (MagnitudeL2-50.0%): Train Loss: 428.8742, Train MSE: 428.58, Val Loss: 219.1794, Val MSE: 218.83\n",
      "Epoch 13/50 (MagnitudeL2-50.0%): Train Loss: 427.9658, Train MSE: 427.98, Val Loss: 194.4621, Val MSE: 194.08 (Best)\n",
      "Epoch 14/50 (MagnitudeL2-50.0%): Train Loss: 417.1379, Train MSE: 417.00, Val Loss: 208.3737, Val MSE: 207.81\n",
      "Epoch 15/50 (MagnitudeL2-50.0%): Train Loss: 415.2431, Train MSE: 414.57, Val Loss: 209.8152, Val MSE: 209.01\n",
      "Epoch 16/50 (MagnitudeL2-50.0%): Train Loss: 394.2638, Train MSE: 395.52, Val Loss: 200.4739, Val MSE: 200.40\n",
      "Epoch 17/50 (MagnitudeL2-50.0%): Train Loss: 414.4645, Train MSE: 413.77, Val Loss: 203.2527, Val MSE: 203.13\n",
      "Epoch 18/50 (MagnitudeL2-50.0%): Train Loss: 401.1972, Train MSE: 401.29, Val Loss: 228.2859, Val MSE: 227.21\n",
      "Epoch 19/50 (MagnitudeL2-50.0%): Train Loss: 401.0290, Train MSE: 401.43, Val Loss: 195.1873, Val MSE: 194.71\n",
      "Epoch 20/50 (MagnitudeL2-50.0%): Train Loss: 390.5332, Train MSE: 391.74, Val Loss: 177.5700, Val MSE: 177.27 (Best)\n",
      "Epoch 21/50 (MagnitudeL2-50.0%): Train Loss: 401.4511, Train MSE: 401.43, Val Loss: 184.9871, Val MSE: 184.43\n",
      "Epoch 22/50 (MagnitudeL2-50.0%): Train Loss: 391.1493, Train MSE: 392.17, Val Loss: 175.8849, Val MSE: 175.44 (Best)\n",
      "Epoch 23/50 (MagnitudeL2-50.0%): Train Loss: 383.0386, Train MSE: 383.58, Val Loss: 176.4634, Val MSE: 176.03\n",
      "Epoch 24/50 (MagnitudeL2-50.0%): Train Loss: 373.8210, Train MSE: 373.08, Val Loss: 186.7568, Val MSE: 186.24\n",
      "Epoch 25/50 (MagnitudeL2-50.0%): Train Loss: 383.2745, Train MSE: 383.57, Val Loss: 195.4122, Val MSE: 195.81\n",
      "Epoch 26/50 (MagnitudeL2-50.0%): Train Loss: 374.6425, Train MSE: 374.77, Val Loss: 168.8173, Val MSE: 168.40 (Best)\n",
      "Epoch 27/50 (MagnitudeL2-50.0%): Train Loss: 356.8777, Train MSE: 357.08, Val Loss: 160.9392, Val MSE: 160.89 (Best)\n",
      "Epoch 28/50 (MagnitudeL2-50.0%): Train Loss: 366.6033, Train MSE: 365.75, Val Loss: 169.5686, Val MSE: 169.10\n",
      "Epoch 29/50 (MagnitudeL2-50.0%): Train Loss: 361.9032, Train MSE: 362.70, Val Loss: 170.8680, Val MSE: 170.96\n",
      "Epoch 30/50 (MagnitudeL2-50.0%): Train Loss: 361.9879, Train MSE: 362.26, Val Loss: 164.4651, Val MSE: 164.23\n",
      "Epoch 31/50 (MagnitudeL2-50.0%): Train Loss: 359.8363, Train MSE: 360.50, Val Loss: 164.8404, Val MSE: 164.59\n",
      "Epoch 32/50 (MagnitudeL2-50.0%): Train Loss: 359.2251, Train MSE: 359.57, Val Loss: 165.7296, Val MSE: 165.73\n",
      "Epoch 33/50 (MagnitudeL2-50.0%): Train Loss: 353.5400, Train MSE: 353.99, Val Loss: 170.6945, Val MSE: 170.15\n",
      "Epoch 34/50 (MagnitudeL2-50.0%): Train Loss: 352.5168, Train MSE: 353.27, Val Loss: 173.2652, Val MSE: 172.48\n",
      "Epoch 35/50 (MagnitudeL2-50.0%): Train Loss: 349.1139, Train MSE: 349.58, Val Loss: 155.7707, Val MSE: 155.91 (Best)\n",
      "Epoch 36/50 (MagnitudeL2-50.0%): Train Loss: 353.2653, Train MSE: 352.06, Val Loss: 162.7791, Val MSE: 162.42\n",
      "Epoch 37/50 (MagnitudeL2-50.0%): Train Loss: 353.4980, Train MSE: 353.81, Val Loss: 156.8694, Val MSE: 157.02\n",
      "Epoch 38/50 (MagnitudeL2-50.0%): Train Loss: 339.2460, Train MSE: 338.38, Val Loss: 181.3452, Val MSE: 181.81\n",
      "Epoch 39/50 (MagnitudeL2-50.0%): Train Loss: 345.6922, Train MSE: 345.90, Val Loss: 159.1447, Val MSE: 159.43\n",
      "Epoch 40/50 (MagnitudeL2-50.0%): Train Loss: 337.6944, Train MSE: 338.43, Val Loss: 153.0725, Val MSE: 152.84 (Best)\n",
      "Epoch 41/50 (MagnitudeL2-50.0%): Train Loss: 334.5539, Train MSE: 335.39, Val Loss: 171.5937, Val MSE: 171.29\n",
      "Epoch 42/50 (MagnitudeL2-50.0%): Train Loss: 328.2454, Train MSE: 328.95, Val Loss: 153.0984, Val MSE: 153.41\n",
      "Epoch 43/50 (MagnitudeL2-50.0%): Train Loss: 333.1416, Train MSE: 333.18, Val Loss: 202.7460, Val MSE: 201.99\n",
      "Epoch 44/50 (MagnitudeL2-50.0%): Train Loss: 336.8816, Train MSE: 336.99, Val Loss: 155.4216, Val MSE: 155.53\n",
      "Epoch 45/50 (MagnitudeL2-50.0%): Train Loss: 344.4296, Train MSE: 345.37, Val Loss: 154.2077, Val MSE: 154.11\n",
      "Epoch 46/50 (MagnitudeL2-50.0%): Train Loss: 341.7303, Train MSE: 341.82, Val Loss: 150.7367, Val MSE: 150.69 (Best)\n",
      "Epoch 47/50 (MagnitudeL2-50.0%): Train Loss: 339.6355, Train MSE: 339.76, Val Loss: 173.1127, Val MSE: 173.04\n",
      "Epoch 48/50 (MagnitudeL2-50.0%): Train Loss: 335.0694, Train MSE: 334.46, Val Loss: 162.9051, Val MSE: 162.82\n",
      "Epoch 49/50 (MagnitudeL2-50.0%): Train Loss: 339.5599, Train MSE: 339.84, Val Loss: 160.3364, Val MSE: 160.07\n",
      "Epoch 50/50 (MagnitudeL2-50.0%): Train Loss: 326.8321, Train MSE: 327.26, Val Loss: 192.6320, Val MSE: 191.79\n",
      "Loaded best model state\n",
      "Results: MSE=277.98, MAE=11.66, MACs=90.45M\n",
      "‚úÖ Model saved to ./models_lstm_nasa/magnitudel2_sparsity_0.5.pth\n",
      "‚úÖ ONNX model saved to ./models_lstm_nasa/magnitudel2_sparsity_0.5.onnx\n",
      "\n",
      "Processing MagnitudeL2 at 70.0% sparsity...\n",
      "‚úÖ Created LSTM with architecture:\n",
      "   LSTM: input_size=14, hidden_size=100, num_layers=2\n",
      "   FC: 100 -> 64 -> 32 -> 1\n",
      "Initial MACs: 90.45M\n",
      "Applying MagnitudeImportance pruning at 70.0% sparsity...\n",
      "Note: Only pruning FC layers, LSTM layers are preserved\n",
      "Final MACs: 90.45M (Reduction: 0.0%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/50 (MagnitudeL2-70.0%): Train Loss: 1881.0800, Train MSE: 1880.44, Val Loss: 1703.8020, Val MSE: 1703.70 (Best)\n",
      "Epoch 2/50 (MagnitudeL2-70.0%): Train Loss: 1310.1800, Train MSE: 1313.32, Val Loss: 639.3939, Val MSE: 639.06 (Best)\n",
      "Epoch 3/50 (MagnitudeL2-70.0%): Train Loss: 656.0057, Train MSE: 655.72, Val Loss: 438.7349, Val MSE: 439.14 (Best)\n",
      "Epoch 4/50 (MagnitudeL2-70.0%): Train Loss: 521.7228, Train MSE: 522.38, Val Loss: 259.3346, Val MSE: 258.60 (Best)\n",
      "Epoch 5/50 (MagnitudeL2-70.0%): Train Loss: 449.5551, Train MSE: 451.06, Val Loss: 222.4695, Val MSE: 221.94 (Best)\n",
      "Epoch 6/50 (MagnitudeL2-70.0%): Train Loss: 429.0948, Train MSE: 429.81, Val Loss: 220.5362, Val MSE: 220.21 (Best)\n",
      "Epoch 7/50 (MagnitudeL2-70.0%): Train Loss: 429.6464, Train MSE: 429.06, Val Loss: 217.2508, Val MSE: 216.39 (Best)\n",
      "Epoch 8/50 (MagnitudeL2-70.0%): Train Loss: 432.7792, Train MSE: 431.89, Val Loss: 200.3880, Val MSE: 199.95 (Best)\n",
      "Epoch 9/50 (MagnitudeL2-70.0%): Train Loss: 405.9210, Train MSE: 406.71, Val Loss: 200.0363, Val MSE: 199.84 (Best)\n",
      "Epoch 10/50 (MagnitudeL2-70.0%): Train Loss: 406.8094, Train MSE: 407.14, Val Loss: 190.7026, Val MSE: 190.19 (Best)\n",
      "Epoch 11/50 (MagnitudeL2-70.0%): Train Loss: 405.8896, Train MSE: 406.56, Val Loss: 201.1291, Val MSE: 200.95\n",
      "Epoch 12/50 (MagnitudeL2-70.0%): Train Loss: 397.7514, Train MSE: 397.90, Val Loss: 189.8928, Val MSE: 190.17 (Best)\n",
      "Epoch 13/50 (MagnitudeL2-70.0%): Train Loss: 390.7027, Train MSE: 390.86, Val Loss: 187.5831, Val MSE: 187.36 (Best)\n",
      "Epoch 14/50 (MagnitudeL2-70.0%): Train Loss: 400.2395, Train MSE: 400.89, Val Loss: 185.4962, Val MSE: 184.79 (Best)\n",
      "Epoch 15/50 (MagnitudeL2-70.0%): Train Loss: 386.5299, Train MSE: 385.90, Val Loss: 210.5603, Val MSE: 211.04\n",
      "Epoch 16/50 (MagnitudeL2-70.0%): Train Loss: 394.4475, Train MSE: 394.25, Val Loss: 202.8702, Val MSE: 202.22\n",
      "Epoch 17/50 (MagnitudeL2-70.0%): Train Loss: 385.4517, Train MSE: 385.75, Val Loss: 213.1157, Val MSE: 212.59\n",
      "Epoch 18/50 (MagnitudeL2-70.0%): Train Loss: 386.2859, Train MSE: 385.81, Val Loss: 186.7231, Val MSE: 186.70\n",
      "Epoch 19/50 (MagnitudeL2-70.0%): Train Loss: 387.9827, Train MSE: 387.71, Val Loss: 185.0756, Val MSE: 184.38 (Best)\n",
      "Epoch 20/50 (MagnitudeL2-70.0%): Train Loss: 379.0297, Train MSE: 378.62, Val Loss: 181.5195, Val MSE: 181.17 (Best)\n",
      "Epoch 21/50 (MagnitudeL2-70.0%): Train Loss: 372.5628, Train MSE: 373.28, Val Loss: 172.9426, Val MSE: 173.12 (Best)\n",
      "Epoch 22/50 (MagnitudeL2-70.0%): Train Loss: 369.1710, Train MSE: 369.32, Val Loss: 169.0264, Val MSE: 169.10 (Best)\n",
      "Epoch 23/50 (MagnitudeL2-70.0%): Train Loss: 374.5638, Train MSE: 375.08, Val Loss: 164.5987, Val MSE: 164.27 (Best)\n",
      "Epoch 24/50 (MagnitudeL2-70.0%): Train Loss: 358.4335, Train MSE: 358.60, Val Loss: 162.3707, Val MSE: 162.21 (Best)\n",
      "Epoch 25/50 (MagnitudeL2-70.0%): Train Loss: 363.8796, Train MSE: 365.15, Val Loss: 169.0820, Val MSE: 168.91\n",
      "Epoch 26/50 (MagnitudeL2-70.0%): Train Loss: 371.2708, Train MSE: 371.75, Val Loss: 167.6686, Val MSE: 167.19\n",
      "Epoch 27/50 (MagnitudeL2-70.0%): Train Loss: 353.8549, Train MSE: 354.70, Val Loss: 181.0124, Val MSE: 181.07\n",
      "Epoch 28/50 (MagnitudeL2-70.0%): Train Loss: 362.7757, Train MSE: 363.08, Val Loss: 164.0112, Val MSE: 163.67\n",
      "Epoch 29/50 (MagnitudeL2-70.0%): Train Loss: 361.9663, Train MSE: 359.29, Val Loss: 155.0834, Val MSE: 154.83 (Best)\n",
      "Epoch 30/50 (MagnitudeL2-70.0%): Train Loss: 362.2084, Train MSE: 361.69, Val Loss: 187.8360, Val MSE: 187.02\n",
      "Epoch 31/50 (MagnitudeL2-70.0%): Train Loss: 364.4266, Train MSE: 364.23, Val Loss: 153.9346, Val MSE: 154.08 (Best)\n",
      "Epoch 32/50 (MagnitudeL2-70.0%): Train Loss: 347.1624, Train MSE: 347.54, Val Loss: 164.8748, Val MSE: 164.80\n",
      "Epoch 33/50 (MagnitudeL2-70.0%): Train Loss: 364.1866, Train MSE: 363.70, Val Loss: 154.7959, Val MSE: 154.48\n",
      "Epoch 34/50 (MagnitudeL2-70.0%): Train Loss: 358.3253, Train MSE: 358.70, Val Loss: 166.8361, Val MSE: 166.78\n",
      "Epoch 35/50 (MagnitudeL2-70.0%): Train Loss: 368.8998, Train MSE: 368.62, Val Loss: 161.3674, Val MSE: 160.97\n",
      "Epoch 36/50 (MagnitudeL2-70.0%): Train Loss: 347.0170, Train MSE: 346.48, Val Loss: 158.8914, Val MSE: 159.21\n",
      "Epoch 37/50 (MagnitudeL2-70.0%): Train Loss: 346.4082, Train MSE: 345.89, Val Loss: 208.3916, Val MSE: 208.03\n",
      "Epoch 38/50 (MagnitudeL2-70.0%): Train Loss: 349.6467, Train MSE: 349.02, Val Loss: 151.9877, Val MSE: 152.16 (Best)\n",
      "Epoch 39/50 (MagnitudeL2-70.0%): Train Loss: 339.3248, Train MSE: 339.76, Val Loss: 158.5980, Val MSE: 158.13\n",
      "Epoch 40/50 (MagnitudeL2-70.0%): Train Loss: 343.7273, Train MSE: 343.86, Val Loss: 143.9810, Val MSE: 144.10 (Best)\n",
      "Epoch 41/50 (MagnitudeL2-70.0%): Train Loss: 347.6028, Train MSE: 348.16, Val Loss: 154.4806, Val MSE: 154.87\n",
      "Epoch 42/50 (MagnitudeL2-70.0%): Train Loss: 349.5528, Train MSE: 350.33, Val Loss: 162.0538, Val MSE: 162.20\n",
      "Epoch 43/50 (MagnitudeL2-70.0%): Train Loss: 335.8201, Train MSE: 336.28, Val Loss: 166.0091, Val MSE: 165.55\n",
      "Epoch 44/50 (MagnitudeL2-70.0%): Train Loss: 343.2027, Train MSE: 343.00, Val Loss: 165.0293, Val MSE: 164.35\n",
      "Epoch 45/50 (MagnitudeL2-70.0%): Train Loss: 335.7413, Train MSE: 336.06, Val Loss: 144.5831, Val MSE: 144.78\n",
      "Epoch 46/50 (MagnitudeL2-70.0%): Train Loss: 340.0284, Train MSE: 340.36, Val Loss: 144.3140, Val MSE: 144.32\n",
      "Epoch 47/50 (MagnitudeL2-70.0%): Train Loss: 338.7864, Train MSE: 339.19, Val Loss: 143.7733, Val MSE: 144.06 (Best)\n",
      "Epoch 48/50 (MagnitudeL2-70.0%): Train Loss: 347.2249, Train MSE: 347.61, Val Loss: 142.4657, Val MSE: 142.28 (Best)\n",
      "Epoch 49/50 (MagnitudeL2-70.0%): Train Loss: 339.6712, Train MSE: 338.12, Val Loss: 163.2856, Val MSE: 162.82\n",
      "Epoch 50/50 (MagnitudeL2-70.0%): Train Loss: 332.3981, Train MSE: 332.59, Val Loss: 142.8559, Val MSE: 142.71\n",
      "Loaded best model state\n",
      "Results: MSE=225.81, MAE=11.07, MACs=90.45M\n",
      "‚úÖ Model saved to ./models_lstm_nasa/magnitudel2_sparsity_0.7.pth\n",
      "‚úÖ ONNX model saved to ./models_lstm_nasa/magnitudel2_sparsity_0.7.onnx\n",
      "\n",
      "--- Strategy: Random ---\n",
      "\n",
      "Processing Random at 20.0% sparsity...\n",
      "‚úÖ Created LSTM with architecture:\n",
      "   LSTM: input_size=14, hidden_size=100, num_layers=2\n",
      "   FC: 100 -> 64 -> 32 -> 1\n",
      "Initial MACs: 90.45M\n",
      "Applying RandomImportance pruning at 20.0% sparsity...\n",
      "Note: Only pruning FC layers, LSTM layers are preserved\n",
      "Final MACs: 90.45M (Reduction: 0.0%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/50 (Random-20.0%): Train Loss: 1936.8858, Train MSE: 1939.08, Val Loss: 1717.7612, Val MSE: 1717.63 (Best)\n",
      "Epoch 2/50 (Random-20.0%): Train Loss: 1829.2466, Train MSE: 1830.08, Val Loss: 1330.6079, Val MSE: 1330.55 (Best)\n",
      "Epoch 3/50 (Random-20.0%): Train Loss: 1052.0688, Train MSE: 1054.34, Val Loss: 579.3465, Val MSE: 579.18 (Best)\n",
      "Epoch 4/50 (Random-20.0%): Train Loss: 609.2012, Train MSE: 610.28, Val Loss: 278.5250, Val MSE: 278.41 (Best)\n",
      "Epoch 5/50 (Random-20.0%): Train Loss: 506.1277, Train MSE: 506.58, Val Loss: 301.5728, Val MSE: 301.96\n",
      "Epoch 6/50 (Random-20.0%): Train Loss: 474.4799, Train MSE: 474.85, Val Loss: 225.3613, Val MSE: 225.51 (Best)\n",
      "Epoch 7/50 (Random-20.0%): Train Loss: 459.8825, Train MSE: 457.18, Val Loss: 213.9444, Val MSE: 214.01 (Best)\n",
      "Epoch 8/50 (Random-20.0%): Train Loss: 440.6591, Train MSE: 441.40, Val Loss: 219.6126, Val MSE: 219.11\n",
      "Epoch 9/50 (Random-20.0%): Train Loss: 435.3731, Train MSE: 435.68, Val Loss: 204.4916, Val MSE: 204.31 (Best)\n",
      "Epoch 10/50 (Random-20.0%): Train Loss: 426.3182, Train MSE: 426.67, Val Loss: 202.2777, Val MSE: 202.24 (Best)\n",
      "Epoch 11/50 (Random-20.0%): Train Loss: 420.9708, Train MSE: 419.88, Val Loss: 194.6416, Val MSE: 195.03 (Best)\n",
      "Epoch 12/50 (Random-20.0%): Train Loss: 435.2850, Train MSE: 435.85, Val Loss: 208.5962, Val MSE: 208.91\n",
      "Epoch 13/50 (Random-20.0%): Train Loss: 413.9336, Train MSE: 412.24, Val Loss: 201.6212, Val MSE: 202.24\n",
      "Epoch 14/50 (Random-20.0%): Train Loss: 422.9688, Train MSE: 423.05, Val Loss: 185.3915, Val MSE: 185.62 (Best)\n",
      "Epoch 15/50 (Random-20.0%): Train Loss: 412.5910, Train MSE: 412.06, Val Loss: 177.7480, Val MSE: 177.88 (Best)\n",
      "Epoch 16/50 (Random-20.0%): Train Loss: 413.5041, Train MSE: 413.80, Val Loss: 176.5798, Val MSE: 176.60 (Best)\n",
      "Epoch 17/50 (Random-20.0%): Train Loss: 409.2880, Train MSE: 407.88, Val Loss: 182.0903, Val MSE: 182.15\n",
      "Epoch 18/50 (Random-20.0%): Train Loss: 410.2780, Train MSE: 409.54, Val Loss: 216.5868, Val MSE: 216.80\n",
      "Epoch 19/50 (Random-20.0%): Train Loss: 402.8478, Train MSE: 403.58, Val Loss: 176.2400, Val MSE: 176.44 (Best)\n",
      "Epoch 20/50 (Random-20.0%): Train Loss: 396.2304, Train MSE: 396.85, Val Loss: 172.9388, Val MSE: 173.15 (Best)\n",
      "Epoch 21/50 (Random-20.0%): Train Loss: 386.2058, Train MSE: 386.80, Val Loss: 184.1429, Val MSE: 183.96\n",
      "Epoch 22/50 (Random-20.0%): Train Loss: 391.6509, Train MSE: 391.84, Val Loss: 208.1797, Val MSE: 208.25\n",
      "Epoch 23/50 (Random-20.0%): Train Loss: 397.3207, Train MSE: 397.47, Val Loss: 173.4615, Val MSE: 173.62\n",
      "Epoch 24/50 (Random-20.0%): Train Loss: 390.0898, Train MSE: 390.74, Val Loss: 173.9677, Val MSE: 174.40\n",
      "Epoch 25/50 (Random-20.0%): Train Loss: 387.2050, Train MSE: 387.53, Val Loss: 164.9563, Val MSE: 165.12 (Best)\n",
      "Epoch 26/50 (Random-20.0%): Train Loss: 379.1530, Train MSE: 379.21, Val Loss: 166.6957, Val MSE: 167.06\n",
      "Epoch 27/50 (Random-20.0%): Train Loss: 376.2971, Train MSE: 377.23, Val Loss: 181.1473, Val MSE: 181.58\n",
      "Epoch 28/50 (Random-20.0%): Train Loss: 378.5790, Train MSE: 377.53, Val Loss: 170.9707, Val MSE: 171.13\n",
      "Epoch 29/50 (Random-20.0%): Train Loss: 383.7600, Train MSE: 384.01, Val Loss: 165.2919, Val MSE: 165.47\n",
      "Epoch 30/50 (Random-20.0%): Train Loss: 388.3666, Train MSE: 388.72, Val Loss: 163.3113, Val MSE: 163.77 (Best)\n",
      "Epoch 31/50 (Random-20.0%): Train Loss: 373.7556, Train MSE: 374.09, Val Loss: 156.9725, Val MSE: 157.01 (Best)\n",
      "Epoch 32/50 (Random-20.0%): Train Loss: 376.4987, Train MSE: 375.39, Val Loss: 191.8199, Val MSE: 192.06\n",
      "Epoch 33/50 (Random-20.0%): Train Loss: 382.4920, Train MSE: 382.06, Val Loss: 158.3682, Val MSE: 158.61\n",
      "Epoch 34/50 (Random-20.0%): Train Loss: 376.2888, Train MSE: 377.33, Val Loss: 176.6849, Val MSE: 176.80\n",
      "Epoch 35/50 (Random-20.0%): Train Loss: 371.6587, Train MSE: 371.78, Val Loss: 157.1893, Val MSE: 157.28\n",
      "Epoch 36/50 (Random-20.0%): Train Loss: 372.3155, Train MSE: 371.95, Val Loss: 152.9870, Val MSE: 153.01 (Best)\n",
      "Epoch 37/50 (Random-20.0%): Train Loss: 363.2723, Train MSE: 364.06, Val Loss: 163.1749, Val MSE: 163.44\n",
      "Epoch 38/50 (Random-20.0%): Train Loss: 377.0456, Train MSE: 377.10, Val Loss: 197.0636, Val MSE: 196.81\n",
      "Epoch 39/50 (Random-20.0%): Train Loss: 373.8507, Train MSE: 371.77, Val Loss: 151.0593, Val MSE: 151.26 (Best)\n",
      "Epoch 40/50 (Random-20.0%): Train Loss: 361.8768, Train MSE: 361.65, Val Loss: 188.9494, Val MSE: 189.35\n",
      "Epoch 41/50 (Random-20.0%): Train Loss: 363.6603, Train MSE: 364.44, Val Loss: 164.7250, Val MSE: 164.66\n",
      "Epoch 42/50 (Random-20.0%): Train Loss: 369.1393, Train MSE: 369.46, Val Loss: 146.9260, Val MSE: 147.08 (Best)\n",
      "Epoch 43/50 (Random-20.0%): Train Loss: 362.9547, Train MSE: 363.00, Val Loss: 157.7414, Val MSE: 158.13\n",
      "Epoch 44/50 (Random-20.0%): Train Loss: 366.2491, Train MSE: 365.88, Val Loss: 153.5157, Val MSE: 153.94\n",
      "Epoch 45/50 (Random-20.0%): Train Loss: 367.6903, Train MSE: 368.26, Val Loss: 153.9591, Val MSE: 154.04\n",
      "Epoch 46/50 (Random-20.0%): Train Loss: 370.8230, Train MSE: 371.56, Val Loss: 149.8842, Val MSE: 150.32\n",
      "Epoch 47/50 (Random-20.0%): Train Loss: 365.4435, Train MSE: 364.92, Val Loss: 156.4008, Val MSE: 156.72\n",
      "Epoch 48/50 (Random-20.0%): Train Loss: 359.5206, Train MSE: 358.42, Val Loss: 165.6069, Val MSE: 166.15\n",
      "Epoch 49/50 (Random-20.0%): Train Loss: 350.7904, Train MSE: 349.59, Val Loss: 159.0134, Val MSE: 159.22\n",
      "Epoch 50/50 (Random-20.0%): Train Loss: 358.2994, Train MSE: 359.26, Val Loss: 159.2886, Val MSE: 159.37\n",
      "Loaded best model state\n",
      "Results: MSE=214.12, MAE=10.69, MACs=90.45M\n",
      "‚úÖ Model saved to ./models_lstm_nasa/random_sparsity_0.2.pth\n",
      "‚úÖ ONNX model saved to ./models_lstm_nasa/random_sparsity_0.2.onnx\n",
      "\n",
      "Processing Random at 50.0% sparsity...\n",
      "‚úÖ Created LSTM with architecture:\n",
      "   LSTM: input_size=14, hidden_size=100, num_layers=2\n",
      "   FC: 100 -> 64 -> 32 -> 1\n",
      "Initial MACs: 90.45M\n",
      "Applying RandomImportance pruning at 50.0% sparsity...\n",
      "Note: Only pruning FC layers, LSTM layers are preserved\n",
      "Final MACs: 90.45M (Reduction: 0.0%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/50 (Random-50.0%): Train Loss: 1965.4474, Train MSE: 1965.50, Val Loss: 1724.9719, Val MSE: 1724.76 (Best)\n",
      "Epoch 2/50 (Random-50.0%): Train Loss: 1854.4283, Train MSE: 1859.43, Val Loss: 1512.8970, Val MSE: 1512.76 (Best)\n",
      "Epoch 3/50 (Random-50.0%): Train Loss: 1150.0966, Train MSE: 1151.43, Val Loss: 606.4988, Val MSE: 605.88 (Best)\n",
      "Epoch 4/50 (Random-50.0%): Train Loss: 667.0163, Train MSE: 668.84, Val Loss: 359.6861, Val MSE: 359.99 (Best)\n",
      "Epoch 5/50 (Random-50.0%): Train Loss: 517.1030, Train MSE: 517.86, Val Loss: 250.2469, Val MSE: 250.15 (Best)\n",
      "Epoch 6/50 (Random-50.0%): Train Loss: 475.0698, Train MSE: 475.57, Val Loss: 247.4492, Val MSE: 247.87 (Best)\n",
      "Epoch 7/50 (Random-50.0%): Train Loss: 456.8631, Train MSE: 457.31, Val Loss: 273.8272, Val MSE: 274.23\n",
      "Epoch 8/50 (Random-50.0%): Train Loss: 445.6061, Train MSE: 442.76, Val Loss: 210.2554, Val MSE: 209.95 (Best)\n",
      "Epoch 9/50 (Random-50.0%): Train Loss: 442.9645, Train MSE: 442.59, Val Loss: 196.1695, Val MSE: 196.22 (Best)\n",
      "Epoch 10/50 (Random-50.0%): Train Loss: 447.9703, Train MSE: 448.64, Val Loss: 196.2137, Val MSE: 196.21\n",
      "Epoch 11/50 (Random-50.0%): Train Loss: 430.7755, Train MSE: 431.14, Val Loss: 187.9542, Val MSE: 188.23 (Best)\n",
      "Epoch 12/50 (Random-50.0%): Train Loss: 409.9120, Train MSE: 410.83, Val Loss: 192.4409, Val MSE: 192.13\n",
      "Epoch 13/50 (Random-50.0%): Train Loss: 427.3703, Train MSE: 428.15, Val Loss: 203.6772, Val MSE: 204.16\n",
      "Epoch 14/50 (Random-50.0%): Train Loss: 428.1072, Train MSE: 427.66, Val Loss: 244.0823, Val MSE: 242.95\n",
      "Epoch 15/50 (Random-50.0%): Train Loss: 417.9888, Train MSE: 417.20, Val Loss: 231.2388, Val MSE: 231.64\n",
      "Epoch 16/50 (Random-50.0%): Train Loss: 414.5500, Train MSE: 413.61, Val Loss: 176.2207, Val MSE: 176.04 (Best)\n",
      "Epoch 17/50 (Random-50.0%): Train Loss: 411.1044, Train MSE: 411.75, Val Loss: 206.6222, Val MSE: 207.33\n",
      "Epoch 18/50 (Random-50.0%): Train Loss: 410.6440, Train MSE: 410.84, Val Loss: 193.2853, Val MSE: 193.89\n",
      "Epoch 19/50 (Random-50.0%): Train Loss: 397.0930, Train MSE: 397.10, Val Loss: 177.8860, Val MSE: 177.50\n",
      "Epoch 20/50 (Random-50.0%): Train Loss: 399.1512, Train MSE: 399.54, Val Loss: 186.7643, Val MSE: 186.63\n",
      "Epoch 21/50 (Random-50.0%): Train Loss: 419.7363, Train MSE: 419.95, Val Loss: 194.0077, Val MSE: 194.16\n",
      "Epoch 22/50 (Random-50.0%): Train Loss: 405.1945, Train MSE: 404.78, Val Loss: 165.5848, Val MSE: 165.74 (Best)\n",
      "Epoch 23/50 (Random-50.0%): Train Loss: 398.5542, Train MSE: 398.13, Val Loss: 172.2853, Val MSE: 172.22\n",
      "Epoch 24/50 (Random-50.0%): Train Loss: 390.7879, Train MSE: 389.78, Val Loss: 176.5377, Val MSE: 176.70\n",
      "Epoch 25/50 (Random-50.0%): Train Loss: 389.0729, Train MSE: 388.86, Val Loss: 180.8521, Val MSE: 181.25\n",
      "Epoch 26/50 (Random-50.0%): Train Loss: 388.8893, Train MSE: 387.67, Val Loss: 169.1635, Val MSE: 169.60\n",
      "Epoch 27/50 (Random-50.0%): Train Loss: 381.5197, Train MSE: 381.73, Val Loss: 172.0966, Val MSE: 172.40\n",
      "Epoch 28/50 (Random-50.0%): Train Loss: 383.5186, Train MSE: 383.85, Val Loss: 161.9248, Val MSE: 162.08 (Best)\n",
      "Epoch 29/50 (Random-50.0%): Train Loss: 379.9024, Train MSE: 379.03, Val Loss: 166.6325, Val MSE: 166.33\n",
      "Epoch 30/50 (Random-50.0%): Train Loss: 378.7642, Train MSE: 380.23, Val Loss: 166.8597, Val MSE: 166.74\n",
      "Epoch 31/50 (Random-50.0%): Train Loss: 385.0006, Train MSE: 385.63, Val Loss: 166.2250, Val MSE: 166.22\n",
      "Epoch 32/50 (Random-50.0%): Train Loss: 372.6787, Train MSE: 372.85, Val Loss: 162.8641, Val MSE: 162.88\n",
      "Epoch 33/50 (Random-50.0%): Train Loss: 369.8290, Train MSE: 370.88, Val Loss: 212.6803, Val MSE: 213.04\n",
      "Epoch 34/50 (Random-50.0%): Train Loss: 378.3181, Train MSE: 378.29, Val Loss: 154.2499, Val MSE: 154.44 (Best)\n",
      "Epoch 35/50 (Random-50.0%): Train Loss: 367.6428, Train MSE: 367.83, Val Loss: 165.5120, Val MSE: 165.81\n",
      "Epoch 36/50 (Random-50.0%): Train Loss: 375.2185, Train MSE: 374.32, Val Loss: 170.2693, Val MSE: 170.62\n",
      "Epoch 37/50 (Random-50.0%): Train Loss: 369.4159, Train MSE: 369.82, Val Loss: 172.3370, Val MSE: 172.56\n",
      "Epoch 38/50 (Random-50.0%): Train Loss: 368.8173, Train MSE: 369.33, Val Loss: 170.5603, Val MSE: 171.12\n",
      "Epoch 39/50 (Random-50.0%): Train Loss: 370.5124, Train MSE: 370.96, Val Loss: 165.8867, Val MSE: 166.27\n",
      "Epoch 40/50 (Random-50.0%): Train Loss: 373.6751, Train MSE: 373.77, Val Loss: 157.0449, Val MSE: 157.66\n",
      "Epoch 41/50 (Random-50.0%): Train Loss: 373.1149, Train MSE: 372.88, Val Loss: 147.7612, Val MSE: 147.94 (Best)\n",
      "Epoch 42/50 (Random-50.0%): Train Loss: 367.2497, Train MSE: 366.81, Val Loss: 167.6280, Val MSE: 167.88\n",
      "Epoch 43/50 (Random-50.0%): Train Loss: 366.7840, Train MSE: 367.57, Val Loss: 157.5348, Val MSE: 157.95\n",
      "Epoch 44/50 (Random-50.0%): Train Loss: 358.1998, Train MSE: 358.99, Val Loss: 154.0169, Val MSE: 154.25\n",
      "Epoch 45/50 (Random-50.0%): Train Loss: 363.9627, Train MSE: 363.98, Val Loss: 156.9551, Val MSE: 156.84\n",
      "Epoch 46/50 (Random-50.0%): Train Loss: 361.0944, Train MSE: 360.12, Val Loss: 148.5186, Val MSE: 148.84\n",
      "Epoch 47/50 (Random-50.0%): Train Loss: 355.7774, Train MSE: 355.13, Val Loss: 146.8239, Val MSE: 146.90 (Best)\n",
      "Epoch 48/50 (Random-50.0%): Train Loss: 357.6764, Train MSE: 358.52, Val Loss: 154.6562, Val MSE: 154.90\n",
      "Epoch 49/50 (Random-50.0%): Train Loss: 357.9118, Train MSE: 358.64, Val Loss: 150.1937, Val MSE: 150.46\n",
      "Epoch 50/50 (Random-50.0%): Train Loss: 360.0134, Train MSE: 359.38, Val Loss: 148.5535, Val MSE: 148.71\n",
      "Loaded best model state\n",
      "Results: MSE=228.97, MAE=10.96, MACs=90.45M\n",
      "‚úÖ Model saved to ./models_lstm_nasa/random_sparsity_0.5.pth\n",
      "‚úÖ ONNX model saved to ./models_lstm_nasa/random_sparsity_0.5.onnx\n",
      "\n",
      "Processing Random at 70.0% sparsity...\n",
      "‚úÖ Created LSTM with architecture:\n",
      "   LSTM: input_size=14, hidden_size=100, num_layers=2\n",
      "   FC: 100 -> 64 -> 32 -> 1\n",
      "Initial MACs: 90.45M\n",
      "Applying RandomImportance pruning at 70.0% sparsity...\n",
      "Note: Only pruning FC layers, LSTM layers are preserved\n",
      "Final MACs: 90.45M (Reduction: 0.0%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/50 (Random-70.0%): Train Loss: 2328.9191, Train MSE: 2330.47, Val Loss: 2020.4240, Val MSE: 2018.83 (Best)\n",
      "Epoch 2/50 (Random-70.0%): Train Loss: 2022.8164, Train MSE: 2020.45, Val Loss: 1782.4169, Val MSE: 1781.78 (Best)\n",
      "Epoch 3/50 (Random-70.0%): Train Loss: 1939.0067, Train MSE: 1938.16, Val Loss: 1724.6852, Val MSE: 1724.48 (Best)\n",
      "Epoch 4/50 (Random-70.0%): Train Loss: 1939.0805, Train MSE: 1936.83, Val Loss: 1715.7208, Val MSE: 1715.61 (Best)\n",
      "Epoch 5/50 (Random-70.0%): Train Loss: 1623.2893, Train MSE: 1624.81, Val Loss: 1023.3969, Val MSE: 1024.09 (Best)\n",
      "Epoch 6/50 (Random-70.0%): Train Loss: 1003.7198, Train MSE: 1006.25, Val Loss: 644.1684, Val MSE: 643.96 (Best)\n",
      "Epoch 7/50 (Random-70.0%): Train Loss: 738.3799, Train MSE: 738.78, Val Loss: 391.6072, Val MSE: 390.92 (Best)\n",
      "Epoch 8/50 (Random-70.0%): Train Loss: 605.3395, Train MSE: 606.41, Val Loss: 459.6693, Val MSE: 460.13\n",
      "Epoch 9/50 (Random-70.0%): Train Loss: 554.2464, Train MSE: 553.26, Val Loss: 295.8078, Val MSE: 296.11 (Best)\n",
      "Epoch 10/50 (Random-70.0%): Train Loss: 559.5300, Train MSE: 560.57, Val Loss: 276.2116, Val MSE: 276.53 (Best)\n",
      "Epoch 11/50 (Random-70.0%): Train Loss: 523.6012, Train MSE: 523.26, Val Loss: 242.3657, Val MSE: 242.70 (Best)\n",
      "Epoch 12/50 (Random-70.0%): Train Loss: 513.9003, Train MSE: 513.02, Val Loss: 310.2467, Val MSE: 310.90\n",
      "Epoch 13/50 (Random-70.0%): Train Loss: 505.3071, Train MSE: 505.67, Val Loss: 219.3292, Val MSE: 219.68 (Best)\n",
      "Epoch 14/50 (Random-70.0%): Train Loss: 492.1588, Train MSE: 492.10, Val Loss: 211.7739, Val MSE: 211.87 (Best)\n",
      "Epoch 15/50 (Random-70.0%): Train Loss: 489.1033, Train MSE: 490.01, Val Loss: 238.0958, Val MSE: 238.38\n",
      "Epoch 16/50 (Random-70.0%): Train Loss: 488.4059, Train MSE: 489.14, Val Loss: 205.8244, Val MSE: 206.38 (Best)\n",
      "Epoch 17/50 (Random-70.0%): Train Loss: 461.4949, Train MSE: 461.29, Val Loss: 230.3784, Val MSE: 230.75\n",
      "Epoch 18/50 (Random-70.0%): Train Loss: 493.2844, Train MSE: 494.07, Val Loss: 262.3620, Val MSE: 262.82\n",
      "Epoch 19/50 (Random-70.0%): Train Loss: 467.9900, Train MSE: 467.38, Val Loss: 195.5055, Val MSE: 195.85 (Best)\n",
      "Epoch 20/50 (Random-70.0%): Train Loss: 455.9431, Train MSE: 456.52, Val Loss: 195.9035, Val MSE: 196.19\n",
      "Epoch 21/50 (Random-70.0%): Train Loss: 462.0993, Train MSE: 462.62, Val Loss: 185.7619, Val MSE: 186.07 (Best)\n",
      "Epoch 22/50 (Random-70.0%): Train Loss: 462.7822, Train MSE: 463.25, Val Loss: 189.4661, Val MSE: 189.55\n",
      "Epoch 23/50 (Random-70.0%): Train Loss: 453.6616, Train MSE: 454.80, Val Loss: 206.0340, Val MSE: 206.30\n",
      "Epoch 24/50 (Random-70.0%): Train Loss: 461.5846, Train MSE: 461.13, Val Loss: 255.2085, Val MSE: 255.75\n",
      "Epoch 25/50 (Random-70.0%): Train Loss: 453.6458, Train MSE: 453.71, Val Loss: 191.6892, Val MSE: 191.74\n",
      "Epoch 26/50 (Random-70.0%): Train Loss: 456.4479, Train MSE: 456.57, Val Loss: 179.8934, Val MSE: 180.26 (Best)\n",
      "Epoch 27/50 (Random-70.0%): Train Loss: 437.2825, Train MSE: 438.68, Val Loss: 183.4213, Val MSE: 183.80\n",
      "Epoch 28/50 (Random-70.0%): Train Loss: 450.5812, Train MSE: 449.96, Val Loss: 198.8071, Val MSE: 198.99\n",
      "Epoch 29/50 (Random-70.0%): Train Loss: 436.8825, Train MSE: 438.41, Val Loss: 184.2679, Val MSE: 184.44\n",
      "Epoch 30/50 (Random-70.0%): Train Loss: 441.7699, Train MSE: 440.82, Val Loss: 177.0869, Val MSE: 177.56 (Best)\n",
      "Epoch 31/50 (Random-70.0%): Train Loss: 457.8209, Train MSE: 457.15, Val Loss: 182.6895, Val MSE: 183.11\n",
      "Epoch 32/50 (Random-70.0%): Train Loss: 433.0119, Train MSE: 434.08, Val Loss: 188.9068, Val MSE: 189.04\n",
      "Epoch 33/50 (Random-70.0%): Train Loss: 423.6127, Train MSE: 423.75, Val Loss: 168.1528, Val MSE: 168.54 (Best)\n",
      "Epoch 34/50 (Random-70.0%): Train Loss: 435.4721, Train MSE: 436.88, Val Loss: 179.8459, Val MSE: 180.39\n",
      "Epoch 35/50 (Random-70.0%): Train Loss: 418.1200, Train MSE: 418.24, Val Loss: 169.0355, Val MSE: 169.50\n",
      "Epoch 36/50 (Random-70.0%): Train Loss: 439.0058, Train MSE: 439.41, Val Loss: 165.4505, Val MSE: 165.93 (Best)\n",
      "Epoch 37/50 (Random-70.0%): Train Loss: 421.6129, Train MSE: 421.48, Val Loss: 177.5860, Val MSE: 177.74\n",
      "Epoch 38/50 (Random-70.0%): Train Loss: 430.1035, Train MSE: 429.28, Val Loss: 163.2006, Val MSE: 163.53 (Best)\n",
      "Epoch 39/50 (Random-70.0%): Train Loss: 422.4802, Train MSE: 421.03, Val Loss: 165.2650, Val MSE: 165.45\n",
      "Epoch 40/50 (Random-70.0%): Train Loss: 418.4028, Train MSE: 419.46, Val Loss: 161.7065, Val MSE: 161.89 (Best)\n",
      "Epoch 41/50 (Random-70.0%): Train Loss: 416.8428, Train MSE: 417.15, Val Loss: 165.2169, Val MSE: 165.64\n",
      "Epoch 42/50 (Random-70.0%): Train Loss: 418.0219, Train MSE: 416.95, Val Loss: 187.8444, Val MSE: 188.00\n",
      "Epoch 43/50 (Random-70.0%): Train Loss: 433.4727, Train MSE: 432.84, Val Loss: 174.4355, Val MSE: 174.47\n",
      "Epoch 44/50 (Random-70.0%): Train Loss: 422.9103, Train MSE: 422.10, Val Loss: 160.7152, Val MSE: 160.82 (Best)\n",
      "Epoch 45/50 (Random-70.0%): Train Loss: 413.5809, Train MSE: 412.55, Val Loss: 183.8560, Val MSE: 183.95\n",
      "Epoch 46/50 (Random-70.0%): Train Loss: 411.9262, Train MSE: 412.05, Val Loss: 174.5194, Val MSE: 174.78\n",
      "Epoch 47/50 (Random-70.0%): Train Loss: 415.4637, Train MSE: 416.03, Val Loss: 156.2110, Val MSE: 156.61 (Best)\n",
      "Epoch 48/50 (Random-70.0%): Train Loss: 429.8165, Train MSE: 426.76, Val Loss: 189.0254, Val MSE: 189.16\n",
      "Epoch 49/50 (Random-70.0%): Train Loss: 415.1302, Train MSE: 415.16, Val Loss: 153.2349, Val MSE: 153.60 (Best)\n",
      "Epoch 50/50 (Random-70.0%): Train Loss: 405.5325, Train MSE: 406.07, Val Loss: 152.6426, Val MSE: 153.13 (Best)\n",
      "Loaded best model state\n",
      "Results: MSE=235.94, MAE=11.38, MACs=90.45M\n",
      "‚úÖ Model saved to ./models_lstm_nasa/random_sparsity_0.7.pth\n",
      "‚úÖ ONNX model saved to ./models_lstm_nasa/random_sparsity_0.7.onnx\n",
      "\n",
      "Saving results...\n",
      "‚úÖ Complete results saved to ./results_lstm_nasa/complete_results.json\n",
      "‚úÖ Summary results saved to ./results_lstm_nasa/summary_results.csv\n",
      "Creating plots...\n",
      "‚úÖ MSE plot saved to ./results_lstm_nasa/mse_vs_sparsity.png\n",
      "‚úÖ Efficiency frontier plot saved to ./results_lstm_nasa/efficiency_frontier.png\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENTAL RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Baseline Performance:\n",
      "  MSE: 1732.94\n",
      "  MAE: 37.11\n",
      "  MACs: 90.45M\n",
      "  Parameters: 0.14M\n",
      "  Model Size: 0.52MB\n",
      "\n",
      "Strategy Comparison at 50% Sparsity (FC Layers Only):\n",
      "   MagnitudeL2: MSE=277.98 (-1454.96, -84.0% increase)\n",
      "        Random: MSE=228.97 (-1503.97, -86.8% increase)\n",
      "\n",
      "Complete Results Table:\n",
      "------------------------------------------------------------------------------------------\n",
      "Strategy     Sparsity MSE      MAE      MACs(M)  Params(M) Size(MB)\n",
      "------------------------------------------------------------------------------------------\n",
      "MagnitudeL2       0% 1732.94   37.11   90.45     0.14    0.52\n",
      "MagnitudeL2      20%  215.62   10.73   90.45     0.14    0.52\n",
      "MagnitudeL2      50%  277.98   11.66   90.45     0.13    0.51\n",
      "MagnitudeL2      70%  225.81   11.07   90.45     0.13    0.51\n",
      "Random            0% 1732.94   37.11   90.45     0.14    0.52\n",
      "Random           20%  214.12   10.69   90.45     0.14    0.52\n",
      "Random           50%  228.97   10.96   90.45     0.13    0.51\n",
      "Random           70%  235.94   11.38   90.45     0.13    0.51\n",
      "\n",
      "Note: Sparsity is applied only to FC layers. LSTM layers remain unchanged.\n",
      "\n",
      "üéâ All experiments completed!\n",
      "üìÅ Results saved to: /home/muis/thesis/github-repo/master-thesis/timeseries/lstm/results_lstm_nasa\n",
      "üìÅ Models saved to: /home/muis/thesis/github-repo/master-thesis/timeseries/lstm/models_lstm_nasa\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
