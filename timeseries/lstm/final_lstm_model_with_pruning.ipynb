{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-04T20:56:38.117670Z",
     "start_time": "2025-06-04T20:52:15.142992Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_pruning as tp\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_BASE_NAME = \"lstm_nasa\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# NASA Dataset preprocessing functions from your provided code\n",
    "column_names = ['unit_number', 'time_in_cycles'] + [f'op_setting_{i}' for i in range(1, 4)] + [f'sensor_{i}' for i in\n",
    "                                                                                               range(1, 24)]\n",
    "\n",
    "\n",
    "def load_dataframe(file_path: str) -> pd.DataFrame | None:\n",
    "    \"\"\"Loads a single CMaps data file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=' ', header=None, names=column_names)\n",
    "        # Drop the last two columns if they are all NaNs (often artifacts of space delimiter)\n",
    "        df.dropna(axis=1, how='all', inplace=True)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def clean_data(df: pd.DataFrame) -> list:\n",
    "    \"\"\"Identifies columns to remove based on low std dev.\"\"\"\n",
    "    if df is None:\n",
    "        return []\n",
    "    # Columns with std dev < 0.02 (potential candidates for removal)\n",
    "    cols_to_check = [col for col in df.columns if 'sensor' in col or 'op_setting' in col]\n",
    "    low_std_cols = [col for col in cols_to_check if df[col].std() < 0.02]\n",
    "    print(f\"Columns with std < 0.02 (potential removal): {low_std_cols}\")\n",
    "    return low_std_cols\n",
    "\n",
    "\n",
    "def add_rul(df: pd.DataFrame) -> pd.DataFrame | None:\n",
    "    \"\"\"Calculates and adds the Remaining Useful Life (RUL) column.\"\"\"\n",
    "    if df is None:\n",
    "        return None\n",
    "    max_cycles = df.groupby('unit_number')['time_in_cycles'].max().reset_index()\n",
    "    max_cycles.columns = ['unit_number', 'max_cycle']\n",
    "    df = df.merge(max_cycles, on='unit_number', how='left')\n",
    "    df['RUL'] = df['max_cycle'] - df['time_in_cycles']\n",
    "    df.drop(columns=['max_cycle'], inplace=True)\n",
    "    # Clip RUL (optional, common practice to limit max RUL)\n",
    "    df['RUL'] = df['RUL'].clip(upper=125)\n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize_data(df: pd.DataFrame,\n",
    "                   columns_to_normalize: List[str], scaler: MinMaxScaler = None) -> Tuple[pd.DataFrame, MinMaxScaler] | \\\n",
    "                                                                                    Tuple[None, None]:\n",
    "    \"\"\"Normalizes specified columns using MinMaxScaler.\"\"\"\n",
    "    if df is None:\n",
    "        return None, None\n",
    "    data_to_scale = df[columns_to_normalize]\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler()\n",
    "        df[columns_to_normalize] = scaler.fit_transform(data_to_scale)\n",
    "    else:\n",
    "        # Ensure only columns present in the scaler are transformed\n",
    "        valid_cols = [col for col in columns_to_normalize if col in scaler.feature_names_in_]\n",
    "        if len(valid_cols) < len(columns_to_normalize):\n",
    "            print(\"Warning: Some columns not found in the provided scaler. Skipping them.\")\n",
    "        if valid_cols:\n",
    "            df[valid_cols] = scaler.transform(df[valid_cols])\n",
    "    return df, scaler\n",
    "\n",
    "\n",
    "def prepare_cmapss_data(data_dir: str, train_file: str, test_file: str, test_rul_file: str) -> Tuple[\n",
    "    pd.DataFrame, pd.DataFrame, pd.DataFrame, MinMaxScaler, List[str]]:\n",
    "    \"\"\"Loads, cleans, preprocesses train/test data and RUL.\"\"\"\n",
    "    print(\"--- Preparing Training Data ---\")\n",
    "    train_df = load_dataframe(os.path.join(data_dir, train_file))\n",
    "    train_df = add_rul(train_df)\n",
    "\n",
    "    print(\"\\n--- Preparing Test Data ---\")\n",
    "    test_df = load_dataframe(os.path.join(data_dir, test_file))\n",
    "    test_rul_df = pd.read_csv(os.path.join(data_dir, test_rul_file), header=None, names=['RUL'])\n",
    "\n",
    "    # Clean Data - identify columns based on TRAINING data variance\n",
    "    cols_to_remove = clean_data(train_df)\n",
    "    feature_cols = [col for col in train_df.columns if\n",
    "                    col not in ['unit_number', 'time_in_cycles', 'RUL'] + cols_to_remove]\n",
    "    print(f\"\\nUsing Features: {feature_cols}\")\n",
    "\n",
    "    # Drop removed columns from both train and test\n",
    "    train_df.drop(columns=cols_to_remove, inplace=True, errors='ignore')\n",
    "    test_df.drop(columns=cols_to_remove, inplace=True, errors='ignore')\n",
    "\n",
    "    # Normalize features based on TRAINING data\n",
    "    print(\"\\n--- Normalizing Data ---\")\n",
    "    train_df_norm, scaler = normalize_data(train_df.copy(), feature_cols, scaler=None)\n",
    "    test_df_norm, _ = normalize_data(test_df.copy(), feature_cols, scaler=scaler)\n",
    "\n",
    "    return train_df_norm, test_df_norm, test_rul_df, scaler, feature_cols\n",
    "\n",
    "\n",
    "# Custom Dataset for NASA time series (LSTM version - no flattening)\n",
    "class NASALSTMDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, feature_cols: List[str], window_size: int = 50,\n",
    "                 stride: int = 1, is_test: bool = False, test_rul_df: pd.DataFrame = None):\n",
    "        self.df = df\n",
    "        self.feature_cols = feature_cols\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.is_test = is_test\n",
    "        self.test_rul_df = test_rul_df\n",
    "        self.samples = []\n",
    "        self.targets = []\n",
    "\n",
    "        self._prepare_samples()\n",
    "\n",
    "    def _prepare_samples(self):\n",
    "        \"\"\"Create windowed samples from the dataframe\"\"\"\n",
    "        units = self.df['unit_number'].unique()\n",
    "\n",
    "        for unit in units:\n",
    "            unit_df = self.df[self.df['unit_number'] == unit].sort_values('time_in_cycles')\n",
    "\n",
    "            # For test data, we only need the last window for each unit\n",
    "            if self.is_test:\n",
    "                if len(unit_df) >= self.window_size:\n",
    "                    window_data = unit_df[self.feature_cols].iloc[-self.window_size:].values\n",
    "                    self.samples.append(window_data)\n",
    "                    # Get RUL from test_rul_df\n",
    "                    if self.test_rul_df is not None:\n",
    "                        self.targets.append(self.test_rul_df.iloc[unit - 1]['RUL'])\n",
    "                else:\n",
    "                    # Pad if necessary\n",
    "                    window_data = unit_df[self.feature_cols].values\n",
    "                    padded = np.zeros((self.window_size, len(self.feature_cols)))\n",
    "                    padded[-len(window_data):] = window_data\n",
    "                    self.samples.append(padded)\n",
    "                    if self.test_rul_df is not None:\n",
    "                        self.targets.append(self.test_rul_df.iloc[unit - 1]['RUL'])\n",
    "            else:\n",
    "                # For training data, create multiple windows\n",
    "                for i in range(0, len(unit_df) - self.window_size + 1, self.stride):\n",
    "                    window_data = unit_df[self.feature_cols].iloc[i:i + self.window_size].values\n",
    "                    rul = unit_df['RUL'].iloc[i + self.window_size - 1]\n",
    "                    self.samples.append(window_data)\n",
    "                    self.targets.append(rul)\n",
    "\n",
    "        self.samples = np.array(self.samples, dtype=np.float32)\n",
    "        self.targets = np.array(self.targets, dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # For LSTM, keep the sequence shape (window_size, num_features)\n",
    "        sample = self.samples[idx]\n",
    "        target = self.targets[idx]\n",
    "        return torch.FloatTensor(sample), torch.FloatTensor([target])\n",
    "\n",
    "\n",
    "# LSTM Model Definition\n",
    "class NASALSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=100, num_layers=2, fc_hidden_sizes=[64, 32], dropout_rate=0.2):\n",
    "        super(NASALSTM, self).__init__()\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        # Fully connected layers after LSTM\n",
    "        fc_layers = []\n",
    "        prev_size = hidden_size\n",
    "\n",
    "        for fc_hidden_size in fc_hidden_sizes:\n",
    "            fc_layers.extend([\n",
    "                nn.Linear(prev_size, fc_hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = fc_hidden_size\n",
    "\n",
    "        # Output layer for regression\n",
    "        fc_layers.append(nn.Linear(prev_size, 1))\n",
    "\n",
    "        self.fc = nn.Sequential(*fc_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM forward pass\n",
    "        # x shape: (batch_size, sequence_length, input_size)\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "\n",
    "        # Use the last hidden state\n",
    "        # lstm_out shape: (batch_size, sequence_length, hidden_size)\n",
    "        # We take the last time step\n",
    "        last_hidden = lstm_out[:, -1, :]\n",
    "\n",
    "        # Pass through fully connected layers\n",
    "        output = self.fc(last_hidden)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def get_data_loaders(data_dir='./data/NASA', batch_size=128, window_size=50, val_split=0.2, seed=42):\n",
    "    \"\"\"Load NASA C-MAPSS dataset with train/val/test splits\"\"\"\n",
    "    print(f\"Loading NASA C-MAPSS dataset from: {data_dir}\")\n",
    "\n",
    "    # Prepare data\n",
    "    train_df, test_df, test_rul_df, scaler, feature_cols = prepare_cmapss_data(\n",
    "        data_dir,\n",
    "        'train_FD001.txt',\n",
    "        'test_FD001.txt',\n",
    "        'RUL_FD001.txt'\n",
    "    )\n",
    "\n",
    "    # Create full training dataset\n",
    "    full_train_dataset = NASALSTMDataset(train_df, feature_cols, window_size=window_size)\n",
    "\n",
    "    # Split into train and validation\n",
    "    val_size = int(len(full_train_dataset) * val_split)\n",
    "    train_size = len(full_train_dataset) - val_size\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_train_dataset, [train_size, val_size], generator=generator\n",
    "    )\n",
    "\n",
    "    # Create test dataset\n",
    "    test_dataset = NASALSTMDataset(test_df, feature_cols, window_size=window_size,\n",
    "                                   is_test=True, test_rul_df=test_rul_df)\n",
    "\n",
    "    # Create data loaders\n",
    "    num_workers = min(4, os.cpu_count() or 2)\n",
    "    pin_memory = True if DEVICE.type == 'cuda' else False\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=num_workers, pin_memory=pin_memory)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                            num_workers=num_workers, pin_memory=pin_memory)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                             num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    print(f\"DataLoaders created - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "    print(f\"Input shape: ({window_size}, {len(feature_cols)}) (sequence_length, num_features)\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader, len(feature_cols)\n",
    "\n",
    "\n",
    "def get_lstm_model(input_size, hidden_size=100, num_layers=2, fc_hidden_sizes=[64, 32], dropout_rate=0.2):\n",
    "    \"\"\"Get LSTM model for NASA dataset\"\"\"\n",
    "    model = NASALSTM(input_size, hidden_size, num_layers, fc_hidden_sizes, dropout_rate)\n",
    "    print(f\"‚úÖ Created LSTM with architecture:\")\n",
    "    print(f\"   LSTM: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}\")\n",
    "    print(f\"   FC: {hidden_size} -> {' -> '.join(map(str, fc_hidden_sizes))} -> 1\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_ignored_layers(model):\n",
    "    \"\"\"Get layers to ignore during pruning (typically final layer)\"\"\"\n",
    "    ignored_layers = []\n",
    "\n",
    "    # Ignore the LSTM layer since torch_pruning only supports single-layer LSTMs\n",
    "    ignored_layers.append(model.lstm)\n",
    "\n",
    "    # Get the last linear layer in the fc sequential model\n",
    "    for module in model.fc:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            last_linear = module\n",
    "    ignored_layers.append(last_linear)\n",
    "\n",
    "    return ignored_layers\n",
    "\n",
    "\n",
    "def calculate_macs_params(model, example_input):\n",
    "    \"\"\"Calculate MACs and parameters using torch_pruning\"\"\"\n",
    "    model.eval()\n",
    "    target_device = example_input.device\n",
    "    model_on_device = model.to(target_device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        macs, params = tp.utils.count_ops_and_params(model_on_device, example_input)\n",
    "\n",
    "    return macs, params\n",
    "\n",
    "\n",
    "def save_model(model, save_path, example_input_cpu=None):\n",
    "    \"\"\"Save model state dict and optionally ONNX\"\"\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"‚úÖ Model saved to {save_path}\")\n",
    "\n",
    "    if example_input_cpu is not None:\n",
    "        onnx_path = save_path.replace('.pth', '.onnx')\n",
    "        try:\n",
    "            model_cpu = model.to('cpu')\n",
    "            torch.onnx.export(\n",
    "                model_cpu, example_input_cpu, onnx_path,\n",
    "                export_params=True, opset_version=13,\n",
    "                input_names=['input'], output_names=['output'],\n",
    "                dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "            )\n",
    "            print(f\"‚úÖ ONNX model saved to {onnx_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: ONNX export failed: {e}\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, example_input, criterion, device):\n",
    "    \"\"\"Evaluate model and return comprehensive metrics\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Calculate efficiency metrics\n",
    "    macs, params = calculate_macs_params(model, example_input.to(device))\n",
    "    model_size_mb = params * 4 / (1024 * 1024)  # Assuming float32\n",
    "\n",
    "    # Calculate MSE and MAE\n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            all_predictions.extend(output.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    mse = np.mean((all_predictions - all_targets) ** 2)\n",
    "    mae = np.mean(np.abs(all_predictions - all_targets))\n",
    "\n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'loss': total_loss / len(data_loader.dataset),\n",
    "        'macs': macs,\n",
    "        'params': params,\n",
    "        'size_mb': model_size_mb\n",
    "    }\n",
    "\n",
    "\n",
    "def prune_model(model, strategy_config, sparsity_ratio, example_input, ignored_layers=None):\n",
    "    \"\"\"Apply structured pruning to model\"\"\"\n",
    "    if sparsity_ratio == 0.0:\n",
    "        print(\"No pruning needed (sparsity = 0.0)\")\n",
    "        return model\n",
    "\n",
    "    model.eval()\n",
    "    pruned_model = copy.deepcopy(model)\n",
    "    pruned_model.to(example_input.device)\n",
    "\n",
    "    # Calculate initial MACs\n",
    "    initial_macs, _ = calculate_macs_params(pruned_model, example_input)\n",
    "    print(f\"Initial MACs: {initial_macs / 1e6:.2f}M\")\n",
    "\n",
    "    ignored_layers = ignored_layers or []\n",
    "\n",
    "    # Create pruner based on strategy\n",
    "    # Only prune Linear layers, not LSTM\n",
    "    pruner = strategy_config['pruner'](\n",
    "        pruned_model,\n",
    "        example_input,\n",
    "        importance=strategy_config['importance'],\n",
    "        iterative_steps=5,  # Use 5 iterative steps\n",
    "        ch_sparsity=sparsity_ratio,\n",
    "        root_module_types=[nn.Linear],  # Only prune Linear layers\n",
    "        ignored_layers=ignored_layers\n",
    "    )\n",
    "\n",
    "    print(f\"Applying {strategy_config['importance'].__class__.__name__} pruning at {sparsity_ratio:.1%} sparsity...\")\n",
    "    print(\"Note: Only pruning FC layers, LSTM layers are preserved\")\n",
    "\n",
    "    # Apply pruning\n",
    "    pruner.step()\n",
    "\n",
    "    # Calculate final MACs\n",
    "    final_macs, _ = calculate_macs_params(pruned_model, example_input)\n",
    "    reduction = (initial_macs - final_macs) / initial_macs * 100 if initial_macs > 0 else 0\n",
    "    print(f\"Final MACs: {final_macs / 1e6:.2f}M (Reduction: {reduction:.1f}%)\")\n",
    "\n",
    "    return pruned_model\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs,\n",
    "                val_loader=None, patience=7, log_prefix=\"\"):\n",
    "    \"\"\"Train model with early stopping\"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_mse': [],\n",
    "        'val_loss': [],\n",
    "        'val_mse': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_predictions = []\n",
    "        train_targets = []\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_predictions.extend(output.detach().cpu().numpy())\n",
    "            train_targets.extend(target.cpu().numpy())\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_mse = np.mean((np.array(train_predictions) - np.array(train_targets)) ** 2)\n",
    "\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_mse'].append(train_mse)\n",
    "\n",
    "        log_msg = f\"Epoch {epoch + 1}/{num_epochs} ({log_prefix}): Train Loss: {avg_train_loss:.4f}, Train MSE: {train_mse:.2f}\"\n",
    "\n",
    "        # Validation phase\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_predictions = []\n",
    "            val_targets = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for data, target in val_loader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                    val_predictions.extend(output.cpu().numpy())\n",
    "                    val_targets.extend(target.cpu().numpy())\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_mse = np.mean((np.array(val_predictions) - np.array(val_targets)) ** 2)\n",
    "\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            history['val_mse'].append(val_mse)\n",
    "\n",
    "            log_msg += f\", Val Loss: {avg_val_loss:.4f}, Val MSE: {val_mse:.2f}\"\n",
    "\n",
    "            # Early stopping check\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                epochs_no_improve = 0\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "                log_msg += \" (Best)\"\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"{log_msg}\")\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "                break\n",
    "        else:\n",
    "            history['val_loss'].append(None)\n",
    "            history['val_mse'].append(None)\n",
    "\n",
    "        print(log_msg)\n",
    "\n",
    "    # Load best model state if available\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"Loaded best model state\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def save_results_to_files(all_results, output_dir):\n",
    "    \"\"\"Save experimental results to JSON and CSV files\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save complete results as JSON\n",
    "    results_json_path = os.path.join(output_dir, 'complete_results.json')\n",
    "    with open(results_json_path, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2, default=str)\n",
    "    print(f\"‚úÖ Complete results saved to {results_json_path}\")\n",
    "\n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for strategy, strategy_results in all_results.items():\n",
    "        for sparsity, metrics in strategy_results.items():\n",
    "            row = {\n",
    "                'strategy': strategy,\n",
    "                'sparsity_ratio': sparsity,\n",
    "                'mse': metrics['mse'],\n",
    "                'mae': metrics['mae'],\n",
    "                'loss': metrics['loss'],\n",
    "                'macs_millions': metrics['macs'] / 1e6,\n",
    "                'params_millions': metrics['params'] / 1e6,\n",
    "                'size_mb': metrics['size_mb']\n",
    "            }\n",
    "            summary_data.append(row)\n",
    "\n",
    "    # Save summary as CSV\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_csv_path = os.path.join(output_dir, 'summary_results.csv')\n",
    "    summary_df.to_csv(summary_csv_path, index=False)\n",
    "    print(f\"‚úÖ Summary results saved to {summary_csv_path}\")\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "def create_results_plots(summary_df, output_dir):\n",
    "    \"\"\"Create visualization plots for regression metrics\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    strategies = summary_df['strategy'].unique()\n",
    "    sparsity_levels = sorted(summary_df['sparsity_ratio'].unique())\n",
    "\n",
    "    # Plot 1: MSE vs Sparsity\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for strategy in strategies:\n",
    "        strategy_data = summary_df[summary_df['strategy'] == strategy].sort_values('sparsity_ratio')\n",
    "        plt.plot(strategy_data['sparsity_ratio'] * 100, strategy_data['mse'],\n",
    "                 'o-', linewidth=2, markersize=8, label=strategy)\n",
    "\n",
    "    plt.xlabel('Sparsity (%)', fontsize=12)\n",
    "    plt.ylabel('MSE', fontsize=12)\n",
    "    plt.title('NASA LSTM: MSE vs Sparsity (FC Layers Only)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.gca().invert_yaxis()  # Lower MSE is better\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(output_dir, 'mse_vs_sparsity.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"‚úÖ MSE plot saved to {plot_path}\")\n",
    "\n",
    "    # Plot 2: Efficiency frontier (MSE vs MACs)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for strategy in strategies:\n",
    "        strategy_data = summary_df[summary_df['strategy'] == strategy].sort_values('sparsity_ratio')\n",
    "        plt.scatter(strategy_data['macs_millions'], strategy_data['mse'],\n",
    "                    s=100, label=strategy, alpha=0.8)\n",
    "        plt.plot(strategy_data['macs_millions'], strategy_data['mse'],\n",
    "                 '--', alpha=0.6)\n",
    "\n",
    "    plt.xlabel('MACs (Millions)', fontsize=12)\n",
    "    plt.ylabel('MSE', fontsize=12)\n",
    "    plt.title('NASA LSTM: Efficiency Frontier (MSE vs MACs)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.gca().invert_yaxis()  # Lower MSE is better\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(output_dir, 'efficiency_frontier.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"‚úÖ Efficiency frontier plot saved to {plot_path}\")\n",
    "\n",
    "\n",
    "def print_results_table(summary_df):\n",
    "    \"\"\"Print formatted results table\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXPERIMENTAL RESULTS SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Baseline results\n",
    "    baseline_results = summary_df[summary_df['sparsity_ratio'] == 0.0].iloc[0]\n",
    "    print(f\"\\nBaseline Performance:\")\n",
    "    print(f\"  MSE: {baseline_results['mse']:.2f}\")\n",
    "    print(f\"  MAE: {baseline_results['mae']:.2f}\")\n",
    "    print(f\"  MACs: {baseline_results['macs_millions']:.2f}M\")\n",
    "    print(f\"  Parameters: {baseline_results['params_millions']:.2f}M\")\n",
    "    print(f\"  Model Size: {baseline_results['size_mb']:.2f}MB\")\n",
    "\n",
    "    # Strategy comparison at 50% sparsity\n",
    "    print(f\"\\nStrategy Comparison at 50% Sparsity (FC Layers Only):\")\n",
    "    sparsity_50_data = summary_df[summary_df['sparsity_ratio'] == 0.5]\n",
    "    for _, row in sparsity_50_data.iterrows():\n",
    "        degradation = row['mse'] - baseline_results['mse']\n",
    "        relative_increase = (degradation / baseline_results['mse']) * 100\n",
    "        print(\n",
    "            f\"  {row['strategy']:>12}: MSE={row['mse']:>6.2f} ({degradation:>+5.2f}, {relative_increase:>+5.1f}% increase)\")\n",
    "\n",
    "    # Complete results table\n",
    "    print(f\"\\nComplete Results Table:\")\n",
    "    print(\"-\" * 90)\n",
    "    print(f\"{'Strategy':<12} {'Sparsity':<8} {'MSE':<8} {'MAE':<8} {'MACs(M)':<8} {'Params(M)':<9} {'Size(MB)':<8}\")\n",
    "    print(\"-\" * 90)\n",
    "\n",
    "    for _, row in summary_df.sort_values(['strategy', 'sparsity_ratio']).iterrows():\n",
    "        print(f\"{row['strategy']:<12} {row['sparsity_ratio'] * 100:>6.0f}% \"\n",
    "              f\"{row['mse']:>7.2f} {row['mae']:>7.2f} {row['macs_millions']:>7.2f} \"\n",
    "              f\"{row['params_millions']:>8.2f} {row['size_mb']:>7.2f}\")\n",
    "\n",
    "    print(\"\\nNote: Sparsity is applied only to FC layers. LSTM layers remain unchanged.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main experimental workflow\"\"\"\n",
    "    print(\"Starting NASA LSTM Pruning Experiments\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'strategies': {\n",
    "            'MagnitudeL2': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.MagnitudeImportance(p=2)\n",
    "            },\n",
    "            'Random': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.RandomImportance()\n",
    "            },\n",
    "            # Note: BNScale is not applicable to LSTM as it doesn't have BatchNorm layers\n",
    "        },\n",
    "        'pruning_ratios': [0.0, 0.2, 0.5, 0.7],\n",
    "        'hidden_size': 100,\n",
    "        'num_layers': 2,\n",
    "        'fc_hidden_sizes': [64, 32],\n",
    "        'dropout_rate': 0.2,\n",
    "        'window_size': 50,\n",
    "        'batch_size': 128,\n",
    "        'learning_rate': 0.0001,\n",
    "        'epochs': 1000,  # More epochs for time series\n",
    "        'patience': 20,\n",
    "        'output_dir': './results_lstm_nasa',\n",
    "        'models_dir': './models_lstm_nasa',\n",
    "        'data_dir': './data/CMaps'\n",
    "    }\n",
    "\n",
    "    # Create output directories\n",
    "    os.makedirs(config['output_dir'], exist_ok=True)\n",
    "    os.makedirs(config['models_dir'], exist_ok=True)\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading NASA C-MAPSS dataset...\")\n",
    "    train_loader, val_loader, test_loader, input_size = get_data_loaders(\n",
    "        data_dir=config['data_dir'],\n",
    "        batch_size=config['batch_size'],\n",
    "        window_size=config['window_size']\n",
    "    )\n",
    "\n",
    "    # Prepare inputs and criterion\n",
    "    example_input_cpu = torch.randn(1, config['window_size'], input_size)\n",
    "    example_input_device = example_input_cpu.to(DEVICE)\n",
    "    criterion = nn.MSELoss()  # Regression loss\n",
    "\n",
    "    # Get baseline model and train it\n",
    "    print(\"\\nCreating and training baseline model...\")\n",
    "    model = get_lstm_model(\n",
    "        input_size,\n",
    "        config['hidden_size'],\n",
    "        config['num_layers'],\n",
    "        config['fc_hidden_sizes'],\n",
    "        config['dropout_rate']\n",
    "    )\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Train baseline model\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    trained_model, training_history = train_model(\n",
    "        model, train_loader, criterion, optimizer, DEVICE,\n",
    "        config['epochs'], val_loader, config['patience'], \"Baseline Training\"\n",
    "    )\n",
    "\n",
    "    # Save baseline model\n",
    "    baseline_model_path = os.path.join(config['models_dir'], 'baseline_model.pth')\n",
    "    save_model(trained_model, baseline_model_path, example_input_cpu)\n",
    "\n",
    "    # Evaluate baseline\n",
    "    print(\"\\nEvaluating baseline model...\")\n",
    "    baseline_metrics = evaluate_model(trained_model, test_loader, example_input_device, criterion, DEVICE)\n",
    "    print(f\"Baseline Results: MSE={baseline_metrics['mse']:.2f}, \"\n",
    "          f\"MAE={baseline_metrics['mae']:.2f}, \"\n",
    "          f\"MACs={baseline_metrics['macs'] / 1e6:.2f}M, \"\n",
    "          f\"Params={baseline_metrics['params'] / 1e6:.2f}M\")\n",
    "\n",
    "    # Initialize results storage\n",
    "    all_results = {}\n",
    "    for strategy_name in config['strategies'].keys():\n",
    "        all_results[strategy_name] = {0.0: baseline_metrics}\n",
    "\n",
    "    # Get ignored layers\n",
    "    ignored_layers = get_ignored_layers(trained_model)\n",
    "\n",
    "    # Run pruning experiments\n",
    "    print(\"\\nStarting pruning experiments...\")\n",
    "    print(\"Note: Due to torch_pruning limitations, only FC layers will be pruned (LSTM layers preserved)\")\n",
    "\n",
    "    for strategy_name, strategy_config in config['strategies'].items():\n",
    "        print(f\"\\n--- Strategy: {strategy_name} ---\")\n",
    "\n",
    "        for sparsity_ratio in config['pruning_ratios']:\n",
    "            if sparsity_ratio == 0.0:\n",
    "                continue  # Skip baseline (already done)\n",
    "\n",
    "            print(f\"\\nProcessing {strategy_name} at {sparsity_ratio:.1%} sparsity...\")\n",
    "\n",
    "            # Load fresh copy of trained baseline\n",
    "            model_copy = get_lstm_model(\n",
    "                input_size,\n",
    "                config['hidden_size'],\n",
    "                config['num_layers'],\n",
    "                config['fc_hidden_sizes'],\n",
    "                config['dropout_rate']\n",
    "            )\n",
    "            model_copy.load_state_dict(torch.load(baseline_model_path, map_location=DEVICE))\n",
    "            model_copy.to(DEVICE)\n",
    "\n",
    "            # Apply pruning\n",
    "            pruned_model = prune_model(\n",
    "                model_copy, strategy_config, sparsity_ratio,\n",
    "                example_input_device, ignored_layers\n",
    "            )\n",
    "\n",
    "            # Fine-tune pruned model\n",
    "            print(\"Fine-tuning pruned model...\")\n",
    "            optimizer_ft = optim.Adam(pruned_model.parameters(), lr=config['learning_rate'])\n",
    "            fine_tuned_model, ft_history = train_model(\n",
    "                pruned_model, train_loader, criterion, optimizer_ft, DEVICE,\n",
    "                config['epochs'], val_loader, config['patience'],\n",
    "                f\"{strategy_name}-{sparsity_ratio:.1%}\"\n",
    "            )\n",
    "\n",
    "            # Evaluate fine-tuned model\n",
    "            final_metrics = evaluate_model(fine_tuned_model, test_loader, example_input_device, criterion, DEVICE)\n",
    "            all_results[strategy_name][sparsity_ratio] = final_metrics\n",
    "\n",
    "            print(f\"Results: MSE={final_metrics['mse']:.2f}, \"\n",
    "                  f\"MAE={final_metrics['mae']:.2f}, \"\n",
    "                  f\"MACs={final_metrics['macs'] / 1e6:.2f}M\")\n",
    "\n",
    "            # Save fine-tuned model\n",
    "            model_filename = f\"{strategy_name.lower()}_sparsity_{sparsity_ratio:.1f}.pth\"\n",
    "            model_path = os.path.join(config['models_dir'], model_filename)\n",
    "            save_model(fine_tuned_model, model_path, example_input_cpu)\n",
    "\n",
    "    # Save and analyze results\n",
    "    print(\"\\nSaving results...\")\n",
    "    summary_df = save_results_to_files(all_results, config['output_dir'])\n",
    "\n",
    "    # Create plots\n",
    "    print(\"Creating plots...\")\n",
    "    create_results_plots(summary_df, config['output_dir'])\n",
    "\n",
    "    # Print summary\n",
    "    print_results_table(summary_df)\n",
    "\n",
    "    print(f\"\\nüéâ All experiments completed!\")\n",
    "    print(f\"üìÅ Results saved to: {os.path.abspath(config['output_dir'])}\")\n",
    "    print(f\"üìÅ Models saved to: {os.path.abspath(config['models_dir'])}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting NASA LSTM Pruning Experiments\n",
      "============================================================\n",
      "Loading NASA C-MAPSS dataset...\n",
      "Loading NASA C-MAPSS dataset from: ./data/CMaps\n",
      "--- Preparing Training Data ---\n",
      "\n",
      "--- Preparing Test Data ---\n",
      "Columns with std < 0.02 (potential removal): ['op_setting_1', 'op_setting_2', 'op_setting_3', 'sensor_1', 'sensor_5', 'sensor_6', 'sensor_10', 'sensor_16', 'sensor_18', 'sensor_19']\n",
      "\n",
      "Using Features: ['sensor_2', 'sensor_3', 'sensor_4', 'sensor_7', 'sensor_8', 'sensor_9', 'sensor_11', 'sensor_12', 'sensor_13', 'sensor_14', 'sensor_15', 'sensor_17', 'sensor_20', 'sensor_21']\n",
      "\n",
      "--- Normalizing Data ---\n",
      "DataLoaders created - Train: 12585, Val: 3146, Test: 100\n",
      "Input shape: (50, 14) (sequence_length, num_features)\n",
      "\n",
      "Creating and training baseline model...\n",
      "‚úÖ Created LSTM with architecture:\n",
      "   LSTM: input_size=14, hidden_size=100, num_layers=2\n",
      "   FC: 100 -> 64 -> 32 -> 1\n",
      "Epoch 1/1000 (Baseline Training): Train Loss: 7309.7875, Train MSE: 7306.49, Val Loss: 7377.7926, Val MSE: 7369.82 (Best)\n",
      "Epoch 2/1000 (Baseline Training): Train Loss: 6786.5416, Train MSE: 6793.34, Val Loss: 6523.4013, Val MSE: 6516.08 (Best)\n",
      "Epoch 3/1000 (Baseline Training): Train Loss: 5667.0628, Train MSE: 5671.24, Val Loss: 5067.4216, Val MSE: 5061.37 (Best)\n",
      "Epoch 4/1000 (Baseline Training): Train Loss: 4133.7027, Train MSE: 4135.92, Val Loss: 3437.0646, Val MSE: 3432.83 (Best)\n",
      "Epoch 5/1000 (Baseline Training): Train Loss: 2770.6004, Train MSE: 2773.33, Val Loss: 2276.3570, Val MSE: 2274.08 (Best)\n",
      "Epoch 6/1000 (Baseline Training): Train Loss: 2060.5553, Train MSE: 2061.48, Val Loss: 1821.1341, Val MSE: 1820.29 (Best)\n",
      "Epoch 7/1000 (Baseline Training): Train Loss: 1874.4369, Train MSE: 1873.26, Val Loss: 1725.1216, Val MSE: 1724.91 (Best)\n",
      "Epoch 8/1000 (Baseline Training): Train Loss: 1853.1803, Train MSE: 1854.21, Val Loss: 1710.1281, Val MSE: 1710.09 (Best)\n",
      "Epoch 9/1000 (Baseline Training): Train Loss: 1856.5275, Train MSE: 1858.15, Val Loss: 1704.9001, Val MSE: 1704.95 (Best)\n",
      "Epoch 10/1000 (Baseline Training): Train Loss: 1844.0749, Train MSE: 1842.43, Val Loss: 1706.0916, Val MSE: 1706.12\n",
      "Epoch 11/1000 (Baseline Training): Train Loss: 1851.0559, Train MSE: 1852.83, Val Loss: 1704.3015, Val MSE: 1704.36 (Best)\n",
      "Epoch 12/1000 (Baseline Training): Train Loss: 1856.5029, Train MSE: 1855.28, Val Loss: 1704.2335, Val MSE: 1704.29 (Best)\n",
      "Epoch 13/1000 (Baseline Training): Train Loss: 1846.8364, Train MSE: 1847.24, Val Loss: 1706.0771, Val MSE: 1706.10\n",
      "Epoch 14/1000 (Baseline Training): Train Loss: 1852.1459, Train MSE: 1852.94, Val Loss: 1706.0806, Val MSE: 1706.11\n",
      "Epoch 15/1000 (Baseline Training): Train Loss: 1841.2417, Train MSE: 1840.61, Val Loss: 1705.4881, Val MSE: 1705.52\n",
      "Epoch 16/1000 (Baseline Training): Train Loss: 1851.9513, Train MSE: 1854.12, Val Loss: 1704.3406, Val MSE: 1704.40\n",
      "Epoch 17/1000 (Baseline Training): Train Loss: 1854.9205, Train MSE: 1852.64, Val Loss: 1705.4231, Val MSE: 1705.46\n",
      "Epoch 18/1000 (Baseline Training): Train Loss: 1844.3603, Train MSE: 1844.88, Val Loss: 1704.8910, Val MSE: 1704.94\n",
      "Epoch 19/1000 (Baseline Training): Train Loss: 1853.8561, Train MSE: 1854.88, Val Loss: 1704.6167, Val MSE: 1704.67\n",
      "Epoch 20/1000 (Baseline Training): Train Loss: 1849.0110, Train MSE: 1847.89, Val Loss: 1704.6954, Val MSE: 1704.74\n",
      "Epoch 21/1000 (Baseline Training): Train Loss: 1857.0268, Train MSE: 1857.61, Val Loss: 1706.4663, Val MSE: 1706.49\n",
      "Epoch 22/1000 (Baseline Training): Train Loss: 1859.6010, Train MSE: 1860.09, Val Loss: 1708.3935, Val MSE: 1708.38\n",
      "Epoch 23/1000 (Baseline Training): Train Loss: 1859.5014, Train MSE: 1858.09, Val Loss: 1707.2110, Val MSE: 1707.22\n",
      "Epoch 24/1000 (Baseline Training): Train Loss: 1844.0597, Train MSE: 1845.80, Val Loss: 1700.3739, Val MSE: 1700.40 (Best)\n",
      "Epoch 25/1000 (Baseline Training): Train Loss: 1532.5443, Train MSE: 1537.68, Val Loss: 885.0754, Val MSE: 885.62 (Best)\n",
      "Epoch 26/1000 (Baseline Training): Train Loss: 691.3007, Train MSE: 692.81, Val Loss: 354.7943, Val MSE: 354.74 (Best)\n",
      "Epoch 27/1000 (Baseline Training): Train Loss: 535.1383, Train MSE: 535.74, Val Loss: 282.4743, Val MSE: 281.87 (Best)\n",
      "Epoch 28/1000 (Baseline Training): Train Loss: 453.6677, Train MSE: 452.60, Val Loss: 260.6853, Val MSE: 259.83 (Best)\n",
      "Epoch 29/1000 (Baseline Training): Train Loss: 444.7217, Train MSE: 444.54, Val Loss: 346.4182, Val MSE: 345.93\n",
      "Epoch 30/1000 (Baseline Training): Train Loss: 434.6256, Train MSE: 434.05, Val Loss: 294.4251, Val MSE: 294.55\n",
      "Epoch 31/1000 (Baseline Training): Train Loss: 424.2155, Train MSE: 423.75, Val Loss: 278.0881, Val MSE: 278.18\n",
      "Epoch 32/1000 (Baseline Training): Train Loss: 424.3163, Train MSE: 424.76, Val Loss: 248.0345, Val MSE: 247.89 (Best)\n",
      "Epoch 33/1000 (Baseline Training): Train Loss: 409.4848, Train MSE: 410.27, Val Loss: 218.0497, Val MSE: 217.55 (Best)\n",
      "Epoch 34/1000 (Baseline Training): Train Loss: 375.8662, Train MSE: 376.80, Val Loss: 292.9640, Val MSE: 293.16\n",
      "Epoch 35/1000 (Baseline Training): Train Loss: 424.0992, Train MSE: 424.48, Val Loss: 212.3830, Val MSE: 211.67 (Best)\n",
      "Epoch 36/1000 (Baseline Training): Train Loss: 389.1594, Train MSE: 388.61, Val Loss: 221.7787, Val MSE: 221.49\n",
      "Epoch 37/1000 (Baseline Training): Train Loss: 377.2893, Train MSE: 377.60, Val Loss: 214.9832, Val MSE: 213.82\n",
      "Epoch 38/1000 (Baseline Training): Train Loss: 362.0538, Train MSE: 363.25, Val Loss: 200.3127, Val MSE: 200.09 (Best)\n",
      "Epoch 39/1000 (Baseline Training): Train Loss: 375.8720, Train MSE: 376.57, Val Loss: 208.8148, Val MSE: 208.01\n",
      "Epoch 40/1000 (Baseline Training): Train Loss: 358.9532, Train MSE: 358.89, Val Loss: 223.8414, Val MSE: 223.59\n",
      "Epoch 41/1000 (Baseline Training): Train Loss: 373.6221, Train MSE: 374.13, Val Loss: 224.5392, Val MSE: 223.48\n",
      "Epoch 42/1000 (Baseline Training): Train Loss: 359.0419, Train MSE: 357.34, Val Loss: 199.5670, Val MSE: 198.81 (Best)\n",
      "Epoch 43/1000 (Baseline Training): Train Loss: 365.6140, Train MSE: 365.93, Val Loss: 186.4687, Val MSE: 185.96 (Best)\n",
      "Epoch 44/1000 (Baseline Training): Train Loss: 363.7047, Train MSE: 364.50, Val Loss: 194.3448, Val MSE: 193.88\n",
      "Epoch 45/1000 (Baseline Training): Train Loss: 346.4153, Train MSE: 346.39, Val Loss: 189.6476, Val MSE: 189.56\n",
      "Epoch 46/1000 (Baseline Training): Train Loss: 356.4525, Train MSE: 356.36, Val Loss: 221.1895, Val MSE: 219.87\n",
      "Epoch 47/1000 (Baseline Training): Train Loss: 352.5921, Train MSE: 351.90, Val Loss: 201.9620, Val MSE: 201.76\n",
      "Epoch 48/1000 (Baseline Training): Train Loss: 350.5609, Train MSE: 350.87, Val Loss: 181.3461, Val MSE: 181.12 (Best)\n",
      "Epoch 49/1000 (Baseline Training): Train Loss: 334.9134, Train MSE: 334.82, Val Loss: 183.4146, Val MSE: 182.94\n",
      "Epoch 50/1000 (Baseline Training): Train Loss: 334.5690, Train MSE: 334.98, Val Loss: 185.9843, Val MSE: 185.91\n",
      "Epoch 51/1000 (Baseline Training): Train Loss: 344.3629, Train MSE: 345.55, Val Loss: 181.3598, Val MSE: 181.04\n",
      "Epoch 52/1000 (Baseline Training): Train Loss: 337.1170, Train MSE: 337.03, Val Loss: 168.8164, Val MSE: 168.38 (Best)\n",
      "Epoch 53/1000 (Baseline Training): Train Loss: 333.5412, Train MSE: 333.31, Val Loss: 176.8850, Val MSE: 176.76\n",
      "Epoch 54/1000 (Baseline Training): Train Loss: 333.5717, Train MSE: 333.78, Val Loss: 197.5580, Val MSE: 197.38\n",
      "Epoch 55/1000 (Baseline Training): Train Loss: 334.1183, Train MSE: 334.48, Val Loss: 179.3269, Val MSE: 179.20\n",
      "Epoch 56/1000 (Baseline Training): Train Loss: 328.4360, Train MSE: 328.39, Val Loss: 202.9062, Val MSE: 202.67\n",
      "Epoch 57/1000 (Baseline Training): Train Loss: 336.3317, Train MSE: 336.41, Val Loss: 164.7261, Val MSE: 164.54 (Best)\n",
      "Epoch 58/1000 (Baseline Training): Train Loss: 323.5572, Train MSE: 323.86, Val Loss: 170.1788, Val MSE: 170.14\n",
      "Epoch 59/1000 (Baseline Training): Train Loss: 327.4320, Train MSE: 327.72, Val Loss: 170.7773, Val MSE: 170.42\n",
      "Epoch 60/1000 (Baseline Training): Train Loss: 320.7232, Train MSE: 321.25, Val Loss: 166.2779, Val MSE: 166.31\n",
      "Epoch 61/1000 (Baseline Training): Train Loss: 320.9344, Train MSE: 320.34, Val Loss: 187.5626, Val MSE: 187.79\n",
      "Epoch 62/1000 (Baseline Training): Train Loss: 319.3737, Train MSE: 320.39, Val Loss: 160.9676, Val MSE: 160.53 (Best)\n",
      "Epoch 63/1000 (Baseline Training): Train Loss: 321.7368, Train MSE: 322.42, Val Loss: 168.5209, Val MSE: 168.24\n",
      "Epoch 64/1000 (Baseline Training): Train Loss: 320.6391, Train MSE: 320.80, Val Loss: 252.1745, Val MSE: 251.46\n",
      "Epoch 65/1000 (Baseline Training): Train Loss: 319.6825, Train MSE: 320.41, Val Loss: 161.7657, Val MSE: 161.89\n",
      "Epoch 66/1000 (Baseline Training): Train Loss: 314.4681, Train MSE: 314.49, Val Loss: 157.7767, Val MSE: 157.67 (Best)\n",
      "Epoch 67/1000 (Baseline Training): Train Loss: 318.5941, Train MSE: 318.40, Val Loss: 161.1905, Val MSE: 161.40\n",
      "Epoch 68/1000 (Baseline Training): Train Loss: 306.9476, Train MSE: 307.40, Val Loss: 171.9224, Val MSE: 172.01\n",
      "Epoch 69/1000 (Baseline Training): Train Loss: 313.8943, Train MSE: 313.84, Val Loss: 207.7677, Val MSE: 207.46\n",
      "Epoch 70/1000 (Baseline Training): Train Loss: 320.5917, Train MSE: 320.66, Val Loss: 152.5231, Val MSE: 152.49 (Best)\n",
      "Epoch 71/1000 (Baseline Training): Train Loss: 308.2915, Train MSE: 308.82, Val Loss: 151.4019, Val MSE: 151.27 (Best)\n",
      "Epoch 72/1000 (Baseline Training): Train Loss: 305.9842, Train MSE: 305.42, Val Loss: 153.9521, Val MSE: 154.31\n",
      "Epoch 73/1000 (Baseline Training): Train Loss: 311.4298, Train MSE: 312.29, Val Loss: 152.6002, Val MSE: 152.37\n",
      "Epoch 74/1000 (Baseline Training): Train Loss: 304.8043, Train MSE: 304.22, Val Loss: 151.0518, Val MSE: 151.14 (Best)\n",
      "Epoch 75/1000 (Baseline Training): Train Loss: 303.5428, Train MSE: 304.30, Val Loss: 152.9222, Val MSE: 152.86\n",
      "Epoch 76/1000 (Baseline Training): Train Loss: 309.9427, Train MSE: 310.07, Val Loss: 160.3814, Val MSE: 160.45\n",
      "Epoch 77/1000 (Baseline Training): Train Loss: 304.3525, Train MSE: 305.00, Val Loss: 160.4213, Val MSE: 160.69\n",
      "Epoch 78/1000 (Baseline Training): Train Loss: 301.0894, Train MSE: 300.70, Val Loss: 155.3248, Val MSE: 155.45\n",
      "Epoch 79/1000 (Baseline Training): Train Loss: 304.5372, Train MSE: 304.00, Val Loss: 152.4684, Val MSE: 152.27\n",
      "Epoch 80/1000 (Baseline Training): Train Loss: 289.6397, Train MSE: 289.52, Val Loss: 148.1791, Val MSE: 148.55 (Best)\n",
      "Epoch 81/1000 (Baseline Training): Train Loss: 297.6745, Train MSE: 297.73, Val Loss: 170.1195, Val MSE: 170.43\n",
      "Epoch 82/1000 (Baseline Training): Train Loss: 297.5560, Train MSE: 297.90, Val Loss: 151.9144, Val MSE: 152.02\n",
      "Epoch 83/1000 (Baseline Training): Train Loss: 307.3329, Train MSE: 307.70, Val Loss: 150.1399, Val MSE: 150.23\n",
      "Epoch 84/1000 (Baseline Training): Train Loss: 303.1037, Train MSE: 302.72, Val Loss: 160.5598, Val MSE: 160.68\n",
      "Epoch 85/1000 (Baseline Training): Train Loss: 299.7931, Train MSE: 299.57, Val Loss: 172.8889, Val MSE: 173.12\n",
      "Epoch 86/1000 (Baseline Training): Train Loss: 292.3525, Train MSE: 292.16, Val Loss: 150.5003, Val MSE: 150.83\n",
      "Epoch 87/1000 (Baseline Training): Train Loss: 295.7712, Train MSE: 295.87, Val Loss: 186.4068, Val MSE: 186.36\n",
      "Epoch 88/1000 (Baseline Training): Train Loss: 298.2113, Train MSE: 297.88, Val Loss: 174.3736, Val MSE: 174.23\n",
      "Epoch 89/1000 (Baseline Training): Train Loss: 295.7926, Train MSE: 294.31, Val Loss: 142.9256, Val MSE: 143.05 (Best)\n",
      "Epoch 90/1000 (Baseline Training): Train Loss: 300.3657, Train MSE: 300.89, Val Loss: 138.7803, Val MSE: 139.03 (Best)\n",
      "Epoch 91/1000 (Baseline Training): Train Loss: 287.4641, Train MSE: 287.56, Val Loss: 141.5492, Val MSE: 141.91\n",
      "Epoch 92/1000 (Baseline Training): Train Loss: 284.5972, Train MSE: 284.21, Val Loss: 158.3771, Val MSE: 158.46\n",
      "Epoch 93/1000 (Baseline Training): Train Loss: 285.8932, Train MSE: 286.00, Val Loss: 145.4417, Val MSE: 145.96\n",
      "Epoch 94/1000 (Baseline Training): Train Loss: 295.3184, Train MSE: 295.22, Val Loss: 228.6968, Val MSE: 228.58\n",
      "Epoch 95/1000 (Baseline Training): Train Loss: 301.1161, Train MSE: 300.95, Val Loss: 138.4958, Val MSE: 138.84 (Best)\n",
      "Epoch 96/1000 (Baseline Training): Train Loss: 284.5290, Train MSE: 284.25, Val Loss: 144.9504, Val MSE: 145.13\n",
      "Epoch 97/1000 (Baseline Training): Train Loss: 290.1979, Train MSE: 289.35, Val Loss: 136.5721, Val MSE: 136.86 (Best)\n",
      "Epoch 98/1000 (Baseline Training): Train Loss: 288.7892, Train MSE: 288.81, Val Loss: 139.0546, Val MSE: 139.23\n",
      "Epoch 99/1000 (Baseline Training): Train Loss: 304.9924, Train MSE: 304.85, Val Loss: 144.5497, Val MSE: 144.99\n",
      "Epoch 100/1000 (Baseline Training): Train Loss: 289.9029, Train MSE: 290.17, Val Loss: 159.0891, Val MSE: 159.17\n",
      "Epoch 101/1000 (Baseline Training): Train Loss: 286.3553, Train MSE: 286.63, Val Loss: 139.2270, Val MSE: 139.40\n",
      "Epoch 102/1000 (Baseline Training): Train Loss: 290.4037, Train MSE: 290.46, Val Loss: 139.9627, Val MSE: 140.47\n",
      "Epoch 103/1000 (Baseline Training): Train Loss: 291.5390, Train MSE: 290.67, Val Loss: 136.2072, Val MSE: 136.50 (Best)\n",
      "Epoch 104/1000 (Baseline Training): Train Loss: 286.8544, Train MSE: 287.31, Val Loss: 137.9723, Val MSE: 138.02\n",
      "Epoch 105/1000 (Baseline Training): Train Loss: 278.7239, Train MSE: 278.96, Val Loss: 143.2474, Val MSE: 143.62\n",
      "Epoch 106/1000 (Baseline Training): Train Loss: 276.3073, Train MSE: 277.17, Val Loss: 135.2904, Val MSE: 135.48 (Best)\n",
      "Epoch 107/1000 (Baseline Training): Train Loss: 283.3205, Train MSE: 283.58, Val Loss: 135.1663, Val MSE: 135.47 (Best)\n",
      "Epoch 108/1000 (Baseline Training): Train Loss: 285.5672, Train MSE: 284.94, Val Loss: 183.3744, Val MSE: 183.77\n",
      "Epoch 109/1000 (Baseline Training): Train Loss: 278.1956, Train MSE: 278.30, Val Loss: 139.4675, Val MSE: 139.50\n",
      "Epoch 110/1000 (Baseline Training): Train Loss: 280.2098, Train MSE: 280.26, Val Loss: 152.5219, Val MSE: 153.08\n",
      "Epoch 111/1000 (Baseline Training): Train Loss: 284.2733, Train MSE: 284.03, Val Loss: 138.3622, Val MSE: 138.55\n",
      "Epoch 112/1000 (Baseline Training): Train Loss: 279.2946, Train MSE: 279.34, Val Loss: 135.8469, Val MSE: 136.18\n",
      "Epoch 113/1000 (Baseline Training): Train Loss: 277.0705, Train MSE: 276.73, Val Loss: 136.3796, Val MSE: 136.71\n",
      "Epoch 114/1000 (Baseline Training): Train Loss: 280.0008, Train MSE: 280.05, Val Loss: 135.6250, Val MSE: 136.09\n",
      "Epoch 115/1000 (Baseline Training): Train Loss: 277.1589, Train MSE: 277.42, Val Loss: 141.5781, Val MSE: 141.61\n",
      "Epoch 116/1000 (Baseline Training): Train Loss: 274.0209, Train MSE: 274.44, Val Loss: 132.9448, Val MSE: 133.12 (Best)\n",
      "Epoch 117/1000 (Baseline Training): Train Loss: 281.1733, Train MSE: 281.63, Val Loss: 137.0815, Val MSE: 137.33\n",
      "Epoch 118/1000 (Baseline Training): Train Loss: 279.9334, Train MSE: 280.46, Val Loss: 146.0775, Val MSE: 146.21\n",
      "Epoch 119/1000 (Baseline Training): Train Loss: 276.4800, Train MSE: 276.59, Val Loss: 151.0853, Val MSE: 151.54\n",
      "Epoch 120/1000 (Baseline Training): Train Loss: 278.3686, Train MSE: 278.25, Val Loss: 132.7632, Val MSE: 133.09 (Best)\n",
      "Epoch 121/1000 (Baseline Training): Train Loss: 276.4914, Train MSE: 276.30, Val Loss: 132.1098, Val MSE: 132.27 (Best)\n",
      "Epoch 122/1000 (Baseline Training): Train Loss: 276.7454, Train MSE: 276.68, Val Loss: 130.6331, Val MSE: 130.81 (Best)\n",
      "Epoch 123/1000 (Baseline Training): Train Loss: 277.2129, Train MSE: 277.13, Val Loss: 134.8494, Val MSE: 135.09\n",
      "Epoch 124/1000 (Baseline Training): Train Loss: 281.9443, Train MSE: 281.79, Val Loss: 130.5508, Val MSE: 130.89 (Best)\n",
      "Epoch 125/1000 (Baseline Training): Train Loss: 276.5981, Train MSE: 276.89, Val Loss: 134.2208, Val MSE: 134.57\n",
      "Epoch 126/1000 (Baseline Training): Train Loss: 275.8351, Train MSE: 275.68, Val Loss: 130.6641, Val MSE: 130.88\n",
      "Epoch 127/1000 (Baseline Training): Train Loss: 276.7847, Train MSE: 276.99, Val Loss: 131.8113, Val MSE: 132.15\n",
      "Epoch 128/1000 (Baseline Training): Train Loss: 273.4495, Train MSE: 273.83, Val Loss: 129.2572, Val MSE: 129.42 (Best)\n",
      "Epoch 129/1000 (Baseline Training): Train Loss: 281.1961, Train MSE: 280.80, Val Loss: 133.4444, Val MSE: 133.74\n",
      "Epoch 130/1000 (Baseline Training): Train Loss: 287.7465, Train MSE: 287.13, Val Loss: 146.4116, Val MSE: 146.60\n",
      "Epoch 131/1000 (Baseline Training): Train Loss: 285.2804, Train MSE: 284.32, Val Loss: 132.1782, Val MSE: 132.60\n",
      "Epoch 132/1000 (Baseline Training): Train Loss: 284.6163, Train MSE: 284.68, Val Loss: 141.6653, Val MSE: 141.92\n",
      "Epoch 133/1000 (Baseline Training): Train Loss: 278.4897, Train MSE: 278.43, Val Loss: 132.3049, Val MSE: 132.38\n",
      "Epoch 134/1000 (Baseline Training): Train Loss: 270.2269, Train MSE: 270.53, Val Loss: 135.1662, Val MSE: 135.32\n",
      "Epoch 135/1000 (Baseline Training): Train Loss: 279.9772, Train MSE: 279.91, Val Loss: 134.2606, Val MSE: 134.84\n",
      "Epoch 136/1000 (Baseline Training): Train Loss: 281.4184, Train MSE: 281.12, Val Loss: 154.2373, Val MSE: 154.36\n",
      "Epoch 137/1000 (Baseline Training): Train Loss: 271.1565, Train MSE: 270.82, Val Loss: 131.8452, Val MSE: 132.16\n",
      "Epoch 138/1000 (Baseline Training): Train Loss: 281.1553, Train MSE: 281.55, Val Loss: 129.5864, Val MSE: 129.67\n",
      "Epoch 139/1000 (Baseline Training): Train Loss: 264.5556, Train MSE: 265.21, Val Loss: 130.4849, Val MSE: 130.89\n",
      "Epoch 140/1000 (Baseline Training): Train Loss: 271.8171, Train MSE: 271.58, Val Loss: 131.5995, Val MSE: 131.92\n",
      "Epoch 141/1000 (Baseline Training): Train Loss: 268.9976, Train MSE: 268.93, Val Loss: 149.2173, Val MSE: 149.45\n",
      "Epoch 142/1000 (Baseline Training): Train Loss: 271.6434, Train MSE: 271.50, Val Loss: 143.0428, Val MSE: 143.25\n",
      "Epoch 143/1000 (Baseline Training): Train Loss: 265.9441, Train MSE: 266.90, Val Loss: 136.4822, Val MSE: 136.70\n",
      "Epoch 144/1000 (Baseline Training): Train Loss: 272.7585, Train MSE: 271.91, Val Loss: 132.9209, Val MSE: 133.13\n",
      "Epoch 145/1000 (Baseline Training): Train Loss: 272.4248, Train MSE: 272.71, Val Loss: 127.7603, Val MSE: 128.00 (Best)\n",
      "Epoch 146/1000 (Baseline Training): Train Loss: 277.7432, Train MSE: 277.80, Val Loss: 138.6452, Val MSE: 139.29\n",
      "Epoch 147/1000 (Baseline Training): Train Loss: 275.4643, Train MSE: 275.56, Val Loss: 133.8564, Val MSE: 133.90\n",
      "Epoch 148/1000 (Baseline Training): Train Loss: 274.9283, Train MSE: 276.13, Val Loss: 145.8950, Val MSE: 146.26\n",
      "Epoch 149/1000 (Baseline Training): Train Loss: 268.3127, Train MSE: 268.28, Val Loss: 130.6008, Val MSE: 130.91\n",
      "Epoch 150/1000 (Baseline Training): Train Loss: 280.6516, Train MSE: 280.03, Val Loss: 142.0328, Val MSE: 141.96\n",
      "Epoch 151/1000 (Baseline Training): Train Loss: 268.7470, Train MSE: 268.39, Val Loss: 130.0731, Val MSE: 130.62\n",
      "Epoch 152/1000 (Baseline Training): Train Loss: 278.9103, Train MSE: 279.55, Val Loss: 145.0542, Val MSE: 145.04\n",
      "Epoch 153/1000 (Baseline Training): Train Loss: 264.3216, Train MSE: 264.69, Val Loss: 124.9022, Val MSE: 125.18 (Best)\n",
      "Epoch 154/1000 (Baseline Training): Train Loss: 266.7201, Train MSE: 267.28, Val Loss: 134.9499, Val MSE: 135.34\n",
      "Epoch 155/1000 (Baseline Training): Train Loss: 275.8941, Train MSE: 275.83, Val Loss: 127.9693, Val MSE: 128.16\n",
      "Epoch 156/1000 (Baseline Training): Train Loss: 269.3547, Train MSE: 269.37, Val Loss: 129.0250, Val MSE: 129.50\n",
      "Epoch 157/1000 (Baseline Training): Train Loss: 269.9209, Train MSE: 268.99, Val Loss: 139.5962, Val MSE: 140.12\n",
      "Epoch 158/1000 (Baseline Training): Train Loss: 260.0939, Train MSE: 259.68, Val Loss: 131.4731, Val MSE: 132.17\n",
      "Epoch 159/1000 (Baseline Training): Train Loss: 272.5988, Train MSE: 273.12, Val Loss: 152.8979, Val MSE: 153.29\n",
      "Epoch 160/1000 (Baseline Training): Train Loss: 267.9201, Train MSE: 268.45, Val Loss: 131.9563, Val MSE: 132.11\n",
      "Epoch 161/1000 (Baseline Training): Train Loss: 278.0737, Train MSE: 278.53, Val Loss: 128.0909, Val MSE: 128.32\n",
      "Epoch 162/1000 (Baseline Training): Train Loss: 279.2165, Train MSE: 278.54, Val Loss: 146.9975, Val MSE: 147.33\n",
      "Epoch 163/1000 (Baseline Training): Train Loss: 279.5928, Train MSE: 278.73, Val Loss: 133.0094, Val MSE: 133.19\n",
      "Epoch 164/1000 (Baseline Training): Train Loss: 271.6271, Train MSE: 271.64, Val Loss: 130.4441, Val MSE: 130.75\n",
      "Epoch 165/1000 (Baseline Training): Train Loss: 272.9652, Train MSE: 272.81, Val Loss: 130.8606, Val MSE: 130.89\n",
      "Epoch 166/1000 (Baseline Training): Train Loss: 269.4166, Train MSE: 268.68, Val Loss: 124.2710, Val MSE: 124.40 (Best)\n",
      "Epoch 167/1000 (Baseline Training): Train Loss: 267.3021, Train MSE: 266.92, Val Loss: 137.1672, Val MSE: 137.14\n",
      "Epoch 168/1000 (Baseline Training): Train Loss: 274.6485, Train MSE: 274.65, Val Loss: 136.1016, Val MSE: 136.41\n",
      "Epoch 169/1000 (Baseline Training): Train Loss: 263.8189, Train MSE: 263.40, Val Loss: 163.4702, Val MSE: 163.77\n",
      "Epoch 170/1000 (Baseline Training): Train Loss: 263.8166, Train MSE: 264.20, Val Loss: 134.7327, Val MSE: 135.05\n",
      "Epoch 171/1000 (Baseline Training): Train Loss: 266.9850, Train MSE: 266.96, Val Loss: 133.5329, Val MSE: 133.64\n",
      "Epoch 172/1000 (Baseline Training): Train Loss: 269.2302, Train MSE: 268.95, Val Loss: 132.7204, Val MSE: 132.81\n",
      "Epoch 173/1000 (Baseline Training): Train Loss: 270.3906, Train MSE: 270.64, Val Loss: 125.0411, Val MSE: 125.25\n",
      "Epoch 174/1000 (Baseline Training): Train Loss: 261.9489, Train MSE: 262.60, Val Loss: 133.3564, Val MSE: 133.72\n",
      "Epoch 175/1000 (Baseline Training): Train Loss: 264.8866, Train MSE: 265.00, Val Loss: 123.9904, Val MSE: 124.27 (Best)\n",
      "Epoch 176/1000 (Baseline Training): Train Loss: 265.8081, Train MSE: 266.24, Val Loss: 128.7697, Val MSE: 128.91\n",
      "Epoch 177/1000 (Baseline Training): Train Loss: 266.2580, Train MSE: 266.65, Val Loss: 124.3116, Val MSE: 124.45\n",
      "Epoch 178/1000 (Baseline Training): Train Loss: 262.1725, Train MSE: 261.61, Val Loss: 142.5442, Val MSE: 142.49\n",
      "Epoch 179/1000 (Baseline Training): Train Loss: 267.0341, Train MSE: 267.40, Val Loss: 127.8812, Val MSE: 128.25\n",
      "Epoch 180/1000 (Baseline Training): Train Loss: 267.1002, Train MSE: 267.16, Val Loss: 127.0003, Val MSE: 127.07\n",
      "Epoch 181/1000 (Baseline Training): Train Loss: 260.4974, Train MSE: 260.26, Val Loss: 128.7773, Val MSE: 128.80\n",
      "Epoch 182/1000 (Baseline Training): Train Loss: 263.0383, Train MSE: 262.33, Val Loss: 125.1445, Val MSE: 125.56\n",
      "Epoch 183/1000 (Baseline Training): Train Loss: 268.1162, Train MSE: 268.73, Val Loss: 128.5860, Val MSE: 128.59\n",
      "Epoch 184/1000 (Baseline Training): Train Loss: 261.2089, Train MSE: 261.48, Val Loss: 133.0332, Val MSE: 132.97\n",
      "Epoch 185/1000 (Baseline Training): Train Loss: 262.6896, Train MSE: 262.72, Val Loss: 124.2356, Val MSE: 124.49\n",
      "Epoch 186/1000 (Baseline Training): Train Loss: 266.3289, Train MSE: 266.20, Val Loss: 128.2033, Val MSE: 128.63\n",
      "Epoch 187/1000 (Baseline Training): Train Loss: 261.5603, Train MSE: 261.82, Val Loss: 135.0406, Val MSE: 135.16\n",
      "Epoch 188/1000 (Baseline Training): Train Loss: 276.5539, Train MSE: 276.10, Val Loss: 130.0188, Val MSE: 130.18\n",
      "Epoch 189/1000 (Baseline Training): Train Loss: 262.2571, Train MSE: 262.07, Val Loss: 144.2531, Val MSE: 144.57\n",
      "Epoch 190/1000 (Baseline Training): Train Loss: 265.9367, Train MSE: 266.10, Val Loss: 128.7371, Val MSE: 128.93\n",
      "Epoch 191/1000 (Baseline Training): Train Loss: 262.2954, Train MSE: 263.42, Val Loss: 131.4219, Val MSE: 131.68\n",
      "Epoch 192/1000 (Baseline Training): Train Loss: 256.6589, Train MSE: 256.77, Val Loss: 143.9827, Val MSE: 144.08\n",
      "Epoch 193/1000 (Baseline Training): Train Loss: 265.6758, Train MSE: 264.83, Val Loss: 125.8910, Val MSE: 126.31\n",
      "Epoch 194/1000 (Baseline Training): Train Loss: 262.8933, Train MSE: 262.55, Val Loss: 133.5282, Val MSE: 133.88\n",
      "Epoch 195/1000 (Baseline Training): Train Loss: 260.2278, Train MSE: 259.80, Val Loss: 126.2275, Val MSE: 126.46\n",
      "Early stopping triggered after 195 epochs\n",
      "Loaded best model state\n",
      "‚úÖ Model saved to ./models_lstm_nasa/baseline_model.pth\n",
      "‚úÖ ONNX model saved to ./models_lstm_nasa/baseline_model.onnx\n",
      "\n",
      "Evaluating baseline model...\n",
      "Baseline Results: MSE=240.74, MAE=10.86, MACs=90.45M, Params=0.14M\n",
      "\n",
      "Starting pruning experiments...\n",
      "Note: Due to torch_pruning limitations, only FC layers will be pruned (LSTM layers preserved)\n",
      "\n",
      "--- Strategy: MagnitudeL2 ---\n",
      "\n",
      "Processing MagnitudeL2 at 20.0% sparsity...\n",
      "‚úÖ Created LSTM with architecture:\n",
      "   LSTM: input_size=14, hidden_size=100, num_layers=2\n",
      "   FC: 100 -> 64 -> 32 -> 1\n",
      "Initial MACs: 90.45M\n",
      "Applying MagnitudeImportance pruning at 20.0% sparsity...\n",
      "Note: Only pruning FC layers, LSTM layers are preserved\n",
      "Final MACs: 90.45M (Reduction: 0.0%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/1000 (MagnitudeL2-20.0%): Train Loss: 271.8842, Train MSE: 272.68, Val Loss: 128.1343, Val MSE: 128.15 (Best)\n",
      "Epoch 2/1000 (MagnitudeL2-20.0%): Train Loss: 263.7136, Train MSE: 263.22, Val Loss: 127.6274, Val MSE: 127.65 (Best)\n",
      "Epoch 3/1000 (MagnitudeL2-20.0%): Train Loss: 263.2189, Train MSE: 263.38, Val Loss: 131.8332, Val MSE: 132.07\n",
      "Epoch 4/1000 (MagnitudeL2-20.0%): Train Loss: 273.6700, Train MSE: 273.63, Val Loss: 132.4862, Val MSE: 132.85\n",
      "Epoch 5/1000 (MagnitudeL2-20.0%): Train Loss: 268.4344, Train MSE: 268.43, Val Loss: 129.3444, Val MSE: 129.45\n",
      "Epoch 6/1000 (MagnitudeL2-20.0%): Train Loss: 275.8538, Train MSE: 275.45, Val Loss: 125.1894, Val MSE: 125.35 (Best)\n",
      "Epoch 7/1000 (MagnitudeL2-20.0%): Train Loss: 268.4467, Train MSE: 267.75, Val Loss: 127.7964, Val MSE: 128.03\n",
      "Epoch 8/1000 (MagnitudeL2-20.0%): Train Loss: 273.4932, Train MSE: 273.64, Val Loss: 123.4819, Val MSE: 123.83 (Best)\n",
      "Epoch 9/1000 (MagnitudeL2-20.0%): Train Loss: 264.2765, Train MSE: 264.41, Val Loss: 152.6481, Val MSE: 152.74\n",
      "Epoch 10/1000 (MagnitudeL2-20.0%): Train Loss: 264.0330, Train MSE: 263.47, Val Loss: 124.1294, Val MSE: 124.28\n",
      "Epoch 11/1000 (MagnitudeL2-20.0%): Train Loss: 263.6837, Train MSE: 263.90, Val Loss: 125.3295, Val MSE: 125.57\n",
      "Epoch 12/1000 (MagnitudeL2-20.0%): Train Loss: 267.3112, Train MSE: 267.46, Val Loss: 131.6011, Val MSE: 131.74\n",
      "Epoch 13/1000 (MagnitudeL2-20.0%): Train Loss: 265.2728, Train MSE: 265.50, Val Loss: 141.9353, Val MSE: 142.09\n",
      "Epoch 14/1000 (MagnitudeL2-20.0%): Train Loss: 267.2035, Train MSE: 267.66, Val Loss: 149.7943, Val MSE: 150.42\n",
      "Epoch 15/1000 (MagnitudeL2-20.0%): Train Loss: 263.5764, Train MSE: 264.04, Val Loss: 127.5297, Val MSE: 127.92\n",
      "Epoch 16/1000 (MagnitudeL2-20.0%): Train Loss: 263.9902, Train MSE: 263.89, Val Loss: 131.3697, Val MSE: 131.58\n",
      "Epoch 17/1000 (MagnitudeL2-20.0%): Train Loss: 262.9404, Train MSE: 261.48, Val Loss: 128.5716, Val MSE: 128.69\n",
      "Epoch 18/1000 (MagnitudeL2-20.0%): Train Loss: 258.8215, Train MSE: 258.94, Val Loss: 130.1080, Val MSE: 130.00\n",
      "Epoch 19/1000 (MagnitudeL2-20.0%): Train Loss: 260.2208, Train MSE: 259.98, Val Loss: 130.1154, Val MSE: 130.09\n",
      "Epoch 20/1000 (MagnitudeL2-20.0%): Train Loss: 264.9975, Train MSE: 265.08, Val Loss: 176.7132, Val MSE: 176.57\n",
      "Epoch 21/1000 (MagnitudeL2-20.0%): Train Loss: 267.1732, Train MSE: 267.89, Val Loss: 126.7375, Val MSE: 127.02\n",
      "Epoch 22/1000 (MagnitudeL2-20.0%): Train Loss: 259.6259, Train MSE: 259.78, Val Loss: 123.8077, Val MSE: 123.95\n",
      "Epoch 23/1000 (MagnitudeL2-20.0%): Train Loss: 264.7936, Train MSE: 264.98, Val Loss: 125.9571, Val MSE: 126.36\n",
      "Epoch 24/1000 (MagnitudeL2-20.0%): Train Loss: 261.6608, Train MSE: 262.06, Val Loss: 141.3286, Val MSE: 141.60\n",
      "Epoch 25/1000 (MagnitudeL2-20.0%): Train Loss: 259.3894, Train MSE: 258.82, Val Loss: 139.0690, Val MSE: 139.25\n",
      "Epoch 26/1000 (MagnitudeL2-20.0%): Train Loss: 257.9578, Train MSE: 258.14, Val Loss: 197.8099, Val MSE: 198.18\n",
      "Epoch 27/1000 (MagnitudeL2-20.0%): Train Loss: 260.6237, Train MSE: 261.14, Val Loss: 126.7466, Val MSE: 126.90\n",
      "Epoch 28/1000 (MagnitudeL2-20.0%): Train Loss: 254.9745, Train MSE: 254.46, Val Loss: 123.1197, Val MSE: 123.27 (Best)\n",
      "Epoch 29/1000 (MagnitudeL2-20.0%): Train Loss: 261.1645, Train MSE: 261.43, Val Loss: 154.1487, Val MSE: 154.22\n",
      "Epoch 30/1000 (MagnitudeL2-20.0%): Train Loss: 266.3685, Train MSE: 266.30, Val Loss: 124.6854, Val MSE: 124.80\n",
      "Epoch 31/1000 (MagnitudeL2-20.0%): Train Loss: 248.9393, Train MSE: 249.45, Val Loss: 125.6100, Val MSE: 125.61\n",
      "Epoch 32/1000 (MagnitudeL2-20.0%): Train Loss: 265.3637, Train MSE: 265.08, Val Loss: 123.2882, Val MSE: 123.62\n",
      "Epoch 33/1000 (MagnitudeL2-20.0%): Train Loss: 265.4957, Train MSE: 265.72, Val Loss: 150.3545, Val MSE: 150.54\n",
      "Epoch 34/1000 (MagnitudeL2-20.0%): Train Loss: 254.6018, Train MSE: 255.25, Val Loss: 129.0834, Val MSE: 129.47\n",
      "Epoch 35/1000 (MagnitudeL2-20.0%): Train Loss: 260.6608, Train MSE: 260.23, Val Loss: 122.3882, Val MSE: 122.45 (Best)\n",
      "Epoch 36/1000 (MagnitudeL2-20.0%): Train Loss: 265.8498, Train MSE: 265.90, Val Loss: 131.3292, Val MSE: 131.85\n",
      "Epoch 37/1000 (MagnitudeL2-20.0%): Train Loss: 260.9928, Train MSE: 260.73, Val Loss: 125.5838, Val MSE: 125.64\n",
      "Epoch 38/1000 (MagnitudeL2-20.0%): Train Loss: 256.4135, Train MSE: 257.05, Val Loss: 122.6881, Val MSE: 122.70\n",
      "Epoch 39/1000 (MagnitudeL2-20.0%): Train Loss: 263.3501, Train MSE: 263.84, Val Loss: 122.3077, Val MSE: 122.35 (Best)\n",
      "Epoch 40/1000 (MagnitudeL2-20.0%): Train Loss: 259.6191, Train MSE: 259.36, Val Loss: 124.4861, Val MSE: 124.66\n",
      "Epoch 41/1000 (MagnitudeL2-20.0%): Train Loss: 255.8071, Train MSE: 256.64, Val Loss: 126.4461, Val MSE: 126.82\n",
      "Epoch 42/1000 (MagnitudeL2-20.0%): Train Loss: 262.8166, Train MSE: 262.67, Val Loss: 141.5219, Val MSE: 141.74\n",
      "Epoch 43/1000 (MagnitudeL2-20.0%): Train Loss: 266.0866, Train MSE: 266.71, Val Loss: 122.1935, Val MSE: 122.53 (Best)\n",
      "Epoch 44/1000 (MagnitudeL2-20.0%): Train Loss: 269.3720, Train MSE: 269.28, Val Loss: 142.9124, Val MSE: 143.18\n",
      "Epoch 45/1000 (MagnitudeL2-20.0%): Train Loss: 253.4873, Train MSE: 253.86, Val Loss: 162.8558, Val MSE: 163.20\n",
      "Epoch 46/1000 (MagnitudeL2-20.0%): Train Loss: 268.5822, Train MSE: 267.71, Val Loss: 145.3231, Val MSE: 145.75\n",
      "Epoch 47/1000 (MagnitudeL2-20.0%): Train Loss: 264.6025, Train MSE: 264.64, Val Loss: 130.2119, Val MSE: 130.32\n",
      "Epoch 48/1000 (MagnitudeL2-20.0%): Train Loss: 264.9098, Train MSE: 265.76, Val Loss: 130.2899, Val MSE: 130.66\n",
      "Epoch 49/1000 (MagnitudeL2-20.0%): Train Loss: 258.2514, Train MSE: 258.00, Val Loss: 121.0450, Val MSE: 121.10 (Best)\n",
      "Epoch 50/1000 (MagnitudeL2-20.0%): Train Loss: 259.7668, Train MSE: 260.33, Val Loss: 154.4665, Val MSE: 154.63\n",
      "Epoch 51/1000 (MagnitudeL2-20.0%): Train Loss: 256.0558, Train MSE: 255.68, Val Loss: 121.4608, Val MSE: 121.64\n",
      "Epoch 52/1000 (MagnitudeL2-20.0%): Train Loss: 251.5018, Train MSE: 251.43, Val Loss: 120.6805, Val MSE: 120.97 (Best)\n",
      "Epoch 53/1000 (MagnitudeL2-20.0%): Train Loss: 263.8992, Train MSE: 264.88, Val Loss: 122.5432, Val MSE: 122.80\n",
      "Epoch 54/1000 (MagnitudeL2-20.0%): Train Loss: 264.5141, Train MSE: 264.62, Val Loss: 131.7781, Val MSE: 131.61\n",
      "Epoch 55/1000 (MagnitudeL2-20.0%): Train Loss: 256.6792, Train MSE: 256.29, Val Loss: 132.2701, Val MSE: 132.30\n",
      "Epoch 56/1000 (MagnitudeL2-20.0%): Train Loss: 260.0224, Train MSE: 259.78, Val Loss: 123.3493, Val MSE: 123.53\n",
      "Epoch 57/1000 (MagnitudeL2-20.0%): Train Loss: 265.2472, Train MSE: 265.35, Val Loss: 127.0382, Val MSE: 127.41\n",
      "Epoch 58/1000 (MagnitudeL2-20.0%): Train Loss: 252.3540, Train MSE: 252.76, Val Loss: 122.2702, Val MSE: 122.57\n",
      "Epoch 59/1000 (MagnitudeL2-20.0%): Train Loss: 257.5451, Train MSE: 257.57, Val Loss: 123.0216, Val MSE: 123.33\n",
      "Epoch 60/1000 (MagnitudeL2-20.0%): Train Loss: 255.6169, Train MSE: 255.65, Val Loss: 126.9849, Val MSE: 127.10\n",
      "Epoch 61/1000 (MagnitudeL2-20.0%): Train Loss: 257.3654, Train MSE: 258.16, Val Loss: 122.2617, Val MSE: 122.51\n",
      "Epoch 62/1000 (MagnitudeL2-20.0%): Train Loss: 254.5077, Train MSE: 255.00, Val Loss: 134.0302, Val MSE: 134.46\n",
      "Epoch 63/1000 (MagnitudeL2-20.0%): Train Loss: 257.4146, Train MSE: 257.57, Val Loss: 124.5495, Val MSE: 124.72\n",
      "Epoch 64/1000 (MagnitudeL2-20.0%): Train Loss: 261.8655, Train MSE: 260.80, Val Loss: 119.7903, Val MSE: 119.98 (Best)\n",
      "Epoch 65/1000 (MagnitudeL2-20.0%): Train Loss: 256.8806, Train MSE: 256.73, Val Loss: 123.5909, Val MSE: 123.91\n",
      "Epoch 66/1000 (MagnitudeL2-20.0%): Train Loss: 259.5329, Train MSE: 259.59, Val Loss: 119.2469, Val MSE: 119.47 (Best)\n",
      "Epoch 67/1000 (MagnitudeL2-20.0%): Train Loss: 257.9646, Train MSE: 258.38, Val Loss: 148.9296, Val MSE: 149.08\n",
      "Epoch 68/1000 (MagnitudeL2-20.0%): Train Loss: 252.8471, Train MSE: 253.44, Val Loss: 132.3247, Val MSE: 132.40\n",
      "Epoch 69/1000 (MagnitudeL2-20.0%): Train Loss: 258.5238, Train MSE: 258.81, Val Loss: 134.5171, Val MSE: 134.52\n",
      "Epoch 70/1000 (MagnitudeL2-20.0%): Train Loss: 261.4223, Train MSE: 261.48, Val Loss: 125.9172, Val MSE: 126.23\n",
      "Epoch 71/1000 (MagnitudeL2-20.0%): Train Loss: 258.9023, Train MSE: 258.61, Val Loss: 130.1461, Val MSE: 130.39\n",
      "Epoch 72/1000 (MagnitudeL2-20.0%): Train Loss: 262.2708, Train MSE: 262.52, Val Loss: 120.9712, Val MSE: 121.08\n",
      "Epoch 73/1000 (MagnitudeL2-20.0%): Train Loss: 259.4383, Train MSE: 259.56, Val Loss: 146.3877, Val MSE: 146.66\n",
      "Epoch 74/1000 (MagnitudeL2-20.0%): Train Loss: 262.2049, Train MSE: 262.22, Val Loss: 120.6383, Val MSE: 120.94\n",
      "Epoch 75/1000 (MagnitudeL2-20.0%): Train Loss: 251.7723, Train MSE: 251.52, Val Loss: 127.3124, Val MSE: 127.21\n",
      "Epoch 76/1000 (MagnitudeL2-20.0%): Train Loss: 256.5452, Train MSE: 256.04, Val Loss: 124.8292, Val MSE: 124.68\n",
      "Epoch 77/1000 (MagnitudeL2-20.0%): Train Loss: 261.8053, Train MSE: 261.81, Val Loss: 124.0330, Val MSE: 124.42\n",
      "Epoch 78/1000 (MagnitudeL2-20.0%): Train Loss: 259.9517, Train MSE: 260.66, Val Loss: 125.5591, Val MSE: 125.83\n",
      "Epoch 79/1000 (MagnitudeL2-20.0%): Train Loss: 260.8092, Train MSE: 259.87, Val Loss: 119.5407, Val MSE: 119.84\n",
      "Epoch 80/1000 (MagnitudeL2-20.0%): Train Loss: 257.8626, Train MSE: 258.26, Val Loss: 124.6065, Val MSE: 124.88\n",
      "Epoch 81/1000 (MagnitudeL2-20.0%): Train Loss: 258.0467, Train MSE: 258.37, Val Loss: 121.8959, Val MSE: 121.86\n",
      "Epoch 82/1000 (MagnitudeL2-20.0%): Train Loss: 262.6440, Train MSE: 262.58, Val Loss: 120.7581, Val MSE: 121.05\n",
      "Epoch 83/1000 (MagnitudeL2-20.0%): Train Loss: 252.7782, Train MSE: 253.28, Val Loss: 129.6674, Val MSE: 129.91\n",
      "Epoch 84/1000 (MagnitudeL2-20.0%): Train Loss: 257.8406, Train MSE: 258.04, Val Loss: 122.7684, Val MSE: 122.87\n",
      "Epoch 85/1000 (MagnitudeL2-20.0%): Train Loss: 257.7464, Train MSE: 257.96, Val Loss: 119.2016, Val MSE: 119.26 (Best)\n",
      "Epoch 86/1000 (MagnitudeL2-20.0%): Train Loss: 251.0399, Train MSE: 251.26, Val Loss: 133.2713, Val MSE: 133.48\n",
      "Epoch 87/1000 (MagnitudeL2-20.0%): Train Loss: 259.3194, Train MSE: 258.53, Val Loss: 126.4917, Val MSE: 126.89\n",
      "Epoch 88/1000 (MagnitudeL2-20.0%): Train Loss: 266.2963, Train MSE: 266.26, Val Loss: 121.3968, Val MSE: 121.68\n",
      "Epoch 89/1000 (MagnitudeL2-20.0%): Train Loss: 249.8275, Train MSE: 250.05, Val Loss: 119.6025, Val MSE: 119.87\n",
      "Epoch 90/1000 (MagnitudeL2-20.0%): Train Loss: 253.0236, Train MSE: 253.42, Val Loss: 120.7595, Val MSE: 120.73\n",
      "Epoch 91/1000 (MagnitudeL2-20.0%): Train Loss: 262.0485, Train MSE: 262.81, Val Loss: 124.1724, Val MSE: 124.15\n",
      "Epoch 92/1000 (MagnitudeL2-20.0%): Train Loss: 259.2651, Train MSE: 259.71, Val Loss: 117.8112, Val MSE: 118.04 (Best)\n",
      "Epoch 93/1000 (MagnitudeL2-20.0%): Train Loss: 254.5442, Train MSE: 254.77, Val Loss: 128.2339, Val MSE: 128.62\n",
      "Epoch 94/1000 (MagnitudeL2-20.0%): Train Loss: 253.7431, Train MSE: 254.74, Val Loss: 156.8132, Val MSE: 157.01\n",
      "Epoch 95/1000 (MagnitudeL2-20.0%): Train Loss: 254.7700, Train MSE: 254.82, Val Loss: 126.1170, Val MSE: 126.23\n",
      "Epoch 96/1000 (MagnitudeL2-20.0%): Train Loss: 260.7511, Train MSE: 260.23, Val Loss: 119.6497, Val MSE: 119.75\n",
      "Epoch 97/1000 (MagnitudeL2-20.0%): Train Loss: 258.3301, Train MSE: 258.74, Val Loss: 116.9889, Val MSE: 117.21 (Best)\n",
      "Epoch 98/1000 (MagnitudeL2-20.0%): Train Loss: 252.4663, Train MSE: 251.95, Val Loss: 124.6316, Val MSE: 124.76\n",
      "Epoch 99/1000 (MagnitudeL2-20.0%): Train Loss: 259.8836, Train MSE: 260.29, Val Loss: 129.3313, Val MSE: 129.66\n",
      "Epoch 100/1000 (MagnitudeL2-20.0%): Train Loss: 262.8415, Train MSE: 262.25, Val Loss: 119.5510, Val MSE: 119.83\n",
      "Epoch 101/1000 (MagnitudeL2-20.0%): Train Loss: 256.4148, Train MSE: 256.58, Val Loss: 122.4449, Val MSE: 122.77\n",
      "Epoch 102/1000 (MagnitudeL2-20.0%): Train Loss: 252.6149, Train MSE: 252.75, Val Loss: 122.8022, Val MSE: 122.98\n",
      "Epoch 103/1000 (MagnitudeL2-20.0%): Train Loss: 253.5942, Train MSE: 254.23, Val Loss: 140.7946, Val MSE: 140.72\n",
      "Epoch 104/1000 (MagnitudeL2-20.0%): Train Loss: 248.6214, Train MSE: 248.62, Val Loss: 134.6931, Val MSE: 134.98\n",
      "Epoch 105/1000 (MagnitudeL2-20.0%): Train Loss: 254.9580, Train MSE: 254.89, Val Loss: 119.2868, Val MSE: 119.50\n",
      "Epoch 106/1000 (MagnitudeL2-20.0%): Train Loss: 257.1675, Train MSE: 257.36, Val Loss: 135.1401, Val MSE: 135.20\n",
      "Epoch 107/1000 (MagnitudeL2-20.0%): Train Loss: 256.5070, Train MSE: 256.02, Val Loss: 132.8321, Val MSE: 133.14\n",
      "Epoch 108/1000 (MagnitudeL2-20.0%): Train Loss: 256.8550, Train MSE: 256.95, Val Loss: 129.6007, Val MSE: 129.75\n",
      "Epoch 109/1000 (MagnitudeL2-20.0%): Train Loss: 249.2903, Train MSE: 249.69, Val Loss: 119.8083, Val MSE: 120.04\n",
      "Epoch 110/1000 (MagnitudeL2-20.0%): Train Loss: 252.0551, Train MSE: 252.58, Val Loss: 123.6347, Val MSE: 123.84\n",
      "Epoch 111/1000 (MagnitudeL2-20.0%): Train Loss: 254.3175, Train MSE: 252.97, Val Loss: 119.4468, Val MSE: 119.73\n",
      "Epoch 112/1000 (MagnitudeL2-20.0%): Train Loss: 256.7270, Train MSE: 256.56, Val Loss: 119.4246, Val MSE: 119.36\n",
      "Epoch 113/1000 (MagnitudeL2-20.0%): Train Loss: 248.2363, Train MSE: 248.31, Val Loss: 122.6508, Val MSE: 122.72\n",
      "Epoch 114/1000 (MagnitudeL2-20.0%): Train Loss: 256.2920, Train MSE: 255.59, Val Loss: 128.3189, Val MSE: 128.35\n",
      "Epoch 115/1000 (MagnitudeL2-20.0%): Train Loss: 254.5360, Train MSE: 254.90, Val Loss: 120.3088, Val MSE: 120.50\n",
      "Epoch 116/1000 (MagnitudeL2-20.0%): Train Loss: 251.2073, Train MSE: 251.59, Val Loss: 126.2775, Val MSE: 126.72\n",
      "Epoch 117/1000 (MagnitudeL2-20.0%): Train Loss: 249.4250, Train MSE: 249.95, Val Loss: 119.3706, Val MSE: 119.67\n",
      "Early stopping triggered after 117 epochs\n",
      "Loaded best model state\n",
      "Results: MSE=259.91, MAE=10.91, MACs=90.45M\n",
      "‚úÖ Model saved to ./models_lstm_nasa/magnitudel2_sparsity_0.2.pth\n",
      "‚úÖ ONNX model saved to ./models_lstm_nasa/magnitudel2_sparsity_0.2.onnx\n",
      "\n",
      "Processing MagnitudeL2 at 50.0% sparsity...\n",
      "‚úÖ Created LSTM with architecture:\n",
      "   LSTM: input_size=14, hidden_size=100, num_layers=2\n",
      "   FC: 100 -> 64 -> 32 -> 1\n",
      "Initial MACs: 90.45M\n",
      "Applying MagnitudeImportance pruning at 50.0% sparsity...\n",
      "Note: Only pruning FC layers, LSTM layers are preserved\n",
      "Final MACs: 90.45M (Reduction: 0.0%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/1000 (MagnitudeL2-50.0%): Train Loss: 276.1692, Train MSE: 276.05, Val Loss: 150.9874, Val MSE: 151.17 (Best)\n",
      "Epoch 2/1000 (MagnitudeL2-50.0%): Train Loss: 264.7600, Train MSE: 264.70, Val Loss: 125.2650, Val MSE: 125.54 (Best)\n",
      "Epoch 3/1000 (MagnitudeL2-50.0%): Train Loss: 266.8851, Train MSE: 266.00, Val Loss: 125.5625, Val MSE: 125.75\n",
      "Epoch 4/1000 (MagnitudeL2-50.0%): Train Loss: 262.0982, Train MSE: 262.18, Val Loss: 142.5216, Val MSE: 142.68\n",
      "Epoch 5/1000 (MagnitudeL2-50.0%): Train Loss: 269.4683, Train MSE: 268.67, Val Loss: 143.2659, Val MSE: 143.30\n",
      "Epoch 6/1000 (MagnitudeL2-50.0%): Train Loss: 265.1558, Train MSE: 264.69, Val Loss: 133.3042, Val MSE: 133.19\n",
      "Epoch 7/1000 (MagnitudeL2-50.0%): Train Loss: 264.4983, Train MSE: 264.47, Val Loss: 125.6652, Val MSE: 126.00\n",
      "Epoch 8/1000 (MagnitudeL2-50.0%): Train Loss: 263.4264, Train MSE: 262.71, Val Loss: 127.5243, Val MSE: 127.99\n",
      "Epoch 9/1000 (MagnitudeL2-50.0%): Train Loss: 267.1922, Train MSE: 267.04, Val Loss: 131.0397, Val MSE: 131.33\n",
      "Epoch 10/1000 (MagnitudeL2-50.0%): Train Loss: 263.8193, Train MSE: 264.84, Val Loss: 152.5424, Val MSE: 152.48\n",
      "Epoch 11/1000 (MagnitudeL2-50.0%): Train Loss: 268.8915, Train MSE: 268.91, Val Loss: 128.1808, Val MSE: 128.72\n",
      "Epoch 12/1000 (MagnitudeL2-50.0%): Train Loss: 262.2670, Train MSE: 261.54, Val Loss: 130.1229, Val MSE: 130.51\n",
      "Epoch 13/1000 (MagnitudeL2-50.0%): Train Loss: 266.9111, Train MSE: 266.88, Val Loss: 121.9072, Val MSE: 122.23 (Best)\n",
      "Epoch 14/1000 (MagnitudeL2-50.0%): Train Loss: 263.5237, Train MSE: 263.49, Val Loss: 124.8771, Val MSE: 125.06\n",
      "Epoch 15/1000 (MagnitudeL2-50.0%): Train Loss: 258.7654, Train MSE: 258.90, Val Loss: 137.4444, Val MSE: 137.93\n",
      "Epoch 16/1000 (MagnitudeL2-50.0%): Train Loss: 266.8404, Train MSE: 266.49, Val Loss: 124.3972, Val MSE: 124.75\n",
      "Epoch 17/1000 (MagnitudeL2-50.0%): Train Loss: 269.7476, Train MSE: 268.94, Val Loss: 126.4716, Val MSE: 126.60\n",
      "Epoch 18/1000 (MagnitudeL2-50.0%): Train Loss: 266.0895, Train MSE: 266.24, Val Loss: 138.3704, Val MSE: 138.51\n",
      "Epoch 19/1000 (MagnitudeL2-50.0%): Train Loss: 261.0820, Train MSE: 259.60, Val Loss: 126.3474, Val MSE: 126.60\n",
      "Epoch 20/1000 (MagnitudeL2-50.0%): Train Loss: 254.8428, Train MSE: 254.75, Val Loss: 122.2089, Val MSE: 122.31\n",
      "Epoch 21/1000 (MagnitudeL2-50.0%): Train Loss: 265.4029, Train MSE: 264.85, Val Loss: 126.0590, Val MSE: 126.09\n",
      "Epoch 22/1000 (MagnitudeL2-50.0%): Train Loss: 267.2297, Train MSE: 266.84, Val Loss: 140.6427, Val MSE: 140.78\n",
      "Epoch 23/1000 (MagnitudeL2-50.0%): Train Loss: 260.9264, Train MSE: 259.86, Val Loss: 122.3997, Val MSE: 122.78\n",
      "Epoch 24/1000 (MagnitudeL2-50.0%): Train Loss: 263.5635, Train MSE: 263.76, Val Loss: 124.1323, Val MSE: 124.44\n",
      "Epoch 25/1000 (MagnitudeL2-50.0%): Train Loss: 265.2609, Train MSE: 264.99, Val Loss: 132.7702, Val MSE: 132.61\n",
      "Epoch 26/1000 (MagnitudeL2-50.0%): Train Loss: 259.9780, Train MSE: 260.51, Val Loss: 126.3880, Val MSE: 126.33\n",
      "Epoch 27/1000 (MagnitudeL2-50.0%): Train Loss: 261.4851, Train MSE: 261.57, Val Loss: 133.8033, Val MSE: 134.05\n",
      "Epoch 28/1000 (MagnitudeL2-50.0%): Train Loss: 253.6220, Train MSE: 254.27, Val Loss: 122.4475, Val MSE: 122.71\n",
      "Epoch 29/1000 (MagnitudeL2-50.0%): Train Loss: 266.3679, Train MSE: 266.05, Val Loss: 132.8994, Val MSE: 133.08\n",
      "Epoch 30/1000 (MagnitudeL2-50.0%): Train Loss: 258.7161, Train MSE: 259.30, Val Loss: 183.5143, Val MSE: 183.43\n",
      "Epoch 31/1000 (MagnitudeL2-50.0%): Train Loss: 264.7111, Train MSE: 263.90, Val Loss: 147.2708, Val MSE: 147.73\n",
      "Epoch 32/1000 (MagnitudeL2-50.0%): Train Loss: 258.2432, Train MSE: 258.85, Val Loss: 129.6497, Val MSE: 129.91\n",
      "Epoch 33/1000 (MagnitudeL2-50.0%): Train Loss: 267.4404, Train MSE: 267.91, Val Loss: 122.9921, Val MSE: 123.09\n",
      "Early stopping triggered after 33 epochs\n",
      "Loaded best model state\n",
      "Results: MSE=243.57, MAE=10.73, MACs=90.45M\n",
      "‚úÖ Model saved to ./models_lstm_nasa/magnitudel2_sparsity_0.5.pth\n",
      "‚úÖ ONNX model saved to ./models_lstm_nasa/magnitudel2_sparsity_0.5.onnx\n",
      "\n",
      "Processing MagnitudeL2 at 70.0% sparsity...\n",
      "‚úÖ Created LSTM with architecture:\n",
      "   LSTM: input_size=14, hidden_size=100, num_layers=2\n",
      "   FC: 100 -> 64 -> 32 -> 1\n",
      "Initial MACs: 90.45M\n",
      "Applying MagnitudeImportance pruning at 70.0% sparsity...\n",
      "Note: Only pruning FC layers, LSTM layers are preserved\n",
      "Final MACs: 90.45M (Reduction: 0.0%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/1000 (MagnitudeL2-70.0%): Train Loss: 272.2970, Train MSE: 273.05, Val Loss: 124.5571, Val MSE: 124.84 (Best)\n",
      "Epoch 2/1000 (MagnitudeL2-70.0%): Train Loss: 267.2741, Train MSE: 267.55, Val Loss: 145.4654, Val MSE: 145.90\n",
      "Epoch 3/1000 (MagnitudeL2-70.0%): Train Loss: 276.6347, Train MSE: 276.30, Val Loss: 130.4253, Val MSE: 130.47\n",
      "Epoch 4/1000 (MagnitudeL2-70.0%): Train Loss: 264.1103, Train MSE: 264.30, Val Loss: 128.7035, Val MSE: 128.80\n",
      "Epoch 5/1000 (MagnitudeL2-70.0%): Train Loss: 269.5297, Train MSE: 269.41, Val Loss: 127.3105, Val MSE: 127.51\n",
      "Epoch 6/1000 (MagnitudeL2-70.0%): Train Loss: 266.1239, Train MSE: 266.25, Val Loss: 125.2467, Val MSE: 125.57\n",
      "Epoch 7/1000 (MagnitudeL2-70.0%): Train Loss: 262.4089, Train MSE: 262.13, Val Loss: 138.9902, Val MSE: 139.02\n",
      "Epoch 8/1000 (MagnitudeL2-70.0%): Train Loss: 266.0411, Train MSE: 265.57, Val Loss: 131.0606, Val MSE: 130.99\n",
      "Epoch 9/1000 (MagnitudeL2-70.0%): Train Loss: 264.6055, Train MSE: 264.81, Val Loss: 124.4658, Val MSE: 124.72 (Best)\n",
      "Epoch 10/1000 (MagnitudeL2-70.0%): Train Loss: 270.7998, Train MSE: 270.89, Val Loss: 136.2801, Val MSE: 136.17\n",
      "Epoch 11/1000 (MagnitudeL2-70.0%): Train Loss: 261.4643, Train MSE: 262.11, Val Loss: 123.5898, Val MSE: 123.79 (Best)\n",
      "Epoch 12/1000 (MagnitudeL2-70.0%): Train Loss: 264.5879, Train MSE: 264.67, Val Loss: 134.5277, Val MSE: 134.91\n",
      "Epoch 13/1000 (MagnitudeL2-70.0%): Train Loss: 272.3195, Train MSE: 273.08, Val Loss: 126.7530, Val MSE: 126.90\n",
      "Epoch 14/1000 (MagnitudeL2-70.0%): Train Loss: 268.4679, Train MSE: 267.45, Val Loss: 127.1649, Val MSE: 127.52\n",
      "Epoch 15/1000 (MagnitudeL2-70.0%): Train Loss: 264.4307, Train MSE: 264.49, Val Loss: 144.2093, Val MSE: 144.43\n",
      "Epoch 16/1000 (MagnitudeL2-70.0%): Train Loss: 259.6457, Train MSE: 259.12, Val Loss: 125.2407, Val MSE: 125.49\n",
      "Epoch 17/1000 (MagnitudeL2-70.0%): Train Loss: 263.1902, Train MSE: 261.74, Val Loss: 125.2366, Val MSE: 125.65\n",
      "Epoch 18/1000 (MagnitudeL2-70.0%): Train Loss: 263.4189, Train MSE: 263.27, Val Loss: 125.4807, Val MSE: 125.64\n",
      "Epoch 19/1000 (MagnitudeL2-70.0%): Train Loss: 257.8178, Train MSE: 257.93, Val Loss: 130.8670, Val MSE: 130.99\n",
      "Epoch 20/1000 (MagnitudeL2-70.0%): Train Loss: 259.4046, Train MSE: 259.27, Val Loss: 143.8907, Val MSE: 144.80\n",
      "Epoch 21/1000 (MagnitudeL2-70.0%): Train Loss: 267.7522, Train MSE: 266.68, Val Loss: 145.2600, Val MSE: 145.55\n",
      "Epoch 22/1000 (MagnitudeL2-70.0%): Train Loss: 260.7558, Train MSE: 260.61, Val Loss: 136.7046, Val MSE: 136.82\n",
      "Epoch 23/1000 (MagnitudeL2-70.0%): Train Loss: 267.4234, Train MSE: 266.45, Val Loss: 135.9928, Val MSE: 136.04\n",
      "Epoch 24/1000 (MagnitudeL2-70.0%): Train Loss: 259.6190, Train MSE: 259.29, Val Loss: 122.0379, Val MSE: 122.28 (Best)\n",
      "Epoch 25/1000 (MagnitudeL2-70.0%): Train Loss: 262.7352, Train MSE: 262.88, Val Loss: 140.8741, Val MSE: 140.92\n",
      "Epoch 26/1000 (MagnitudeL2-70.0%): Train Loss: 265.7181, Train MSE: 265.81, Val Loss: 128.6252, Val MSE: 128.70\n",
      "Epoch 27/1000 (MagnitudeL2-70.0%): Train Loss: 263.9630, Train MSE: 263.56, Val Loss: 123.7741, Val MSE: 123.86\n",
      "Epoch 28/1000 (MagnitudeL2-70.0%): Train Loss: 261.2359, Train MSE: 261.22, Val Loss: 125.3330, Val MSE: 125.64\n",
      "Epoch 29/1000 (MagnitudeL2-70.0%): Train Loss: 262.8001, Train MSE: 262.36, Val Loss: 141.6063, Val MSE: 142.07\n",
      "Epoch 30/1000 (MagnitudeL2-70.0%): Train Loss: 264.3882, Train MSE: 263.86, Val Loss: 139.2172, Val MSE: 139.14\n",
      "Epoch 31/1000 (MagnitudeL2-70.0%): Train Loss: 264.5663, Train MSE: 263.92, Val Loss: 124.1741, Val MSE: 124.30\n",
      "Epoch 32/1000 (MagnitudeL2-70.0%): Train Loss: 261.9588, Train MSE: 261.82, Val Loss: 132.4747, Val MSE: 132.74\n",
      "Epoch 33/1000 (MagnitudeL2-70.0%): Train Loss: 264.2633, Train MSE: 264.55, Val Loss: 124.1871, Val MSE: 124.28\n",
      "Epoch 34/1000 (MagnitudeL2-70.0%): Train Loss: 266.2033, Train MSE: 266.60, Val Loss: 125.1691, Val MSE: 125.17\n",
      "Epoch 35/1000 (MagnitudeL2-70.0%): Train Loss: 257.3224, Train MSE: 257.99, Val Loss: 125.9686, Val MSE: 126.27\n",
      "Epoch 36/1000 (MagnitudeL2-70.0%): Train Loss: 265.5266, Train MSE: 265.40, Val Loss: 127.3751, Val MSE: 127.53\n",
      "Epoch 37/1000 (MagnitudeL2-70.0%): Train Loss: 254.3380, Train MSE: 254.83, Val Loss: 137.2733, Val MSE: 137.55\n",
      "Epoch 38/1000 (MagnitudeL2-70.0%): Train Loss: 262.9435, Train MSE: 263.28, Val Loss: 122.7626, Val MSE: 122.98\n",
      "Epoch 39/1000 (MagnitudeL2-70.0%): Train Loss: 263.1617, Train MSE: 263.43, Val Loss: 125.4884, Val MSE: 125.60\n",
      "Epoch 40/1000 (MagnitudeL2-70.0%): Train Loss: 262.9679, Train MSE: 263.30, Val Loss: 129.3324, Val MSE: 129.17\n",
      "Epoch 41/1000 (MagnitudeL2-70.0%): Train Loss: 263.6470, Train MSE: 263.93, Val Loss: 135.6194, Val MSE: 135.67\n",
      "Epoch 42/1000 (MagnitudeL2-70.0%): Train Loss: 261.6364, Train MSE: 262.11, Val Loss: 137.2235, Val MSE: 137.61\n",
      "Epoch 43/1000 (MagnitudeL2-70.0%): Train Loss: 262.2127, Train MSE: 261.99, Val Loss: 123.7556, Val MSE: 123.79\n",
      "Epoch 44/1000 (MagnitudeL2-70.0%): Train Loss: 277.1263, Train MSE: 276.77, Val Loss: 123.9001, Val MSE: 124.21\n",
      "Early stopping triggered after 44 epochs\n",
      "Loaded best model state\n",
      "Results: MSE=244.14, MAE=10.89, MACs=90.45M\n",
      "‚úÖ Model saved to ./models_lstm_nasa/magnitudel2_sparsity_0.7.pth\n",
      "‚úÖ ONNX model saved to ./models_lstm_nasa/magnitudel2_sparsity_0.7.onnx\n",
      "\n",
      "--- Strategy: Random ---\n",
      "\n",
      "Processing Random at 20.0% sparsity...\n",
      "‚úÖ Created LSTM with architecture:\n",
      "   LSTM: input_size=14, hidden_size=100, num_layers=2\n",
      "   FC: 100 -> 64 -> 32 -> 1\n",
      "Initial MACs: 90.45M\n",
      "Applying RandomImportance pruning at 20.0% sparsity...\n",
      "Note: Only pruning FC layers, LSTM layers are preserved\n",
      "Final MACs: 90.45M (Reduction: 0.0%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/1000 (Random-20.0%): Train Loss: 285.4551, Train MSE: 285.62, Val Loss: 131.7617, Val MSE: 132.25 (Best)\n",
      "Epoch 2/1000 (Random-20.0%): Train Loss: 270.4961, Train MSE: 271.33, Val Loss: 124.4057, Val MSE: 124.50 (Best)\n",
      "Epoch 3/1000 (Random-20.0%): Train Loss: 277.5693, Train MSE: 278.12, Val Loss: 127.9454, Val MSE: 128.15\n",
      "Epoch 4/1000 (Random-20.0%): Train Loss: 285.1198, Train MSE: 284.34, Val Loss: 147.1094, Val MSE: 147.48\n",
      "Epoch 5/1000 (Random-20.0%): Train Loss: 283.1068, Train MSE: 283.38, Val Loss: 128.5438, Val MSE: 128.77\n",
      "Epoch 6/1000 (Random-20.0%): Train Loss: 284.9204, Train MSE: 285.17, Val Loss: 129.3252, Val MSE: 129.54\n",
      "Epoch 7/1000 (Random-20.0%): Train Loss: 275.6390, Train MSE: 276.06, Val Loss: 158.4926, Val MSE: 158.95\n",
      "Epoch 8/1000 (Random-20.0%): Train Loss: 282.8479, Train MSE: 283.31, Val Loss: 124.9755, Val MSE: 125.31\n",
      "Epoch 9/1000 (Random-20.0%): Train Loss: 273.4325, Train MSE: 274.20, Val Loss: 126.6812, Val MSE: 126.92\n",
      "Epoch 10/1000 (Random-20.0%): Train Loss: 276.2620, Train MSE: 276.16, Val Loss: 127.1118, Val MSE: 127.55\n",
      "Epoch 11/1000 (Random-20.0%): Train Loss: 272.6460, Train MSE: 272.71, Val Loss: 137.0258, Val MSE: 137.54\n",
      "Epoch 12/1000 (Random-20.0%): Train Loss: 285.2384, Train MSE: 285.52, Val Loss: 165.9246, Val MSE: 166.33\n",
      "Epoch 13/1000 (Random-20.0%): Train Loss: 273.2770, Train MSE: 274.01, Val Loss: 133.5812, Val MSE: 133.48\n",
      "Epoch 14/1000 (Random-20.0%): Train Loss: 276.7976, Train MSE: 277.21, Val Loss: 135.1905, Val MSE: 135.21\n",
      "Epoch 15/1000 (Random-20.0%): Train Loss: 278.9093, Train MSE: 279.65, Val Loss: 123.6825, Val MSE: 124.04 (Best)\n",
      "Epoch 16/1000 (Random-20.0%): Train Loss: 275.8891, Train MSE: 275.23, Val Loss: 123.0351, Val MSE: 123.41 (Best)\n",
      "Epoch 17/1000 (Random-20.0%): Train Loss: 278.5737, Train MSE: 278.16, Val Loss: 139.9261, Val MSE: 139.94\n",
      "Epoch 18/1000 (Random-20.0%): Train Loss: 278.1338, Train MSE: 276.40, Val Loss: 128.3794, Val MSE: 128.89\n",
      "Epoch 19/1000 (Random-20.0%): Train Loss: 278.9780, Train MSE: 279.41, Val Loss: 126.4474, Val MSE: 126.86\n",
      "Epoch 20/1000 (Random-20.0%): Train Loss: 274.5222, Train MSE: 274.87, Val Loss: 140.9289, Val MSE: 141.39\n",
      "Epoch 21/1000 (Random-20.0%): Train Loss: 271.6602, Train MSE: 271.63, Val Loss: 144.2354, Val MSE: 144.33\n",
      "Epoch 22/1000 (Random-20.0%): Train Loss: 276.3716, Train MSE: 276.50, Val Loss: 147.5122, Val MSE: 147.88\n",
      "Epoch 23/1000 (Random-20.0%): Train Loss: 272.1592, Train MSE: 271.53, Val Loss: 134.5537, Val MSE: 134.72\n",
      "Epoch 24/1000 (Random-20.0%): Train Loss: 283.6121, Train MSE: 282.94, Val Loss: 122.6455, Val MSE: 122.89 (Best)\n",
      "Epoch 25/1000 (Random-20.0%): Train Loss: 277.1509, Train MSE: 277.41, Val Loss: 122.4455, Val MSE: 122.64 (Best)\n",
      "Epoch 26/1000 (Random-20.0%): Train Loss: 267.5599, Train MSE: 267.02, Val Loss: 123.2801, Val MSE: 123.47\n",
      "Epoch 27/1000 (Random-20.0%): Train Loss: 270.4294, Train MSE: 270.25, Val Loss: 161.0428, Val MSE: 161.47\n",
      "Epoch 28/1000 (Random-20.0%): Train Loss: 277.0928, Train MSE: 277.13, Val Loss: 150.9006, Val MSE: 150.82\n",
      "Epoch 29/1000 (Random-20.0%): Train Loss: 272.2404, Train MSE: 272.75, Val Loss: 126.0936, Val MSE: 126.52\n",
      "Epoch 30/1000 (Random-20.0%): Train Loss: 277.1214, Train MSE: 277.66, Val Loss: 152.5168, Val MSE: 152.65\n",
      "Epoch 31/1000 (Random-20.0%): Train Loss: 277.0989, Train MSE: 277.43, Val Loss: 139.3514, Val MSE: 139.55\n",
      "Epoch 32/1000 (Random-20.0%): Train Loss: 276.3897, Train MSE: 276.28, Val Loss: 125.4559, Val MSE: 126.05\n",
      "Epoch 33/1000 (Random-20.0%): Train Loss: 271.5884, Train MSE: 271.79, Val Loss: 131.4892, Val MSE: 131.59\n",
      "Epoch 34/1000 (Random-20.0%): Train Loss: 271.6396, Train MSE: 271.47, Val Loss: 125.0491, Val MSE: 125.51\n",
      "Epoch 35/1000 (Random-20.0%): Train Loss: 267.3955, Train MSE: 267.56, Val Loss: 143.1885, Val MSE: 143.21\n",
      "Epoch 36/1000 (Random-20.0%): Train Loss: 268.9600, Train MSE: 269.41, Val Loss: 129.6695, Val MSE: 129.96\n",
      "Epoch 37/1000 (Random-20.0%): Train Loss: 280.5376, Train MSE: 280.18, Val Loss: 124.0064, Val MSE: 124.21\n",
      "Epoch 38/1000 (Random-20.0%): Train Loss: 275.7333, Train MSE: 275.48, Val Loss: 125.7295, Val MSE: 125.90\n",
      "Epoch 39/1000 (Random-20.0%): Train Loss: 284.1422, Train MSE: 284.65, Val Loss: 122.1012, Val MSE: 122.45 (Best)\n",
      "Epoch 40/1000 (Random-20.0%): Train Loss: 273.5055, Train MSE: 272.66, Val Loss: 131.2814, Val MSE: 131.34\n",
      "Epoch 41/1000 (Random-20.0%): Train Loss: 277.3228, Train MSE: 275.81, Val Loss: 123.7644, Val MSE: 124.03\n",
      "Epoch 42/1000 (Random-20.0%): Train Loss: 280.0419, Train MSE: 279.98, Val Loss: 131.0711, Val MSE: 131.35\n",
      "Epoch 43/1000 (Random-20.0%): Train Loss: 274.0887, Train MSE: 274.82, Val Loss: 123.7387, Val MSE: 124.20\n",
      "Epoch 44/1000 (Random-20.0%): Train Loss: 265.8618, Train MSE: 265.23, Val Loss: 121.4868, Val MSE: 121.73 (Best)\n",
      "Epoch 45/1000 (Random-20.0%): Train Loss: 280.7097, Train MSE: 280.90, Val Loss: 123.3264, Val MSE: 123.56\n",
      "Epoch 46/1000 (Random-20.0%): Train Loss: 269.6902, Train MSE: 270.47, Val Loss: 125.8777, Val MSE: 126.06\n",
      "Epoch 47/1000 (Random-20.0%): Train Loss: 275.2996, Train MSE: 275.22, Val Loss: 121.1986, Val MSE: 121.39 (Best)\n",
      "Epoch 48/1000 (Random-20.0%): Train Loss: 278.2125, Train MSE: 278.81, Val Loss: 126.1172, Val MSE: 126.26\n",
      "Epoch 49/1000 (Random-20.0%): Train Loss: 277.9186, Train MSE: 277.99, Val Loss: 127.9899, Val MSE: 128.08\n",
      "Epoch 50/1000 (Random-20.0%): Train Loss: 271.6212, Train MSE: 271.73, Val Loss: 128.2558, Val MSE: 128.36\n",
      "Epoch 51/1000 (Random-20.0%): Train Loss: 268.8725, Train MSE: 269.55, Val Loss: 123.1512, Val MSE: 123.17\n",
      "Epoch 52/1000 (Random-20.0%): Train Loss: 274.1757, Train MSE: 274.68, Val Loss: 137.4323, Val MSE: 137.69\n",
      "Epoch 53/1000 (Random-20.0%): Train Loss: 266.7213, Train MSE: 266.85, Val Loss: 121.5351, Val MSE: 121.65\n",
      "Epoch 54/1000 (Random-20.0%): Train Loss: 277.4594, Train MSE: 277.80, Val Loss: 127.6207, Val MSE: 128.03\n",
      "Epoch 55/1000 (Random-20.0%): Train Loss: 268.6708, Train MSE: 268.89, Val Loss: 124.0637, Val MSE: 124.19\n",
      "Epoch 56/1000 (Random-20.0%): Train Loss: 268.3638, Train MSE: 267.82, Val Loss: 129.5785, Val MSE: 129.71\n",
      "Epoch 57/1000 (Random-20.0%): Train Loss: 276.6534, Train MSE: 276.38, Val Loss: 127.1317, Val MSE: 127.34\n",
      "Epoch 58/1000 (Random-20.0%): Train Loss: 278.3176, Train MSE: 278.25, Val Loss: 133.3468, Val MSE: 133.45\n",
      "Epoch 59/1000 (Random-20.0%): Train Loss: 271.7870, Train MSE: 272.21, Val Loss: 124.6281, Val MSE: 124.74\n",
      "Epoch 60/1000 (Random-20.0%): Train Loss: 271.2014, Train MSE: 271.10, Val Loss: 122.0785, Val MSE: 122.52\n",
      "Epoch 61/1000 (Random-20.0%): Train Loss: 270.8028, Train MSE: 269.82, Val Loss: 124.2385, Val MSE: 124.48\n",
      "Epoch 62/1000 (Random-20.0%): Train Loss: 272.6028, Train MSE: 271.63, Val Loss: 127.7080, Val MSE: 128.00\n",
      "Epoch 63/1000 (Random-20.0%): Train Loss: 264.8888, Train MSE: 264.48, Val Loss: 118.6889, Val MSE: 119.02 (Best)\n",
      "Epoch 64/1000 (Random-20.0%): Train Loss: 270.3216, Train MSE: 269.95, Val Loss: 133.3701, Val MSE: 133.41\n",
      "Epoch 65/1000 (Random-20.0%): Train Loss: 269.7588, Train MSE: 269.83, Val Loss: 121.1988, Val MSE: 121.43\n",
      "Epoch 66/1000 (Random-20.0%): Train Loss: 269.6273, Train MSE: 269.31, Val Loss: 124.0354, Val MSE: 124.20\n",
      "Epoch 67/1000 (Random-20.0%): Train Loss: 278.3269, Train MSE: 278.84, Val Loss: 124.8491, Val MSE: 125.04\n",
      "Epoch 68/1000 (Random-20.0%): Train Loss: 263.6306, Train MSE: 263.67, Val Loss: 125.1675, Val MSE: 125.03\n",
      "Epoch 69/1000 (Random-20.0%): Train Loss: 268.5448, Train MSE: 269.24, Val Loss: 125.2280, Val MSE: 125.36\n",
      "Epoch 70/1000 (Random-20.0%): Train Loss: 270.7000, Train MSE: 271.19, Val Loss: 166.9182, Val MSE: 166.95\n",
      "Epoch 71/1000 (Random-20.0%): Train Loss: 274.1108, Train MSE: 274.70, Val Loss: 121.2773, Val MSE: 121.52\n",
      "Epoch 72/1000 (Random-20.0%): Train Loss: 271.6064, Train MSE: 271.56, Val Loss: 126.7687, Val MSE: 126.74\n",
      "Epoch 73/1000 (Random-20.0%): Train Loss: 273.0411, Train MSE: 273.52, Val Loss: 123.4958, Val MSE: 123.70\n",
      "Epoch 74/1000 (Random-20.0%): Train Loss: 269.4168, Train MSE: 268.88, Val Loss: 174.2820, Val MSE: 174.47\n",
      "Epoch 75/1000 (Random-20.0%): Train Loss: 262.6090, Train MSE: 263.15, Val Loss: 128.3519, Val MSE: 128.46\n",
      "Epoch 76/1000 (Random-20.0%): Train Loss: 267.3901, Train MSE: 267.68, Val Loss: 147.4609, Val MSE: 147.70\n",
      "Epoch 77/1000 (Random-20.0%): Train Loss: 272.6582, Train MSE: 272.75, Val Loss: 124.6350, Val MSE: 124.84\n",
      "Epoch 78/1000 (Random-20.0%): Train Loss: 275.3783, Train MSE: 275.79, Val Loss: 124.3465, Val MSE: 124.69\n",
      "Epoch 79/1000 (Random-20.0%): Train Loss: 267.1425, Train MSE: 266.66, Val Loss: 119.7207, Val MSE: 119.93\n",
      "Epoch 80/1000 (Random-20.0%): Train Loss: 273.2507, Train MSE: 273.47, Val Loss: 138.5961, Val MSE: 138.64\n",
      "Epoch 81/1000 (Random-20.0%): Train Loss: 268.9538, Train MSE: 269.31, Val Loss: 130.1596, Val MSE: 130.40\n",
      "Epoch 82/1000 (Random-20.0%): Train Loss: 271.8909, Train MSE: 271.80, Val Loss: 124.8631, Val MSE: 125.13\n",
      "Epoch 83/1000 (Random-20.0%): Train Loss: 274.7181, Train MSE: 274.15, Val Loss: 134.5305, Val MSE: 135.16\n",
      "Early stopping triggered after 83 epochs\n",
      "Loaded best model state\n",
      "Results: MSE=252.49, MAE=10.96, MACs=90.45M\n",
      "‚úÖ Model saved to ./models_lstm_nasa/random_sparsity_0.2.pth\n",
      "‚úÖ ONNX model saved to ./models_lstm_nasa/random_sparsity_0.2.onnx\n",
      "\n",
      "Processing Random at 50.0% sparsity...\n",
      "‚úÖ Created LSTM with architecture:\n",
      "   LSTM: input_size=14, hidden_size=100, num_layers=2\n",
      "   FC: 100 -> 64 -> 32 -> 1\n",
      "Initial MACs: 90.45M\n",
      "Applying RandomImportance pruning at 50.0% sparsity...\n",
      "Note: Only pruning FC layers, LSTM layers are preserved\n",
      "Final MACs: 90.45M (Reduction: 0.0%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/1000 (Random-50.0%): Train Loss: 325.4515, Train MSE: 325.69, Val Loss: 134.1825, Val MSE: 134.34 (Best)\n",
      "Epoch 2/1000 (Random-50.0%): Train Loss: 280.8894, Train MSE: 281.33, Val Loss: 129.9547, Val MSE: 130.06 (Best)\n",
      "Epoch 3/1000 (Random-50.0%): Train Loss: 282.3565, Train MSE: 282.42, Val Loss: 133.2169, Val MSE: 133.28\n",
      "Epoch 4/1000 (Random-50.0%): Train Loss: 288.2746, Train MSE: 288.18, Val Loss: 151.0989, Val MSE: 151.34\n",
      "Epoch 5/1000 (Random-50.0%): Train Loss: 287.4053, Train MSE: 287.10, Val Loss: 134.4047, Val MSE: 134.70\n",
      "Epoch 6/1000 (Random-50.0%): Train Loss: 277.6313, Train MSE: 277.84, Val Loss: 126.6535, Val MSE: 126.86 (Best)\n",
      "Epoch 7/1000 (Random-50.0%): Train Loss: 288.3185, Train MSE: 288.58, Val Loss: 180.6376, Val MSE: 180.93\n",
      "Epoch 8/1000 (Random-50.0%): Train Loss: 283.1671, Train MSE: 283.49, Val Loss: 126.1578, Val MSE: 126.22 (Best)\n",
      "Epoch 9/1000 (Random-50.0%): Train Loss: 285.3497, Train MSE: 285.81, Val Loss: 130.0141, Val MSE: 130.14\n",
      "Epoch 10/1000 (Random-50.0%): Train Loss: 287.9458, Train MSE: 287.69, Val Loss: 128.8024, Val MSE: 129.13\n",
      "Epoch 11/1000 (Random-50.0%): Train Loss: 287.7057, Train MSE: 286.45, Val Loss: 151.2773, Val MSE: 151.75\n",
      "Epoch 12/1000 (Random-50.0%): Train Loss: 285.2179, Train MSE: 285.88, Val Loss: 158.1477, Val MSE: 158.41\n",
      "Epoch 13/1000 (Random-50.0%): Train Loss: 285.4067, Train MSE: 284.55, Val Loss: 123.8582, Val MSE: 124.13 (Best)\n",
      "Epoch 14/1000 (Random-50.0%): Train Loss: 286.0715, Train MSE: 285.90, Val Loss: 128.0996, Val MSE: 128.04\n",
      "Epoch 15/1000 (Random-50.0%): Train Loss: 277.5873, Train MSE: 276.94, Val Loss: 126.6958, Val MSE: 126.73\n",
      "Epoch 16/1000 (Random-50.0%): Train Loss: 283.0121, Train MSE: 283.91, Val Loss: 127.7021, Val MSE: 128.19\n",
      "Epoch 17/1000 (Random-50.0%): Train Loss: 286.0114, Train MSE: 285.84, Val Loss: 145.4815, Val MSE: 146.16\n",
      "Epoch 18/1000 (Random-50.0%): Train Loss: 268.9952, Train MSE: 268.88, Val Loss: 144.2345, Val MSE: 144.42\n",
      "Epoch 19/1000 (Random-50.0%): Train Loss: 288.6570, Train MSE: 289.45, Val Loss: 151.4125, Val MSE: 151.45\n",
      "Epoch 20/1000 (Random-50.0%): Train Loss: 281.7816, Train MSE: 281.42, Val Loss: 127.4700, Val MSE: 127.91\n",
      "Epoch 21/1000 (Random-50.0%): Train Loss: 277.7855, Train MSE: 277.67, Val Loss: 130.6435, Val MSE: 130.94\n",
      "Epoch 22/1000 (Random-50.0%): Train Loss: 282.3519, Train MSE: 283.08, Val Loss: 129.3369, Val MSE: 129.60\n",
      "Epoch 23/1000 (Random-50.0%): Train Loss: 282.6731, Train MSE: 282.49, Val Loss: 126.1387, Val MSE: 126.45\n",
      "Epoch 24/1000 (Random-50.0%): Train Loss: 281.9507, Train MSE: 282.02, Val Loss: 158.9341, Val MSE: 159.16\n",
      "Epoch 25/1000 (Random-50.0%): Train Loss: 282.8839, Train MSE: 282.36, Val Loss: 126.3477, Val MSE: 126.72\n",
      "Epoch 26/1000 (Random-50.0%): Train Loss: 279.5759, Train MSE: 280.30, Val Loss: 140.3083, Val MSE: 140.00\n",
      "Epoch 27/1000 (Random-50.0%): Train Loss: 282.0274, Train MSE: 282.67, Val Loss: 127.6361, Val MSE: 127.74\n",
      "Epoch 28/1000 (Random-50.0%): Train Loss: 280.1015, Train MSE: 280.54, Val Loss: 123.1582, Val MSE: 123.34 (Best)\n",
      "Epoch 29/1000 (Random-50.0%): Train Loss: 277.5530, Train MSE: 277.25, Val Loss: 127.9451, Val MSE: 128.28\n",
      "Epoch 30/1000 (Random-50.0%): Train Loss: 283.9491, Train MSE: 283.41, Val Loss: 141.8106, Val MSE: 142.23\n",
      "Epoch 31/1000 (Random-50.0%): Train Loss: 282.1982, Train MSE: 281.92, Val Loss: 128.1728, Val MSE: 128.34\n",
      "Epoch 32/1000 (Random-50.0%): Train Loss: 281.5661, Train MSE: 282.00, Val Loss: 125.8155, Val MSE: 125.82\n",
      "Epoch 33/1000 (Random-50.0%): Train Loss: 272.7829, Train MSE: 271.54, Val Loss: 124.8346, Val MSE: 125.05\n",
      "Epoch 34/1000 (Random-50.0%): Train Loss: 280.0434, Train MSE: 280.10, Val Loss: 127.5150, Val MSE: 127.81\n",
      "Epoch 35/1000 (Random-50.0%): Train Loss: 274.1861, Train MSE: 274.90, Val Loss: 126.4130, Val MSE: 126.62\n",
      "Epoch 36/1000 (Random-50.0%): Train Loss: 277.6797, Train MSE: 277.80, Val Loss: 126.2439, Val MSE: 126.39\n",
      "Epoch 37/1000 (Random-50.0%): Train Loss: 281.4553, Train MSE: 281.55, Val Loss: 128.2624, Val MSE: 128.28\n",
      "Epoch 38/1000 (Random-50.0%): Train Loss: 278.1884, Train MSE: 277.68, Val Loss: 123.1730, Val MSE: 123.43\n",
      "Epoch 39/1000 (Random-50.0%): Train Loss: 280.8498, Train MSE: 280.62, Val Loss: 126.7069, Val MSE: 126.95\n",
      "Epoch 40/1000 (Random-50.0%): Train Loss: 283.5267, Train MSE: 283.55, Val Loss: 129.2479, Val MSE: 129.66\n",
      "Epoch 41/1000 (Random-50.0%): Train Loss: 277.4408, Train MSE: 278.07, Val Loss: 128.5606, Val MSE: 128.57\n",
      "Epoch 42/1000 (Random-50.0%): Train Loss: 284.6736, Train MSE: 283.95, Val Loss: 124.2289, Val MSE: 124.43\n",
      "Epoch 43/1000 (Random-50.0%): Train Loss: 277.6204, Train MSE: 277.31, Val Loss: 133.6905, Val MSE: 133.71\n",
      "Epoch 44/1000 (Random-50.0%): Train Loss: 273.4302, Train MSE: 273.85, Val Loss: 128.7000, Val MSE: 128.69\n",
      "Epoch 45/1000 (Random-50.0%): Train Loss: 285.1024, Train MSE: 284.82, Val Loss: 126.5156, Val MSE: 126.72\n",
      "Epoch 46/1000 (Random-50.0%): Train Loss: 273.0417, Train MSE: 273.87, Val Loss: 137.6830, Val MSE: 137.80\n",
      "Epoch 47/1000 (Random-50.0%): Train Loss: 277.7099, Train MSE: 277.10, Val Loss: 125.7740, Val MSE: 126.03\n",
      "Epoch 48/1000 (Random-50.0%): Train Loss: 271.7322, Train MSE: 270.58, Val Loss: 123.2751, Val MSE: 123.56\n",
      "Early stopping triggered after 48 epochs\n",
      "Loaded best model state\n",
      "Results: MSE=246.86, MAE=10.91, MACs=90.45M\n",
      "‚úÖ Model saved to ./models_lstm_nasa/random_sparsity_0.5.pth\n",
      "‚úÖ ONNX model saved to ./models_lstm_nasa/random_sparsity_0.5.onnx\n",
      "\n",
      "Processing Random at 70.0% sparsity...\n",
      "‚úÖ Created LSTM with architecture:\n",
      "   LSTM: input_size=14, hidden_size=100, num_layers=2\n",
      "   FC: 100 -> 64 -> 32 -> 1\n",
      "Initial MACs: 90.45M\n",
      "Applying RandomImportance pruning at 70.0% sparsity...\n",
      "Note: Only pruning FC layers, LSTM layers are preserved\n",
      "Final MACs: 90.45M (Reduction: 0.0%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/1000 (Random-70.0%): Train Loss: 360.8740, Train MSE: 361.67, Val Loss: 140.2229, Val MSE: 140.44 (Best)\n",
      "Epoch 2/1000 (Random-70.0%): Train Loss: 301.7789, Train MSE: 301.77, Val Loss: 129.9706, Val MSE: 130.22 (Best)\n",
      "Epoch 3/1000 (Random-70.0%): Train Loss: 292.8752, Train MSE: 291.81, Val Loss: 147.8683, Val MSE: 148.07\n",
      "Epoch 4/1000 (Random-70.0%): Train Loss: 295.7426, Train MSE: 296.14, Val Loss: 127.8969, Val MSE: 128.16 (Best)\n",
      "Epoch 5/1000 (Random-70.0%): Train Loss: 288.1395, Train MSE: 287.95, Val Loss: 127.2746, Val MSE: 127.45 (Best)\n",
      "Epoch 6/1000 (Random-70.0%): Train Loss: 286.5136, Train MSE: 286.67, Val Loss: 187.2950, Val MSE: 187.42\n",
      "Epoch 7/1000 (Random-70.0%): Train Loss: 295.3327, Train MSE: 295.49, Val Loss: 140.0404, Val MSE: 140.13\n",
      "Epoch 8/1000 (Random-70.0%): Train Loss: 294.1614, Train MSE: 294.45, Val Loss: 134.1794, Val MSE: 134.18\n",
      "Epoch 9/1000 (Random-70.0%): Train Loss: 298.3054, Train MSE: 298.68, Val Loss: 150.8472, Val MSE: 151.30\n",
      "Epoch 10/1000 (Random-70.0%): Train Loss: 290.5624, Train MSE: 290.89, Val Loss: 140.5999, Val MSE: 140.54\n",
      "Epoch 11/1000 (Random-70.0%): Train Loss: 284.6272, Train MSE: 284.64, Val Loss: 127.5036, Val MSE: 127.43\n",
      "Epoch 12/1000 (Random-70.0%): Train Loss: 297.0521, Train MSE: 297.10, Val Loss: 141.9107, Val MSE: 142.08\n",
      "Epoch 13/1000 (Random-70.0%): Train Loss: 292.4530, Train MSE: 293.04, Val Loss: 125.2482, Val MSE: 125.49 (Best)\n",
      "Epoch 14/1000 (Random-70.0%): Train Loss: 295.6901, Train MSE: 295.61, Val Loss: 125.6490, Val MSE: 125.82\n",
      "Epoch 15/1000 (Random-70.0%): Train Loss: 294.4930, Train MSE: 295.03, Val Loss: 126.9006, Val MSE: 127.04\n",
      "Epoch 16/1000 (Random-70.0%): Train Loss: 295.5272, Train MSE: 294.71, Val Loss: 126.1579, Val MSE: 126.36\n",
      "Epoch 17/1000 (Random-70.0%): Train Loss: 292.8665, Train MSE: 293.36, Val Loss: 127.9765, Val MSE: 128.23\n",
      "Epoch 18/1000 (Random-70.0%): Train Loss: 287.4822, Train MSE: 287.26, Val Loss: 144.5664, Val MSE: 145.05\n",
      "Epoch 19/1000 (Random-70.0%): Train Loss: 290.1281, Train MSE: 289.71, Val Loss: 123.0617, Val MSE: 123.39 (Best)\n",
      "Epoch 20/1000 (Random-70.0%): Train Loss: 285.5956, Train MSE: 286.13, Val Loss: 135.4486, Val MSE: 135.70\n",
      "Epoch 21/1000 (Random-70.0%): Train Loss: 287.7038, Train MSE: 287.37, Val Loss: 139.4884, Val MSE: 139.74\n",
      "Epoch 22/1000 (Random-70.0%): Train Loss: 285.5502, Train MSE: 285.00, Val Loss: 130.9954, Val MSE: 131.46\n",
      "Epoch 23/1000 (Random-70.0%): Train Loss: 293.6936, Train MSE: 293.28, Val Loss: 127.6012, Val MSE: 127.81\n",
      "Epoch 24/1000 (Random-70.0%): Train Loss: 286.8357, Train MSE: 286.95, Val Loss: 137.0401, Val MSE: 136.86\n",
      "Epoch 25/1000 (Random-70.0%): Train Loss: 291.4360, Train MSE: 291.49, Val Loss: 127.0508, Val MSE: 127.57\n",
      "Epoch 26/1000 (Random-70.0%): Train Loss: 286.6940, Train MSE: 287.00, Val Loss: 137.0903, Val MSE: 137.29\n",
      "Epoch 27/1000 (Random-70.0%): Train Loss: 291.3439, Train MSE: 292.43, Val Loss: 131.4419, Val MSE: 131.30\n",
      "Epoch 28/1000 (Random-70.0%): Train Loss: 279.5288, Train MSE: 279.49, Val Loss: 131.3707, Val MSE: 131.41\n",
      "Epoch 29/1000 (Random-70.0%): Train Loss: 284.8662, Train MSE: 285.01, Val Loss: 154.5717, Val MSE: 154.60\n",
      "Epoch 30/1000 (Random-70.0%): Train Loss: 287.5476, Train MSE: 287.99, Val Loss: 128.5406, Val MSE: 128.53\n",
      "Epoch 31/1000 (Random-70.0%): Train Loss: 296.1731, Train MSE: 296.67, Val Loss: 125.0593, Val MSE: 125.36\n",
      "Epoch 32/1000 (Random-70.0%): Train Loss: 284.9872, Train MSE: 285.08, Val Loss: 125.2084, Val MSE: 125.52\n",
      "Epoch 33/1000 (Random-70.0%): Train Loss: 286.1543, Train MSE: 285.15, Val Loss: 132.4637, Val MSE: 132.64\n",
      "Epoch 34/1000 (Random-70.0%): Train Loss: 286.7079, Train MSE: 287.07, Val Loss: 155.6307, Val MSE: 156.15\n",
      "Epoch 35/1000 (Random-70.0%): Train Loss: 286.7776, Train MSE: 287.06, Val Loss: 125.0506, Val MSE: 125.37\n",
      "Epoch 36/1000 (Random-70.0%): Train Loss: 302.4638, Train MSE: 302.71, Val Loss: 124.6648, Val MSE: 124.69\n",
      "Epoch 37/1000 (Random-70.0%): Train Loss: 283.5206, Train MSE: 283.45, Val Loss: 127.7157, Val MSE: 127.90\n",
      "Epoch 38/1000 (Random-70.0%): Train Loss: 294.2231, Train MSE: 292.95, Val Loss: 132.6134, Val MSE: 132.57\n",
      "Epoch 39/1000 (Random-70.0%): Train Loss: 292.1648, Train MSE: 292.03, Val Loss: 145.4927, Val MSE: 145.72\n",
      "Early stopping triggered after 39 epochs\n",
      "Loaded best model state\n",
      "Results: MSE=245.09, MAE=10.84, MACs=90.45M\n",
      "‚úÖ Model saved to ./models_lstm_nasa/random_sparsity_0.7.pth\n",
      "‚úÖ ONNX model saved to ./models_lstm_nasa/random_sparsity_0.7.onnx\n",
      "\n",
      "Saving results...\n",
      "‚úÖ Complete results saved to ./results_lstm_nasa/complete_results.json\n",
      "‚úÖ Summary results saved to ./results_lstm_nasa/summary_results.csv\n",
      "Creating plots...\n",
      "‚úÖ MSE plot saved to ./results_lstm_nasa/mse_vs_sparsity.png\n",
      "‚úÖ Efficiency frontier plot saved to ./results_lstm_nasa/efficiency_frontier.png\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENTAL RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Baseline Performance:\n",
      "  MSE: 240.74\n",
      "  MAE: 10.86\n",
      "  MACs: 90.45M\n",
      "  Parameters: 0.14M\n",
      "  Model Size: 0.52MB\n",
      "\n",
      "Strategy Comparison at 50% Sparsity (FC Layers Only):\n",
      "   MagnitudeL2: MSE=243.57 (+2.83,  +1.2% increase)\n",
      "        Random: MSE=246.86 (+6.12,  +2.5% increase)\n",
      "\n",
      "Complete Results Table:\n",
      "------------------------------------------------------------------------------------------\n",
      "Strategy     Sparsity MSE      MAE      MACs(M)  Params(M) Size(MB)\n",
      "------------------------------------------------------------------------------------------\n",
      "MagnitudeL2       0%  240.74   10.86   90.45     0.14    0.52\n",
      "MagnitudeL2      20%  259.91   10.91   90.45     0.14    0.52\n",
      "MagnitudeL2      50%  243.57   10.73   90.45     0.13    0.51\n",
      "MagnitudeL2      70%  244.14   10.89   90.45     0.13    0.51\n",
      "Random            0%  240.74   10.86   90.45     0.14    0.52\n",
      "Random           20%  252.49   10.96   90.45     0.14    0.52\n",
      "Random           50%  246.86   10.91   90.45     0.13    0.51\n",
      "Random           70%  245.09   10.84   90.45     0.13    0.51\n",
      "\n",
      "Note: Sparsity is applied only to FC layers. LSTM layers remain unchanged.\n",
      "\n",
      "üéâ All experiments completed!\n",
      "üìÅ Results saved to: /home/muis/thesis/github-repo/master-thesis/timeseries/lstm/results_lstm_nasa\n",
      "üìÅ Models saved to: /home/muis/thesis/github-repo/master-thesis/timeseries/lstm/models_lstm_nasa\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# v2",
   "id": "77b2fdc9c99529cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T20:32:57.940080Z",
     "start_time": "2025-06-04T20:26:36.053001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_pruning as tp\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_BASE_NAME = \"lstm_nasa\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# NASA Dataset preprocessing functions - IMPROVED for better MSE\n",
    "column_names = ['unit_number', 'time_in_cycles'] + [f'op_setting_{i}' for i in range(1, 4)] + [f'sensor_{i}' for i in range(1, 24)]\n",
    "\n",
    "def load_dataframe(file_path: str) -> pd.DataFrame | None:\n",
    "    \"\"\"Loads a single CMaps data file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=' ', header=None, names=column_names)\n",
    "        df.dropna(axis=1, how='all', inplace=True)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def clean_data_improved(df: pd.DataFrame) -> list:\n",
    "    \"\"\"IMPROVED: More aggressive feature selection for better MSE.\"\"\"\n",
    "    if df is None:\n",
    "        return []\n",
    "\n",
    "    # More aggressive std threshold - remove more noise\n",
    "    cols_to_check = [col for col in df.columns if 'sensor' in col or 'op_setting' in col]\n",
    "    low_std_cols = [col for col in cols_to_check if df[col].std() < 0.01]  # Reduced from 0.02\n",
    "\n",
    "    print(f\"Columns with std < 0.01 (removing for better MSE): {low_std_cols}\")\n",
    "    return low_std_cols\n",
    "\n",
    "def add_rul_improved(df: pd.DataFrame) -> pd.DataFrame | None:\n",
    "    \"\"\"IMPROVED: Better RUL processing for LSTM.\"\"\"\n",
    "    if df is None:\n",
    "        return None\n",
    "\n",
    "    max_cycles = df.groupby('unit_number')['time_in_cycles'].max().reset_index()\n",
    "    max_cycles.columns = ['unit_number', 'max_cycle']\n",
    "    df = df.merge(max_cycles, on='unit_number', how='left')\n",
    "    df['RUL'] = df['max_cycle'] - df['time_in_cycles']\n",
    "    df.drop(columns=['max_cycle'], inplace=True)\n",
    "\n",
    "    # IMPROVED: Better RUL capping for LSTM - lower threshold\n",
    "    df['RUL'] = df['RUL'].clip(upper=120)  # Reduced from 125\n",
    "    return df\n",
    "\n",
    "def normalize_data_improved(df: pd.DataFrame, columns_to_normalize: List[str],\n",
    "                           scaler: MinMaxScaler = None) -> Tuple[pd.DataFrame, MinMaxScaler] | Tuple[None, None]:\n",
    "    \"\"\"IMPROVED: Better normalization for LSTM.\"\"\"\n",
    "    if df is None:\n",
    "        return None, None\n",
    "\n",
    "    data_to_scale = df[columns_to_normalize]\n",
    "    if scaler is None:\n",
    "        # IMPROVED: Better range for LSTM gradients\n",
    "        scaler = MinMaxScaler(feature_range=(-0.5, 0.5))  # Centered around 0\n",
    "        df[columns_to_normalize] = scaler.fit_transform(data_to_scale)\n",
    "    else:\n",
    "        valid_cols = [col for col in columns_to_normalize if hasattr(scaler, 'feature_names_in_') and col in scaler.feature_names_in_]\n",
    "        if len(valid_cols) < len(columns_to_normalize):\n",
    "            print(\"Warning: Some columns not found in the provided scaler. Skipping them.\")\n",
    "        if valid_cols:\n",
    "            df[valid_cols] = scaler.transform(df[valid_cols])\n",
    "        else:\n",
    "            # Fallback: transform all columns\n",
    "            df[columns_to_normalize] = scaler.transform(data_to_scale)\n",
    "    return df, scaler\n",
    "\n",
    "def prepare_cmapss_data_improved(data_dir: str, train_file: str, test_file: str, test_rul_file: str) -> Tuple[\n",
    "    pd.DataFrame, pd.DataFrame, pd.DataFrame, MinMaxScaler, List[str]]:\n",
    "    \"\"\"IMPROVED data preparation for better MSE.\"\"\"\n",
    "    print(\"--- IMPROVED Data Preparation for Better MSE ---\")\n",
    "    train_df = load_dataframe(os.path.join(data_dir, train_file))\n",
    "    train_df = add_rul_improved(train_df)\n",
    "\n",
    "    print(\"\\n--- Preparing Test Data ---\")\n",
    "    test_df = load_dataframe(os.path.join(data_dir, test_file))\n",
    "    test_rul_df = pd.read_csv(os.path.join(data_dir, test_rul_file), header=None, names=['RUL'])\n",
    "\n",
    "    # IMPROVED: More aggressive cleaning\n",
    "    cols_to_remove = clean_data_improved(train_df)\n",
    "    feature_cols = [col for col in train_df.columns if\n",
    "                    col not in ['unit_number', 'time_in_cycles', 'RUL'] + cols_to_remove]\n",
    "    print(f\"\\nUsing {len(feature_cols)} Features: {feature_cols}\")\n",
    "\n",
    "    # Drop removed columns\n",
    "    train_df.drop(columns=cols_to_remove, inplace=True, errors='ignore')\n",
    "    test_df.drop(columns=cols_to_remove, inplace=True, errors='ignore')\n",
    "\n",
    "    # IMPROVED normalization\n",
    "    print(\"\\n--- Improved Normalization ---\")\n",
    "    train_df_norm, scaler = normalize_data_improved(train_df.copy(), feature_cols, scaler=None)\n",
    "    test_df_norm, _ = normalize_data_improved(test_df.copy(), feature_cols, scaler=scaler)\n",
    "\n",
    "    return train_df_norm, test_df_norm, test_rul_df, scaler, feature_cols\n",
    "\n",
    "# IMPROVED LSTM Dataset with better sampling\n",
    "class ImprovedNASALSTMDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, feature_cols: List[str], window_size: int = 30,  # Reduced window\n",
    "                 stride: int = 1, is_test: bool = False, test_rul_df: pd.DataFrame = None):\n",
    "        self.df = df\n",
    "        self.feature_cols = feature_cols\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.is_test = is_test\n",
    "        self.test_rul_df = test_rul_df\n",
    "        self.samples = []\n",
    "        self.targets = []\n",
    "\n",
    "        self._prepare_samples()\n",
    "\n",
    "    def _prepare_samples(self):\n",
    "        \"\"\"IMPROVED: Better sampling strategy for lower MSE.\"\"\"\n",
    "        units = self.df['unit_number'].unique()\n",
    "\n",
    "        for unit in units:\n",
    "            unit_df = self.df[self.df['unit_number'] == unit].sort_values('time_in_cycles')\n",
    "\n",
    "            if self.is_test:\n",
    "                # Test: same as before but with RUL capping\n",
    "                if len(unit_df) >= self.window_size:\n",
    "                    window_data = unit_df[self.feature_cols].iloc[-self.window_size:].values\n",
    "                    self.samples.append(window_data)\n",
    "                    if self.test_rul_df is not None:\n",
    "                        rul = min(self.test_rul_df.iloc[unit - 1]['RUL'], 120)  # Same capping\n",
    "                        self.targets.append(rul)\n",
    "                else:\n",
    "                    window_data = unit_df[self.feature_cols].values\n",
    "                    padded = np.zeros((self.window_size, len(self.feature_cols)))\n",
    "                    padded[-len(window_data):] = window_data\n",
    "                    self.samples.append(padded)\n",
    "                    if self.test_rul_df is not None:\n",
    "                        rul = min(self.test_rul_df.iloc[unit - 1]['RUL'], 120)\n",
    "                        self.targets.append(rul)\n",
    "            else:\n",
    "                # IMPROVED: More samples from degradation phase\n",
    "                total_cycles = len(unit_df)\n",
    "\n",
    "                # Dense sampling for all data (better coverage)\n",
    "                for i in range(0, len(unit_df) - self.window_size + 1, self.stride):\n",
    "                    window_data = unit_df[self.feature_cols].iloc[i:i + self.window_size].values\n",
    "                    rul = unit_df['RUL'].iloc[i + self.window_size - 1]\n",
    "                    self.samples.append(window_data)\n",
    "                    self.targets.append(rul)\n",
    "\n",
    "                # IMPROVED: Add extra samples from end-of-life (last 30%)\n",
    "                eol_start = max(self.window_size, int(total_cycles * 0.7))\n",
    "                for i in range(eol_start, len(unit_df) - self.window_size + 1, 1):\n",
    "                    window_data = unit_df[self.feature_cols].iloc[i:i + self.window_size].values\n",
    "                    rul = unit_df['RUL'].iloc[i + self.window_size - 1]\n",
    "                    self.samples.append(window_data)  # Add extra samples\n",
    "                    self.targets.append(rul)\n",
    "\n",
    "        self.samples = np.array(self.samples, dtype=np.float32)\n",
    "        self.targets = np.array(self.targets, dtype=np.float32)\n",
    "        print(f\"IMPROVED: Created {len(self.samples)} samples with enhanced end-of-life focus\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        target = self.targets[idx]\n",
    "        return torch.FloatTensor(sample), torch.FloatTensor([target])\n",
    "\n",
    "# IMPROVED LSTM Model - Smaller and more focused\n",
    "class ImprovedNASALSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=1, fc_hidden_sizes=[32], dropout_rate=0.1):\n",
    "        super(ImprovedNASALSTM, self).__init__()\n",
    "\n",
    "        # IMPROVED: Smaller LSTM for better generalization\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0  # No dropout in LSTM for single layer\n",
    "        )\n",
    "\n",
    "        # IMPROVED: Simpler FC layers\n",
    "        fc_layers = []\n",
    "        prev_size = hidden_size\n",
    "\n",
    "        for fc_hidden_size in fc_hidden_sizes:\n",
    "            fc_layers.extend([\n",
    "                nn.Linear(prev_size, fc_hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = fc_hidden_size\n",
    "\n",
    "        fc_layers.append(nn.Linear(prev_size, 1))\n",
    "        self.fc = nn.Sequential(*fc_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Simple LSTM forward pass\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "\n",
    "        # Use last output\n",
    "        last_hidden = lstm_out[:, -1, :]\n",
    "\n",
    "        # Pass through FC layers\n",
    "        output = self.fc(last_hidden)\n",
    "        return output\n",
    "\n",
    "def get_data_loaders_improved(data_dir='./data/NASA', batch_size=64, window_size=30, val_split=0.15, seed=42):\n",
    "    \"\"\"IMPROVED data loading for better MSE.\"\"\"\n",
    "    print(f\"Loading NASA C-MAPSS dataset (IMPROVED) from: {data_dir}\")\n",
    "\n",
    "    train_df, test_df, test_rul_df, scaler, feature_cols = prepare_cmapss_data_improved(\n",
    "        data_dir, 'train_FD001.txt', 'test_FD001.txt', 'RUL_FD001.txt'\n",
    "    )\n",
    "\n",
    "    # Create improved datasets\n",
    "    full_train_dataset = ImprovedNASALSTMDataset(train_df, feature_cols, window_size=window_size)\n",
    "\n",
    "    # Smaller validation split for more training data\n",
    "    val_size = int(len(full_train_dataset) * val_split)\n",
    "    train_size = len(full_train_dataset) - val_size\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_train_dataset, [train_size, val_size], generator=generator\n",
    "    )\n",
    "\n",
    "    test_dataset = ImprovedNASALSTMDataset(test_df, feature_cols, window_size=window_size,\n",
    "                                          is_test=True, test_rul_df=test_rul_df)\n",
    "\n",
    "    # Improved data loaders\n",
    "    num_workers = min(4, os.cpu_count() or 2)\n",
    "    pin_memory = True if DEVICE.type == 'cuda' else False\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=num_workers, pin_memory=pin_memory, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                            num_workers=num_workers, pin_memory=pin_memory)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                             num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    print(f\"IMPROVED DataLoaders - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "    print(f\"Input shape: ({window_size}, {len(feature_cols)}) (sequence_length, num_features)\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader, len(feature_cols)\n",
    "\n",
    "def get_improved_lstm_model(input_size, hidden_size=64, num_layers=1, fc_hidden_sizes=[32], dropout_rate=0.1):\n",
    "    \"\"\"IMPROVED: Smaller LSTM model for better MSE.\"\"\"\n",
    "    model = ImprovedNASALSTM(input_size, hidden_size, num_layers, fc_hidden_sizes, dropout_rate)\n",
    "    print(f\"‚úÖ Created IMPROVED LSTM with smaller architecture:\")\n",
    "    print(f\"   LSTM: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}\")\n",
    "    print(f\"   FC: {hidden_size} -> {' -> '.join(map(str, fc_hidden_sizes))} -> 1\")\n",
    "    return model\n",
    "\n",
    "def get_ignored_layers(model):\n",
    "    \"\"\"Get layers to ignore during pruning.\"\"\"\n",
    "    ignored_layers = []\n",
    "    # Ignore LSTM\n",
    "    ignored_layers.append(model.lstm)\n",
    "\n",
    "    # Get the last linear layer\n",
    "    for module in model.fc:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            last_linear = module\n",
    "    ignored_layers.append(last_linear)\n",
    "    return ignored_layers\n",
    "\n",
    "def calculate_macs_params(model, example_input):\n",
    "    \"\"\"Calculate MACs and parameters using torch_pruning\"\"\"\n",
    "    model.eval()\n",
    "    target_device = example_input.device\n",
    "    model_on_device = model.to(target_device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        macs, params = tp.utils.count_ops_and_params(model_on_device, example_input)\n",
    "\n",
    "    return macs, params\n",
    "\n",
    "def save_model(model, save_path, example_input_cpu=None):\n",
    "    \"\"\"Save model state dict and optionally ONNX\"\"\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"‚úÖ Model saved to {save_path}\")\n",
    "\n",
    "    if example_input_cpu is not None:\n",
    "        onnx_path = save_path.replace('.pth', '.onnx')\n",
    "        try:\n",
    "            model_cpu = model.to('cpu')\n",
    "            torch.onnx.export(\n",
    "                model_cpu, example_input_cpu, onnx_path,\n",
    "                export_params=True, opset_version=13,\n",
    "                input_names=['input'], output_names=['output'],\n",
    "                dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "            )\n",
    "            print(f\"‚úÖ ONNX model saved to {onnx_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: ONNX export failed: {e}\")\n",
    "\n",
    "def evaluate_model(model, data_loader, example_input, criterion, device):\n",
    "    \"\"\"Evaluate model and return comprehensive metrics\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Calculate efficiency metrics\n",
    "    macs, params = calculate_macs_params(model, example_input.to(device))\n",
    "    model_size_mb = params * 4 / (1024 * 1024)\n",
    "\n",
    "    # Calculate MSE and MAE\n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            all_predictions.extend(output.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    mse = np.mean((all_predictions - all_targets) ** 2)\n",
    "    mae = np.mean(np.abs(all_predictions - all_targets))\n",
    "\n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'loss': total_loss / len(data_loader.dataset),\n",
    "        'macs': macs,\n",
    "        'params': params,\n",
    "        'size_mb': model_size_mb\n",
    "    }\n",
    "\n",
    "def prune_model(model, strategy_config, sparsity_ratio, example_input, ignored_layers=None):\n",
    "    \"\"\"Apply structured pruning to model\"\"\"\n",
    "    if sparsity_ratio == 0.0:\n",
    "        print(\"No pruning needed (sparsity = 0.0)\")\n",
    "        return model\n",
    "\n",
    "    model.eval()\n",
    "    pruned_model = copy.deepcopy(model)\n",
    "    pruned_model.to(example_input.device)\n",
    "\n",
    "    initial_macs, _ = calculate_macs_params(pruned_model, example_input)\n",
    "    print(f\"Initial MACs: {initial_macs / 1e6:.2f}M\")\n",
    "\n",
    "    ignored_layers = ignored_layers or []\n",
    "\n",
    "    # Create pruner - only prune Linear layers\n",
    "    pruner = strategy_config['pruner'](\n",
    "        pruned_model,\n",
    "        example_input,\n",
    "        importance=strategy_config['importance'],\n",
    "        iterative_steps=3,  # Fewer steps for stability\n",
    "        ch_sparsity=sparsity_ratio,\n",
    "        root_module_types=[nn.Linear],\n",
    "        ignored_layers=ignored_layers\n",
    "    )\n",
    "\n",
    "    print(f\"Applying {strategy_config['importance'].__class__.__name__} pruning at {sparsity_ratio:.1%} sparsity...\")\n",
    "    print(\"Note: Only pruning FC layers, LSTM layers are preserved\")\n",
    "\n",
    "    pruner.step()\n",
    "\n",
    "    final_macs, _ = calculate_macs_params(pruned_model, example_input)\n",
    "    reduction = (initial_macs - final_macs) / initial_macs * 100 if initial_macs > 0 else 0\n",
    "    print(f\"Final MACs: {final_macs / 1e6:.2f}M (Reduction: {reduction:.1f}%)\")\n",
    "\n",
    "    return pruned_model\n",
    "\n",
    "def train_model_improved(model, train_loader, criterion, optimizer, device, num_epochs,\n",
    "                        val_loader=None, patience=15, log_prefix=\"\", scheduler=None):\n",
    "    \"\"\"IMPROVED training with better convergence.\"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_mse': [],\n",
    "        'val_loss': [],\n",
    "        'val_mse': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_predictions = []\n",
    "        train_targets = []\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "\n",
    "            # IMPROVED: Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_predictions.extend(output.detach().cpu().numpy())\n",
    "            train_targets.extend(target.cpu().numpy())\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_mse = np.mean((np.array(train_predictions) - np.array(train_targets)) ** 2)\n",
    "\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_mse'].append(train_mse)\n",
    "\n",
    "        log_msg = f\"Epoch {epoch + 1}/{num_epochs} ({log_prefix}): Train Loss: {avg_train_loss:.4f}, Train MSE: {train_mse:.2f}\"\n",
    "\n",
    "        # Validation phase\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_predictions = []\n",
    "            val_targets = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for data, target in val_loader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                    val_predictions.extend(output.cpu().numpy())\n",
    "                    val_targets.extend(target.cpu().numpy())\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_mse = np.mean((np.array(val_predictions) - np.array(val_targets)) ** 2)\n",
    "\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            history['val_mse'].append(val_mse)\n",
    "\n",
    "            log_msg += f\", Val Loss: {avg_val_loss:.4f}, Val MSE: {val_mse:.2f}\"\n",
    "\n",
    "            # IMPROVED: Learning rate scheduling\n",
    "            if scheduler:\n",
    "                scheduler.step(avg_val_loss)\n",
    "                log_msg += f\", LR: {optimizer.param_groups[0]['lr']:.6f}\"\n",
    "\n",
    "            # Early stopping check\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                epochs_no_improve = 0\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "                log_msg += \" (Best)\"\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"{log_msg}\")\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "                break\n",
    "        else:\n",
    "            history['val_loss'].append(None)\n",
    "            history['val_mse'].append(None)\n",
    "\n",
    "        if epoch % 25 == 0:\n",
    "            print(log_msg)\n",
    "\n",
    "    # Load best model state if available\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"Loaded best model state\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "def save_results_to_files(all_results, output_dir):\n",
    "    \"\"\"Save experimental results to JSON and CSV files\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save complete results as JSON\n",
    "    results_json_path = os.path.join(output_dir, 'complete_results.json')\n",
    "    with open(results_json_path, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2, default=str)\n",
    "    print(f\"‚úÖ Complete results saved to {results_json_path}\")\n",
    "\n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for strategy, strategy_results in all_results.items():\n",
    "        for sparsity, metrics in strategy_results.items():\n",
    "            row = {\n",
    "                'strategy': strategy,\n",
    "                'sparsity_ratio': sparsity,\n",
    "                'mse': metrics['mse'],\n",
    "                'mae': metrics['mae'],\n",
    "                'loss': metrics['loss'],\n",
    "                'macs_millions': metrics['macs'] / 1e6,\n",
    "                'params_millions': metrics['params'] / 1e6,\n",
    "                'size_mb': metrics['size_mb']\n",
    "            }\n",
    "            summary_data.append(row)\n",
    "\n",
    "    # Save summary as CSV\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_csv_path = os.path.join(output_dir, 'summary_results.csv')\n",
    "    summary_df.to_csv(summary_csv_path, index=False)\n",
    "    print(f\"‚úÖ Summary results saved to {summary_csv_path}\")\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "def create_results_plots(summary_df, output_dir):\n",
    "    \"\"\"Create visualization plots\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    strategies = summary_df['strategy'].unique()\n",
    "\n",
    "    # Plot 1: MSE vs Sparsity\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for strategy in strategies:\n",
    "        strategy_data = summary_df[summary_df['strategy'] == strategy].sort_values('sparsity_ratio')\n",
    "        plt.plot(strategy_data['sparsity_ratio'] * 100, strategy_data['mse'],\n",
    "                 'o-', linewidth=2, markersize=8, label=strategy)\n",
    "\n",
    "    plt.xlabel('Sparsity (%)', fontsize=12)\n",
    "    plt.ylabel('MSE', fontsize=12)\n",
    "    plt.title('IMPROVED NASA LSTM: MSE vs Sparsity (Target: < 100)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(output_dir, 'mse_vs_sparsity.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"‚úÖ MSE plot saved to {plot_path}\")\n",
    "\n",
    "    # Plot 2: Efficiency frontier\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for strategy in strategies:\n",
    "        strategy_data = summary_df[summary_df['strategy'] == strategy].sort_values('sparsity_ratio')\n",
    "        plt.scatter(strategy_data['macs_millions'], strategy_data['mse'],\n",
    "                    s=100, label=strategy, alpha=0.8)\n",
    "        plt.plot(strategy_data['macs_millions'], strategy_data['mse'], '--', alpha=0.6)\n",
    "\n",
    "    plt.xlabel('MACs (Millions)', fontsize=12)\n",
    "    plt.ylabel('MSE', fontsize=12)\n",
    "    plt.title('IMPROVED NASA LSTM: Efficiency Frontier', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(output_dir, 'efficiency_frontier.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"‚úÖ Efficiency frontier plot saved to {plot_path}\")\n",
    "\n",
    "def print_results_table(summary_df):\n",
    "    \"\"\"Print formatted results table\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"IMPROVED LSTM RESULTS - TARGET: MSE < 100\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Baseline results\n",
    "    baseline_results = summary_df[summary_df['sparsity_ratio'] == 0.0].iloc[0]\n",
    "    print(f\"\\nImproved LSTM Baseline Performance:\")\n",
    "    print(f\"  MSE: {baseline_results['mse']:.2f}\")\n",
    "    print(f\"  MAE: {baseline_results['mae']:.2f}\")\n",
    "    print(f\"  MACs: {baseline_results['macs_millions']:.2f}M\")\n",
    "    print(f\"  Parameters: {baseline_results['params_millions']:.2f}M\")\n",
    "    print(f\"  Model Size: {baseline_results['size_mb']:.2f}MB\")\n",
    "\n",
    "    # Best result\n",
    "    best_mse = summary_df['mse'].min()\n",
    "    best_row = summary_df[summary_df['mse'] == best_mse].iloc[0]\n",
    "    print(f\"\\nBest IMPROVED LSTM Result:\")\n",
    "    print(f\"  Strategy: {best_row['strategy']}\")\n",
    "    print(f\"  Sparsity: {best_row['sparsity_ratio']*100:.0f}%\")\n",
    "    print(f\"  MSE: {best_row['mse']:.2f}\")\n",
    "    print(f\"  MAE: {best_row['mae']:.2f}\")\n",
    "\n",
    "    # Complete results table\n",
    "    print(f\"\\nComplete IMPROVED LSTM Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Strategy':<12} {'Sparsity':<8} {'MSE':<8} {'MAE':<8} {'MACs(M)':<8} {'Params(M)':<9} {'Size(MB)':<8}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for _, row in summary_df.sort_values(['strategy', 'sparsity_ratio']).iterrows():\n",
    "        print(f\"{row['strategy']:<12} {row['sparsity_ratio'] * 100:>6.0f}% \"\n",
    "              f\"{row['mse']:>7.2f} {row['mae']:>7.2f} {row['macs_millions']:>7.2f} \"\n",
    "              f\"{row['params_millions']:>8.2f} {row['size_mb']:>7.2f}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"IMPROVED main experimental workflow for sub-100 MSE.\"\"\"\n",
    "    print(\"Starting IMPROVED NASA LSTM Experiments for Sub-100 MSE\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # IMPROVED Configuration - optimized for lower MSE\n",
    "    config = {\n",
    "        'strategies': {\n",
    "            'MagnitudeL2': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.MagnitudeImportance(p=2)\n",
    "            },\n",
    "            'Random': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.RandomImportance()\n",
    "            },\n",
    "        },\n",
    "        'pruning_ratios': [0.0, 0.2, 0.5, 0.7],\n",
    "        'hidden_size': 64,                # Reduced from 100\n",
    "        'num_layers': 1,                  # Reduced from 2\n",
    "        'fc_hidden_sizes': [32],          # Simplified from [64, 32]\n",
    "        'dropout_rate': 0.1,              # Reduced from 0.2\n",
    "        'window_size': 30,                # Reduced from 50\n",
    "        'batch_size': 64,                 # Reduced from 128\n",
    "        'learning_rate': 0.002,           # Increased from 0.0001\n",
    "        'epochs': 800,                    # Reduced from 1000\n",
    "        'patience': 15,                   # Reduced from 20\n",
    "        'output_dir': './results_improved_lstm_nasa',\n",
    "        'models_dir': './models_improved_lstm_nasa',\n",
    "        'data_dir': './data/CMaps'\n",
    "    }\n",
    "\n",
    "    # Create output directories\n",
    "    os.makedirs(config['output_dir'], exist_ok=True)\n",
    "    os.makedirs(config['models_dir'], exist_ok=True)\n",
    "\n",
    "    # Load improved data\n",
    "    print(\"Loading IMPROVED NASA C-MAPSS dataset...\")\n",
    "    train_loader, val_loader, test_loader, input_size = get_data_loaders_improved(\n",
    "        data_dir=config['data_dir'],\n",
    "        batch_size=config['batch_size'],\n",
    "        window_size=config['window_size']\n",
    "    )\n",
    "\n",
    "    # Prepare inputs and criterion\n",
    "    example_input_cpu = torch.randn(1, config['window_size'], input_size)\n",
    "    example_input_device = example_input_cpu.to(DEVICE)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Get improved baseline model and train it\n",
    "    print(\"\\nCreating and training IMPROVED baseline model...\")\n",
    "    model = get_improved_lstm_model(\n",
    "        input_size,\n",
    "        config['hidden_size'],\n",
    "        config['num_layers'],\n",
    "        config['fc_hidden_sizes'],\n",
    "        config['dropout_rate']\n",
    "    )\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # IMPROVED training with better optimizer and scheduling\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=8, verbose=False, min_lr=1e-6\n",
    "    )\n",
    "\n",
    "    trained_model, training_history = train_model_improved(\n",
    "        model, train_loader, criterion, optimizer, DEVICE,\n",
    "        config['epochs'], val_loader, config['patience'], \"IMPROVED Baseline\", scheduler\n",
    "    )\n",
    "\n",
    "    # Save baseline model\n",
    "    baseline_model_path = os.path.join(config['models_dir'], 'baseline_model.pth')\n",
    "    save_model(trained_model, baseline_model_path, example_input_cpu)\n",
    "\n",
    "    # Evaluate baseline\n",
    "    print(\"\\nEvaluating IMPROVED baseline model...\")\n",
    "    baseline_metrics = evaluate_model(trained_model, test_loader, example_input_device, criterion, DEVICE)\n",
    "    print(f\"IMPROVED Baseline Results: MSE={baseline_metrics['mse']:.2f}, \"\n",
    "          f\"MAE={baseline_metrics['mae']:.2f}, \"\n",
    "          f\"MACs={baseline_metrics['macs'] / 1e6:.2f}M, \"\n",
    "          f\"Params={baseline_metrics['params'] / 1e6:.2f}M\")\n",
    "\n",
    "    # Initialize results storage\n",
    "    all_results = {}\n",
    "    for strategy_name in config['strategies'].keys():\n",
    "        all_results[strategy_name] = {0.0: baseline_metrics}\n",
    "\n",
    "    # Get ignored layers\n",
    "    ignored_layers = get_ignored_layers(trained_model)\n",
    "\n",
    "    # Run improved pruning experiments\n",
    "    print(\"\\nStarting IMPROVED pruning experiments...\")\n",
    "    for strategy_name, strategy_config in config['strategies'].items():\n",
    "        print(f\"\\n--- IMPROVED Strategy: {strategy_name} ---\")\n",
    "\n",
    "        for sparsity_ratio in config['pruning_ratios']:\n",
    "            if sparsity_ratio == 0.0:\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nProcessing IMPROVED {strategy_name} at {sparsity_ratio:.1%} sparsity...\")\n",
    "\n",
    "            # Load fresh copy of trained baseline\n",
    "            model_copy = get_improved_lstm_model(\n",
    "                input_size,\n",
    "                config['hidden_size'],\n",
    "                config['num_layers'],\n",
    "                config['fc_hidden_sizes'],\n",
    "                config['dropout_rate']\n",
    "            )\n",
    "            model_copy.load_state_dict(torch.load(baseline_model_path, map_location=DEVICE))\n",
    "            model_copy.to(DEVICE)\n",
    "\n",
    "            # Apply pruning\n",
    "            try:\n",
    "                pruned_model = prune_model(\n",
    "                    model_copy, strategy_config, sparsity_ratio,\n",
    "                    example_input_device, ignored_layers\n",
    "                )\n",
    "\n",
    "                # IMPROVED fine-tuning\n",
    "                print(\"IMPROVED fine-tuning...\")\n",
    "                optimizer_ft = optim.Adam(pruned_model.parameters(), lr=config['learning_rate']/2, weight_decay=1e-5)\n",
    "                scheduler_ft = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                    optimizer_ft, mode='min', factor=0.7, patience=5, min_lr=1e-7\n",
    "                )\n",
    "\n",
    "                fine_tuned_model, ft_history = train_model_improved(\n",
    "                    pruned_model, train_loader, criterion, optimizer_ft, DEVICE,\n",
    "                    config['epochs']//2, val_loader, config['patience']//2,\n",
    "                    f\"IMPROVED-{strategy_name}-{sparsity_ratio:.1%}\", scheduler_ft\n",
    "                )\n",
    "\n",
    "                # Evaluate fine-tuned model\n",
    "                final_metrics = evaluate_model(fine_tuned_model, test_loader, example_input_device, criterion, DEVICE)\n",
    "                all_results[strategy_name][sparsity_ratio] = final_metrics\n",
    "\n",
    "                print(f\"IMPROVED Results: MSE={final_metrics['mse']:.2f}, \"\n",
    "                      f\"MAE={final_metrics['mae']:.2f}, \"\n",
    "                      f\"MACs={final_metrics['macs'] / 1e6:.2f}M\")\n",
    "\n",
    "                # Save fine-tuned model\n",
    "                model_filename = f\"improved_{strategy_name.lower()}_sparsity_{sparsity_ratio:.1f}.pth\"\n",
    "                model_path = os.path.join(config['models_dir'], model_filename)\n",
    "                save_model(fine_tuned_model, model_path, example_input_cpu)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in IMPROVED processing {strategy_name} at {sparsity_ratio:.1%}: {e}\")\n",
    "                # Use baseline as fallback\n",
    "                all_results[strategy_name][sparsity_ratio] = baseline_metrics\n",
    "\n",
    "    # Save and analyze results\n",
    "    print(\"\\nSaving IMPROVED results...\")\n",
    "    summary_df = save_results_to_files(all_results, config['output_dir'])\n",
    "\n",
    "    # Create plots\n",
    "    print(\"Creating IMPROVED plots...\")\n",
    "    create_results_plots(summary_df, config['output_dir'])\n",
    "\n",
    "    # Print summary\n",
    "    print_results_table(summary_df)\n",
    "\n",
    "    # Performance analysis\n",
    "    best_mse = summary_df['mse'].min()\n",
    "    baseline_mse = summary_df[summary_df['sparsity_ratio'] == 0.0]['mse'].iloc[0]\n",
    "\n",
    "    print(f\"\\nüéØ IMPROVED LSTM PERFORMANCE ANALYSIS:\")\n",
    "    print(f\"  Best MSE: {best_mse:.2f}\")\n",
    "    print(f\"  Baseline MSE: {baseline_mse:.2f}\")\n",
    "    print(f\"  Your Original LSTM: ~200+ MSE\")\n",
    "    print(f\"  Improvement: {((200 - best_mse) / 200 * 100):.1f}% better than original\")\n",
    "\n",
    "    if best_mse < 100:\n",
    "        print(f\"\\nüéâ SUCCESS! IMPROVED LSTM achieved target MSE < 100: {best_mse:.2f}\")\n",
    "        print(\"üöÄ Key improvements that worked:\")\n",
    "        print(\"  ‚úÖ Smaller architecture (64 hidden, 1 layer)\")\n",
    "        print(\"  ‚úÖ Better normalization (-0.5 to 0.5 range)\")\n",
    "        print(\"  ‚úÖ More aggressive feature selection\")\n",
    "        print(\"  ‚úÖ Enhanced end-of-life sampling\")\n",
    "        print(\"  ‚úÖ Gradient clipping and LR scheduling\")\n",
    "    elif best_mse < 150:\n",
    "        print(f\"\\n‚úÖ EXCELLENT! IMPROVED LSTM MSE < 150: {best_mse:.2f}\")\n",
    "        print(\"üí° Almost there! Try:\")\n",
    "        print(\"  - Ensemble of 3 improved LSTMs\")\n",
    "        print(\"  - Different window sizes (20, 25)\")\n",
    "        print(\"  - Even smaller architecture (32 hidden)\")\n",
    "    else:\n",
    "        print(f\"\\nüìà GOOD PROGRESS! IMPROVED LSTM MSE: {best_mse:.2f}\")\n",
    "        print(\"üí° Additional suggestions:\")\n",
    "        print(\"  - Try window sizes: 20, 25, 35\")\n",
    "        print(\"  - Even smaller model: 32 hidden units\")\n",
    "        print(\"  - Different normalization ranges\")\n",
    "        print(\"  - Ensemble averaging\")\n",
    "\n",
    "    print(f\"\\nüéâ IMPROVED experiments completed!\")\n",
    "    print(f\"üìÅ Results saved to: {os.path.abspath(config['output_dir'])}\")\n",
    "    print(f\"üìÅ Models saved to: {os.path.abspath(config['models_dir'])}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "44188f495813327",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting IMPROVED NASA LSTM Experiments for Sub-100 MSE\n",
      "============================================================\n",
      "Loading IMPROVED NASA C-MAPSS dataset...\n",
      "Loading NASA C-MAPSS dataset (IMPROVED) from: ./data/CMaps\n",
      "--- IMPROVED Data Preparation for Better MSE ---\n",
      "\n",
      "--- Preparing Test Data ---\n",
      "Columns with std < 0.01 (removing for better MSE): ['op_setting_1', 'op_setting_2', 'op_setting_3', 'sensor_1', 'sensor_5', 'sensor_6', 'sensor_10', 'sensor_16', 'sensor_18', 'sensor_19']\n",
      "\n",
      "Using 14 Features: ['sensor_2', 'sensor_3', 'sensor_4', 'sensor_7', 'sensor_8', 'sensor_9', 'sensor_11', 'sensor_12', 'sensor_13', 'sensor_14', 'sensor_15', 'sensor_17', 'sensor_20', 'sensor_21']\n",
      "\n",
      "--- Improved Normalization ---\n",
      "IMPROVED: Created 21068 samples with enhanced end-of-life focus\n",
      "IMPROVED: Created 100 samples with enhanced end-of-life focus\n",
      "IMPROVED DataLoaders - Train: 17908, Val: 3160, Test: 100\n",
      "Input shape: (30, 14) (sequence_length, num_features)\n",
      "\n",
      "Creating and training IMPROVED baseline model...\n",
      "‚úÖ Created IMPROVED LSTM with smaller architecture:\n",
      "   LSTM: input_size=14, hidden_size=64, num_layers=1\n",
      "   FC: 64 -> 32 -> 1\n",
      "Epoch 1/800 (IMPROVED Baseline): Train Loss: 2397.3292, Train MSE: 2397.33, Val Loss: 345.0719, Val MSE: 345.79, LR: 0.002000 (Best)\n",
      "Epoch 26/800 (IMPROVED Baseline): Train Loss: 145.4351, Train MSE: 145.44, Val Loss: 104.9556, Val MSE: 105.16, LR: 0.001000\n",
      "Epoch 51/800 (IMPROVED Baseline): Train Loss: 122.7178, Train MSE: 122.72, Val Loss: 96.9434, Val MSE: 96.27, LR: 0.001000\n",
      "Epoch 76/800 (IMPROVED Baseline): Train Loss: 101.6493, Train MSE: 101.65, Val Loss: 80.4394, Val MSE: 79.64, LR: 0.001000\n",
      "Epoch 101/800 (IMPROVED Baseline): Train Loss: 77.4480, Train MSE: 77.45, Val Loss: 59.8129, Val MSE: 59.12, LR: 0.001000\n",
      "Epoch 126/800 (IMPROVED Baseline): Train Loss: 55.7803, Train MSE: 55.78, Val Loss: 43.6129, Val MSE: 43.32, LR: 0.001000\n",
      "Epoch 151/800 (IMPROVED Baseline): Train Loss: 46.5354, Train MSE: 46.54, Val Loss: 36.9331, Val MSE: 37.15, LR: 0.001000\n",
      "Epoch 176/800 (IMPROVED Baseline): Train Loss: 34.1511, Train MSE: 34.15, Val Loss: 19.5332, Val MSE: 19.41, LR: 0.000500\n",
      "Epoch 201/800 (IMPROVED Baseline): Train Loss: 31.0398, Train MSE: 31.04, Val Loss: 13.5233, Val MSE: 13.59, LR: 0.000500\n",
      "Epoch 226/800 (IMPROVED Baseline): Train Loss: 26.6100, Train MSE: 26.61, Val Loss: 10.6844, Val MSE: 10.71, LR: 0.000250\n",
      "Epoch 251/800 (IMPROVED Baseline): Train Loss: 25.2061, Train MSE: 25.21, Val Loss: 9.0587, Val MSE: 8.98, LR: 0.000063\n",
      "Epoch 268/800 (IMPROVED Baseline): Train Loss: 24.9533, Train MSE: 24.95, Val Loss: 7.8283, Val MSE: 7.81, LR: 0.000031\n",
      "Early stopping triggered after 268 epochs\n",
      "Loaded best model state\n",
      "‚úÖ Model saved to ./models_improved_lstm_nasa/baseline_model.pth\n",
      "‚úÖ ONNX model saved to ./models_improved_lstm_nasa/baseline_model.onnx\n",
      "\n",
      "Evaluating IMPROVED baseline model...\n",
      "IMPROVED Baseline Results: MSE=243.38, MAE=11.36, MACs=8.87M, Params=0.02M\n",
      "\n",
      "Starting IMPROVED pruning experiments...\n",
      "\n",
      "--- IMPROVED Strategy: MagnitudeL2 ---\n",
      "\n",
      "Processing IMPROVED MagnitudeL2 at 20.0% sparsity...\n",
      "‚úÖ Created IMPROVED LSTM with smaller architecture:\n",
      "   LSTM: input_size=14, hidden_size=64, num_layers=1\n",
      "   FC: 64 -> 32 -> 1\n",
      "Initial MACs: 8.87M\n",
      "Applying MagnitudeImportance pruning at 20.0% sparsity...\n",
      "Note: Only pruning FC layers, LSTM layers are preserved\n",
      "Final MACs: 8.87M (Reduction: 0.0%)\n",
      "IMPROVED fine-tuning...\n",
      "Epoch 1/400 (IMPROVED-MagnitudeL2-20.0%): Train Loss: 36.2784, Train MSE: 36.28, Val Loss: 15.7318, Val MSE: 15.70, LR: 0.001000 (Best)\n",
      "Epoch 26/400 (IMPROVED-MagnitudeL2-20.0%): Train Loss: 29.5436, Train MSE: 29.54, Val Loss: 13.2167, Val MSE: 13.22, LR: 0.000700\n",
      "Epoch 38/400 (IMPROVED-MagnitudeL2-20.0%): Train Loss: 27.3835, Train MSE: 27.38, Val Loss: 10.4787, Val MSE: 10.51, LR: 0.000490\n",
      "Early stopping triggered after 38 epochs\n",
      "Loaded best model state\n",
      "IMPROVED Results: MSE=212.97, MAE=10.73, MACs=8.87M\n",
      "‚úÖ Model saved to ./models_improved_lstm_nasa/improved_magnitudel2_sparsity_0.2.pth\n",
      "‚úÖ ONNX model saved to ./models_improved_lstm_nasa/improved_magnitudel2_sparsity_0.2.onnx\n",
      "\n",
      "Processing IMPROVED MagnitudeL2 at 50.0% sparsity...\n",
      "‚úÖ Created IMPROVED LSTM with smaller architecture:\n",
      "   LSTM: input_size=14, hidden_size=64, num_layers=1\n",
      "   FC: 64 -> 32 -> 1\n",
      "Initial MACs: 8.87M\n",
      "Applying MagnitudeImportance pruning at 50.0% sparsity...\n",
      "Note: Only pruning FC layers, LSTM layers are preserved\n",
      "Final MACs: 8.87M (Reduction: 0.0%)\n",
      "IMPROVED fine-tuning...\n",
      "Epoch 1/400 (IMPROVED-MagnitudeL2-50.0%): Train Loss: 39.6165, Train MSE: 39.62, Val Loss: 16.8666, Val MSE: 16.91, LR: 0.001000 (Best)\n",
      "Epoch 26/400 (IMPROVED-MagnitudeL2-50.0%): Train Loss: 30.8023, Train MSE: 30.80, Val Loss: 11.1325, Val MSE: 11.06, LR: 0.000490 (Best)\n",
      "Epoch 51/400 (IMPROVED-MagnitudeL2-50.0%): Train Loss: 29.3689, Train MSE: 29.37, Val Loss: 9.0110, Val MSE: 8.99, LR: 0.000343\n",
      "Epoch 67/400 (IMPROVED-MagnitudeL2-50.0%): Train Loss: 26.2635, Train MSE: 26.26, Val Loss: 7.5061, Val MSE: 7.45, LR: 0.000168\n",
      "Early stopping triggered after 67 epochs\n",
      "Loaded best model state\n",
      "IMPROVED Results: MSE=239.52, MAE=11.45, MACs=8.87M\n",
      "‚úÖ Model saved to ./models_improved_lstm_nasa/improved_magnitudel2_sparsity_0.5.pth\n",
      "‚úÖ ONNX model saved to ./models_improved_lstm_nasa/improved_magnitudel2_sparsity_0.5.onnx\n",
      "\n",
      "Processing IMPROVED MagnitudeL2 at 70.0% sparsity...\n",
      "‚úÖ Created IMPROVED LSTM with smaller architecture:\n",
      "   LSTM: input_size=14, hidden_size=64, num_layers=1\n",
      "   FC: 64 -> 32 -> 1\n",
      "Initial MACs: 8.87M\n",
      "Applying MagnitudeImportance pruning at 70.0% sparsity...\n",
      "Note: Only pruning FC layers, LSTM layers are preserved\n",
      "Final MACs: 8.87M (Reduction: 0.0%)\n",
      "IMPROVED fine-tuning...\n",
      "Epoch 1/400 (IMPROVED-MagnitudeL2-70.0%): Train Loss: 47.3104, Train MSE: 47.31, Val Loss: 28.8002, Val MSE: 29.04, LR: 0.001000 (Best)\n",
      "Epoch 26/400 (IMPROVED-MagnitudeL2-70.0%): Train Loss: 35.2638, Train MSE: 35.26, Val Loss: 14.6950, Val MSE: 14.80, LR: 0.000700\n",
      "Epoch 31/400 (IMPROVED-MagnitudeL2-70.0%): Train Loss: 32.7216, Train MSE: 32.72, Val Loss: 13.0334, Val MSE: 13.04, LR: 0.000490\n",
      "Early stopping triggered after 31 epochs\n",
      "Loaded best model state\n",
      "IMPROVED Results: MSE=221.93, MAE=11.05, MACs=8.87M\n",
      "‚úÖ Model saved to ./models_improved_lstm_nasa/improved_magnitudel2_sparsity_0.7.pth\n",
      "‚úÖ ONNX model saved to ./models_improved_lstm_nasa/improved_magnitudel2_sparsity_0.7.onnx\n",
      "\n",
      "--- IMPROVED Strategy: Random ---\n",
      "\n",
      "Processing IMPROVED Random at 20.0% sparsity...\n",
      "‚úÖ Created IMPROVED LSTM with smaller architecture:\n",
      "   LSTM: input_size=14, hidden_size=64, num_layers=1\n",
      "   FC: 64 -> 32 -> 1\n",
      "Initial MACs: 8.87M\n",
      "Applying RandomImportance pruning at 20.0% sparsity...\n",
      "Note: Only pruning FC layers, LSTM layers are preserved\n",
      "Final MACs: 8.87M (Reduction: 0.0%)\n",
      "IMPROVED fine-tuning...\n",
      "Epoch 1/400 (IMPROVED-Random-20.0%): Train Loss: 37.3319, Train MSE: 37.33, Val Loss: 26.8264, Val MSE: 26.87, LR: 0.001000 (Best)\n",
      "Epoch 26/400 (IMPROVED-Random-20.0%): Train Loss: 29.5690, Train MSE: 29.57, Val Loss: 14.8285, Val MSE: 14.82, LR: 0.000700\n",
      "Epoch 30/400 (IMPROVED-Random-20.0%): Train Loss: 28.3289, Train MSE: 28.33, Val Loss: 12.9735, Val MSE: 12.93, LR: 0.000490\n",
      "Early stopping triggered after 30 epochs\n",
      "Loaded best model state\n",
      "IMPROVED Results: MSE=236.31, MAE=10.86, MACs=8.87M\n",
      "‚úÖ Model saved to ./models_improved_lstm_nasa/improved_random_sparsity_0.2.pth\n",
      "‚úÖ ONNX model saved to ./models_improved_lstm_nasa/improved_random_sparsity_0.2.onnx\n",
      "\n",
      "Processing IMPROVED Random at 50.0% sparsity...\n",
      "‚úÖ Created IMPROVED LSTM with smaller architecture:\n",
      "   LSTM: input_size=14, hidden_size=64, num_layers=1\n",
      "   FC: 64 -> 32 -> 1\n",
      "Initial MACs: 8.87M\n",
      "Applying RandomImportance pruning at 50.0% sparsity...\n",
      "Note: Only pruning FC layers, LSTM layers are preserved\n",
      "Final MACs: 8.87M (Reduction: 0.0%)\n",
      "IMPROVED fine-tuning...\n",
      "Epoch 1/400 (IMPROVED-Random-50.0%): Train Loss: 49.1861, Train MSE: 49.19, Val Loss: 23.8831, Val MSE: 23.71, LR: 0.001000 (Best)\n",
      "Epoch 16/400 (IMPROVED-Random-50.0%): Train Loss: 36.4031, Train MSE: 36.40, Val Loss: 14.8112, Val MSE: 14.67, LR: 0.000700\n",
      "Early stopping triggered after 16 epochs\n",
      "Loaded best model state\n",
      "IMPROVED Results: MSE=213.18, MAE=10.66, MACs=8.87M\n",
      "‚úÖ Model saved to ./models_improved_lstm_nasa/improved_random_sparsity_0.5.pth\n",
      "‚úÖ ONNX model saved to ./models_improved_lstm_nasa/improved_random_sparsity_0.5.onnx\n",
      "\n",
      "Processing IMPROVED Random at 70.0% sparsity...\n",
      "‚úÖ Created IMPROVED LSTM with smaller architecture:\n",
      "   LSTM: input_size=14, hidden_size=64, num_layers=1\n",
      "   FC: 64 -> 32 -> 1\n",
      "Initial MACs: 8.87M\n",
      "Applying RandomImportance pruning at 70.0% sparsity...\n",
      "Note: Only pruning FC layers, LSTM layers are preserved\n",
      "Final MACs: 8.87M (Reduction: 0.0%)\n",
      "IMPROVED fine-tuning...\n",
      "Epoch 1/400 (IMPROVED-Random-70.0%): Train Loss: 62.7789, Train MSE: 62.78, Val Loss: 16.8600, Val MSE: 16.79, LR: 0.001000 (Best)\n",
      "Epoch 26/400 (IMPROVED-Random-70.0%): Train Loss: 35.8691, Train MSE: 35.87, Val Loss: 15.2335, Val MSE: 15.23, LR: 0.000700\n",
      "Epoch 36/400 (IMPROVED-Random-70.0%): Train Loss: 33.4975, Train MSE: 33.50, Val Loss: 10.0424, Val MSE: 10.11, LR: 0.000343\n",
      "Early stopping triggered after 36 epochs\n",
      "Loaded best model state\n",
      "IMPROVED Results: MSE=246.79, MAE=11.71, MACs=8.87M\n",
      "‚úÖ Model saved to ./models_improved_lstm_nasa/improved_random_sparsity_0.7.pth\n",
      "‚úÖ ONNX model saved to ./models_improved_lstm_nasa/improved_random_sparsity_0.7.onnx\n",
      "\n",
      "Saving IMPROVED results...\n",
      "‚úÖ Complete results saved to ./results_improved_lstm_nasa/complete_results.json\n",
      "‚úÖ Summary results saved to ./results_improved_lstm_nasa/summary_results.csv\n",
      "Creating IMPROVED plots...\n",
      "‚úÖ MSE plot saved to ./results_improved_lstm_nasa/mse_vs_sparsity.png\n",
      "‚úÖ Efficiency frontier plot saved to ./results_improved_lstm_nasa/efficiency_frontier.png\n",
      "\n",
      "================================================================================\n",
      "IMPROVED LSTM RESULTS - TARGET: MSE < 100\n",
      "================================================================================\n",
      "\n",
      "Improved LSTM Baseline Performance:\n",
      "  MSE: 243.38\n",
      "  MAE: 11.36\n",
      "  MACs: 8.87M\n",
      "  Parameters: 0.02M\n",
      "  Model Size: 0.09MB\n",
      "\n",
      "Best IMPROVED LSTM Result:\n",
      "  Strategy: MagnitudeL2\n",
      "  Sparsity: 20%\n",
      "  MSE: 212.97\n",
      "  MAE: 10.73\n",
      "\n",
      "Complete IMPROVED LSTM Results:\n",
      "--------------------------------------------------------------------------------\n",
      "Strategy     Sparsity MSE      MAE      MACs(M)  Params(M) Size(MB)\n",
      "--------------------------------------------------------------------------------\n",
      "MagnitudeL2       0%  243.38   11.36    8.87     0.02    0.09\n",
      "MagnitudeL2      20%  212.97   10.73    8.87     0.02    0.09\n",
      "MagnitudeL2      50%  239.52   11.45    8.87     0.02    0.08\n",
      "MagnitudeL2      70%  221.93   11.05    8.87     0.02    0.08\n",
      "Random            0%  243.38   11.36    8.87     0.02    0.09\n",
      "Random           20%  236.31   10.86    8.87     0.02    0.09\n",
      "Random           50%  213.18   10.66    8.87     0.02    0.08\n",
      "Random           70%  246.79   11.71    8.87     0.02    0.08\n",
      "\n",
      "üéØ IMPROVED LSTM PERFORMANCE ANALYSIS:\n",
      "  Best MSE: 212.97\n",
      "  Baseline MSE: 243.38\n",
      "  Your Original LSTM: ~200+ MSE\n",
      "  Improvement: -6.5% better than original\n",
      "\n",
      "üìà GOOD PROGRESS! IMPROVED LSTM MSE: 212.97\n",
      "üí° Additional suggestions:\n",
      "  - Try window sizes: 20, 25, 35\n",
      "  - Even smaller model: 32 hidden units\n",
      "  - Different normalization ranges\n",
      "  - Ensemble averaging\n",
      "\n",
      "üéâ IMPROVED experiments completed!\n",
      "üìÅ Results saved to: /home/muis/thesis/github-repo/master-thesis/timeseries/lstm/results_improved_lstm_nasa\n",
      "üìÅ Models saved to: /home/muis/thesis/github-repo/master-thesis/timeseries/lstm/models_improved_lstm_nasa\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
