{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-30T11:14:04.040769Z",
     "start_time": "2025-05-30T11:09:36.766597Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_pruning as tp\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_BASE_NAME = \"lstm_nasa\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# NASA Dataset preprocessing functions from your provided code\n",
    "column_names = ['unit_number', 'time_in_cycles'] + [f'op_setting_{i}' for i in range(1, 4)] + [f'sensor_{i}' for i in\n",
    "                                                                                               range(1, 24)]\n",
    "\n",
    "\n",
    "def load_dataframe(file_path: str) -> pd.DataFrame | None:\n",
    "    \"\"\"Loads a single CMaps data file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=' ', header=None, names=column_names)\n",
    "        # Drop the last two columns if they are all NaNs (often artifacts of space delimiter)\n",
    "        df.dropna(axis=1, how='all', inplace=True)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def clean_data(df: pd.DataFrame) -> list:\n",
    "    \"\"\"Identifies columns to remove based on low std dev.\"\"\"\n",
    "    if df is None:\n",
    "        return []\n",
    "    # Columns with std dev < 0.02 (potential candidates for removal)\n",
    "    cols_to_check = [col for col in df.columns if 'sensor' in col or 'op_setting' in col]\n",
    "    low_std_cols = [col for col in cols_to_check if df[col].std() < 0.02]\n",
    "    print(f\"Columns with std < 0.02 (potential removal): {low_std_cols}\")\n",
    "    return low_std_cols\n",
    "\n",
    "\n",
    "def add_rul(df: pd.DataFrame) -> pd.DataFrame | None:\n",
    "    \"\"\"Calculates and adds the Remaining Useful Life (RUL) column.\"\"\"\n",
    "    if df is None:\n",
    "        return None\n",
    "    max_cycles = df.groupby('unit_number')['time_in_cycles'].max().reset_index()\n",
    "    max_cycles.columns = ['unit_number', 'max_cycle']\n",
    "    df = df.merge(max_cycles, on='unit_number', how='left')\n",
    "    df['RUL'] = df['max_cycle'] - df['time_in_cycles']\n",
    "    df.drop(columns=['max_cycle'], inplace=True)\n",
    "    # Clip RUL (optional, common practice to limit max RUL)\n",
    "    df['RUL'] = df['RUL'].clip(upper=125)\n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize_data(df: pd.DataFrame,\n",
    "                   columns_to_normalize: List[str], scaler: MinMaxScaler = None) -> Tuple[pd.DataFrame, MinMaxScaler] | \\\n",
    "                                                                                    Tuple[None, None]:\n",
    "    \"\"\"Normalizes specified columns using MinMaxScaler.\"\"\"\n",
    "    if df is None:\n",
    "        return None, None\n",
    "    data_to_scale = df[columns_to_normalize]\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler()\n",
    "        df[columns_to_normalize] = scaler.fit_transform(data_to_scale)\n",
    "    else:\n",
    "        # Ensure only columns present in the scaler are transformed\n",
    "        valid_cols = [col for col in columns_to_normalize if col in scaler.feature_names_in_]\n",
    "        if len(valid_cols) < len(columns_to_normalize):\n",
    "            print(\"Warning: Some columns not found in the provided scaler. Skipping them.\")\n",
    "        if valid_cols:\n",
    "            df[valid_cols] = scaler.transform(df[valid_cols])\n",
    "    return df, scaler\n",
    "\n",
    "\n",
    "def prepare_cmapss_data(data_dir: str, train_file: str, test_file: str, test_rul_file: str) -> Tuple[\n",
    "    pd.DataFrame, pd.DataFrame, pd.DataFrame, MinMaxScaler, List[str]]:\n",
    "    \"\"\"Loads, cleans, preprocesses train/test data and RUL.\"\"\"\n",
    "    print(\"--- Preparing Training Data ---\")\n",
    "    train_df = load_dataframe(os.path.join(data_dir, train_file))\n",
    "    train_df = add_rul(train_df)\n",
    "\n",
    "    print(\"\\n--- Preparing Test Data ---\")\n",
    "    test_df = load_dataframe(os.path.join(data_dir, test_file))\n",
    "    test_rul_df = pd.read_csv(os.path.join(data_dir, test_rul_file), header=None, names=['RUL'])\n",
    "\n",
    "    # Clean Data - identify columns based on TRAINING data variance\n",
    "    cols_to_remove = clean_data(train_df)\n",
    "    feature_cols = [col for col in train_df.columns if\n",
    "                    col not in ['unit_number', 'time_in_cycles', 'RUL'] + cols_to_remove]\n",
    "    print(f\"\\nUsing Features: {feature_cols}\")\n",
    "\n",
    "    # Drop removed columns from both train and test\n",
    "    train_df.drop(columns=cols_to_remove, inplace=True, errors='ignore')\n",
    "    test_df.drop(columns=cols_to_remove, inplace=True, errors='ignore')\n",
    "\n",
    "    # Normalize features based on TRAINING data\n",
    "    print(\"\\n--- Normalizing Data ---\")\n",
    "    train_df_norm, scaler = normalize_data(train_df.copy(), feature_cols, scaler=None)\n",
    "    test_df_norm, _ = normalize_data(test_df.copy(), feature_cols, scaler=scaler)\n",
    "\n",
    "    return train_df_norm, test_df_norm, test_rul_df, scaler, feature_cols\n",
    "\n",
    "\n",
    "# Custom Dataset for NASA time series (LSTM version - no flattening)\n",
    "class NASALSTMDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, feature_cols: List[str], window_size: int = 50,\n",
    "                 stride: int = 1, is_test: bool = False, test_rul_df: pd.DataFrame = None):\n",
    "        self.df = df\n",
    "        self.feature_cols = feature_cols\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.is_test = is_test\n",
    "        self.test_rul_df = test_rul_df\n",
    "        self.samples = []\n",
    "        self.targets = []\n",
    "\n",
    "        self._prepare_samples()\n",
    "\n",
    "    def _prepare_samples(self):\n",
    "        \"\"\"Create windowed samples from the dataframe\"\"\"\n",
    "        units = self.df['unit_number'].unique()\n",
    "\n",
    "        for unit in units:\n",
    "            unit_df = self.df[self.df['unit_number'] == unit].sort_values('time_in_cycles')\n",
    "\n",
    "            # For test data, we only need the last window for each unit\n",
    "            if self.is_test:\n",
    "                if len(unit_df) >= self.window_size:\n",
    "                    window_data = unit_df[self.feature_cols].iloc[-self.window_size:].values\n",
    "                    self.samples.append(window_data)\n",
    "                    # Get RUL from test_rul_df\n",
    "                    if self.test_rul_df is not None:\n",
    "                        self.targets.append(self.test_rul_df.iloc[unit - 1]['RUL'])\n",
    "                else:\n",
    "                    # Pad if necessary\n",
    "                    window_data = unit_df[self.feature_cols].values\n",
    "                    padded = np.zeros((self.window_size, len(self.feature_cols)))\n",
    "                    padded[-len(window_data):] = window_data\n",
    "                    self.samples.append(padded)\n",
    "                    if self.test_rul_df is not None:\n",
    "                        self.targets.append(self.test_rul_df.iloc[unit - 1]['RUL'])\n",
    "            else:\n",
    "                # For training data, create multiple windows\n",
    "                for i in range(0, len(unit_df) - self.window_size + 1, self.stride):\n",
    "                    window_data = unit_df[self.feature_cols].iloc[i:i + self.window_size].values\n",
    "                    rul = unit_df['RUL'].iloc[i + self.window_size - 1]\n",
    "                    self.samples.append(window_data)\n",
    "                    self.targets.append(rul)\n",
    "\n",
    "        self.samples = np.array(self.samples, dtype=np.float32)\n",
    "        self.targets = np.array(self.targets, dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # For LSTM, keep the sequence shape (window_size, num_features)\n",
    "        sample = self.samples[idx]\n",
    "        target = self.targets[idx]\n",
    "        return torch.FloatTensor(sample), torch.FloatTensor([target])\n",
    "\n",
    "\n",
    "# LSTM Model Definition\n",
    "class NASALSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=100, num_layers=2, fc_hidden_sizes=[64, 32], dropout_rate=0.2):\n",
    "        super(NASALSTM, self).__init__()\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        # Fully connected layers after LSTM\n",
    "        fc_layers = []\n",
    "        prev_size = hidden_size\n",
    "\n",
    "        for fc_hidden_size in fc_hidden_sizes:\n",
    "            fc_layers.extend([\n",
    "                nn.Linear(prev_size, fc_hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = fc_hidden_size\n",
    "\n",
    "        # Output layer for regression\n",
    "        fc_layers.append(nn.Linear(prev_size, 1))\n",
    "\n",
    "        self.fc = nn.Sequential(*fc_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM forward pass\n",
    "        # x shape: (batch_size, sequence_length, input_size)\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "\n",
    "        # Use the last hidden state\n",
    "        # lstm_out shape: (batch_size, sequence_length, hidden_size)\n",
    "        # We take the last time step\n",
    "        last_hidden = lstm_out[:, -1, :]\n",
    "\n",
    "        # Pass through fully connected layers\n",
    "        output = self.fc(last_hidden)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def get_data_loaders(data_dir='./data/NASA', batch_size=128, window_size=50, val_split=0.2, seed=42):\n",
    "    \"\"\"Load NASA C-MAPSS dataset with train/val/test splits\"\"\"\n",
    "    print(f\"Loading NASA C-MAPSS dataset from: {data_dir}\")\n",
    "\n",
    "    # Prepare data\n",
    "    train_df, test_df, test_rul_df, scaler, feature_cols = prepare_cmapss_data(\n",
    "        data_dir,\n",
    "        'train_FD001.txt',\n",
    "        'test_FD001.txt',\n",
    "        'RUL_FD001.txt'\n",
    "    )\n",
    "\n",
    "    # Create full training dataset\n",
    "    full_train_dataset = NASALSTMDataset(train_df, feature_cols, window_size=window_size)\n",
    "\n",
    "    # Split into train and validation\n",
    "    val_size = int(len(full_train_dataset) * val_split)\n",
    "    train_size = len(full_train_dataset) - val_size\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_train_dataset, [train_size, val_size], generator=generator\n",
    "    )\n",
    "\n",
    "    # Create test dataset\n",
    "    test_dataset = NASALSTMDataset(test_df, feature_cols, window_size=window_size,\n",
    "                                   is_test=True, test_rul_df=test_rul_df)\n",
    "\n",
    "    # Create data loaders\n",
    "    num_workers = min(4, os.cpu_count() or 2)\n",
    "    pin_memory = True if DEVICE.type == 'cuda' else False\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=num_workers, pin_memory=pin_memory)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                            num_workers=num_workers, pin_memory=pin_memory)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                             num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    print(f\"DataLoaders created - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "    print(f\"Input shape: ({window_size}, {len(feature_cols)}) (sequence_length, num_features)\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader, len(feature_cols)\n",
    "\n",
    "\n",
    "def get_lstm_model(input_size, hidden_size=100, num_layers=2, fc_hidden_sizes=[64, 32], dropout_rate=0.2):\n",
    "    \"\"\"Get LSTM model for NASA dataset\"\"\"\n",
    "    model = NASALSTM(input_size, hidden_size, num_layers, fc_hidden_sizes, dropout_rate)\n",
    "    print(f\"✅ Created LSTM with architecture:\")\n",
    "    print(f\"   LSTM: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}\")\n",
    "    print(f\"   FC: {hidden_size} -> {' -> '.join(map(str, fc_hidden_sizes))} -> 1\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_ignored_layers(model):\n",
    "    \"\"\"Get layers to ignore during pruning (typically final layer)\"\"\"\n",
    "    ignored_layers = []\n",
    "\n",
    "    # Ignore the LSTM layer since torch_pruning only supports single-layer LSTMs\n",
    "    ignored_layers.append(model.lstm)\n",
    "\n",
    "    # Get the last linear layer in the fc sequential model\n",
    "    for module in model.fc:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            last_linear = module\n",
    "    ignored_layers.append(last_linear)\n",
    "\n",
    "    return ignored_layers\n",
    "\n",
    "\n",
    "def calculate_macs_params(model, example_input):\n",
    "    \"\"\"Calculate MACs and parameters using torch_pruning\"\"\"\n",
    "    model.eval()\n",
    "    target_device = example_input.device\n",
    "    model_on_device = model.to(target_device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        macs, params = tp.utils.count_ops_and_params(model_on_device, example_input)\n",
    "\n",
    "    return macs, params\n",
    "\n",
    "\n",
    "def save_model(model, save_path, example_input_cpu=None):\n",
    "    \"\"\"Save model state dict and optionally ONNX\"\"\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"✅ Model saved to {save_path}\")\n",
    "\n",
    "    if example_input_cpu is not None:\n",
    "        onnx_path = save_path.replace('.pth', '.onnx')\n",
    "        try:\n",
    "            model_cpu = model.to('cpu')\n",
    "            torch.onnx.export(\n",
    "                model_cpu, example_input_cpu, onnx_path,\n",
    "                export_params=True, opset_version=13,\n",
    "                input_names=['input'], output_names=['output'],\n",
    "                dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "            )\n",
    "            print(f\"✅ ONNX model saved to {onnx_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: ONNX export failed: {e}\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, example_input, criterion, device):\n",
    "    \"\"\"Evaluate model and return comprehensive metrics\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Calculate efficiency metrics\n",
    "    macs, params = calculate_macs_params(model, example_input.to(device))\n",
    "    model_size_mb = params * 4 / (1024 * 1024)  # Assuming float32\n",
    "\n",
    "    # Calculate MSE and MAE\n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            all_predictions.extend(output.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    mse = np.mean((all_predictions - all_targets) ** 2)\n",
    "    mae = np.mean(np.abs(all_predictions - all_targets))\n",
    "\n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'loss': total_loss / len(data_loader.dataset),\n",
    "        'macs': macs,\n",
    "        'params': params,\n",
    "        'size_mb': model_size_mb\n",
    "    }\n",
    "\n",
    "\n",
    "def prune_model(model, strategy_config, sparsity_ratio, example_input, ignored_layers=None):\n",
    "    \"\"\"Apply structured pruning to model\"\"\"\n",
    "    if sparsity_ratio == 0.0:\n",
    "        print(\"No pruning needed (sparsity = 0.0)\")\n",
    "        return model\n",
    "\n",
    "    model.eval()\n",
    "    pruned_model = copy.deepcopy(model)\n",
    "    pruned_model.to(example_input.device)\n",
    "\n",
    "    # Calculate initial MACs\n",
    "    initial_macs, _ = calculate_macs_params(pruned_model, example_input)\n",
    "    print(f\"Initial MACs: {initial_macs / 1e6:.2f}M\")\n",
    "\n",
    "    ignored_layers = ignored_layers or []\n",
    "\n",
    "    # Create pruner based on strategy\n",
    "    # Only prune Linear layers, not LSTM\n",
    "    pruner = strategy_config['pruner'](\n",
    "        pruned_model,\n",
    "        example_input,\n",
    "        importance=strategy_config['importance'],\n",
    "        iterative_steps=5,  # Use 5 iterative steps\n",
    "        ch_sparsity=sparsity_ratio,\n",
    "        root_module_types=[nn.Linear],  # Only prune Linear layers\n",
    "        ignored_layers=ignored_layers\n",
    "    )\n",
    "\n",
    "    print(f\"Applying {strategy_config['importance'].__class__.__name__} pruning at {sparsity_ratio:.1%} sparsity...\")\n",
    "    print(\"Note: Only pruning FC layers, LSTM layers are preserved\")\n",
    "\n",
    "    # Apply pruning\n",
    "    pruner.step()\n",
    "\n",
    "    # Calculate final MACs\n",
    "    final_macs, _ = calculate_macs_params(pruned_model, example_input)\n",
    "    reduction = (initial_macs - final_macs) / initial_macs * 100 if initial_macs > 0 else 0\n",
    "    print(f\"Final MACs: {final_macs / 1e6:.2f}M (Reduction: {reduction:.1f}%)\")\n",
    "\n",
    "    return pruned_model\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs,\n",
    "                val_loader=None, patience=7, log_prefix=\"\"):\n",
    "    \"\"\"Train model with early stopping\"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_mse': [],\n",
    "        'val_loss': [],\n",
    "        'val_mse': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_predictions = []\n",
    "        train_targets = []\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_predictions.extend(output.detach().cpu().numpy())\n",
    "            train_targets.extend(target.cpu().numpy())\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_mse = np.mean((np.array(train_predictions) - np.array(train_targets)) ** 2)\n",
    "\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_mse'].append(train_mse)\n",
    "\n",
    "        log_msg = f\"Epoch {epoch + 1}/{num_epochs} ({log_prefix}): Train Loss: {avg_train_loss:.4f}, Train MSE: {train_mse:.2f}\"\n",
    "\n",
    "        # Validation phase\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_predictions = []\n",
    "            val_targets = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for data, target in val_loader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                    val_predictions.extend(output.cpu().numpy())\n",
    "                    val_targets.extend(target.cpu().numpy())\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_mse = np.mean((np.array(val_predictions) - np.array(val_targets)) ** 2)\n",
    "\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            history['val_mse'].append(val_mse)\n",
    "\n",
    "            log_msg += f\", Val Loss: {avg_val_loss:.4f}, Val MSE: {val_mse:.2f}\"\n",
    "\n",
    "            # Early stopping check\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                epochs_no_improve = 0\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "                log_msg += \" (Best)\"\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"{log_msg}\")\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "                break\n",
    "        else:\n",
    "            history['val_loss'].append(None)\n",
    "            history['val_mse'].append(None)\n",
    "\n",
    "        print(log_msg)\n",
    "\n",
    "    # Load best model state if available\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"Loaded best model state\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def save_results_to_files(all_results, output_dir):\n",
    "    \"\"\"Save experimental results to JSON and CSV files\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save complete results as JSON\n",
    "    results_json_path = os.path.join(output_dir, 'complete_results.json')\n",
    "    with open(results_json_path, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2, default=str)\n",
    "    print(f\"✅ Complete results saved to {results_json_path}\")\n",
    "\n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for strategy, strategy_results in all_results.items():\n",
    "        for sparsity, metrics in strategy_results.items():\n",
    "            row = {\n",
    "                'strategy': strategy,\n",
    "                'sparsity_ratio': sparsity,\n",
    "                'mse': metrics['mse'],\n",
    "                'mae': metrics['mae'],\n",
    "                'loss': metrics['loss'],\n",
    "                'macs_millions': metrics['macs'] / 1e6,\n",
    "                'params_millions': metrics['params'] / 1e6,\n",
    "                'size_mb': metrics['size_mb']\n",
    "            }\n",
    "            summary_data.append(row)\n",
    "\n",
    "    # Save summary as CSV\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_csv_path = os.path.join(output_dir, 'summary_results.csv')\n",
    "    summary_df.to_csv(summary_csv_path, index=False)\n",
    "    print(f\"✅ Summary results saved to {summary_csv_path}\")\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "def create_results_plots(summary_df, output_dir):\n",
    "    \"\"\"Create visualization plots for regression metrics\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    strategies = summary_df['strategy'].unique()\n",
    "    sparsity_levels = sorted(summary_df['sparsity_ratio'].unique())\n",
    "\n",
    "    # Plot 1: MSE vs Sparsity\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for strategy in strategies:\n",
    "        strategy_data = summary_df[summary_df['strategy'] == strategy].sort_values('sparsity_ratio')\n",
    "        plt.plot(strategy_data['sparsity_ratio'] * 100, strategy_data['mse'],\n",
    "                 'o-', linewidth=2, markersize=8, label=strategy)\n",
    "\n",
    "    plt.xlabel('Sparsity (%)', fontsize=12)\n",
    "    plt.ylabel('MSE', fontsize=12)\n",
    "    plt.title('NASA LSTM: MSE vs Sparsity (FC Layers Only)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.gca().invert_yaxis()  # Lower MSE is better\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(output_dir, 'mse_vs_sparsity.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✅ MSE plot saved to {plot_path}\")\n",
    "\n",
    "    # Plot 2: Efficiency frontier (MSE vs MACs)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for strategy in strategies:\n",
    "        strategy_data = summary_df[summary_df['strategy'] == strategy].sort_values('sparsity_ratio')\n",
    "        plt.scatter(strategy_data['macs_millions'], strategy_data['mse'],\n",
    "                    s=100, label=strategy, alpha=0.8)\n",
    "        plt.plot(strategy_data['macs_millions'], strategy_data['mse'],\n",
    "                 '--', alpha=0.6)\n",
    "\n",
    "    plt.xlabel('MACs (Millions)', fontsize=12)\n",
    "    plt.ylabel('MSE', fontsize=12)\n",
    "    plt.title('NASA LSTM: Efficiency Frontier (MSE vs MACs)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.gca().invert_yaxis()  # Lower MSE is better\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(output_dir, 'efficiency_frontier.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✅ Efficiency frontier plot saved to {plot_path}\")\n",
    "\n",
    "\n",
    "def print_results_table(summary_df):\n",
    "    \"\"\"Print formatted results table\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXPERIMENTAL RESULTS SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Baseline results\n",
    "    baseline_results = summary_df[summary_df['sparsity_ratio'] == 0.0].iloc[0]\n",
    "    print(f\"\\nBaseline Performance:\")\n",
    "    print(f\"  MSE: {baseline_results['mse']:.2f}\")\n",
    "    print(f\"  MAE: {baseline_results['mae']:.2f}\")\n",
    "    print(f\"  MACs: {baseline_results['macs_millions']:.2f}M\")\n",
    "    print(f\"  Parameters: {baseline_results['params_millions']:.2f}M\")\n",
    "    print(f\"  Model Size: {baseline_results['size_mb']:.2f}MB\")\n",
    "\n",
    "    # Strategy comparison at 50% sparsity\n",
    "    print(f\"\\nStrategy Comparison at 50% Sparsity (FC Layers Only):\")\n",
    "    sparsity_50_data = summary_df[summary_df['sparsity_ratio'] == 0.5]\n",
    "    for _, row in sparsity_50_data.iterrows():\n",
    "        degradation = row['mse'] - baseline_results['mse']\n",
    "        relative_increase = (degradation / baseline_results['mse']) * 100\n",
    "        print(\n",
    "            f\"  {row['strategy']:>12}: MSE={row['mse']:>6.2f} ({degradation:>+5.2f}, {relative_increase:>+5.1f}% increase)\")\n",
    "\n",
    "    # Complete results table\n",
    "    print(f\"\\nComplete Results Table:\")\n",
    "    print(\"-\" * 90)\n",
    "    print(f\"{'Strategy':<12} {'Sparsity':<8} {'MSE':<8} {'MAE':<8} {'MACs(M)':<8} {'Params(M)':<9} {'Size(MB)':<8}\")\n",
    "    print(\"-\" * 90)\n",
    "\n",
    "    for _, row in summary_df.sort_values(['strategy', 'sparsity_ratio']).iterrows():\n",
    "        print(f\"{row['strategy']:<12} {row['sparsity_ratio'] * 100:>6.0f}% \"\n",
    "              f\"{row['mse']:>7.2f} {row['mae']:>7.2f} {row['macs_millions']:>7.2f} \"\n",
    "              f\"{row['params_millions']:>8.2f} {row['size_mb']:>7.2f}\")\n",
    "\n",
    "    print(\"\\nNote: Sparsity is applied only to FC layers. LSTM layers remain unchanged.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main experimental workflow\"\"\"\n",
    "    print(\"Starting NASA LSTM Pruning Experiments\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'strategies': {\n",
    "            'MagnitudeL2': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.MagnitudeImportance(p=2)\n",
    "            },\n",
    "            'Random': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.RandomImportance()\n",
    "            },\n",
    "            # Note: BNScale is not applicable to LSTM as it doesn't have BatchNorm layers\n",
    "        },\n",
    "        'pruning_ratios': [0.0, 0.2, 0.5, 0.7],\n",
    "        'hidden_size': 100,\n",
    "        'num_layers': 2,\n",
    "        'fc_hidden_sizes': [64, 32],\n",
    "        'dropout_rate': 0.2,\n",
    "        'window_size': 50,\n",
    "        'batch_size': 128,\n",
    "        'learning_rate': 0.0001,\n",
    "        'epochs': 1000,  # More epochs for time series\n",
    "        'patience': 20,\n",
    "        'output_dir': './results_lstm_nasa',\n",
    "        'models_dir': './models_lstm_nasa',\n",
    "        'data_dir': './data/CMaps'\n",
    "    }\n",
    "\n",
    "    # Create output directories\n",
    "    os.makedirs(config['output_dir'], exist_ok=True)\n",
    "    os.makedirs(config['models_dir'], exist_ok=True)\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading NASA C-MAPSS dataset...\")\n",
    "    train_loader, val_loader, test_loader, input_size = get_data_loaders(\n",
    "        data_dir=config['data_dir'],\n",
    "        batch_size=config['batch_size'],\n",
    "        window_size=config['window_size']\n",
    "    )\n",
    "\n",
    "    # Prepare inputs and criterion\n",
    "    example_input_cpu = torch.randn(1, config['window_size'], input_size)\n",
    "    example_input_device = example_input_cpu.to(DEVICE)\n",
    "    criterion = nn.MSELoss()  # Regression loss\n",
    "\n",
    "    # Get baseline model and train it\n",
    "    print(\"\\nCreating and training baseline model...\")\n",
    "    model = get_lstm_model(\n",
    "        input_size,\n",
    "        config['hidden_size'],\n",
    "        config['num_layers'],\n",
    "        config['fc_hidden_sizes'],\n",
    "        config['dropout_rate']\n",
    "    )\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Train baseline model\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    trained_model, training_history = train_model(\n",
    "        model, train_loader, criterion, optimizer, DEVICE,\n",
    "        config['epochs'], val_loader, config['patience'], \"Baseline Training\"\n",
    "    )\n",
    "\n",
    "    # Save baseline model\n",
    "    baseline_model_path = os.path.join(config['models_dir'], 'baseline_model.pth')\n",
    "    save_model(trained_model, baseline_model_path, example_input_cpu)\n",
    "\n",
    "    # Evaluate baseline\n",
    "    print(\"\\nEvaluating baseline model...\")\n",
    "    baseline_metrics = evaluate_model(trained_model, test_loader, example_input_device, criterion, DEVICE)\n",
    "    print(f\"Baseline Results: MSE={baseline_metrics['mse']:.2f}, \"\n",
    "          f\"MAE={baseline_metrics['mae']:.2f}, \"\n",
    "          f\"MACs={baseline_metrics['macs'] / 1e6:.2f}M, \"\n",
    "          f\"Params={baseline_metrics['params'] / 1e6:.2f}M\")\n",
    "\n",
    "    # Initialize results storage\n",
    "    all_results = {}\n",
    "    for strategy_name in config['strategies'].keys():\n",
    "        all_results[strategy_name] = {0.0: baseline_metrics}\n",
    "\n",
    "    # Get ignored layers\n",
    "    ignored_layers = get_ignored_layers(trained_model)\n",
    "\n",
    "    # Run pruning experiments\n",
    "    print(\"\\nStarting pruning experiments...\")\n",
    "    print(\"Note: Due to torch_pruning limitations, only FC layers will be pruned (LSTM layers preserved)\")\n",
    "\n",
    "    for strategy_name, strategy_config in config['strategies'].items():\n",
    "        print(f\"\\n--- Strategy: {strategy_name} ---\")\n",
    "\n",
    "        for sparsity_ratio in config['pruning_ratios']:\n",
    "            if sparsity_ratio == 0.0:\n",
    "                continue  # Skip baseline (already done)\n",
    "\n",
    "            print(f\"\\nProcessing {strategy_name} at {sparsity_ratio:.1%} sparsity...\")\n",
    "\n",
    "            # Load fresh copy of trained baseline\n",
    "            model_copy = get_lstm_model(\n",
    "                input_size,\n",
    "                config['hidden_size'],\n",
    "                config['num_layers'],\n",
    "                config['fc_hidden_sizes'],\n",
    "                config['dropout_rate']\n",
    "            )\n",
    "            model_copy.load_state_dict(torch.load(baseline_model_path, map_location=DEVICE))\n",
    "            model_copy.to(DEVICE)\n",
    "\n",
    "            # Apply pruning\n",
    "            pruned_model = prune_model(\n",
    "                model_copy, strategy_config, sparsity_ratio,\n",
    "                example_input_device, ignored_layers\n",
    "            )\n",
    "\n",
    "            # Fine-tune pruned model\n",
    "            print(\"Fine-tuning pruned model...\")\n",
    "            optimizer_ft = optim.Adam(pruned_model.parameters(), lr=config['learning_rate'])\n",
    "            fine_tuned_model, ft_history = train_model(\n",
    "                pruned_model, train_loader, criterion, optimizer_ft, DEVICE,\n",
    "                config['epochs'], val_loader, config['patience'],\n",
    "                f\"{strategy_name}-{sparsity_ratio:.1%}\"\n",
    "            )\n",
    "\n",
    "            # Evaluate fine-tuned model\n",
    "            final_metrics = evaluate_model(fine_tuned_model, test_loader, example_input_device, criterion, DEVICE)\n",
    "            all_results[strategy_name][sparsity_ratio] = final_metrics\n",
    "\n",
    "            print(f\"Results: MSE={final_metrics['mse']:.2f}, \"\n",
    "                  f\"MAE={final_metrics['mae']:.2f}, \"\n",
    "                  f\"MACs={final_metrics['macs'] / 1e6:.2f}M\")\n",
    "\n",
    "            # Save fine-tuned model\n",
    "            model_filename = f\"{strategy_name.lower()}_sparsity_{sparsity_ratio:.1f}.pth\"\n",
    "            model_path = os.path.join(config['models_dir'], model_filename)\n",
    "            save_model(fine_tuned_model, model_path, example_input_cpu)\n",
    "\n",
    "    # Save and analyze results\n",
    "    print(\"\\nSaving results...\")\n",
    "    summary_df = save_results_to_files(all_results, config['output_dir'])\n",
    "\n",
    "    # Create plots\n",
    "    print(\"Creating plots...\")\n",
    "    create_results_plots(summary_df, config['output_dir'])\n",
    "\n",
    "    # Print summary\n",
    "    print_results_table(summary_df)\n",
    "\n",
    "    print(f\"\\n🎉 All experiments completed!\")\n",
    "    print(f\"📁 Results saved to: {os.path.abspath(config['output_dir'])}\")\n",
    "    print(f\"📁 Models saved to: {os.path.abspath(config['models_dir'])}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting NASA LSTM Pruning Experiments\n",
      "============================================================\n",
      "Loading NASA C-MAPSS dataset...\n",
      "Loading NASA C-MAPSS dataset from: ./data/CMaps\n",
      "--- Preparing Training Data ---\n",
      "\n",
      "--- Preparing Test Data ---\n",
      "Columns with std < 0.02 (potential removal): ['op_setting_1', 'op_setting_2', 'op_setting_3', 'sensor_1', 'sensor_5', 'sensor_6', 'sensor_10', 'sensor_16', 'sensor_18', 'sensor_19']\n",
      "\n",
      "Using Features: ['sensor_2', 'sensor_3', 'sensor_4', 'sensor_7', 'sensor_8', 'sensor_9', 'sensor_11', 'sensor_12', 'sensor_13', 'sensor_14', 'sensor_15', 'sensor_17', 'sensor_20', 'sensor_21']\n",
      "\n",
      "--- Normalizing Data ---\n",
      "DataLoaders created - Train: 12585, Val: 3146, Test: 100\n",
      "Input shape: (50, 14) (sequence_length, num_features)\n",
      "\n",
      "Creating and training baseline model...\n",
      "✅ Created LSTM with architecture:\n",
      "   LSTM: input_size=14, hidden_size=100, num_layers=2\n",
      "   FC: 100 -> 64 -> 32 -> 1\n",
      "Epoch 1/1000 (Baseline Training): Train Loss: 7318.2809, Train MSE: 7316.14, Val Loss: 7437.0442, Val MSE: 7429.02 (Best)\n",
      "Epoch 2/1000 (Baseline Training): Train Loss: 6962.5041, Train MSE: 6966.33, Val Loss: 6846.0318, Val MSE: 6838.46 (Best)\n",
      "Epoch 3/1000 (Baseline Training): Train Loss: 6186.7068, Train MSE: 6194.98, Val Loss: 5847.4043, Val MSE: 5840.65 (Best)\n",
      "Epoch 4/1000 (Baseline Training): Train Loss: 5060.3206, Train MSE: 5069.41, Val Loss: 4549.1009, Val MSE: 4543.57 (Best)\n",
      "Epoch 5/1000 (Baseline Training): Train Loss: 3807.8962, Train MSE: 3809.65, Val Loss: 3262.2096, Val MSE: 3258.21 (Best)\n",
      "Epoch 6/1000 (Baseline Training): Train Loss: 2735.0793, Train MSE: 2742.08, Val Loss: 2334.7137, Val MSE: 2332.31 (Best)\n",
      "Epoch 7/1000 (Baseline Training): Train Loss: 2162.2189, Train MSE: 2159.03, Val Loss: 1892.6716, Val MSE: 1891.51 (Best)\n",
      "Epoch 8/1000 (Baseline Training): Train Loss: 1941.5370, Train MSE: 1941.65, Val Loss: 1754.3663, Val MSE: 1753.91 (Best)\n",
      "Epoch 9/1000 (Baseline Training): Train Loss: 1914.6896, Train MSE: 1915.53, Val Loss: 1720.8282, Val MSE: 1720.66 (Best)\n",
      "Epoch 10/1000 (Baseline Training): Train Loss: 1916.3630, Train MSE: 1913.71, Val Loss: 1713.9564, Val MSE: 1713.87 (Best)\n",
      "Epoch 11/1000 (Baseline Training): Train Loss: 1922.9365, Train MSE: 1919.32, Val Loss: 1713.4866, Val MSE: 1713.41 (Best)\n",
      "Epoch 12/1000 (Baseline Training): Train Loss: 1899.3907, Train MSE: 1897.34, Val Loss: 1710.4122, Val MSE: 1710.37 (Best)\n",
      "Epoch 13/1000 (Baseline Training): Train Loss: 1899.4199, Train MSE: 1898.74, Val Loss: 1710.6432, Val MSE: 1710.60\n",
      "Epoch 14/1000 (Baseline Training): Train Loss: 1910.7049, Train MSE: 1908.52, Val Loss: 1710.5493, Val MSE: 1710.51\n",
      "Epoch 15/1000 (Baseline Training): Train Loss: 1899.7105, Train MSE: 1900.48, Val Loss: 1711.2711, Val MSE: 1711.22\n",
      "Epoch 16/1000 (Baseline Training): Train Loss: 1895.0010, Train MSE: 1896.09, Val Loss: 1706.9790, Val MSE: 1706.99 (Best)\n",
      "Epoch 17/1000 (Baseline Training): Train Loss: 1893.1146, Train MSE: 1894.43, Val Loss: 1708.5687, Val MSE: 1708.56\n",
      "Epoch 18/1000 (Baseline Training): Train Loss: 1894.7548, Train MSE: 1893.88, Val Loss: 1710.0089, Val MSE: 1709.98\n",
      "Epoch 19/1000 (Baseline Training): Train Loss: 1903.8599, Train MSE: 1905.18, Val Loss: 1709.7949, Val MSE: 1709.77\n",
      "Epoch 20/1000 (Baseline Training): Train Loss: 1885.9857, Train MSE: 1886.42, Val Loss: 1708.1956, Val MSE: 1708.19\n",
      "Epoch 21/1000 (Baseline Training): Train Loss: 1889.2369, Train MSE: 1888.70, Val Loss: 1708.2317, Val MSE: 1708.22\n",
      "Epoch 22/1000 (Baseline Training): Train Loss: 1875.4221, Train MSE: 1873.85, Val Loss: 1708.5636, Val MSE: 1708.55\n",
      "Epoch 23/1000 (Baseline Training): Train Loss: 1881.0338, Train MSE: 1878.68, Val Loss: 1707.2526, Val MSE: 1707.26\n",
      "Epoch 24/1000 (Baseline Training): Train Loss: 1900.5464, Train MSE: 1902.17, Val Loss: 1711.5588, Val MSE: 1711.50\n",
      "Epoch 25/1000 (Baseline Training): Train Loss: 1875.5735, Train MSE: 1874.27, Val Loss: 1560.4902, Val MSE: 1560.72 (Best)\n",
      "Epoch 26/1000 (Baseline Training): Train Loss: 1555.4122, Train MSE: 1555.57, Val Loss: 1251.9524, Val MSE: 1250.86 (Best)\n",
      "Epoch 27/1000 (Baseline Training): Train Loss: 1167.2971, Train MSE: 1167.80, Val Loss: 697.1803, Val MSE: 697.43 (Best)\n",
      "Epoch 28/1000 (Baseline Training): Train Loss: 816.7718, Train MSE: 816.41, Val Loss: 693.1316, Val MSE: 690.48 (Best)\n",
      "Epoch 29/1000 (Baseline Training): Train Loss: 706.6368, Train MSE: 707.20, Val Loss: 548.9544, Val MSE: 549.50 (Best)\n",
      "Epoch 30/1000 (Baseline Training): Train Loss: 632.8920, Train MSE: 632.61, Val Loss: 304.6646, Val MSE: 304.64 (Best)\n",
      "Epoch 31/1000 (Baseline Training): Train Loss: 528.0048, Train MSE: 526.60, Val Loss: 243.4888, Val MSE: 243.60 (Best)\n",
      "Epoch 32/1000 (Baseline Training): Train Loss: 506.3920, Train MSE: 507.17, Val Loss: 233.4716, Val MSE: 233.17 (Best)\n",
      "Epoch 33/1000 (Baseline Training): Train Loss: 483.4999, Train MSE: 484.23, Val Loss: 254.5894, Val MSE: 254.52\n",
      "Epoch 34/1000 (Baseline Training): Train Loss: 453.1814, Train MSE: 452.73, Val Loss: 208.3698, Val MSE: 208.43 (Best)\n",
      "Epoch 35/1000 (Baseline Training): Train Loss: 463.0733, Train MSE: 464.17, Val Loss: 260.9304, Val MSE: 261.15\n",
      "Epoch 36/1000 (Baseline Training): Train Loss: 451.6867, Train MSE: 452.02, Val Loss: 200.2957, Val MSE: 200.19 (Best)\n",
      "Epoch 37/1000 (Baseline Training): Train Loss: 428.7141, Train MSE: 428.35, Val Loss: 209.4201, Val MSE: 209.83\n",
      "Epoch 38/1000 (Baseline Training): Train Loss: 434.3859, Train MSE: 434.35, Val Loss: 222.1322, Val MSE: 221.90\n",
      "Epoch 39/1000 (Baseline Training): Train Loss: 428.2922, Train MSE: 427.80, Val Loss: 193.0907, Val MSE: 193.19 (Best)\n",
      "Epoch 40/1000 (Baseline Training): Train Loss: 432.7612, Train MSE: 433.21, Val Loss: 189.6617, Val MSE: 189.41 (Best)\n",
      "Epoch 41/1000 (Baseline Training): Train Loss: 429.2838, Train MSE: 429.58, Val Loss: 193.4197, Val MSE: 193.16\n",
      "Epoch 42/1000 (Baseline Training): Train Loss: 417.3095, Train MSE: 418.89, Val Loss: 205.2205, Val MSE: 204.63\n",
      "Epoch 43/1000 (Baseline Training): Train Loss: 414.0359, Train MSE: 414.95, Val Loss: 172.9475, Val MSE: 172.82 (Best)\n",
      "Epoch 44/1000 (Baseline Training): Train Loss: 397.9712, Train MSE: 397.92, Val Loss: 170.7726, Val MSE: 170.51 (Best)\n",
      "Epoch 45/1000 (Baseline Training): Train Loss: 403.9344, Train MSE: 402.13, Val Loss: 171.6684, Val MSE: 171.42\n",
      "Epoch 46/1000 (Baseline Training): Train Loss: 408.8375, Train MSE: 409.30, Val Loss: 177.3409, Val MSE: 177.50\n",
      "Epoch 47/1000 (Baseline Training): Train Loss: 386.9522, Train MSE: 386.78, Val Loss: 232.6593, Val MSE: 232.09\n",
      "Epoch 48/1000 (Baseline Training): Train Loss: 388.3909, Train MSE: 388.27, Val Loss: 173.0189, Val MSE: 172.55\n",
      "Epoch 49/1000 (Baseline Training): Train Loss: 396.9195, Train MSE: 396.80, Val Loss: 171.2455, Val MSE: 171.09\n",
      "Epoch 50/1000 (Baseline Training): Train Loss: 394.7001, Train MSE: 393.47, Val Loss: 203.8277, Val MSE: 203.06\n",
      "Epoch 51/1000 (Baseline Training): Train Loss: 379.7820, Train MSE: 380.57, Val Loss: 186.8617, Val MSE: 186.42\n",
      "Epoch 52/1000 (Baseline Training): Train Loss: 378.2340, Train MSE: 378.32, Val Loss: 161.6197, Val MSE: 161.39 (Best)\n",
      "Epoch 53/1000 (Baseline Training): Train Loss: 382.2615, Train MSE: 381.85, Val Loss: 167.2363, Val MSE: 167.38\n",
      "Epoch 54/1000 (Baseline Training): Train Loss: 383.0952, Train MSE: 382.55, Val Loss: 167.1932, Val MSE: 167.31\n",
      "Epoch 55/1000 (Baseline Training): Train Loss: 386.3193, Train MSE: 386.31, Val Loss: 163.8515, Val MSE: 163.91\n",
      "Epoch 56/1000 (Baseline Training): Train Loss: 371.5190, Train MSE: 370.42, Val Loss: 166.5064, Val MSE: 166.05\n",
      "Epoch 57/1000 (Baseline Training): Train Loss: 378.5823, Train MSE: 379.63, Val Loss: 180.2432, Val MSE: 180.32\n",
      "Epoch 58/1000 (Baseline Training): Train Loss: 369.6522, Train MSE: 370.09, Val Loss: 155.2256, Val MSE: 155.19 (Best)\n",
      "Epoch 59/1000 (Baseline Training): Train Loss: 370.0861, Train MSE: 371.02, Val Loss: 154.8535, Val MSE: 154.71 (Best)\n",
      "Epoch 60/1000 (Baseline Training): Train Loss: 376.5740, Train MSE: 377.48, Val Loss: 163.2860, Val MSE: 162.85\n",
      "Epoch 61/1000 (Baseline Training): Train Loss: 377.9921, Train MSE: 377.74, Val Loss: 172.3348, Val MSE: 172.17\n",
      "Epoch 62/1000 (Baseline Training): Train Loss: 358.9673, Train MSE: 359.43, Val Loss: 156.4023, Val MSE: 156.07\n",
      "Epoch 63/1000 (Baseline Training): Train Loss: 357.8109, Train MSE: 357.57, Val Loss: 205.1129, Val MSE: 204.75\n",
      "Epoch 64/1000 (Baseline Training): Train Loss: 366.1980, Train MSE: 367.39, Val Loss: 156.1064, Val MSE: 155.93\n",
      "Epoch 65/1000 (Baseline Training): Train Loss: 368.1234, Train MSE: 366.67, Val Loss: 160.8338, Val MSE: 161.06\n",
      "Epoch 66/1000 (Baseline Training): Train Loss: 357.8181, Train MSE: 357.45, Val Loss: 150.9393, Val MSE: 150.71 (Best)\n",
      "Epoch 67/1000 (Baseline Training): Train Loss: 355.0450, Train MSE: 354.16, Val Loss: 152.5312, Val MSE: 152.74\n",
      "Epoch 68/1000 (Baseline Training): Train Loss: 362.6718, Train MSE: 362.24, Val Loss: 150.7122, Val MSE: 150.83 (Best)\n",
      "Epoch 69/1000 (Baseline Training): Train Loss: 354.3277, Train MSE: 354.65, Val Loss: 145.8164, Val MSE: 145.67 (Best)\n",
      "Epoch 70/1000 (Baseline Training): Train Loss: 359.1695, Train MSE: 358.80, Val Loss: 156.8804, Val MSE: 156.54\n",
      "Epoch 71/1000 (Baseline Training): Train Loss: 348.4831, Train MSE: 348.94, Val Loss: 155.2912, Val MSE: 154.97\n",
      "Epoch 72/1000 (Baseline Training): Train Loss: 350.4759, Train MSE: 350.20, Val Loss: 219.5255, Val MSE: 219.17\n",
      "Epoch 73/1000 (Baseline Training): Train Loss: 354.2544, Train MSE: 353.57, Val Loss: 164.9563, Val MSE: 164.71\n",
      "Epoch 74/1000 (Baseline Training): Train Loss: 345.2571, Train MSE: 344.61, Val Loss: 146.5172, Val MSE: 146.34\n",
      "Epoch 75/1000 (Baseline Training): Train Loss: 342.1144, Train MSE: 342.77, Val Loss: 151.6596, Val MSE: 151.72\n",
      "Epoch 76/1000 (Baseline Training): Train Loss: 351.4220, Train MSE: 349.45, Val Loss: 170.3380, Val MSE: 169.97\n",
      "Epoch 77/1000 (Baseline Training): Train Loss: 344.5464, Train MSE: 345.52, Val Loss: 141.1944, Val MSE: 141.16 (Best)\n",
      "Epoch 78/1000 (Baseline Training): Train Loss: 344.7674, Train MSE: 344.97, Val Loss: 142.9654, Val MSE: 142.93\n",
      "Epoch 79/1000 (Baseline Training): Train Loss: 339.2663, Train MSE: 338.89, Val Loss: 145.2986, Val MSE: 145.05\n",
      "Epoch 80/1000 (Baseline Training): Train Loss: 342.7008, Train MSE: 341.85, Val Loss: 141.0947, Val MSE: 141.02 (Best)\n",
      "Epoch 81/1000 (Baseline Training): Train Loss: 345.1466, Train MSE: 344.75, Val Loss: 144.7468, Val MSE: 144.77\n",
      "Epoch 82/1000 (Baseline Training): Train Loss: 344.4766, Train MSE: 344.26, Val Loss: 147.9576, Val MSE: 147.94\n",
      "Epoch 83/1000 (Baseline Training): Train Loss: 341.4981, Train MSE: 341.48, Val Loss: 144.1773, Val MSE: 144.18\n",
      "Epoch 84/1000 (Baseline Training): Train Loss: 343.5252, Train MSE: 343.49, Val Loss: 142.0863, Val MSE: 141.92\n",
      "Epoch 85/1000 (Baseline Training): Train Loss: 335.4787, Train MSE: 335.83, Val Loss: 151.2049, Val MSE: 151.29\n",
      "Epoch 86/1000 (Baseline Training): Train Loss: 346.6549, Train MSE: 347.18, Val Loss: 161.9910, Val MSE: 161.90\n",
      "Epoch 87/1000 (Baseline Training): Train Loss: 338.6588, Train MSE: 338.52, Val Loss: 173.0801, Val MSE: 173.15\n",
      "Epoch 88/1000 (Baseline Training): Train Loss: 333.4336, Train MSE: 333.37, Val Loss: 147.4155, Val MSE: 147.21\n",
      "Epoch 89/1000 (Baseline Training): Train Loss: 339.8636, Train MSE: 340.54, Val Loss: 140.5869, Val MSE: 140.52 (Best)\n",
      "Epoch 90/1000 (Baseline Training): Train Loss: 335.4230, Train MSE: 334.28, Val Loss: 143.0103, Val MSE: 143.02\n",
      "Epoch 91/1000 (Baseline Training): Train Loss: 330.9597, Train MSE: 331.30, Val Loss: 139.3718, Val MSE: 139.30 (Best)\n",
      "Epoch 92/1000 (Baseline Training): Train Loss: 329.5123, Train MSE: 330.10, Val Loss: 142.1644, Val MSE: 141.87\n",
      "Epoch 93/1000 (Baseline Training): Train Loss: 340.8464, Train MSE: 340.81, Val Loss: 138.0177, Val MSE: 138.13 (Best)\n",
      "Epoch 94/1000 (Baseline Training): Train Loss: 323.0059, Train MSE: 322.48, Val Loss: 166.8331, Val MSE: 166.66\n",
      "Epoch 95/1000 (Baseline Training): Train Loss: 328.3784, Train MSE: 329.45, Val Loss: 139.3330, Val MSE: 139.43\n",
      "Epoch 96/1000 (Baseline Training): Train Loss: 339.5600, Train MSE: 339.08, Val Loss: 156.2520, Val MSE: 156.54\n",
      "Epoch 97/1000 (Baseline Training): Train Loss: 337.1804, Train MSE: 337.01, Val Loss: 141.9456, Val MSE: 142.02\n",
      "Epoch 98/1000 (Baseline Training): Train Loss: 318.6959, Train MSE: 319.37, Val Loss: 137.1538, Val MSE: 137.34 (Best)\n",
      "Epoch 99/1000 (Baseline Training): Train Loss: 329.6191, Train MSE: 329.44, Val Loss: 146.0962, Val MSE: 145.93\n",
      "Epoch 100/1000 (Baseline Training): Train Loss: 335.1917, Train MSE: 333.71, Val Loss: 141.9750, Val MSE: 141.88\n",
      "Epoch 101/1000 (Baseline Training): Train Loss: 329.9771, Train MSE: 329.16, Val Loss: 136.4119, Val MSE: 136.57 (Best)\n",
      "Epoch 102/1000 (Baseline Training): Train Loss: 323.3355, Train MSE: 323.86, Val Loss: 141.6874, Val MSE: 142.03\n",
      "Epoch 103/1000 (Baseline Training): Train Loss: 326.8611, Train MSE: 326.06, Val Loss: 137.7934, Val MSE: 137.88\n",
      "Epoch 104/1000 (Baseline Training): Train Loss: 332.5442, Train MSE: 331.56, Val Loss: 133.8641, Val MSE: 133.79 (Best)\n",
      "Epoch 105/1000 (Baseline Training): Train Loss: 332.6559, Train MSE: 333.23, Val Loss: 146.1328, Val MSE: 145.93\n",
      "Epoch 106/1000 (Baseline Training): Train Loss: 331.0581, Train MSE: 331.24, Val Loss: 146.1724, Val MSE: 146.01\n",
      "Epoch 107/1000 (Baseline Training): Train Loss: 329.8390, Train MSE: 330.26, Val Loss: 145.3391, Val MSE: 145.10\n",
      "Epoch 108/1000 (Baseline Training): Train Loss: 324.7666, Train MSE: 325.98, Val Loss: 139.6771, Val MSE: 139.99\n",
      "Epoch 109/1000 (Baseline Training): Train Loss: 319.6763, Train MSE: 320.23, Val Loss: 134.5781, Val MSE: 134.43\n",
      "Epoch 110/1000 (Baseline Training): Train Loss: 322.9688, Train MSE: 323.06, Val Loss: 147.8755, Val MSE: 147.56\n",
      "Epoch 111/1000 (Baseline Training): Train Loss: 328.3270, Train MSE: 328.01, Val Loss: 135.1308, Val MSE: 135.16\n",
      "Epoch 112/1000 (Baseline Training): Train Loss: 316.7526, Train MSE: 317.32, Val Loss: 166.5321, Val MSE: 166.36\n",
      "Epoch 113/1000 (Baseline Training): Train Loss: 325.3273, Train MSE: 324.29, Val Loss: 145.2465, Val MSE: 145.30\n",
      "Epoch 114/1000 (Baseline Training): Train Loss: 328.6150, Train MSE: 326.84, Val Loss: 132.8986, Val MSE: 132.79 (Best)\n",
      "Epoch 115/1000 (Baseline Training): Train Loss: 322.6216, Train MSE: 322.47, Val Loss: 151.5224, Val MSE: 151.76\n",
      "Epoch 116/1000 (Baseline Training): Train Loss: 315.0049, Train MSE: 314.98, Val Loss: 155.1414, Val MSE: 155.04\n",
      "Epoch 117/1000 (Baseline Training): Train Loss: 332.3158, Train MSE: 332.27, Val Loss: 169.9290, Val MSE: 169.50\n",
      "Epoch 118/1000 (Baseline Training): Train Loss: 320.4186, Train MSE: 319.48, Val Loss: 149.9919, Val MSE: 149.74\n",
      "Epoch 119/1000 (Baseline Training): Train Loss: 330.5095, Train MSE: 330.68, Val Loss: 133.5017, Val MSE: 133.45\n",
      "Epoch 120/1000 (Baseline Training): Train Loss: 322.0483, Train MSE: 321.82, Val Loss: 156.0009, Val MSE: 156.29\n",
      "Epoch 121/1000 (Baseline Training): Train Loss: 327.8911, Train MSE: 327.51, Val Loss: 185.6585, Val MSE: 185.65\n",
      "Epoch 122/1000 (Baseline Training): Train Loss: 318.4456, Train MSE: 318.43, Val Loss: 142.2152, Val MSE: 142.18\n",
      "Epoch 123/1000 (Baseline Training): Train Loss: 325.1971, Train MSE: 325.59, Val Loss: 133.5357, Val MSE: 133.83\n",
      "Epoch 124/1000 (Baseline Training): Train Loss: 328.1357, Train MSE: 328.60, Val Loss: 133.7823, Val MSE: 133.79\n",
      "Epoch 125/1000 (Baseline Training): Train Loss: 311.8743, Train MSE: 311.59, Val Loss: 133.8557, Val MSE: 133.82\n",
      "Epoch 126/1000 (Baseline Training): Train Loss: 321.5347, Train MSE: 320.40, Val Loss: 127.4078, Val MSE: 127.47 (Best)\n",
      "Epoch 127/1000 (Baseline Training): Train Loss: 321.9582, Train MSE: 321.13, Val Loss: 136.1073, Val MSE: 136.11\n",
      "Epoch 128/1000 (Baseline Training): Train Loss: 319.5646, Train MSE: 319.64, Val Loss: 125.4362, Val MSE: 125.50 (Best)\n",
      "Epoch 129/1000 (Baseline Training): Train Loss: 318.4655, Train MSE: 319.18, Val Loss: 132.6373, Val MSE: 132.74\n",
      "Epoch 130/1000 (Baseline Training): Train Loss: 317.9833, Train MSE: 317.16, Val Loss: 140.6493, Val MSE: 140.72\n",
      "Epoch 131/1000 (Baseline Training): Train Loss: 329.3474, Train MSE: 329.97, Val Loss: 132.7204, Val MSE: 132.89\n",
      "Epoch 132/1000 (Baseline Training): Train Loss: 325.6166, Train MSE: 326.28, Val Loss: 148.6113, Val MSE: 148.77\n",
      "Epoch 133/1000 (Baseline Training): Train Loss: 319.3363, Train MSE: 318.66, Val Loss: 147.5150, Val MSE: 147.66\n",
      "Epoch 134/1000 (Baseline Training): Train Loss: 311.4928, Train MSE: 311.74, Val Loss: 160.4000, Val MSE: 160.47\n",
      "Epoch 135/1000 (Baseline Training): Train Loss: 306.9194, Train MSE: 307.45, Val Loss: 141.0769, Val MSE: 141.18\n",
      "Epoch 136/1000 (Baseline Training): Train Loss: 312.4555, Train MSE: 313.00, Val Loss: 141.5111, Val MSE: 141.90\n",
      "Epoch 137/1000 (Baseline Training): Train Loss: 318.7505, Train MSE: 318.55, Val Loss: 151.8648, Val MSE: 151.76\n",
      "Epoch 138/1000 (Baseline Training): Train Loss: 321.9890, Train MSE: 321.39, Val Loss: 135.5613, Val MSE: 135.44\n",
      "Epoch 139/1000 (Baseline Training): Train Loss: 318.1331, Train MSE: 318.30, Val Loss: 134.1539, Val MSE: 134.22\n",
      "Epoch 140/1000 (Baseline Training): Train Loss: 318.2861, Train MSE: 318.25, Val Loss: 157.2052, Val MSE: 157.55\n",
      "Epoch 141/1000 (Baseline Training): Train Loss: 322.8031, Train MSE: 323.14, Val Loss: 146.2074, Val MSE: 146.09\n",
      "Epoch 142/1000 (Baseline Training): Train Loss: 320.4517, Train MSE: 320.20, Val Loss: 133.2385, Val MSE: 133.41\n",
      "Epoch 143/1000 (Baseline Training): Train Loss: 319.4997, Train MSE: 320.27, Val Loss: 128.7916, Val MSE: 128.81\n",
      "Epoch 144/1000 (Baseline Training): Train Loss: 321.5376, Train MSE: 322.00, Val Loss: 129.3652, Val MSE: 129.44\n",
      "Epoch 145/1000 (Baseline Training): Train Loss: 308.3103, Train MSE: 308.53, Val Loss: 130.4909, Val MSE: 130.53\n",
      "Epoch 146/1000 (Baseline Training): Train Loss: 311.9159, Train MSE: 312.19, Val Loss: 149.9944, Val MSE: 150.00\n",
      "Epoch 147/1000 (Baseline Training): Train Loss: 315.8903, Train MSE: 316.05, Val Loss: 126.5786, Val MSE: 126.48\n",
      "Epoch 148/1000 (Baseline Training): Train Loss: 312.9281, Train MSE: 313.51, Val Loss: 165.9320, Val MSE: 165.99\n",
      "Early stopping triggered after 148 epochs\n",
      "Loaded best model state\n",
      "✅ Model saved to ./models_lstm_nasa/baseline_model.pth\n",
      "✅ ONNX model saved to ./models_lstm_nasa/baseline_model.onnx\n",
      "\n",
      "Evaluating baseline model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/onnx/symbolic_opset9.py:4277: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n",
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
      "  result = _VF.lstm(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Results: MSE=205.94, MAE=10.50, MACs=90.45M, Params=0.14M\n",
      "\n",
      "Starting pruning experiments...\n",
      "Note: Due to torch_pruning limitations, only FC layers will be pruned (LSTM layers preserved)\n",
      "\n",
      "--- Strategy: MagnitudeL2 ---\n",
      "\n",
      "Processing MagnitudeL2 at 20.0% sparsity...\n",
      "✅ Created LSTM with architecture:\n",
      "   LSTM: input_size=14, hidden_size=100, num_layers=2\n",
      "   FC: 100 -> 64 -> 32 -> 1\n",
      "Initial MACs: 90.45M\n",
      "Applying MagnitudeImportance pruning at 20.0% sparsity...\n",
      "Note: Only pruning FC layers, LSTM layers are preserved\n",
      "Final MACs: 90.45M (Reduction: 0.0%)\n",
      "Fine-tuning pruned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch_pruning/pruner/algorithms/base_pruner.py:87: UserWarning: ch_sparsity is deprecated in v1.3.0. Please use pruning_ratio.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 (MagnitudeL2-20.0%): Train Loss: 320.8533, Train MSE: 321.48, Val Loss: 127.4979, Val MSE: 127.70 (Best)\n",
      "Epoch 2/1000 (MagnitudeL2-20.0%): Train Loss: 326.8225, Train MSE: 327.41, Val Loss: 134.5940, Val MSE: 134.82\n",
      "Epoch 3/1000 (MagnitudeL2-20.0%): Train Loss: 327.7182, Train MSE: 328.15, Val Loss: 165.8171, Val MSE: 165.68\n",
      "Epoch 4/1000 (MagnitudeL2-20.0%): Train Loss: 318.7881, Train MSE: 318.59, Val Loss: 147.8816, Val MSE: 147.97\n",
      "Epoch 5/1000 (MagnitudeL2-20.0%): Train Loss: 313.9571, Train MSE: 314.50, Val Loss: 138.1769, Val MSE: 137.98\n",
      "Epoch 6/1000 (MagnitudeL2-20.0%): Train Loss: 319.8437, Train MSE: 319.51, Val Loss: 136.4381, Val MSE: 136.70\n",
      "Epoch 7/1000 (MagnitudeL2-20.0%): Train Loss: 329.2139, Train MSE: 329.45, Val Loss: 131.1303, Val MSE: 131.07\n",
      "Epoch 8/1000 (MagnitudeL2-20.0%): Train Loss: 317.8225, Train MSE: 317.75, Val Loss: 137.5211, Val MSE: 137.27\n",
      "Epoch 9/1000 (MagnitudeL2-20.0%): Train Loss: 308.6468, Train MSE: 307.95, Val Loss: 129.9778, Val MSE: 130.10\n",
      "Epoch 10/1000 (MagnitudeL2-20.0%): Train Loss: 318.8413, Train MSE: 317.89, Val Loss: 129.1181, Val MSE: 129.22\n",
      "Epoch 11/1000 (MagnitudeL2-20.0%): Train Loss: 318.5472, Train MSE: 318.95, Val Loss: 144.2738, Val MSE: 144.75\n",
      "Epoch 12/1000 (MagnitudeL2-20.0%): Train Loss: 321.6223, Train MSE: 321.96, Val Loss: 127.2697, Val MSE: 127.25 (Best)\n",
      "Epoch 13/1000 (MagnitudeL2-20.0%): Train Loss: 321.7083, Train MSE: 322.44, Val Loss: 132.9655, Val MSE: 132.77\n",
      "Epoch 14/1000 (MagnitudeL2-20.0%): Train Loss: 312.1743, Train MSE: 312.25, Val Loss: 127.0547, Val MSE: 127.21 (Best)\n",
      "Epoch 15/1000 (MagnitudeL2-20.0%): Train Loss: 321.2957, Train MSE: 321.40, Val Loss: 151.2917, Val MSE: 151.51\n",
      "Epoch 16/1000 (MagnitudeL2-20.0%): Train Loss: 315.2954, Train MSE: 314.94, Val Loss: 124.6883, Val MSE: 124.80 (Best)\n",
      "Epoch 17/1000 (MagnitudeL2-20.0%): Train Loss: 310.3916, Train MSE: 310.57, Val Loss: 132.7154, Val MSE: 132.66\n",
      "Epoch 18/1000 (MagnitudeL2-20.0%): Train Loss: 308.4639, Train MSE: 308.38, Val Loss: 134.3103, Val MSE: 134.40\n",
      "Epoch 19/1000 (MagnitudeL2-20.0%): Train Loss: 312.7637, Train MSE: 313.50, Val Loss: 139.2520, Val MSE: 139.07\n",
      "Epoch 20/1000 (MagnitudeL2-20.0%): Train Loss: 317.0664, Train MSE: 317.51, Val Loss: 129.7343, Val MSE: 129.98\n",
      "Epoch 21/1000 (MagnitudeL2-20.0%): Train Loss: 308.7536, Train MSE: 309.06, Val Loss: 154.6793, Val MSE: 154.66\n",
      "Epoch 22/1000 (MagnitudeL2-20.0%): Train Loss: 321.5421, Train MSE: 322.06, Val Loss: 125.8033, Val MSE: 125.90\n",
      "Epoch 23/1000 (MagnitudeL2-20.0%): Train Loss: 320.8868, Train MSE: 320.24, Val Loss: 139.2171, Val MSE: 139.47\n",
      "Epoch 24/1000 (MagnitudeL2-20.0%): Train Loss: 314.8041, Train MSE: 314.29, Val Loss: 138.6268, Val MSE: 138.63\n",
      "Epoch 25/1000 (MagnitudeL2-20.0%): Train Loss: 322.2776, Train MSE: 322.78, Val Loss: 153.7440, Val MSE: 153.78\n",
      "Epoch 26/1000 (MagnitudeL2-20.0%): Train Loss: 307.6509, Train MSE: 308.34, Val Loss: 131.7036, Val MSE: 131.84\n",
      "Epoch 27/1000 (MagnitudeL2-20.0%): Train Loss: 317.2271, Train MSE: 317.34, Val Loss: 126.4834, Val MSE: 126.65\n",
      "Epoch 28/1000 (MagnitudeL2-20.0%): Train Loss: 314.5207, Train MSE: 315.35, Val Loss: 152.5630, Val MSE: 152.66\n",
      "Epoch 29/1000 (MagnitudeL2-20.0%): Train Loss: 320.5206, Train MSE: 321.01, Val Loss: 129.7325, Val MSE: 129.88\n",
      "Epoch 30/1000 (MagnitudeL2-20.0%): Train Loss: 312.1935, Train MSE: 313.37, Val Loss: 155.2157, Val MSE: 155.59\n",
      "Epoch 31/1000 (MagnitudeL2-20.0%): Train Loss: 317.7138, Train MSE: 317.55, Val Loss: 131.5635, Val MSE: 131.56\n",
      "Epoch 32/1000 (MagnitudeL2-20.0%): Train Loss: 315.1004, Train MSE: 314.84, Val Loss: 143.3474, Val MSE: 143.35\n",
      "Epoch 33/1000 (MagnitudeL2-20.0%): Train Loss: 310.3602, Train MSE: 310.57, Val Loss: 148.9957, Val MSE: 149.30\n",
      "Epoch 34/1000 (MagnitudeL2-20.0%): Train Loss: 307.4586, Train MSE: 307.64, Val Loss: 137.0745, Val MSE: 136.72\n",
      "Epoch 35/1000 (MagnitudeL2-20.0%): Train Loss: 306.8553, Train MSE: 307.05, Val Loss: 125.9557, Val MSE: 126.31\n",
      "Epoch 36/1000 (MagnitudeL2-20.0%): Train Loss: 322.4230, Train MSE: 320.92, Val Loss: 129.7767, Val MSE: 129.71\n",
      "Early stopping triggered after 36 epochs\n",
      "Loaded best model state\n",
      "Results: MSE=207.10, MAE=10.54, MACs=90.45M\n",
      "✅ Model saved to ./models_lstm_nasa/magnitudel2_sparsity_0.2.pth\n",
      "✅ ONNX model saved to ./models_lstm_nasa/magnitudel2_sparsity_0.2.onnx\n",
      "\n",
      "Processing MagnitudeL2 at 50.0% sparsity...\n",
      "✅ Created LSTM with architecture:\n",
      "   LSTM: input_size=14, hidden_size=100, num_layers=2\n",
      "   FC: 100 -> 64 -> 32 -> 1\n",
      "Initial MACs: 90.45M\n",
      "Applying MagnitudeImportance pruning at 50.0% sparsity...\n",
      "Note: Only pruning FC layers, LSTM layers are preserved\n",
      "Final MACs: 90.45M (Reduction: 0.0%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/1000 (MagnitudeL2-50.0%): Train Loss: 319.6800, Train MSE: 320.38, Val Loss: 132.5396, Val MSE: 132.50 (Best)\n",
      "Epoch 2/1000 (MagnitudeL2-50.0%): Train Loss: 317.0449, Train MSE: 316.23, Val Loss: 133.9860, Val MSE: 134.20\n",
      "Epoch 3/1000 (MagnitudeL2-50.0%): Train Loss: 316.8037, Train MSE: 317.32, Val Loss: 129.0371, Val MSE: 129.14 (Best)\n",
      "Epoch 4/1000 (MagnitudeL2-50.0%): Train Loss: 328.6964, Train MSE: 327.79, Val Loss: 130.8306, Val MSE: 130.85\n",
      "Epoch 5/1000 (MagnitudeL2-50.0%): Train Loss: 322.4730, Train MSE: 322.26, Val Loss: 131.5456, Val MSE: 131.36\n",
      "Epoch 6/1000 (MagnitudeL2-50.0%): Train Loss: 313.6296, Train MSE: 314.85, Val Loss: 167.5737, Val MSE: 167.75\n",
      "Epoch 7/1000 (MagnitudeL2-50.0%): Train Loss: 325.4264, Train MSE: 325.43, Val Loss: 140.7074, Val MSE: 140.46\n",
      "Epoch 8/1000 (MagnitudeL2-50.0%): Train Loss: 317.9034, Train MSE: 318.49, Val Loss: 132.6529, Val MSE: 132.54\n",
      "Epoch 9/1000 (MagnitudeL2-50.0%): Train Loss: 309.4982, Train MSE: 309.81, Val Loss: 142.3320, Val MSE: 142.55\n",
      "Epoch 10/1000 (MagnitudeL2-50.0%): Train Loss: 323.0570, Train MSE: 323.57, Val Loss: 151.1502, Val MSE: 151.41\n",
      "Epoch 11/1000 (MagnitudeL2-50.0%): Train Loss: 322.8432, Train MSE: 322.49, Val Loss: 138.7429, Val MSE: 139.04\n",
      "Epoch 12/1000 (MagnitudeL2-50.0%): Train Loss: 317.3610, Train MSE: 316.79, Val Loss: 136.8897, Val MSE: 136.66\n",
      "Epoch 13/1000 (MagnitudeL2-50.0%): Train Loss: 319.2466, Train MSE: 318.66, Val Loss: 130.2331, Val MSE: 130.15\n",
      "Epoch 14/1000 (MagnitudeL2-50.0%): Train Loss: 315.9463, Train MSE: 315.75, Val Loss: 132.4630, Val MSE: 132.32\n",
      "Epoch 15/1000 (MagnitudeL2-50.0%): Train Loss: 317.0459, Train MSE: 317.04, Val Loss: 138.9674, Val MSE: 139.03\n",
      "Epoch 16/1000 (MagnitudeL2-50.0%): Train Loss: 316.4774, Train MSE: 317.23, Val Loss: 176.2540, Val MSE: 176.46\n",
      "Epoch 17/1000 (MagnitudeL2-50.0%): Train Loss: 312.9182, Train MSE: 312.57, Val Loss: 130.4330, Val MSE: 130.42\n",
      "Epoch 18/1000 (MagnitudeL2-50.0%): Train Loss: 312.3509, Train MSE: 312.56, Val Loss: 127.0797, Val MSE: 127.25 (Best)\n",
      "Epoch 19/1000 (MagnitudeL2-50.0%): Train Loss: 309.0402, Train MSE: 308.65, Val Loss: 157.5379, Val MSE: 157.56\n",
      "Epoch 20/1000 (MagnitudeL2-50.0%): Train Loss: 316.6497, Train MSE: 316.44, Val Loss: 142.8618, Val MSE: 142.76\n",
      "Epoch 21/1000 (MagnitudeL2-50.0%): Train Loss: 318.1580, Train MSE: 317.99, Val Loss: 157.3686, Val MSE: 156.82\n",
      "Epoch 22/1000 (MagnitudeL2-50.0%): Train Loss: 308.9929, Train MSE: 308.01, Val Loss: 147.7508, Val MSE: 147.88\n",
      "Epoch 23/1000 (MagnitudeL2-50.0%): Train Loss: 314.1834, Train MSE: 313.67, Val Loss: 131.0040, Val MSE: 130.90\n",
      "Epoch 24/1000 (MagnitudeL2-50.0%): Train Loss: 320.0304, Train MSE: 320.16, Val Loss: 132.6571, Val MSE: 132.81\n",
      "Epoch 25/1000 (MagnitudeL2-50.0%): Train Loss: 307.3265, Train MSE: 308.06, Val Loss: 133.5259, Val MSE: 133.49\n",
      "Epoch 26/1000 (MagnitudeL2-50.0%): Train Loss: 315.9905, Train MSE: 315.58, Val Loss: 169.5170, Val MSE: 169.75\n",
      "Epoch 27/1000 (MagnitudeL2-50.0%): Train Loss: 306.1234, Train MSE: 306.16, Val Loss: 126.3049, Val MSE: 126.37 (Best)\n",
      "Epoch 28/1000 (MagnitudeL2-50.0%): Train Loss: 311.9537, Train MSE: 312.27, Val Loss: 129.0170, Val MSE: 129.15\n",
      "Epoch 29/1000 (MagnitudeL2-50.0%): Train Loss: 312.8439, Train MSE: 312.23, Val Loss: 126.9167, Val MSE: 127.19\n",
      "Epoch 30/1000 (MagnitudeL2-50.0%): Train Loss: 314.8163, Train MSE: 315.70, Val Loss: 131.0331, Val MSE: 131.22\n",
      "Epoch 31/1000 (MagnitudeL2-50.0%): Train Loss: 316.5778, Train MSE: 316.50, Val Loss: 127.1516, Val MSE: 126.97\n",
      "Epoch 32/1000 (MagnitudeL2-50.0%): Train Loss: 322.2324, Train MSE: 321.10, Val Loss: 161.6493, Val MSE: 161.97\n",
      "Epoch 33/1000 (MagnitudeL2-50.0%): Train Loss: 309.0063, Train MSE: 309.24, Val Loss: 137.1962, Val MSE: 137.08\n",
      "Epoch 34/1000 (MagnitudeL2-50.0%): Train Loss: 310.0283, Train MSE: 309.87, Val Loss: 146.1620, Val MSE: 146.19\n",
      "Epoch 35/1000 (MagnitudeL2-50.0%): Train Loss: 314.6285, Train MSE: 313.45, Val Loss: 148.5816, Val MSE: 148.45\n",
      "Epoch 36/1000 (MagnitudeL2-50.0%): Train Loss: 307.5230, Train MSE: 306.69, Val Loss: 135.5238, Val MSE: 135.59\n",
      "Epoch 37/1000 (MagnitudeL2-50.0%): Train Loss: 317.8551, Train MSE: 318.19, Val Loss: 133.9956, Val MSE: 134.03\n",
      "Epoch 38/1000 (MagnitudeL2-50.0%): Train Loss: 311.8427, Train MSE: 311.55, Val Loss: 124.5054, Val MSE: 124.68 (Best)\n",
      "Epoch 39/1000 (MagnitudeL2-50.0%): Train Loss: 318.1138, Train MSE: 317.67, Val Loss: 141.3826, Val MSE: 141.30\n",
      "Epoch 40/1000 (MagnitudeL2-50.0%): Train Loss: 310.8034, Train MSE: 310.26, Val Loss: 137.5629, Val MSE: 137.28\n",
      "Epoch 41/1000 (MagnitudeL2-50.0%): Train Loss: 308.0397, Train MSE: 307.83, Val Loss: 131.8243, Val MSE: 131.99\n",
      "Epoch 42/1000 (MagnitudeL2-50.0%): Train Loss: 305.6796, Train MSE: 306.00, Val Loss: 138.5091, Val MSE: 138.26\n",
      "Epoch 43/1000 (MagnitudeL2-50.0%): Train Loss: 311.7269, Train MSE: 311.84, Val Loss: 144.8375, Val MSE: 144.53\n",
      "Epoch 44/1000 (MagnitudeL2-50.0%): Train Loss: 324.8147, Train MSE: 323.19, Val Loss: 127.5742, Val MSE: 127.91\n",
      "Epoch 45/1000 (MagnitudeL2-50.0%): Train Loss: 311.1224, Train MSE: 310.15, Val Loss: 127.7502, Val MSE: 127.78\n",
      "Epoch 46/1000 (MagnitudeL2-50.0%): Train Loss: 306.5160, Train MSE: 306.44, Val Loss: 142.2456, Val MSE: 141.94\n",
      "Epoch 47/1000 (MagnitudeL2-50.0%): Train Loss: 310.1316, Train MSE: 310.87, Val Loss: 124.2947, Val MSE: 124.25 (Best)\n",
      "Epoch 48/1000 (MagnitudeL2-50.0%): Train Loss: 310.6499, Train MSE: 309.54, Val Loss: 138.4656, Val MSE: 138.16\n",
      "Epoch 49/1000 (MagnitudeL2-50.0%): Train Loss: 309.9638, Train MSE: 310.65, Val Loss: 127.1757, Val MSE: 127.08\n",
      "Epoch 50/1000 (MagnitudeL2-50.0%): Train Loss: 310.8404, Train MSE: 310.49, Val Loss: 124.5274, Val MSE: 124.66\n",
      "Epoch 51/1000 (MagnitudeL2-50.0%): Train Loss: 310.3478, Train MSE: 310.19, Val Loss: 126.1812, Val MSE: 126.40\n",
      "Epoch 52/1000 (MagnitudeL2-50.0%): Train Loss: 308.8978, Train MSE: 309.64, Val Loss: 156.0251, Val MSE: 155.88\n",
      "Epoch 53/1000 (MagnitudeL2-50.0%): Train Loss: 310.4652, Train MSE: 311.00, Val Loss: 131.6444, Val MSE: 131.56\n",
      "Epoch 54/1000 (MagnitudeL2-50.0%): Train Loss: 302.0316, Train MSE: 301.87, Val Loss: 126.1545, Val MSE: 126.28\n",
      "Epoch 55/1000 (MagnitudeL2-50.0%): Train Loss: 299.3602, Train MSE: 299.21, Val Loss: 127.6484, Val MSE: 127.48\n",
      "Epoch 56/1000 (MagnitudeL2-50.0%): Train Loss: 311.1261, Train MSE: 312.12, Val Loss: 137.3619, Val MSE: 137.14\n",
      "Epoch 57/1000 (MagnitudeL2-50.0%): Train Loss: 311.0786, Train MSE: 311.12, Val Loss: 128.7858, Val MSE: 128.65\n",
      "Epoch 58/1000 (MagnitudeL2-50.0%): Train Loss: 302.0975, Train MSE: 301.56, Val Loss: 135.9438, Val MSE: 136.18\n",
      "Epoch 59/1000 (MagnitudeL2-50.0%): Train Loss: 308.8529, Train MSE: 309.03, Val Loss: 144.3256, Val MSE: 144.58\n",
      "Epoch 60/1000 (MagnitudeL2-50.0%): Train Loss: 311.2459, Train MSE: 312.04, Val Loss: 132.5279, Val MSE: 132.49\n",
      "Epoch 61/1000 (MagnitudeL2-50.0%): Train Loss: 306.2877, Train MSE: 305.01, Val Loss: 134.9478, Val MSE: 135.23\n",
      "Epoch 62/1000 (MagnitudeL2-50.0%): Train Loss: 301.5180, Train MSE: 302.23, Val Loss: 151.0167, Val MSE: 151.07\n",
      "Epoch 63/1000 (MagnitudeL2-50.0%): Train Loss: 301.8974, Train MSE: 300.51, Val Loss: 122.8838, Val MSE: 123.04 (Best)\n",
      "Epoch 64/1000 (MagnitudeL2-50.0%): Train Loss: 306.8309, Train MSE: 307.56, Val Loss: 136.3160, Val MSE: 136.44\n",
      "Epoch 65/1000 (MagnitudeL2-50.0%): Train Loss: 304.3728, Train MSE: 304.10, Val Loss: 141.5199, Val MSE: 141.76\n",
      "Epoch 66/1000 (MagnitudeL2-50.0%): Train Loss: 305.0324, Train MSE: 305.01, Val Loss: 134.9939, Val MSE: 135.18\n",
      "Epoch 67/1000 (MagnitudeL2-50.0%): Train Loss: 307.4054, Train MSE: 306.59, Val Loss: 124.8454, Val MSE: 124.80\n",
      "Epoch 68/1000 (MagnitudeL2-50.0%): Train Loss: 300.5675, Train MSE: 301.16, Val Loss: 121.4390, Val MSE: 121.64 (Best)\n",
      "Epoch 69/1000 (MagnitudeL2-50.0%): Train Loss: 295.1617, Train MSE: 295.50, Val Loss: 132.2718, Val MSE: 132.29\n",
      "Epoch 70/1000 (MagnitudeL2-50.0%): Train Loss: 293.9014, Train MSE: 294.11, Val Loss: 128.8618, Val MSE: 128.77\n",
      "Epoch 71/1000 (MagnitudeL2-50.0%): Train Loss: 302.9382, Train MSE: 303.28, Val Loss: 135.8306, Val MSE: 136.01\n",
      "Epoch 72/1000 (MagnitudeL2-50.0%): Train Loss: 290.1938, Train MSE: 290.76, Val Loss: 131.4843, Val MSE: 131.53\n",
      "Epoch 73/1000 (MagnitudeL2-50.0%): Train Loss: 296.4083, Train MSE: 295.73, Val Loss: 161.7508, Val MSE: 161.75\n",
      "Epoch 74/1000 (MagnitudeL2-50.0%): Train Loss: 297.0751, Train MSE: 297.27, Val Loss: 138.1088, Val MSE: 138.23\n",
      "Epoch 75/1000 (MagnitudeL2-50.0%): Train Loss: 293.3757, Train MSE: 293.86, Val Loss: 138.3424, Val MSE: 138.51\n",
      "Epoch 76/1000 (MagnitudeL2-50.0%): Train Loss: 294.0508, Train MSE: 293.92, Val Loss: 123.9796, Val MSE: 124.18\n",
      "Epoch 77/1000 (MagnitudeL2-50.0%): Train Loss: 299.8830, Train MSE: 299.68, Val Loss: 131.2681, Val MSE: 131.67\n",
      "Epoch 78/1000 (MagnitudeL2-50.0%): Train Loss: 294.9293, Train MSE: 295.44, Val Loss: 128.1009, Val MSE: 128.07\n",
      "Epoch 79/1000 (MagnitudeL2-50.0%): Train Loss: 300.8562, Train MSE: 301.69, Val Loss: 131.4524, Val MSE: 131.19\n",
      "Epoch 80/1000 (MagnitudeL2-50.0%): Train Loss: 296.8549, Train MSE: 296.15, Val Loss: 136.1146, Val MSE: 135.94\n",
      "Epoch 81/1000 (MagnitudeL2-50.0%): Train Loss: 291.4311, Train MSE: 292.21, Val Loss: 121.1805, Val MSE: 121.28 (Best)\n",
      "Epoch 82/1000 (MagnitudeL2-50.0%): Train Loss: 288.5418, Train MSE: 289.08, Val Loss: 120.8522, Val MSE: 120.99 (Best)\n",
      "Epoch 83/1000 (MagnitudeL2-50.0%): Train Loss: 295.1501, Train MSE: 294.22, Val Loss: 124.4033, Val MSE: 124.28\n",
      "Epoch 84/1000 (MagnitudeL2-50.0%): Train Loss: 286.0474, Train MSE: 286.72, Val Loss: 126.0916, Val MSE: 126.17\n",
      "Epoch 85/1000 (MagnitudeL2-50.0%): Train Loss: 289.2066, Train MSE: 289.78, Val Loss: 137.7055, Val MSE: 137.85\n",
      "Epoch 86/1000 (MagnitudeL2-50.0%): Train Loss: 291.4296, Train MSE: 292.58, Val Loss: 135.1367, Val MSE: 135.42\n",
      "Epoch 87/1000 (MagnitudeL2-50.0%): Train Loss: 287.1896, Train MSE: 287.03, Val Loss: 128.7869, Val MSE: 128.90\n",
      "Epoch 88/1000 (MagnitudeL2-50.0%): Train Loss: 292.3465, Train MSE: 292.39, Val Loss: 134.4853, Val MSE: 134.27\n",
      "Epoch 89/1000 (MagnitudeL2-50.0%): Train Loss: 287.6645, Train MSE: 288.14, Val Loss: 124.6519, Val MSE: 124.72\n",
      "Epoch 90/1000 (MagnitudeL2-50.0%): Train Loss: 295.6462, Train MSE: 295.89, Val Loss: 126.3767, Val MSE: 126.39\n",
      "Epoch 91/1000 (MagnitudeL2-50.0%): Train Loss: 287.8352, Train MSE: 286.93, Val Loss: 124.1213, Val MSE: 124.01\n",
      "Epoch 92/1000 (MagnitudeL2-50.0%): Train Loss: 300.1169, Train MSE: 298.54, Val Loss: 127.1269, Val MSE: 127.40\n",
      "Epoch 93/1000 (MagnitudeL2-50.0%): Train Loss: 285.8862, Train MSE: 285.97, Val Loss: 119.1151, Val MSE: 119.20 (Best)\n",
      "Epoch 94/1000 (MagnitudeL2-50.0%): Train Loss: 286.8204, Train MSE: 286.57, Val Loss: 145.7305, Val MSE: 145.65\n",
      "Epoch 95/1000 (MagnitudeL2-50.0%): Train Loss: 289.8756, Train MSE: 289.23, Val Loss: 127.7461, Val MSE: 127.63\n",
      "Epoch 96/1000 (MagnitudeL2-50.0%): Train Loss: 291.7448, Train MSE: 291.72, Val Loss: 123.3076, Val MSE: 123.38\n",
      "Epoch 97/1000 (MagnitudeL2-50.0%): Train Loss: 291.3109, Train MSE: 291.65, Val Loss: 126.8669, Val MSE: 126.96\n",
      "Epoch 98/1000 (MagnitudeL2-50.0%): Train Loss: 288.9805, Train MSE: 289.50, Val Loss: 131.0195, Val MSE: 131.13\n",
      "Epoch 99/1000 (MagnitudeL2-50.0%): Train Loss: 289.1625, Train MSE: 289.57, Val Loss: 133.5566, Val MSE: 133.76\n",
      "Epoch 100/1000 (MagnitudeL2-50.0%): Train Loss: 292.5686, Train MSE: 293.13, Val Loss: 143.6777, Val MSE: 143.60\n",
      "Epoch 101/1000 (MagnitudeL2-50.0%): Train Loss: 295.5190, Train MSE: 295.01, Val Loss: 123.2203, Val MSE: 123.48\n",
      "Epoch 102/1000 (MagnitudeL2-50.0%): Train Loss: 291.9671, Train MSE: 292.75, Val Loss: 123.2297, Val MSE: 123.15\n",
      "Epoch 103/1000 (MagnitudeL2-50.0%): Train Loss: 290.6054, Train MSE: 290.49, Val Loss: 120.8697, Val MSE: 121.08\n",
      "Epoch 104/1000 (MagnitudeL2-50.0%): Train Loss: 295.2699, Train MSE: 295.59, Val Loss: 131.6122, Val MSE: 131.77\n",
      "Epoch 105/1000 (MagnitudeL2-50.0%): Train Loss: 296.8359, Train MSE: 297.87, Val Loss: 140.3239, Val MSE: 140.25\n",
      "Epoch 106/1000 (MagnitudeL2-50.0%): Train Loss: 295.2090, Train MSE: 294.86, Val Loss: 122.6367, Val MSE: 122.72\n",
      "Epoch 107/1000 (MagnitudeL2-50.0%): Train Loss: 291.6543, Train MSE: 292.03, Val Loss: 120.8534, Val MSE: 120.95\n",
      "Epoch 108/1000 (MagnitudeL2-50.0%): Train Loss: 287.2731, Train MSE: 287.52, Val Loss: 142.7511, Val MSE: 142.97\n",
      "Epoch 109/1000 (MagnitudeL2-50.0%): Train Loss: 288.9846, Train MSE: 288.12, Val Loss: 142.4032, Val MSE: 142.39\n",
      "Epoch 110/1000 (MagnitudeL2-50.0%): Train Loss: 287.4366, Train MSE: 287.87, Val Loss: 128.0921, Val MSE: 128.39\n",
      "Epoch 111/1000 (MagnitudeL2-50.0%): Train Loss: 293.5740, Train MSE: 293.60, Val Loss: 126.7383, Val MSE: 126.66\n",
      "Epoch 112/1000 (MagnitudeL2-50.0%): Train Loss: 283.1511, Train MSE: 283.76, Val Loss: 143.4205, Val MSE: 143.62\n",
      "Epoch 113/1000 (MagnitudeL2-50.0%): Train Loss: 293.0656, Train MSE: 292.89, Val Loss: 126.8832, Val MSE: 127.00\n",
      "Early stopping triggered after 113 epochs\n",
      "Loaded best model state\n",
      "Results: MSE=222.37, MAE=10.77, MACs=90.45M\n",
      "✅ Model saved to ./models_lstm_nasa/magnitudel2_sparsity_0.5.pth\n",
      "✅ ONNX model saved to ./models_lstm_nasa/magnitudel2_sparsity_0.5.onnx\n",
      "\n",
      "Processing MagnitudeL2 at 70.0% sparsity...\n",
      "✅ Created LSTM with architecture:\n",
      "   LSTM: input_size=14, hidden_size=100, num_layers=2\n",
      "   FC: 100 -> 64 -> 32 -> 1\n",
      "Initial MACs: 90.45M\n",
      "Applying MagnitudeImportance pruning at 70.0% sparsity...\n",
      "Note: Only pruning FC layers, LSTM layers are preserved\n",
      "Final MACs: 90.45M (Reduction: 0.0%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/1000 (MagnitudeL2-70.0%): Train Loss: 318.3305, Train MSE: 318.83, Val Loss: 134.0083, Val MSE: 134.30 (Best)\n",
      "Epoch 2/1000 (MagnitudeL2-70.0%): Train Loss: 316.0012, Train MSE: 316.04, Val Loss: 126.9644, Val MSE: 127.02 (Best)\n",
      "Epoch 3/1000 (MagnitudeL2-70.0%): Train Loss: 308.4020, Train MSE: 308.43, Val Loss: 134.6617, Val MSE: 134.51\n",
      "Epoch 4/1000 (MagnitudeL2-70.0%): Train Loss: 319.0158, Train MSE: 319.13, Val Loss: 153.8196, Val MSE: 154.17\n",
      "Epoch 5/1000 (MagnitudeL2-70.0%): Train Loss: 329.5819, Train MSE: 328.64, Val Loss: 129.7439, Val MSE: 130.04\n",
      "Epoch 6/1000 (MagnitudeL2-70.0%): Train Loss: 325.7653, Train MSE: 325.67, Val Loss: 139.1042, Val MSE: 138.92\n",
      "Epoch 7/1000 (MagnitudeL2-70.0%): Train Loss: 314.6912, Train MSE: 315.03, Val Loss: 128.9138, Val MSE: 128.86\n",
      "Epoch 8/1000 (MagnitudeL2-70.0%): Train Loss: 316.9949, Train MSE: 317.36, Val Loss: 134.8880, Val MSE: 134.73\n",
      "Epoch 9/1000 (MagnitudeL2-70.0%): Train Loss: 318.1994, Train MSE: 318.91, Val Loss: 130.7596, Val MSE: 130.88\n",
      "Epoch 10/1000 (MagnitudeL2-70.0%): Train Loss: 322.4131, Train MSE: 321.17, Val Loss: 141.6795, Val MSE: 141.68\n",
      "Epoch 11/1000 (MagnitudeL2-70.0%): Train Loss: 312.9452, Train MSE: 312.43, Val Loss: 129.3879, Val MSE: 129.37\n",
      "Epoch 12/1000 (MagnitudeL2-70.0%): Train Loss: 317.0745, Train MSE: 316.70, Val Loss: 134.8519, Val MSE: 134.57\n",
      "Epoch 13/1000 (MagnitudeL2-70.0%): Train Loss: 311.9568, Train MSE: 311.49, Val Loss: 135.5158, Val MSE: 135.94\n",
      "Epoch 14/1000 (MagnitudeL2-70.0%): Train Loss: 316.3933, Train MSE: 315.88, Val Loss: 156.6281, Val MSE: 156.35\n",
      "Epoch 15/1000 (MagnitudeL2-70.0%): Train Loss: 310.6010, Train MSE: 311.33, Val Loss: 134.0149, Val MSE: 134.11\n",
      "Epoch 16/1000 (MagnitudeL2-70.0%): Train Loss: 315.5264, Train MSE: 315.76, Val Loss: 135.3021, Val MSE: 135.31\n",
      "Epoch 17/1000 (MagnitudeL2-70.0%): Train Loss: 311.1349, Train MSE: 311.26, Val Loss: 126.5415, Val MSE: 126.69 (Best)\n",
      "Epoch 18/1000 (MagnitudeL2-70.0%): Train Loss: 326.2205, Train MSE: 326.79, Val Loss: 127.9001, Val MSE: 128.06\n",
      "Epoch 19/1000 (MagnitudeL2-70.0%): Train Loss: 318.8542, Train MSE: 320.13, Val Loss: 136.4176, Val MSE: 136.26\n",
      "Epoch 20/1000 (MagnitudeL2-70.0%): Train Loss: 318.6726, Train MSE: 318.88, Val Loss: 152.2206, Val MSE: 152.26\n",
      "Epoch 21/1000 (MagnitudeL2-70.0%): Train Loss: 307.3962, Train MSE: 307.70, Val Loss: 131.7816, Val MSE: 131.59\n",
      "Epoch 22/1000 (MagnitudeL2-70.0%): Train Loss: 318.9407, Train MSE: 319.42, Val Loss: 125.7982, Val MSE: 125.93 (Best)\n",
      "Epoch 23/1000 (MagnitudeL2-70.0%): Train Loss: 317.5888, Train MSE: 317.80, Val Loss: 140.7857, Val MSE: 140.82\n",
      "Epoch 24/1000 (MagnitudeL2-70.0%): Train Loss: 318.3550, Train MSE: 318.81, Val Loss: 173.7030, Val MSE: 173.79\n",
      "Epoch 25/1000 (MagnitudeL2-70.0%): Train Loss: 314.6563, Train MSE: 314.79, Val Loss: 162.8852, Val MSE: 162.67\n",
      "Epoch 26/1000 (MagnitudeL2-70.0%): Train Loss: 308.4640, Train MSE: 309.25, Val Loss: 138.9502, Val MSE: 139.20\n",
      "Epoch 27/1000 (MagnitudeL2-70.0%): Train Loss: 308.6365, Train MSE: 308.47, Val Loss: 132.1626, Val MSE: 132.37\n",
      "Epoch 28/1000 (MagnitudeL2-70.0%): Train Loss: 318.8757, Train MSE: 319.92, Val Loss: 137.6567, Val MSE: 138.04\n",
      "Epoch 29/1000 (MagnitudeL2-70.0%): Train Loss: 314.1654, Train MSE: 313.62, Val Loss: 130.0365, Val MSE: 130.01\n",
      "Epoch 30/1000 (MagnitudeL2-70.0%): Train Loss: 314.4012, Train MSE: 315.44, Val Loss: 128.1319, Val MSE: 127.93\n",
      "Epoch 31/1000 (MagnitudeL2-70.0%): Train Loss: 310.6695, Train MSE: 311.16, Val Loss: 134.5699, Val MSE: 134.84\n",
      "Epoch 32/1000 (MagnitudeL2-70.0%): Train Loss: 320.3165, Train MSE: 320.57, Val Loss: 141.8098, Val MSE: 141.84\n",
      "Epoch 33/1000 (MagnitudeL2-70.0%): Train Loss: 310.9002, Train MSE: 310.31, Val Loss: 142.7943, Val MSE: 143.23\n",
      "Epoch 34/1000 (MagnitudeL2-70.0%): Train Loss: 311.9574, Train MSE: 312.52, Val Loss: 128.1801, Val MSE: 128.15\n",
      "Epoch 35/1000 (MagnitudeL2-70.0%): Train Loss: 323.6720, Train MSE: 323.55, Val Loss: 135.2053, Val MSE: 135.24\n",
      "Epoch 36/1000 (MagnitudeL2-70.0%): Train Loss: 317.7115, Train MSE: 318.08, Val Loss: 126.6690, Val MSE: 126.73\n",
      "Epoch 37/1000 (MagnitudeL2-70.0%): Train Loss: 315.6131, Train MSE: 316.24, Val Loss: 143.3671, Val MSE: 143.11\n",
      "Epoch 38/1000 (MagnitudeL2-70.0%): Train Loss: 317.9533, Train MSE: 317.77, Val Loss: 129.7738, Val MSE: 129.81\n",
      "Epoch 39/1000 (MagnitudeL2-70.0%): Train Loss: 307.4110, Train MSE: 306.60, Val Loss: 134.8987, Val MSE: 134.97\n",
      "Epoch 40/1000 (MagnitudeL2-70.0%): Train Loss: 312.1026, Train MSE: 311.30, Val Loss: 125.0751, Val MSE: 125.23 (Best)\n",
      "Epoch 41/1000 (MagnitudeL2-70.0%): Train Loss: 308.2810, Train MSE: 307.67, Val Loss: 134.7703, Val MSE: 134.56\n",
      "Epoch 42/1000 (MagnitudeL2-70.0%): Train Loss: 316.8465, Train MSE: 315.72, Val Loss: 129.6357, Val MSE: 129.69\n",
      "Epoch 43/1000 (MagnitudeL2-70.0%): Train Loss: 307.0556, Train MSE: 307.27, Val Loss: 141.2952, Val MSE: 141.41\n",
      "Epoch 44/1000 (MagnitudeL2-70.0%): Train Loss: 316.7499, Train MSE: 316.61, Val Loss: 129.0696, Val MSE: 129.28\n",
      "Epoch 45/1000 (MagnitudeL2-70.0%): Train Loss: 320.6567, Train MSE: 319.66, Val Loss: 125.1454, Val MSE: 125.26\n",
      "Epoch 46/1000 (MagnitudeL2-70.0%): Train Loss: 306.8651, Train MSE: 307.13, Val Loss: 162.9839, Val MSE: 162.88\n",
      "Epoch 47/1000 (MagnitudeL2-70.0%): Train Loss: 321.7248, Train MSE: 320.52, Val Loss: 144.1485, Val MSE: 143.91\n",
      "Epoch 48/1000 (MagnitudeL2-70.0%): Train Loss: 309.3873, Train MSE: 309.23, Val Loss: 133.7742, Val MSE: 134.14\n",
      "Epoch 49/1000 (MagnitudeL2-70.0%): Train Loss: 304.4636, Train MSE: 304.68, Val Loss: 129.5423, Val MSE: 129.56\n",
      "Epoch 50/1000 (MagnitudeL2-70.0%): Train Loss: 311.1647, Train MSE: 310.06, Val Loss: 133.9489, Val MSE: 134.17\n",
      "Epoch 51/1000 (MagnitudeL2-70.0%): Train Loss: 307.4950, Train MSE: 306.71, Val Loss: 129.2847, Val MSE: 129.53\n",
      "Epoch 52/1000 (MagnitudeL2-70.0%): Train Loss: 302.1118, Train MSE: 302.84, Val Loss: 131.6336, Val MSE: 132.13\n",
      "Epoch 53/1000 (MagnitudeL2-70.0%): Train Loss: 308.9693, Train MSE: 308.37, Val Loss: 127.0301, Val MSE: 127.12\n",
      "Epoch 54/1000 (MagnitudeL2-70.0%): Train Loss: 310.0999, Train MSE: 310.49, Val Loss: 132.5926, Val MSE: 132.63\n",
      "Epoch 55/1000 (MagnitudeL2-70.0%): Train Loss: 311.4939, Train MSE: 311.90, Val Loss: 169.7862, Val MSE: 169.36\n",
      "Epoch 56/1000 (MagnitudeL2-70.0%): Train Loss: 305.2054, Train MSE: 306.53, Val Loss: 127.0219, Val MSE: 127.24\n",
      "Epoch 57/1000 (MagnitudeL2-70.0%): Train Loss: 306.1466, Train MSE: 306.18, Val Loss: 125.1675, Val MSE: 125.40\n",
      "Epoch 58/1000 (MagnitudeL2-70.0%): Train Loss: 313.0383, Train MSE: 313.73, Val Loss: 131.6468, Val MSE: 131.57\n",
      "Epoch 59/1000 (MagnitudeL2-70.0%): Train Loss: 309.0302, Train MSE: 308.88, Val Loss: 126.2598, Val MSE: 126.22\n",
      "Epoch 60/1000 (MagnitudeL2-70.0%): Train Loss: 304.7681, Train MSE: 305.41, Val Loss: 136.0224, Val MSE: 136.11\n",
      "Early stopping triggered after 60 epochs\n",
      "Loaded best model state\n",
      "Results: MSE=217.27, MAE=10.73, MACs=90.45M\n",
      "✅ Model saved to ./models_lstm_nasa/magnitudel2_sparsity_0.7.pth\n",
      "✅ ONNX model saved to ./models_lstm_nasa/magnitudel2_sparsity_0.7.onnx\n",
      "\n",
      "--- Strategy: Random ---\n",
      "\n",
      "Processing Random at 20.0% sparsity...\n",
      "✅ Created LSTM with architecture:\n",
      "   LSTM: input_size=14, hidden_size=100, num_layers=2\n",
      "   FC: 100 -> 64 -> 32 -> 1\n",
      "Initial MACs: 90.45M\n",
      "Applying RandomImportance pruning at 20.0% sparsity...\n",
      "Note: Only pruning FC layers, LSTM layers are preserved\n",
      "Final MACs: 90.45M (Reduction: 0.0%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/1000 (Random-20.0%): Train Loss: 325.8268, Train MSE: 325.27, Val Loss: 143.6485, Val MSE: 143.69 (Best)\n",
      "Epoch 2/1000 (Random-20.0%): Train Loss: 329.2736, Train MSE: 329.84, Val Loss: 131.3268, Val MSE: 131.43 (Best)\n",
      "Epoch 3/1000 (Random-20.0%): Train Loss: 331.2966, Train MSE: 331.95, Val Loss: 134.3845, Val MSE: 134.61\n",
      "Epoch 4/1000 (Random-20.0%): Train Loss: 330.3433, Train MSE: 330.96, Val Loss: 135.8539, Val MSE: 136.13\n",
      "Epoch 5/1000 (Random-20.0%): Train Loss: 328.7731, Train MSE: 327.79, Val Loss: 151.9566, Val MSE: 151.97\n",
      "Epoch 6/1000 (Random-20.0%): Train Loss: 313.8688, Train MSE: 314.14, Val Loss: 129.1509, Val MSE: 129.30 (Best)\n",
      "Epoch 7/1000 (Random-20.0%): Train Loss: 326.2838, Train MSE: 326.58, Val Loss: 144.2784, Val MSE: 143.82\n",
      "Epoch 8/1000 (Random-20.0%): Train Loss: 322.7394, Train MSE: 322.97, Val Loss: 134.0061, Val MSE: 133.92\n",
      "Epoch 9/1000 (Random-20.0%): Train Loss: 310.4077, Train MSE: 310.05, Val Loss: 139.3080, Val MSE: 139.17\n",
      "Epoch 10/1000 (Random-20.0%): Train Loss: 327.6586, Train MSE: 327.66, Val Loss: 137.0228, Val MSE: 137.07\n",
      "Epoch 11/1000 (Random-20.0%): Train Loss: 325.3948, Train MSE: 324.58, Val Loss: 130.1150, Val MSE: 129.94\n",
      "Epoch 12/1000 (Random-20.0%): Train Loss: 331.9731, Train MSE: 331.92, Val Loss: 125.9380, Val MSE: 125.99 (Best)\n",
      "Epoch 13/1000 (Random-20.0%): Train Loss: 323.0916, Train MSE: 322.60, Val Loss: 162.7548, Val MSE: 162.94\n",
      "Epoch 14/1000 (Random-20.0%): Train Loss: 319.6864, Train MSE: 319.56, Val Loss: 130.8255, Val MSE: 131.17\n",
      "Epoch 15/1000 (Random-20.0%): Train Loss: 318.5819, Train MSE: 318.74, Val Loss: 163.2398, Val MSE: 163.12\n",
      "Epoch 16/1000 (Random-20.0%): Train Loss: 313.5218, Train MSE: 313.47, Val Loss: 164.4329, Val MSE: 164.56\n",
      "Epoch 17/1000 (Random-20.0%): Train Loss: 318.6350, Train MSE: 319.60, Val Loss: 198.1533, Val MSE: 198.48\n",
      "Epoch 18/1000 (Random-20.0%): Train Loss: 333.7567, Train MSE: 333.05, Val Loss: 135.4217, Val MSE: 135.19\n",
      "Epoch 19/1000 (Random-20.0%): Train Loss: 323.5246, Train MSE: 322.61, Val Loss: 136.1808, Val MSE: 136.51\n",
      "Epoch 20/1000 (Random-20.0%): Train Loss: 320.4435, Train MSE: 319.52, Val Loss: 156.3582, Val MSE: 156.42\n",
      "Epoch 21/1000 (Random-20.0%): Train Loss: 319.1051, Train MSE: 319.64, Val Loss: 130.3124, Val MSE: 130.33\n",
      "Epoch 22/1000 (Random-20.0%): Train Loss: 327.3491, Train MSE: 327.43, Val Loss: 137.6457, Val MSE: 137.75\n",
      "Epoch 23/1000 (Random-20.0%): Train Loss: 323.1443, Train MSE: 323.38, Val Loss: 149.0505, Val MSE: 149.21\n",
      "Epoch 24/1000 (Random-20.0%): Train Loss: 327.0529, Train MSE: 326.05, Val Loss: 128.7052, Val MSE: 129.12\n",
      "Epoch 25/1000 (Random-20.0%): Train Loss: 316.5031, Train MSE: 316.37, Val Loss: 131.1090, Val MSE: 131.14\n",
      "Epoch 26/1000 (Random-20.0%): Train Loss: 308.7296, Train MSE: 308.91, Val Loss: 132.0096, Val MSE: 132.27\n",
      "Epoch 27/1000 (Random-20.0%): Train Loss: 310.2898, Train MSE: 310.88, Val Loss: 128.4748, Val MSE: 128.72\n",
      "Epoch 28/1000 (Random-20.0%): Train Loss: 310.5723, Train MSE: 310.12, Val Loss: 135.7110, Val MSE: 135.55\n",
      "Epoch 29/1000 (Random-20.0%): Train Loss: 315.4669, Train MSE: 314.08, Val Loss: 130.9458, Val MSE: 131.19\n",
      "Epoch 30/1000 (Random-20.0%): Train Loss: 314.7570, Train MSE: 315.54, Val Loss: 151.8791, Val MSE: 151.58\n",
      "Epoch 31/1000 (Random-20.0%): Train Loss: 317.4119, Train MSE: 316.74, Val Loss: 125.9447, Val MSE: 126.13\n",
      "Epoch 32/1000 (Random-20.0%): Train Loss: 307.6709, Train MSE: 307.56, Val Loss: 141.8351, Val MSE: 142.01\n",
      "Early stopping triggered after 32 epochs\n",
      "Loaded best model state\n",
      "Results: MSE=204.06, MAE=10.45, MACs=90.45M\n",
      "✅ Model saved to ./models_lstm_nasa/random_sparsity_0.2.pth\n",
      "✅ ONNX model saved to ./models_lstm_nasa/random_sparsity_0.2.onnx\n",
      "\n",
      "Processing Random at 50.0% sparsity...\n",
      "✅ Created LSTM with architecture:\n",
      "   LSTM: input_size=14, hidden_size=100, num_layers=2\n",
      "   FC: 100 -> 64 -> 32 -> 1\n",
      "Initial MACs: 90.45M\n",
      "Applying RandomImportance pruning at 50.0% sparsity...\n",
      "Note: Only pruning FC layers, LSTM layers are preserved\n",
      "Final MACs: 90.45M (Reduction: 0.0%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/1000 (Random-50.0%): Train Loss: 397.6036, Train MSE: 398.14, Val Loss: 147.6251, Val MSE: 147.76 (Best)\n",
      "Epoch 2/1000 (Random-50.0%): Train Loss: 359.0556, Train MSE: 358.06, Val Loss: 138.1912, Val MSE: 138.42 (Best)\n",
      "Epoch 3/1000 (Random-50.0%): Train Loss: 351.8134, Train MSE: 352.67, Val Loss: 131.9230, Val MSE: 132.03 (Best)\n",
      "Epoch 4/1000 (Random-50.0%): Train Loss: 347.5236, Train MSE: 348.51, Val Loss: 133.6263, Val MSE: 133.83\n",
      "Epoch 5/1000 (Random-50.0%): Train Loss: 356.2991, Train MSE: 355.60, Val Loss: 138.0853, Val MSE: 138.47\n",
      "Epoch 6/1000 (Random-50.0%): Train Loss: 358.3970, Train MSE: 358.75, Val Loss: 136.0438, Val MSE: 136.18\n",
      "Epoch 7/1000 (Random-50.0%): Train Loss: 350.7309, Train MSE: 351.11, Val Loss: 134.5676, Val MSE: 134.56\n",
      "Epoch 8/1000 (Random-50.0%): Train Loss: 351.3785, Train MSE: 351.75, Val Loss: 132.7841, Val MSE: 132.85\n",
      "Epoch 9/1000 (Random-50.0%): Train Loss: 357.5966, Train MSE: 356.85, Val Loss: 133.6586, Val MSE: 133.58\n",
      "Epoch 10/1000 (Random-50.0%): Train Loss: 345.2041, Train MSE: 344.48, Val Loss: 137.1574, Val MSE: 137.18\n",
      "Epoch 11/1000 (Random-50.0%): Train Loss: 347.9027, Train MSE: 348.77, Val Loss: 135.1676, Val MSE: 135.49\n",
      "Epoch 12/1000 (Random-50.0%): Train Loss: 350.6399, Train MSE: 351.08, Val Loss: 130.4717, Val MSE: 130.58 (Best)\n",
      "Epoch 13/1000 (Random-50.0%): Train Loss: 354.9718, Train MSE: 355.22, Val Loss: 135.4471, Val MSE: 135.62\n",
      "Epoch 14/1000 (Random-50.0%): Train Loss: 343.5535, Train MSE: 343.87, Val Loss: 143.8177, Val MSE: 143.89\n",
      "Epoch 15/1000 (Random-50.0%): Train Loss: 352.7462, Train MSE: 353.57, Val Loss: 155.3541, Val MSE: 155.22\n",
      "Epoch 16/1000 (Random-50.0%): Train Loss: 350.9886, Train MSE: 351.31, Val Loss: 135.7842, Val MSE: 136.10\n",
      "Epoch 17/1000 (Random-50.0%): Train Loss: 359.0428, Train MSE: 359.10, Val Loss: 167.0850, Val MSE: 167.23\n",
      "Epoch 18/1000 (Random-50.0%): Train Loss: 340.5671, Train MSE: 340.29, Val Loss: 130.2561, Val MSE: 130.26 (Best)\n",
      "Epoch 19/1000 (Random-50.0%): Train Loss: 345.5884, Train MSE: 345.39, Val Loss: 129.3278, Val MSE: 129.64 (Best)\n",
      "Epoch 20/1000 (Random-50.0%): Train Loss: 344.1572, Train MSE: 344.68, Val Loss: 150.7511, Val MSE: 150.65\n",
      "Epoch 21/1000 (Random-50.0%): Train Loss: 343.6069, Train MSE: 344.04, Val Loss: 135.3940, Val MSE: 135.36\n",
      "Epoch 22/1000 (Random-50.0%): Train Loss: 346.3798, Train MSE: 347.13, Val Loss: 140.3252, Val MSE: 140.53\n",
      "Epoch 23/1000 (Random-50.0%): Train Loss: 343.9695, Train MSE: 344.10, Val Loss: 138.0676, Val MSE: 138.07\n",
      "Epoch 24/1000 (Random-50.0%): Train Loss: 344.2739, Train MSE: 344.25, Val Loss: 155.3905, Val MSE: 155.57\n",
      "Epoch 25/1000 (Random-50.0%): Train Loss: 350.4511, Train MSE: 351.43, Val Loss: 131.5280, Val MSE: 131.47\n",
      "Epoch 26/1000 (Random-50.0%): Train Loss: 349.6668, Train MSE: 349.63, Val Loss: 134.6072, Val MSE: 134.27\n",
      "Epoch 27/1000 (Random-50.0%): Train Loss: 346.5653, Train MSE: 347.40, Val Loss: 127.7788, Val MSE: 128.08 (Best)\n",
      "Epoch 28/1000 (Random-50.0%): Train Loss: 339.2976, Train MSE: 337.42, Val Loss: 156.9703, Val MSE: 157.09\n",
      "Epoch 29/1000 (Random-50.0%): Train Loss: 341.7551, Train MSE: 342.39, Val Loss: 199.4531, Val MSE: 199.36\n",
      "Epoch 30/1000 (Random-50.0%): Train Loss: 345.6490, Train MSE: 346.54, Val Loss: 127.7290, Val MSE: 127.62 (Best)\n",
      "Epoch 31/1000 (Random-50.0%): Train Loss: 333.1294, Train MSE: 332.74, Val Loss: 143.3473, Val MSE: 143.62\n",
      "Epoch 32/1000 (Random-50.0%): Train Loss: 343.3500, Train MSE: 342.16, Val Loss: 133.3305, Val MSE: 133.24\n",
      "Epoch 33/1000 (Random-50.0%): Train Loss: 339.5411, Train MSE: 339.99, Val Loss: 145.1786, Val MSE: 144.83\n",
      "Epoch 34/1000 (Random-50.0%): Train Loss: 327.3944, Train MSE: 327.48, Val Loss: 166.6874, Val MSE: 166.60\n",
      "Epoch 35/1000 (Random-50.0%): Train Loss: 334.5882, Train MSE: 335.15, Val Loss: 136.6158, Val MSE: 136.49\n",
      "Epoch 36/1000 (Random-50.0%): Train Loss: 341.9862, Train MSE: 341.08, Val Loss: 131.3792, Val MSE: 131.49\n",
      "Epoch 37/1000 (Random-50.0%): Train Loss: 330.0275, Train MSE: 329.62, Val Loss: 142.0814, Val MSE: 142.40\n",
      "Epoch 38/1000 (Random-50.0%): Train Loss: 337.4307, Train MSE: 336.45, Val Loss: 163.8703, Val MSE: 164.13\n",
      "Epoch 39/1000 (Random-50.0%): Train Loss: 345.7976, Train MSE: 345.29, Val Loss: 130.2536, Val MSE: 130.18\n",
      "Epoch 40/1000 (Random-50.0%): Train Loss: 325.8714, Train MSE: 325.54, Val Loss: 131.9466, Val MSE: 132.07\n",
      "Epoch 41/1000 (Random-50.0%): Train Loss: 326.1295, Train MSE: 325.95, Val Loss: 129.5236, Val MSE: 129.84\n",
      "Epoch 42/1000 (Random-50.0%): Train Loss: 321.5272, Train MSE: 322.28, Val Loss: 148.4032, Val MSE: 148.66\n",
      "Epoch 43/1000 (Random-50.0%): Train Loss: 326.3988, Train MSE: 325.46, Val Loss: 138.7100, Val MSE: 139.15\n",
      "Epoch 44/1000 (Random-50.0%): Train Loss: 339.4656, Train MSE: 339.12, Val Loss: 130.9623, Val MSE: 131.24\n",
      "Epoch 45/1000 (Random-50.0%): Train Loss: 322.6460, Train MSE: 322.82, Val Loss: 125.3586, Val MSE: 125.38 (Best)\n",
      "Epoch 46/1000 (Random-50.0%): Train Loss: 333.9496, Train MSE: 334.40, Val Loss: 134.3348, Val MSE: 134.33\n",
      "Epoch 47/1000 (Random-50.0%): Train Loss: 325.2751, Train MSE: 325.75, Val Loss: 129.6364, Val MSE: 129.59\n",
      "Epoch 48/1000 (Random-50.0%): Train Loss: 331.6980, Train MSE: 332.51, Val Loss: 126.5503, Val MSE: 126.84\n",
      "Epoch 49/1000 (Random-50.0%): Train Loss: 331.1802, Train MSE: 331.41, Val Loss: 126.3204, Val MSE: 126.53\n",
      "Epoch 50/1000 (Random-50.0%): Train Loss: 322.6803, Train MSE: 322.79, Val Loss: 143.2865, Val MSE: 143.34\n",
      "Epoch 51/1000 (Random-50.0%): Train Loss: 327.0898, Train MSE: 327.48, Val Loss: 143.3965, Val MSE: 143.50\n",
      "Epoch 52/1000 (Random-50.0%): Train Loss: 321.9825, Train MSE: 322.19, Val Loss: 129.0613, Val MSE: 129.36\n",
      "Epoch 53/1000 (Random-50.0%): Train Loss: 317.7873, Train MSE: 317.38, Val Loss: 125.5473, Val MSE: 125.43\n",
      "Epoch 54/1000 (Random-50.0%): Train Loss: 327.0632, Train MSE: 327.84, Val Loss: 148.4093, Val MSE: 148.33\n",
      "Epoch 55/1000 (Random-50.0%): Train Loss: 324.8373, Train MSE: 323.90, Val Loss: 146.1336, Val MSE: 145.71\n",
      "Epoch 56/1000 (Random-50.0%): Train Loss: 336.6063, Train MSE: 336.13, Val Loss: 128.7166, Val MSE: 128.73\n",
      "Epoch 57/1000 (Random-50.0%): Train Loss: 325.0045, Train MSE: 325.75, Val Loss: 131.3547, Val MSE: 131.37\n",
      "Epoch 58/1000 (Random-50.0%): Train Loss: 327.4865, Train MSE: 325.79, Val Loss: 126.3619, Val MSE: 126.59\n",
      "Epoch 59/1000 (Random-50.0%): Train Loss: 325.2074, Train MSE: 325.07, Val Loss: 137.6436, Val MSE: 137.76\n",
      "Epoch 60/1000 (Random-50.0%): Train Loss: 317.2371, Train MSE: 318.00, Val Loss: 126.3982, Val MSE: 126.47\n",
      "Epoch 61/1000 (Random-50.0%): Train Loss: 312.9556, Train MSE: 312.48, Val Loss: 175.2156, Val MSE: 174.75\n",
      "Epoch 62/1000 (Random-50.0%): Train Loss: 319.0179, Train MSE: 318.93, Val Loss: 123.5265, Val MSE: 123.63 (Best)\n",
      "Epoch 63/1000 (Random-50.0%): Train Loss: 328.0704, Train MSE: 327.14, Val Loss: 130.7908, Val MSE: 131.01\n",
      "Epoch 64/1000 (Random-50.0%): Train Loss: 312.1911, Train MSE: 311.66, Val Loss: 130.0951, Val MSE: 130.39\n",
      "Epoch 65/1000 (Random-50.0%): Train Loss: 320.3398, Train MSE: 320.54, Val Loss: 142.9407, Val MSE: 143.22\n",
      "Epoch 66/1000 (Random-50.0%): Train Loss: 323.0023, Train MSE: 322.30, Val Loss: 142.1759, Val MSE: 141.85\n",
      "Epoch 67/1000 (Random-50.0%): Train Loss: 325.8029, Train MSE: 325.07, Val Loss: 131.1486, Val MSE: 131.43\n",
      "Epoch 68/1000 (Random-50.0%): Train Loss: 313.8109, Train MSE: 314.27, Val Loss: 150.9404, Val MSE: 150.78\n",
      "Epoch 69/1000 (Random-50.0%): Train Loss: 323.4393, Train MSE: 322.52, Val Loss: 128.6324, Val MSE: 128.60\n",
      "Epoch 70/1000 (Random-50.0%): Train Loss: 311.2144, Train MSE: 311.31, Val Loss: 133.7364, Val MSE: 133.78\n",
      "Epoch 71/1000 (Random-50.0%): Train Loss: 320.8048, Train MSE: 320.57, Val Loss: 142.3807, Val MSE: 142.72\n",
      "Epoch 72/1000 (Random-50.0%): Train Loss: 318.7614, Train MSE: 319.16, Val Loss: 127.2860, Val MSE: 127.16\n",
      "Epoch 73/1000 (Random-50.0%): Train Loss: 318.6584, Train MSE: 318.69, Val Loss: 135.0154, Val MSE: 134.87\n",
      "Epoch 74/1000 (Random-50.0%): Train Loss: 315.6910, Train MSE: 315.81, Val Loss: 129.5864, Val MSE: 129.60\n",
      "Epoch 75/1000 (Random-50.0%): Train Loss: 318.2809, Train MSE: 317.41, Val Loss: 126.9244, Val MSE: 126.96\n",
      "Epoch 76/1000 (Random-50.0%): Train Loss: 301.5197, Train MSE: 301.45, Val Loss: 140.8094, Val MSE: 141.01\n",
      "Epoch 77/1000 (Random-50.0%): Train Loss: 313.8040, Train MSE: 313.26, Val Loss: 128.9810, Val MSE: 129.12\n",
      "Epoch 78/1000 (Random-50.0%): Train Loss: 312.2443, Train MSE: 312.00, Val Loss: 134.7358, Val MSE: 134.44\n",
      "Epoch 79/1000 (Random-50.0%): Train Loss: 311.8329, Train MSE: 311.47, Val Loss: 134.5917, Val MSE: 135.04\n",
      "Epoch 80/1000 (Random-50.0%): Train Loss: 316.6400, Train MSE: 315.73, Val Loss: 122.7345, Val MSE: 122.81 (Best)\n",
      "Epoch 81/1000 (Random-50.0%): Train Loss: 313.6792, Train MSE: 313.02, Val Loss: 131.1102, Val MSE: 131.01\n",
      "Epoch 82/1000 (Random-50.0%): Train Loss: 306.2061, Train MSE: 306.68, Val Loss: 140.6791, Val MSE: 140.56\n",
      "Epoch 83/1000 (Random-50.0%): Train Loss: 317.3222, Train MSE: 317.18, Val Loss: 156.8285, Val MSE: 157.39\n",
      "Epoch 84/1000 (Random-50.0%): Train Loss: 312.5292, Train MSE: 313.22, Val Loss: 147.5373, Val MSE: 147.65\n",
      "Epoch 85/1000 (Random-50.0%): Train Loss: 308.3339, Train MSE: 309.01, Val Loss: 167.2938, Val MSE: 167.41\n",
      "Epoch 86/1000 (Random-50.0%): Train Loss: 313.3823, Train MSE: 313.67, Val Loss: 133.8696, Val MSE: 133.99\n",
      "Epoch 87/1000 (Random-50.0%): Train Loss: 310.1552, Train MSE: 309.88, Val Loss: 125.4869, Val MSE: 125.54\n",
      "Epoch 88/1000 (Random-50.0%): Train Loss: 310.3954, Train MSE: 309.58, Val Loss: 129.8417, Val MSE: 130.13\n",
      "Epoch 89/1000 (Random-50.0%): Train Loss: 308.0375, Train MSE: 309.04, Val Loss: 125.2317, Val MSE: 125.53\n",
      "Epoch 90/1000 (Random-50.0%): Train Loss: 303.9179, Train MSE: 304.32, Val Loss: 127.9820, Val MSE: 128.23\n",
      "Epoch 91/1000 (Random-50.0%): Train Loss: 310.1113, Train MSE: 310.38, Val Loss: 140.1602, Val MSE: 140.31\n",
      "Epoch 92/1000 (Random-50.0%): Train Loss: 305.8691, Train MSE: 305.69, Val Loss: 122.8137, Val MSE: 122.81\n",
      "Epoch 93/1000 (Random-50.0%): Train Loss: 311.9656, Train MSE: 312.24, Val Loss: 123.6806, Val MSE: 123.80\n",
      "Epoch 94/1000 (Random-50.0%): Train Loss: 303.1063, Train MSE: 303.07, Val Loss: 134.1938, Val MSE: 133.90\n",
      "Epoch 95/1000 (Random-50.0%): Train Loss: 314.0730, Train MSE: 314.47, Val Loss: 128.3566, Val MSE: 128.16\n",
      "Epoch 96/1000 (Random-50.0%): Train Loss: 303.0118, Train MSE: 302.05, Val Loss: 148.0180, Val MSE: 148.12\n",
      "Epoch 97/1000 (Random-50.0%): Train Loss: 312.5684, Train MSE: 311.94, Val Loss: 122.0221, Val MSE: 122.06 (Best)\n",
      "Epoch 98/1000 (Random-50.0%): Train Loss: 311.0029, Train MSE: 310.70, Val Loss: 120.3300, Val MSE: 120.45 (Best)\n",
      "Epoch 99/1000 (Random-50.0%): Train Loss: 309.1050, Train MSE: 309.20, Val Loss: 148.1186, Val MSE: 148.66\n",
      "Epoch 100/1000 (Random-50.0%): Train Loss: 315.0833, Train MSE: 315.32, Val Loss: 134.7023, Val MSE: 135.14\n",
      "Epoch 101/1000 (Random-50.0%): Train Loss: 307.1592, Train MSE: 306.81, Val Loss: 150.9222, Val MSE: 150.73\n",
      "Epoch 102/1000 (Random-50.0%): Train Loss: 307.7264, Train MSE: 307.58, Val Loss: 126.3347, Val MSE: 126.45\n",
      "Epoch 103/1000 (Random-50.0%): Train Loss: 311.3408, Train MSE: 311.41, Val Loss: 120.6542, Val MSE: 120.81\n",
      "Epoch 104/1000 (Random-50.0%): Train Loss: 300.5510, Train MSE: 300.82, Val Loss: 124.5783, Val MSE: 124.60\n",
      "Epoch 105/1000 (Random-50.0%): Train Loss: 310.9483, Train MSE: 309.63, Val Loss: 122.8135, Val MSE: 122.83\n",
      "Epoch 106/1000 (Random-50.0%): Train Loss: 295.1628, Train MSE: 295.72, Val Loss: 128.9019, Val MSE: 129.07\n",
      "Epoch 107/1000 (Random-50.0%): Train Loss: 304.4095, Train MSE: 305.21, Val Loss: 142.6992, Val MSE: 142.51\n",
      "Epoch 108/1000 (Random-50.0%): Train Loss: 313.1655, Train MSE: 312.56, Val Loss: 120.5502, Val MSE: 120.56\n",
      "Epoch 109/1000 (Random-50.0%): Train Loss: 307.3237, Train MSE: 307.43, Val Loss: 121.5211, Val MSE: 121.50\n",
      "Epoch 110/1000 (Random-50.0%): Train Loss: 308.3824, Train MSE: 308.54, Val Loss: 126.3748, Val MSE: 126.36\n",
      "Epoch 111/1000 (Random-50.0%): Train Loss: 309.3072, Train MSE: 309.02, Val Loss: 121.2722, Val MSE: 121.46\n",
      "Epoch 112/1000 (Random-50.0%): Train Loss: 316.3884, Train MSE: 317.17, Val Loss: 124.9867, Val MSE: 125.27\n",
      "Epoch 113/1000 (Random-50.0%): Train Loss: 304.9795, Train MSE: 304.63, Val Loss: 123.4024, Val MSE: 123.57\n",
      "Epoch 114/1000 (Random-50.0%): Train Loss: 303.5119, Train MSE: 304.54, Val Loss: 122.4753, Val MSE: 122.59\n",
      "Epoch 115/1000 (Random-50.0%): Train Loss: 302.6735, Train MSE: 304.12, Val Loss: 133.4316, Val MSE: 133.14\n",
      "Epoch 116/1000 (Random-50.0%): Train Loss: 306.4330, Train MSE: 305.14, Val Loss: 123.4467, Val MSE: 123.60\n",
      "Epoch 117/1000 (Random-50.0%): Train Loss: 301.3967, Train MSE: 300.63, Val Loss: 122.6178, Val MSE: 122.61\n",
      "Epoch 118/1000 (Random-50.0%): Train Loss: 302.8368, Train MSE: 302.61, Val Loss: 122.5436, Val MSE: 122.46\n",
      "Early stopping triggered after 118 epochs\n",
      "Loaded best model state\n",
      "Results: MSE=217.27, MAE=10.67, MACs=90.45M\n",
      "✅ Model saved to ./models_lstm_nasa/random_sparsity_0.5.pth\n",
      "✅ ONNX model saved to ./models_lstm_nasa/random_sparsity_0.5.onnx\n",
      "\n",
      "Processing Random at 70.0% sparsity...\n",
      "✅ Created LSTM with architecture:\n",
      "   LSTM: input_size=14, hidden_size=100, num_layers=2\n",
      "   FC: 100 -> 64 -> 32 -> 1\n",
      "Initial MACs: 90.45M\n",
      "Applying RandomImportance pruning at 70.0% sparsity...\n",
      "Note: Only pruning FC layers, LSTM layers are preserved\n",
      "Final MACs: 90.45M (Reduction: 0.0%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/1000 (Random-70.0%): Train Loss: 500.2598, Train MSE: 501.95, Val Loss: 172.5026, Val MSE: 172.57 (Best)\n",
      "Epoch 2/1000 (Random-70.0%): Train Loss: 391.4763, Train MSE: 391.27, Val Loss: 143.8862, Val MSE: 143.98 (Best)\n",
      "Epoch 3/1000 (Random-70.0%): Train Loss: 375.6222, Train MSE: 375.39, Val Loss: 183.9496, Val MSE: 184.20\n",
      "Epoch 4/1000 (Random-70.0%): Train Loss: 383.4336, Train MSE: 383.90, Val Loss: 141.1965, Val MSE: 141.28 (Best)\n",
      "Epoch 5/1000 (Random-70.0%): Train Loss: 380.3580, Train MSE: 379.94, Val Loss: 154.3161, Val MSE: 154.22\n",
      "Epoch 6/1000 (Random-70.0%): Train Loss: 372.4606, Train MSE: 371.47, Val Loss: 168.8205, Val MSE: 168.90\n",
      "Epoch 7/1000 (Random-70.0%): Train Loss: 365.5708, Train MSE: 366.11, Val Loss: 143.5418, Val MSE: 143.62\n",
      "Epoch 8/1000 (Random-70.0%): Train Loss: 367.9552, Train MSE: 369.18, Val Loss: 130.4862, Val MSE: 130.53 (Best)\n",
      "Epoch 9/1000 (Random-70.0%): Train Loss: 373.6932, Train MSE: 373.55, Val Loss: 140.1715, Val MSE: 140.17\n",
      "Epoch 10/1000 (Random-70.0%): Train Loss: 366.5171, Train MSE: 367.34, Val Loss: 176.5506, Val MSE: 176.73\n",
      "Epoch 11/1000 (Random-70.0%): Train Loss: 374.2326, Train MSE: 373.92, Val Loss: 136.1035, Val MSE: 135.97\n",
      "Epoch 12/1000 (Random-70.0%): Train Loss: 369.8915, Train MSE: 369.62, Val Loss: 138.1138, Val MSE: 138.12\n",
      "Epoch 13/1000 (Random-70.0%): Train Loss: 358.4567, Train MSE: 359.00, Val Loss: 140.2555, Val MSE: 140.27\n",
      "Epoch 14/1000 (Random-70.0%): Train Loss: 363.7329, Train MSE: 364.33, Val Loss: 141.1711, Val MSE: 141.14\n",
      "Epoch 15/1000 (Random-70.0%): Train Loss: 370.2063, Train MSE: 370.39, Val Loss: 136.9083, Val MSE: 136.70\n",
      "Epoch 16/1000 (Random-70.0%): Train Loss: 368.8379, Train MSE: 368.24, Val Loss: 130.0137, Val MSE: 130.20 (Best)\n",
      "Epoch 17/1000 (Random-70.0%): Train Loss: 361.4863, Train MSE: 361.13, Val Loss: 128.3264, Val MSE: 128.46 (Best)\n",
      "Epoch 18/1000 (Random-70.0%): Train Loss: 359.6991, Train MSE: 359.39, Val Loss: 143.3032, Val MSE: 143.28\n",
      "Epoch 19/1000 (Random-70.0%): Train Loss: 356.7307, Train MSE: 357.63, Val Loss: 133.5975, Val MSE: 133.54\n",
      "Epoch 20/1000 (Random-70.0%): Train Loss: 359.6017, Train MSE: 359.41, Val Loss: 151.4207, Val MSE: 151.16\n",
      "Epoch 21/1000 (Random-70.0%): Train Loss: 357.8120, Train MSE: 357.18, Val Loss: 153.8101, Val MSE: 153.58\n",
      "Epoch 22/1000 (Random-70.0%): Train Loss: 355.2003, Train MSE: 354.52, Val Loss: 152.8402, Val MSE: 152.79\n",
      "Epoch 23/1000 (Random-70.0%): Train Loss: 357.6547, Train MSE: 358.20, Val Loss: 135.9666, Val MSE: 135.81\n",
      "Epoch 24/1000 (Random-70.0%): Train Loss: 353.5574, Train MSE: 353.32, Val Loss: 126.9169, Val MSE: 127.14 (Best)\n",
      "Epoch 25/1000 (Random-70.0%): Train Loss: 352.5009, Train MSE: 352.16, Val Loss: 187.6384, Val MSE: 187.28\n",
      "Epoch 26/1000 (Random-70.0%): Train Loss: 357.5113, Train MSE: 357.52, Val Loss: 138.2703, Val MSE: 138.23\n",
      "Epoch 27/1000 (Random-70.0%): Train Loss: 356.9460, Train MSE: 357.26, Val Loss: 135.4888, Val MSE: 135.27\n",
      "Epoch 28/1000 (Random-70.0%): Train Loss: 343.8775, Train MSE: 344.64, Val Loss: 133.1284, Val MSE: 132.92\n",
      "Epoch 29/1000 (Random-70.0%): Train Loss: 345.8603, Train MSE: 344.93, Val Loss: 137.5929, Val MSE: 138.00\n",
      "Epoch 30/1000 (Random-70.0%): Train Loss: 345.3651, Train MSE: 345.66, Val Loss: 134.9645, Val MSE: 134.95\n",
      "Epoch 31/1000 (Random-70.0%): Train Loss: 354.4903, Train MSE: 355.11, Val Loss: 157.2480, Val MSE: 156.97\n",
      "Epoch 32/1000 (Random-70.0%): Train Loss: 346.8957, Train MSE: 346.41, Val Loss: 180.8870, Val MSE: 181.00\n",
      "Epoch 33/1000 (Random-70.0%): Train Loss: 347.1334, Train MSE: 346.14, Val Loss: 126.7409, Val MSE: 126.93 (Best)\n",
      "Epoch 34/1000 (Random-70.0%): Train Loss: 342.3527, Train MSE: 342.98, Val Loss: 146.7859, Val MSE: 146.82\n",
      "Epoch 35/1000 (Random-70.0%): Train Loss: 345.3650, Train MSE: 346.42, Val Loss: 141.6482, Val MSE: 141.67\n",
      "Epoch 36/1000 (Random-70.0%): Train Loss: 343.3804, Train MSE: 342.13, Val Loss: 126.9236, Val MSE: 127.02\n",
      "Epoch 37/1000 (Random-70.0%): Train Loss: 354.1889, Train MSE: 354.94, Val Loss: 147.6589, Val MSE: 148.01\n",
      "Epoch 38/1000 (Random-70.0%): Train Loss: 345.3572, Train MSE: 345.65, Val Loss: 129.0949, Val MSE: 129.11\n",
      "Epoch 39/1000 (Random-70.0%): Train Loss: 352.0945, Train MSE: 353.10, Val Loss: 127.7768, Val MSE: 127.87\n",
      "Epoch 40/1000 (Random-70.0%): Train Loss: 344.9625, Train MSE: 346.29, Val Loss: 139.3556, Val MSE: 139.56\n",
      "Epoch 41/1000 (Random-70.0%): Train Loss: 350.0579, Train MSE: 350.20, Val Loss: 126.7016, Val MSE: 126.74 (Best)\n",
      "Epoch 42/1000 (Random-70.0%): Train Loss: 348.3135, Train MSE: 349.15, Val Loss: 138.6069, Val MSE: 139.09\n",
      "Epoch 43/1000 (Random-70.0%): Train Loss: 345.2588, Train MSE: 345.75, Val Loss: 130.5892, Val MSE: 130.75\n",
      "Epoch 44/1000 (Random-70.0%): Train Loss: 343.9897, Train MSE: 344.18, Val Loss: 171.5238, Val MSE: 171.32\n",
      "Epoch 45/1000 (Random-70.0%): Train Loss: 342.9762, Train MSE: 342.92, Val Loss: 159.8116, Val MSE: 160.21\n",
      "Epoch 46/1000 (Random-70.0%): Train Loss: 348.6986, Train MSE: 349.10, Val Loss: 146.5355, Val MSE: 146.51\n",
      "Epoch 47/1000 (Random-70.0%): Train Loss: 333.6877, Train MSE: 333.86, Val Loss: 133.1632, Val MSE: 133.32\n",
      "Epoch 48/1000 (Random-70.0%): Train Loss: 341.3462, Train MSE: 339.93, Val Loss: 133.9536, Val MSE: 133.68\n",
      "Epoch 49/1000 (Random-70.0%): Train Loss: 343.0543, Train MSE: 343.10, Val Loss: 141.0906, Val MSE: 141.22\n",
      "Epoch 50/1000 (Random-70.0%): Train Loss: 331.2204, Train MSE: 331.56, Val Loss: 133.9391, Val MSE: 133.75\n",
      "Epoch 51/1000 (Random-70.0%): Train Loss: 342.9787, Train MSE: 343.89, Val Loss: 130.8970, Val MSE: 130.87\n",
      "Epoch 52/1000 (Random-70.0%): Train Loss: 333.5229, Train MSE: 333.43, Val Loss: 144.0595, Val MSE: 144.21\n",
      "Epoch 53/1000 (Random-70.0%): Train Loss: 346.0565, Train MSE: 346.29, Val Loss: 164.1099, Val MSE: 164.19\n",
      "Epoch 54/1000 (Random-70.0%): Train Loss: 348.4218, Train MSE: 346.00, Val Loss: 152.5726, Val MSE: 152.93\n",
      "Epoch 55/1000 (Random-70.0%): Train Loss: 335.0544, Train MSE: 335.28, Val Loss: 134.4009, Val MSE: 134.32\n",
      "Epoch 56/1000 (Random-70.0%): Train Loss: 348.9400, Train MSE: 349.43, Val Loss: 125.5576, Val MSE: 125.52 (Best)\n",
      "Epoch 57/1000 (Random-70.0%): Train Loss: 343.2074, Train MSE: 343.20, Val Loss: 127.6320, Val MSE: 127.84\n",
      "Epoch 58/1000 (Random-70.0%): Train Loss: 337.6345, Train MSE: 338.10, Val Loss: 129.6618, Val MSE: 129.98\n",
      "Epoch 59/1000 (Random-70.0%): Train Loss: 333.3815, Train MSE: 333.20, Val Loss: 143.8519, Val MSE: 144.12\n",
      "Epoch 60/1000 (Random-70.0%): Train Loss: 334.2131, Train MSE: 332.86, Val Loss: 133.1140, Val MSE: 133.44\n",
      "Epoch 61/1000 (Random-70.0%): Train Loss: 336.2842, Train MSE: 335.85, Val Loss: 140.0700, Val MSE: 140.03\n",
      "Epoch 62/1000 (Random-70.0%): Train Loss: 338.9338, Train MSE: 339.09, Val Loss: 173.5440, Val MSE: 173.85\n",
      "Epoch 63/1000 (Random-70.0%): Train Loss: 338.9125, Train MSE: 339.15, Val Loss: 125.2531, Val MSE: 125.31 (Best)\n",
      "Epoch 64/1000 (Random-70.0%): Train Loss: 334.6838, Train MSE: 332.78, Val Loss: 137.8740, Val MSE: 137.91\n",
      "Epoch 65/1000 (Random-70.0%): Train Loss: 342.8112, Train MSE: 343.47, Val Loss: 162.9739, Val MSE: 163.16\n",
      "Epoch 66/1000 (Random-70.0%): Train Loss: 329.7242, Train MSE: 330.52, Val Loss: 129.3303, Val MSE: 129.69\n",
      "Epoch 67/1000 (Random-70.0%): Train Loss: 337.5652, Train MSE: 336.67, Val Loss: 125.4447, Val MSE: 125.54\n",
      "Epoch 68/1000 (Random-70.0%): Train Loss: 336.9819, Train MSE: 336.34, Val Loss: 134.8706, Val MSE: 134.81\n",
      "Epoch 69/1000 (Random-70.0%): Train Loss: 336.9920, Train MSE: 336.78, Val Loss: 137.9602, Val MSE: 138.51\n",
      "Epoch 70/1000 (Random-70.0%): Train Loss: 336.2700, Train MSE: 335.79, Val Loss: 143.0227, Val MSE: 143.16\n",
      "Epoch 71/1000 (Random-70.0%): Train Loss: 334.2717, Train MSE: 334.71, Val Loss: 135.8682, Val MSE: 136.04\n",
      "Epoch 72/1000 (Random-70.0%): Train Loss: 339.3619, Train MSE: 337.94, Val Loss: 132.4794, Val MSE: 132.88\n",
      "Epoch 73/1000 (Random-70.0%): Train Loss: 345.7140, Train MSE: 345.72, Val Loss: 124.7953, Val MSE: 124.82 (Best)\n",
      "Epoch 74/1000 (Random-70.0%): Train Loss: 331.1246, Train MSE: 331.79, Val Loss: 137.4617, Val MSE: 137.67\n",
      "Epoch 75/1000 (Random-70.0%): Train Loss: 335.6193, Train MSE: 335.57, Val Loss: 126.7368, Val MSE: 126.82\n",
      "Epoch 76/1000 (Random-70.0%): Train Loss: 340.8895, Train MSE: 340.01, Val Loss: 138.3738, Val MSE: 138.68\n",
      "Epoch 77/1000 (Random-70.0%): Train Loss: 336.1289, Train MSE: 337.16, Val Loss: 128.3515, Val MSE: 128.23\n",
      "Epoch 78/1000 (Random-70.0%): Train Loss: 340.2463, Train MSE: 341.44, Val Loss: 128.1304, Val MSE: 127.96\n",
      "Epoch 79/1000 (Random-70.0%): Train Loss: 341.6763, Train MSE: 341.88, Val Loss: 128.8476, Val MSE: 129.01\n",
      "Epoch 80/1000 (Random-70.0%): Train Loss: 329.9975, Train MSE: 330.34, Val Loss: 142.9306, Val MSE: 143.39\n",
      "Epoch 81/1000 (Random-70.0%): Train Loss: 328.8153, Train MSE: 329.21, Val Loss: 147.9455, Val MSE: 148.01\n",
      "Epoch 82/1000 (Random-70.0%): Train Loss: 336.1322, Train MSE: 335.14, Val Loss: 134.3508, Val MSE: 134.28\n",
      "Epoch 83/1000 (Random-70.0%): Train Loss: 338.0338, Train MSE: 338.56, Val Loss: 131.7908, Val MSE: 132.09\n",
      "Epoch 84/1000 (Random-70.0%): Train Loss: 330.1126, Train MSE: 330.44, Val Loss: 125.9772, Val MSE: 125.99\n",
      "Epoch 85/1000 (Random-70.0%): Train Loss: 332.0979, Train MSE: 333.16, Val Loss: 136.1654, Val MSE: 136.23\n",
      "Epoch 86/1000 (Random-70.0%): Train Loss: 338.7552, Train MSE: 337.74, Val Loss: 131.5438, Val MSE: 131.45\n",
      "Epoch 87/1000 (Random-70.0%): Train Loss: 343.8825, Train MSE: 343.64, Val Loss: 122.5978, Val MSE: 122.60 (Best)\n",
      "Epoch 88/1000 (Random-70.0%): Train Loss: 338.1499, Train MSE: 338.54, Val Loss: 125.7508, Val MSE: 125.89\n",
      "Epoch 89/1000 (Random-70.0%): Train Loss: 333.2117, Train MSE: 333.24, Val Loss: 140.0699, Val MSE: 140.38\n",
      "Epoch 90/1000 (Random-70.0%): Train Loss: 328.6164, Train MSE: 328.52, Val Loss: 136.6243, Val MSE: 136.97\n",
      "Epoch 91/1000 (Random-70.0%): Train Loss: 333.2014, Train MSE: 331.32, Val Loss: 129.9948, Val MSE: 130.03\n",
      "Epoch 92/1000 (Random-70.0%): Train Loss: 346.6415, Train MSE: 346.98, Val Loss: 125.7144, Val MSE: 125.68\n",
      "Epoch 93/1000 (Random-70.0%): Train Loss: 338.2535, Train MSE: 338.73, Val Loss: 131.8116, Val MSE: 131.88\n",
      "Epoch 94/1000 (Random-70.0%): Train Loss: 331.6032, Train MSE: 331.32, Val Loss: 129.5770, Val MSE: 129.87\n",
      "Epoch 95/1000 (Random-70.0%): Train Loss: 327.2974, Train MSE: 327.36, Val Loss: 128.5579, Val MSE: 128.69\n",
      "Epoch 96/1000 (Random-70.0%): Train Loss: 331.7334, Train MSE: 331.27, Val Loss: 126.1558, Val MSE: 126.56\n",
      "Epoch 97/1000 (Random-70.0%): Train Loss: 341.2259, Train MSE: 341.28, Val Loss: 140.1591, Val MSE: 140.34\n",
      "Epoch 98/1000 (Random-70.0%): Train Loss: 329.4200, Train MSE: 329.88, Val Loss: 146.6712, Val MSE: 147.02\n",
      "Epoch 99/1000 (Random-70.0%): Train Loss: 332.4370, Train MSE: 332.64, Val Loss: 129.1252, Val MSE: 129.27\n",
      "Epoch 100/1000 (Random-70.0%): Train Loss: 328.2213, Train MSE: 327.75, Val Loss: 134.3200, Val MSE: 134.64\n",
      "Epoch 101/1000 (Random-70.0%): Train Loss: 336.7951, Train MSE: 336.29, Val Loss: 149.1293, Val MSE: 148.79\n",
      "Epoch 102/1000 (Random-70.0%): Train Loss: 330.4161, Train MSE: 330.18, Val Loss: 157.5666, Val MSE: 157.85\n",
      "Epoch 103/1000 (Random-70.0%): Train Loss: 335.9041, Train MSE: 337.38, Val Loss: 131.8992, Val MSE: 132.21\n",
      "Epoch 104/1000 (Random-70.0%): Train Loss: 333.8683, Train MSE: 333.98, Val Loss: 132.1587, Val MSE: 132.44\n",
      "Epoch 105/1000 (Random-70.0%): Train Loss: 330.5971, Train MSE: 330.10, Val Loss: 130.0286, Val MSE: 130.04\n",
      "Epoch 106/1000 (Random-70.0%): Train Loss: 328.0328, Train MSE: 327.50, Val Loss: 149.2191, Val MSE: 149.36\n",
      "Epoch 107/1000 (Random-70.0%): Train Loss: 331.5089, Train MSE: 331.03, Val Loss: 150.7297, Val MSE: 150.90\n",
      "Early stopping triggered after 107 epochs\n",
      "Loaded best model state\n",
      "Results: MSE=218.70, MAE=10.68, MACs=90.45M\n",
      "✅ Model saved to ./models_lstm_nasa/random_sparsity_0.7.pth\n",
      "✅ ONNX model saved to ./models_lstm_nasa/random_sparsity_0.7.onnx\n",
      "\n",
      "Saving results...\n",
      "✅ Complete results saved to ./results_lstm_nasa/complete_results.json\n",
      "✅ Summary results saved to ./results_lstm_nasa/summary_results.csv\n",
      "Creating plots...\n",
      "✅ MSE plot saved to ./results_lstm_nasa/mse_vs_sparsity.png\n",
      "✅ Efficiency frontier plot saved to ./results_lstm_nasa/efficiency_frontier.png\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENTAL RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Baseline Performance:\n",
      "  MSE: 205.94\n",
      "  MAE: 10.50\n",
      "  MACs: 90.45M\n",
      "  Parameters: 0.14M\n",
      "  Model Size: 0.52MB\n",
      "\n",
      "Strategy Comparison at 50% Sparsity (FC Layers Only):\n",
      "   MagnitudeL2: MSE=222.37 (+16.43,  +8.0% increase)\n",
      "        Random: MSE=217.27 (+11.33,  +5.5% increase)\n",
      "\n",
      "Complete Results Table:\n",
      "------------------------------------------------------------------------------------------\n",
      "Strategy     Sparsity MSE      MAE      MACs(M)  Params(M) Size(MB)\n",
      "------------------------------------------------------------------------------------------\n",
      "MagnitudeL2       0%  205.94   10.50   90.45     0.14    0.52\n",
      "MagnitudeL2      20%  207.10   10.54   90.45     0.14    0.52\n",
      "MagnitudeL2      50%  222.37   10.77   90.45     0.13    0.51\n",
      "MagnitudeL2      70%  217.27   10.73   90.45     0.13    0.51\n",
      "Random            0%  205.94   10.50   90.45     0.14    0.52\n",
      "Random           20%  204.06   10.45   90.45     0.14    0.52\n",
      "Random           50%  217.27   10.67   90.45     0.13    0.51\n",
      "Random           70%  218.70   10.68   90.45     0.13    0.51\n",
      "\n",
      "Note: Sparsity is applied only to FC layers. LSTM layers remain unchanged.\n",
      "\n",
      "🎉 All experiments completed!\n",
      "📁 Results saved to: /home/muis/thesis/github-repo/master-thesis/timeseries/lstm/results_lstm_nasa\n",
      "📁 Models saved to: /home/muis/thesis/github-repo/master-thesis/timeseries/lstm/models_lstm_nasa\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
