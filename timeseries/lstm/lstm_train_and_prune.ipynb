{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a09f8662",
   "metadata": {},
   "source": [
    "# Pruning Experiment for LSTM on Energy Prediction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-04T04:11:25.915143Z",
     "start_time": "2025-05-04T04:11:25.911030Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_pruning as tp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "import os\n",
    "import time\n",
    "import copy"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. LSTM Model Definition",
   "id": "d34501d960e6ef22"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T04:14:32.066164Z",
     "start_time": "2025-05-04T04:14:32.061244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TimeSeriesLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob=0.2):\n",
    "        super(TimeSeriesLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                            batch_first=True, dropout=dropout_prob if num_layers > 1 else 0)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, input_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Use output of the last time step\n",
    "        last_time_step_out = lstm_out[:, -1, :]\n",
    "        out = self.dropout(last_time_step_out)\n",
    "        final_out = self.fc(out)\n",
    "        # final_out shape: (batch_size, output_size=1)\n",
    "        return final_out"
   ],
   "id": "32c8b58d641d5cc9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. Data Handling for Appliances Energy Dataset",
   "id": "dcf0ae5256b38f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### --- Data Configuration ---",
   "id": "b0b7dfa7ecfa623b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T04:14:36.430384Z",
     "start_time": "2025-05-04T04:14:36.427026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATASET_PATH = './data/energydata_complete.csv' # ADJUST PATH AS NEEDED\n",
    "SEQUENCE_LENGTH = 6 * 12 # Use 12 hours of past data (12 hours * 6 samples/hour)\n",
    "TARGET_COLUMN = 'Appliances'\n",
    "# Features to use (excluding target, date, and others)\n",
    "FEATURE_COLUMNS = [\n",
    "    'lights', 'T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4', 'RH_4', 'T5', 'RH_5',\n",
    "    'T6', 'RH_6', 'T7', 'RH_7', 'T8', 'RH_8', 'T9', 'RH_9', 'T_out',\n",
    "    'Press_mm_hg', 'RH_out', 'Windspeed', 'Visibility', 'Tdewpoint'\n",
    "]"
   ],
   "id": "346c330705147c24",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### --- Helper function to create sequences ---",
   "id": "d474b0a04d72dd4c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T04:14:40.718883Z",
     "start_time": "2025-05-04T04:14:40.715143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_sequences(input_data, target_data, seq_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    # Stop seq_length steps early to ensure target data is available\n",
    "    for i in range(len(input_data) - seq_length):\n",
    "        sequences.append(input_data[i:i + seq_length])\n",
    "        targets.append(target_data[i + seq_length])\n",
    "    return np.array(sequences), np.array(targets)"
   ],
   "id": "a5184c2440abb655",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### --- Custom Dataset ---",
   "id": "3df380163964e5f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T04:16:20.642791Z",
     "start_time": "2025-05-04T04:16:20.638854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EnergyDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32).unsqueeze(1) # Target shape [N, 1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.targets[idx]"
   ],
   "id": "a9e29c58eaace567",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### --- Main Data Loading Function ---",
   "id": "60c367dad6b9ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T04:16:54.063580Z",
     "start_time": "2025-05-04T04:16:54.052382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_energy_data_loaders(\n",
    "    file_path=DATASET_PATH,\n",
    "    feature_cols=FEATURE_COLUMNS,\n",
    "    target_col=TARGET_COLUMN,\n",
    "    seq_length=SEQUENCE_LENGTH,\n",
    "    batch_size=64,\n",
    "    test_size=0.2,\n",
    "    val_size=0.1 # Proportion of the *remaining* data after test split\n",
    "    ):\n",
    "    \"\"\"Loads, preprocesses, scales, and creates sequences for the energy dataset.\"\"\"\n",
    "    print(f\"Loading dataset from: {file_path}\")\n",
    "    try:\n",
    "        # 1. Load Data & Initial Processing\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Original data shape: {df.shape}\")\n",
    "        df['date'] = pd.to_datetime(df['date']) # Parse date\n",
    "        df = df.sort_values('date') # Ensure chronological order\n",
    "        df = df.set_index('date') # Optional: use date index\n",
    "        df = df[feature_cols + [target_col]].dropna() # Select columns and drop NaNs\n",
    "        print(f\"Data shape after selecting columns & dropping NaNs: {df.shape}\")\n",
    "        if df.empty:\n",
    "            raise ValueError(\"DataFrame is empty after selecting columns and dropping NaNs.\")\n",
    "\n",
    "        # 2. Separate Features and Target\n",
    "        X = df[feature_cols].values\n",
    "        y = df[[target_col]].values # Keep as 2D: [N, 1]\n",
    "\n",
    "        # 3. Splitting (Chronological)\n",
    "        n_total = len(X)\n",
    "        n_test = int(n_total * test_size)\n",
    "        n_val = int((n_total - n_test) * val_size)\n",
    "        n_train = n_total - n_test - n_val\n",
    "\n",
    "        if n_train <= seq_length or n_val <= seq_length or n_test <= seq_length:\n",
    "             raise ValueError(f\"Not enough data for sequence length {seq_length} after splitting. \"\n",
    "                              f\"Train={n_train}, Val={n_val}, Test={n_test}\")\n",
    "\n",
    "\n",
    "        X_train, y_train = X[:n_train], y[:n_train]\n",
    "        X_val, y_val = X[n_train:n_train + n_val], y[n_train:n_train + n_val]\n",
    "        X_test, y_test = X[n_train + n_val:], y[n_train + n_val:]\n",
    "\n",
    "        print(f\"Data split: Train={X_train.shape[0]}, Val={X_val.shape[0]}, Test={X_test.shape[0]}\")\n",
    "\n",
    "        # 4. Scaling\n",
    "        scaler_features = MinMaxScaler()\n",
    "        scaler_target = MinMaxScaler()\n",
    "\n",
    "        X_train_scaled = scaler_features.fit_transform(X_train)\n",
    "        X_val_scaled = scaler_features.transform(X_val)\n",
    "        X_test_scaled = scaler_features.transform(X_test)\n",
    "\n",
    "        y_train_scaled = scaler_target.fit_transform(y_train)\n",
    "        # Flatten y for sequence creation, EnergyDataset will unsqueeze later\n",
    "        y_val_scaled = scaler_target.transform(y_val)\n",
    "        y_test_scaled = scaler_target.transform(y_test)\n",
    "\n",
    "        # 5. Create Sequences\n",
    "        X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled.flatten(), seq_length)\n",
    "        X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled.flatten(), seq_length)\n",
    "        X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled.flatten(), seq_length)\n",
    "\n",
    "        # 6. Create Datasets and DataLoaders\n",
    "        train_dataset = EnergyDataset(X_train_seq, y_train_seq)\n",
    "        val_dataset = EnergyDataset(X_val_seq, y_val_seq)\n",
    "        test_dataset = EnergyDataset(X_test_seq, y_test_seq)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        input_size = X_train_scaled.shape[1] # Number of features\n",
    "        scalers = {'features': scaler_features, 'target': scaler_target} # Store scalers\n",
    "\n",
    "        print(f\"Data loaded successfully:\")\n",
    "        print(f\"  Input size (features): {input_size}\")\n",
    "        print(f\"  Sequence length: {seq_length}\")\n",
    "        print(f\"  Train sequences: {len(train_dataset)}\")\n",
    "        print(f\"  Validation sequences: {len(val_dataset)}\")\n",
    "        print(f\"  Test sequences: {len(test_dataset)}\")\n",
    "\n",
    "        return train_loader, val_loader, test_loader, input_size, seq_length, scalers\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Dataset file not found at {file_path}\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError during data processing: {ve}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during data loading: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    return None, None, None, 0, 0, None # Return None on error\n"
   ],
   "id": "26bf2344c6cd369a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Training Function (Regression)",
   "id": "b534f34b2a0a5436"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T04:17:58.963202Z",
     "start_time": "2025-05-04T04:17:58.956523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model_regression(model, train_loader, criterion, optimizer, device, num_epochs, val_loader=None, model_path_prefix=\"best_model\", grad_clip=None):\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    print(f\"Starting training for {num_epochs} epochs...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            if grad_clip:\n",
    "                 torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        log_msg = f\"Epoch {epoch+1}/{num_epochs}: Train Loss={epoch_loss:.6f}, Time: {epoch_time:.2f}s\"\n",
    "\n",
    "        # Validation Step\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for inputs_val, labels_val in val_loader:\n",
    "                    inputs_val, labels_val = inputs_val.to(device), labels_val.to(device)\n",
    "                    outputs_val = model(inputs_val)\n",
    "                    val_loss += criterion(outputs_val, labels_val).item() * inputs_val.size(0)\n",
    "            val_loss /= len(val_loader.dataset)\n",
    "            val_losses.append(val_loss)\n",
    "            log_msg += f\", Val Loss={val_loss:.6f}\"\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                save_path = f\"{model_path_prefix}_best_val.pth\"\n",
    "                try:\n",
    "                    torch.save(model.state_dict(), save_path)\n",
    "                    log_msg += f\" (Best model saved)\"\n",
    "                except Exception as e:\n",
    "                    log_msg += f\" (Error saving model: {e})\"\n",
    "            model.train() # Switch back\n",
    "\n",
    "        print(log_msg)\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "    return model, train_losses, val_losses"
   ],
   "id": "3394e5dd05204081",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Evaluation Function (Regression)",
   "id": "c62ad1d00d793849"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T04:20:33.094227Z",
     "start_time": "2025-05-04T04:20:33.085697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_macs_params(model, example_input):\n",
    "    # Ensure example_input is on the right device\n",
    "    device = next(model.parameters()).device\n",
    "    example_input = example_input.to(device)\n",
    "    # tp.utils.count_ops_and_params can fail with LSTMs sometimes. Use torchinfo as fallback.\n",
    "    try:\n",
    "         macs, params = tp.utils.count_ops_and_params(model, example_input)\n",
    "         return macs, params\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: torch_pruning MACs calculation failed ({e}). Falling back to torchinfo estimate.\")\n",
    "        try:\n",
    "             from torchinfo import summary\n",
    "             # Correct input format for torchinfo might be needed depending on version\n",
    "             # Try with tuple (common format) or just the tensor\n",
    "             input_data_shape = example_input.shape\n",
    "             model_summary = summary(model, input_size=input_data_shape, verbose=0)\n",
    "             params = model_summary.total_params\n",
    "             macs = model_summary.total_mult_adds\n",
    "             print(f\"torchinfo estimate: Params={params}, MACs={macs}\")\n",
    "             return macs, params\n",
    "        except Exception as e2:\n",
    "            print(f\"Warning: torchinfo calculation also failed ({e2}). Returning 0 for MACs/Params.\")\n",
    "            return 0, sum(p.numel() for p in model.parameters()) # Return at least params\n",
    "\n",
    "def evaluate_model_regression(model, test_loader, example_input, device, scalers=None):\n",
    "    model.eval()\n",
    "    macs, params = calculate_macs_params(model, example_input) # Handles potential LSTM issues\n",
    "    size_mb = params * 4 / 1e6 # Assumes float32\n",
    "\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    print(\"Evaluating on test set...\")\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            all_outputs.append(outputs.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    all_outputs = np.concatenate(all_outputs, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    outputs_eval = all_outputs\n",
    "    labels_eval = all_labels\n",
    "\n",
    "    # Inverse transform for interpretable metrics\n",
    "    if scalers and 'target' in scalers:\n",
    "        try:\n",
    "            outputs_eval = scalers['target'].inverse_transform(all_outputs)\n",
    "            labels_eval = scalers['target'].inverse_transform(all_labels)\n",
    "            print(\"Metrics calculated on original scale.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not inverse transform. Metrics on scaled data. Error: {e}\")\n",
    "    else:\n",
    "         print(\"Warning: Target scaler not provided. Metrics calculated on scaled data.\")\n",
    "\n",
    "\n",
    "    mse = mean_squared_error(labels_eval, outputs_eval)\n",
    "    mae = mean_absolute_error(labels_eval, outputs_eval)\n",
    "    r2 = r2_score(labels_eval, outputs_eval)\n",
    "    rmse = np.sqrt(mse)\n",
    "    # MAPE calculation - handle potential zeros in labels_eval\n",
    "    epsilon = 1e-8 # Small value to avoid division by zero\n",
    "    mape = np.mean(np.abs((labels_eval - outputs_eval) / (labels_eval + epsilon))) * 100\n",
    "\n",
    "\n",
    "    print(f\"Evaluation Metrics: MSE={mse:.4f}, MAE={mae:.4f}, RMSE={rmse:.4f}, R2={r2:.4f}, MAPE={mape:.2f}%\")\n",
    "\n",
    "    return {\n",
    "        'macs': macs,\n",
    "        'params': params,\n",
    "        'size_mb': size_mb,\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2,\n",
    "        'mape': mape,\n",
    "        'performance': mse # Use MSE as the primary performance metric (lower is better)\n",
    "    }"
   ],
   "id": "d6de516508982311",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5. Pruning Function (Adapted for LSTM)",
   "id": "212c6003f1c20c89"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T05:02:27.504596Z",
     "start_time": "2025-05-04T05:02:27.490257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prune_lstm_model_by_threshold(\n",
    "    model,\n",
    "    example_input_bs1, # BS=1 for MACs/Params calc & non-grad strategies\n",
    "    target_macs,\n",
    "    target_params,\n",
    "    strategy,\n",
    "    max_iterations=50,\n",
    "    step_pruning_ratio=0.1,\n",
    "    gradient_batch=None, # Dict {'inputs': T, 'labels': T} with BS > 1\n",
    "    prunable_modules=None # List of specific layers (e.g., [model.fc])\n",
    "    ):\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    needs_gradient = isinstance(strategy['importance'], (\n",
    "        tp.importance.TaylorImportance,\n",
    "        tp.importance.GroupHessianImportance\n",
    "    ))\n",
    "    if needs_gradient:\n",
    "        if gradient_batch is None: raise ValueError(f\"Strategy needs 'gradient_batch'.\")\n",
    "        if gradient_batch['inputs'].shape[0] <= 1: raise ValueError(f\"Need BS > 1 in gradient_batch\")\n",
    "        gradient_inputs = gradient_batch['inputs'].to(device)\n",
    "        gradient_labels = gradient_batch['labels'].to(device)\n",
    "\n",
    "    print(f\"--- Starting Pruning ({strategy['importance'].__class__.__name__}) ---\")\n",
    "    print(f\"Target MACs: <= {target_macs:,.0f}, Target Params: <= {target_params:,.0f}\")\n",
    "    print(f\"Step Ratio: {step_pruning_ratio:.2f}, Max Iter: {max_iterations}\")\n",
    "\n",
    "    # Determine layers to prune/ignore\n",
    "    if not prunable_modules:\n",
    "         prunable_modules = [m for m in model.modules() if isinstance(m, nn.Linear)]\n",
    "         print(f\"Defaulting to pruning nn.Linear layers: {[m.__class__.__name__ for m in prunable_modules]}\")\n",
    "    else:\n",
    "         print(f\"Targeting specific modules for pruning: {[m.__class__.__name__ for m in prunable_modules]}\")\n",
    "\n",
    "    modules_to_ignore = [m for m in model.modules() if isinstance(m, (nn.Linear, nn.Conv2d, nn.LSTM)) and m not in prunable_modules]\n",
    "    root_types = list(set(type(m) for m in prunable_modules))\n",
    "    if not root_types:\n",
    "        print(\"Warning: No prunable module types identified. Pruning may fail.\")\n",
    "        root_types = [nn.Linear] # Fallback guess\n",
    "\n",
    "\n",
    "    pruner = strategy['pruner'](\n",
    "        model,\n",
    "        example_input_bs1.to(device),\n",
    "        importance=strategy['importance'],\n",
    "        pruning_ratio=step_pruning_ratio,\n",
    "        root_module_types=root_types,\n",
    "        ignored_layers=modules_to_ignore,\n",
    "    )\n",
    "\n",
    "    initial_macs, initial_params = calculate_macs_params(model, example_input_bs1.to(device))\n",
    "    current_macs, current_params = initial_macs, initial_params\n",
    "    print(f\"Initial State | MACs: {current_macs:,.0f}, Params: {current_params:,.0f}\")\n",
    "    # --- Remove or comment out these two lines ---\n",
    "    # prunable_layer_names = [layer.__class__.__name__ for layer in pruner.get_pruning_layers()] # <--- ERROR HERE\n",
    "    # print(f\"Detected Prunable Layers by tp: {prunable_layer_names}\")\n",
    "    # --------------------------------------------\n",
    "    if initial_macs == 0 and initial_params == 0: # Check if initial calc failed\n",
    "         print(\"Warning: Initial MACs/Params calculation failed or returned zero. Cannot proceed.\")\n",
    "         return model # Or raise error\n",
    "\n",
    "    # Existing check (adjust slightly): If no prunable layers are implicitly handled by pruner.step, it will just return empty groups.\n",
    "    # We don't need the explicit check here anymore. The loop checking `if not pruning_groups:` will handle it.\n",
    "    # if not prunable_layer_names and initial_macs > 0:\n",
    "    #     print(\"Warning: torch-pruning did not detect any prunable layers matching criteria.\")\n",
    "\n",
    "\n",
    "    iteration = 0\n",
    "    model.eval()\n",
    "    criterion = nn.MSELoss().to(device) # Loss for gradient calculation\n",
    "\n",
    "    while (current_macs > target_macs or current_params > target_params) and iteration < max_iterations:\n",
    "        iteration += 1\n",
    "        macs_before_step = current_macs\n",
    "        params_before_step = current_params\n",
    "\n",
    "        if needs_gradient:\n",
    "            model.train()\n",
    "            input_for_grad = gradient_inputs.detach().clone()\n",
    "            labels_for_grad = gradient_labels.detach().clone()\n",
    "            try:\n",
    "                for param in model.parameters(): param.requires_grad_(True)\n",
    "                outputs = model(input_for_grad)\n",
    "                loss = criterion(outputs, labels_for_grad)\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError during backward (Iter {iteration}): {e}. Stopping.\")\n",
    "                model.eval() ; break\n",
    "            finally:\n",
    "                model.eval() ; model.zero_grad(set_to_none=True)\n",
    "\n",
    "        try:\n",
    "            pruning_groups = list(pruner.step(interactive=True))\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during pruner.step() (Iter {iteration}): {e}. Stopping.\")\n",
    "            break\n",
    "\n",
    "        if not pruning_groups:\n",
    "            print(f\"Iter {iteration}: No more candidates found by pruner. Stopping.\")\n",
    "            break\n",
    "\n",
    "        for group in pruning_groups: group.prune()\n",
    "\n",
    "        current_macs, current_params = calculate_macs_params(model, example_input_bs1.to(device))\n",
    "        macs_reduced_pct = (macs_before_step - current_macs) / initial_macs * 100 if initial_macs > 0 else 0\n",
    "        params_reduced_pct = (params_before_step - current_params) / initial_params * 100 if initial_params > 0 else 0\n",
    "\n",
    "        print(\n",
    "            f\"Iter {iteration: >3}/{max_iterations} | \"\n",
    "            f\"MACs: {current_macs:,.0f} ({macs_reduced_pct:+6.1f}% R) | \"\n",
    "            f\"Params: {current_params:,.0f} ({params_reduced_pct:+6.1f}% R)\"\n",
    "        )\n",
    "\n",
    "        if current_macs >= macs_before_step and current_params >= params_before_step:\n",
    "            if not (current_macs <= target_macs and current_params <= target_params):\n",
    "                print(f\"Iter {iteration}: No reduction. Stopping.\") ; break\n",
    "            else: break # Targets met\n",
    "\n",
    "    # --- Final Report ---\n",
    "    print(f\"--- Finished Pruning ({strategy['importance'].__class__.__name__}) ---\")\n",
    "    if iteration >= max_iterations: print(f\"Warning: Reached max iterations ({max_iterations}).\")\n",
    "    final_macs, final_params = calculate_macs_params(model, example_input_bs1.to(device))\n",
    "    macs_reduction = (initial_macs - final_macs) / initial_macs * 100 if initial_macs > 0 else 0\n",
    "    params_reduction = (initial_params - final_params) / initial_params * 100 if initial_params > 0 else 0\n",
    "    print(f\"Initial | MACs: {initial_macs:,.0f}, Params: {initial_params:,.0f}\")\n",
    "    print(f\"Final   | MACs: {final_macs:,.0f} (Reduction: {macs_reduction:.2f}%)\")\n",
    "    print(f\"        | Params: {final_params:,.0f} (Reduction: {params_reduction:.2f}%)\")\n",
    "    print(f\"Target  | MACs <= {target_macs:,.0f}, Params <= {target_params:,.0f}\")\n",
    "    if final_macs > target_macs or final_params > target_params:\n",
    "         print(\"Warning: Pruning finished, but target threshold(s) were not fully met.\")\n",
    "\n",
    "    model.eval()\n",
    "    return model"
   ],
   "id": "9b726ab96d7993b7",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6. Comparison and Plotting Function (Regression)",
   "id": "b067dfeb3c7e56f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T04:27:18.218241Z",
     "start_time": "2025-05-04T04:27:18.207933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compare_results_and_plot_regression(results, metric_key='mse', lower_is_better=True, output_dir='output'):\n",
    "    if not results: print(\"No results to plot.\") ; return\n",
    "    valid_results = {k: v for k, v in results.items() if isinstance(v, dict) and all(m in v for m in ['macs', 'params', metric_key])}\n",
    "    if not valid_results: print(\"No valid results entries found for plotting.\") ; return\n",
    "\n",
    "    strategy_order = []\n",
    "    if 'initial' in valid_results: strategy_order.append('initial')\n",
    "    strategy_order.extend([s for s in valid_results if s != 'initial'])\n",
    "    if not strategy_order: print(\"No strategies to plot.\"); return\n",
    "\n",
    "    # --- Print Table ---\n",
    "    metric_name = metric_key.upper()\n",
    "    print(f\"\\n=== Pruning Strategy Comparison (Metric: {metric_name}) ===\")\n",
    "    header = f\"{'Strategy':<15} | {'MACs (M)':<10} | {'Params (K)':<10} | {'Size (MB)':<10} | {metric_name:<12}\"\n",
    "    print(header); print(\"-\" * len(header))\n",
    "    for strategy in strategy_order:\n",
    "        metrics = valid_results[strategy]\n",
    "        macs_m = metrics['macs']/1e6 if metrics['macs'] is not None else 0\n",
    "        params_k = metrics['params']/1e3 if metrics['params'] is not None else 0\n",
    "        print(f\"{strategy:<15} | {macs_m:<10.2f} | {params_k:<10.1f} | {metrics.get('size_mb', 0):>10.2f} | {metrics[metric_key]:>12.4f}\")\n",
    "\n",
    "    # --- Generate Bar Charts ---\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    metrics_to_plot = ['macs', 'params', 'size_mb', metric_key]\n",
    "    base_titles = {'macs': 'MACs', 'params': 'Parameters', 'size_mb': 'Model Size (MB)', metric_key: metric_name}\n",
    "    plot_titles = {k: f'{v} Comparison (Lower is Better)' for k, v in base_titles.items()}\n",
    "    if not lower_is_better:\n",
    "        plot_titles[metric_key] = f'{base_titles[metric_key]} Comparison (Higher is Better)'\n",
    "\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(strategy_order)))\n",
    "    initial_metrics = valid_results.get('initial', None)\n",
    "\n",
    "    for plot_metric in metrics_to_plot:\n",
    "        if not all(plot_metric in valid_results[s] for s in strategy_order):\n",
    "             print(f\"Skipping plot for {plot_metric} as it's missing from some results.\")\n",
    "             continue\n",
    "        values = [valid_results[strategy][plot_metric] for strategy in strategy_order]\n",
    "\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        bars = plt.bar(strategy_order, values, color=colors)\n",
    "        plt.ylabel(base_titles[plot_metric])\n",
    "        plt.title(plot_titles[plot_metric])\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "        # Add value labels\n",
    "        max_val = max(values) if values else 0\n",
    "        for i, bar in enumerate(bars):\n",
    "            yval = bar.get_height()\n",
    "            label = \"\"\n",
    "            # ... (Use the formatting logic from previous `compare_results_and_plot_regression` function) ...\n",
    "            if plot_metric == 'macs': label = f'{yval/1e6:.2f}M' if yval > 1e5 else f'{yval:,.0f}' # Adjust format\n",
    "            elif plot_metric == 'params': label = f'{yval/1e3:.1f}K' if yval > 100 else f'{yval:,.0f}' # Adjust format\n",
    "            elif plot_metric == 'size_mb': label = f'{yval:.2f}'\n",
    "            else: label = f'{yval:.4f}' # Regression metric\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., yval + 0.01 * max_val, label, ha='center', va='bottom', fontsize=8, rotation=0)\n",
    "\n",
    "        if initial_metrics and plot_metric in initial_metrics:\n",
    "             initial_value = initial_metrics[plot_metric]\n",
    "             plt.axhline(y=initial_value, color='r', linestyle='--', label=f'Initial Value')\n",
    "             plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(output_dir, f'lstm_energy_{plot_metric}_comparison.png')\n",
    "        try:\n",
    "            plt.savefig(save_path)\n",
    "        except Exception as e:\n",
    "             print(f\"Error saving plot {save_path}: {e}\")\n",
    "        plt.close()\n",
    "    print(f\"Comparison plots saved to {output_dir}\")"
   ],
   "id": "6eeda7c2629cea21",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7. Main Workflow (LSTM Energy Prediction)",
   "id": "597afc3d99ec2631"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T04:27:23.736854Z",
     "start_time": "2025-05-04T04:27:23.720590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main_lstm():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- Configuration ---\n",
    "    config = {\n",
    "        'strategies': {\n",
    "            'Magnitude_L1': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.MagnitudeImportance(p=1)},\n",
    "            'Magnitude_L2': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.MagnitudeImportance(p=2)},\n",
    "            'Random': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.RandomImportance()},\n",
    "            'Taylor': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.TaylorImportance()},\n",
    "            # 'LAMP': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.LAMPImportance(p=2)}, # LAMP might need careful tuning for regression\n",
    "            'FPGM': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.FPGMImportance()},\n",
    "        },\n",
    "        'target_macs_sparsity': 0.3,   # Example: Target 30% MAC reduction\n",
    "        'target_params_sparsity': 0.4, # Example: Target 40% Params reduction\n",
    "        # --- LSTM Hyperparameters ---\n",
    "        'lstm_hidden_size': 64,     # Smaller hidden size to start\n",
    "        'lstm_num_layers': 1,       # Single layer often works for simpler tasks\n",
    "        'lstm_dropout': 0.1,\n",
    "        # --- Training ---\n",
    "        'train_epochs': 30,         # Fewer initial epochs, more for fine-tune\n",
    "        'fine_tune_epochs': 40,\n",
    "        'batch_size': 128,\n",
    "        'learning_rate_initial': 0.002,\n",
    "        'learning_rate_finetune': 0.0005, # Lower LR for fine-tuning\n",
    "        'grad_clip': 1.0, # Use gradient clipping\n",
    "        # --- Paths & Pruning ---\n",
    "        'output_dir': './output/lstm_energy_pruning',\n",
    "        'pruning_max_iterations': 50,\n",
    "        'pruning_step_ratio': 0.2,\n",
    "        'pruning_primary_metric': 'mse', # Metric to compare for pruning plots\n",
    "    }\n",
    "    os.makedirs(config['output_dir'], exist_ok=True)\n",
    "\n",
    "    # --- Initialize Data ---\n",
    "    print(\"Loading and processing data...\")\n",
    "    train_loader, val_loader, test_loader, input_size, seq_length, scalers = get_energy_data_loaders(\n",
    "        batch_size=config['batch_size'], seq_length=SEQUENCE_LENGTH\n",
    "    )\n",
    "    if train_loader is None: return # Exit if data loading failed\n",
    "\n",
    "    # --- Initialize Model ---\n",
    "    model = TimeSeriesLSTM(\n",
    "        input_size=input_size, hidden_size=config['lstm_hidden_size'],\n",
    "        num_layers=config['lstm_num_layers'], output_size=1,\n",
    "        dropout_prob=config['lstm_dropout']\n",
    "    ).to(device)\n",
    "    print(\"\\nModel Architecture:\")\n",
    "    print(model)\n",
    "\n",
    "    # --- Create Example Inputs ---\n",
    "    example_input_bs1 = torch.randn(1, seq_length, input_size) # No need to send to device yet\n",
    "    example_gradient_batch = None\n",
    "    try:\n",
    "        grad_batch_data = next(iter(train_loader))\n",
    "        if grad_batch_data[0].shape[0] > 1:\n",
    "            example_gradient_batch = {'inputs': grad_batch_data[0], 'labels': grad_batch_data[1]}\n",
    "            print(f\"Obtained gradient batch with BS={example_gradient_batch['inputs'].shape[0]}\")\n",
    "        else: print(\"Warning: First train batch has BS=1, cannot use for gradient importance.\")\n",
    "    except Exception as e: print(f\"Could not get gradient batch: {e}\")\n",
    "\n",
    "\n",
    "    # --- Initial Training ---\n",
    "    initial_model_path = os.path.join(config['output_dir'], \"lstm_energy_initial.pth\")\n",
    "    initial_best_model_path = os.path.join(config['output_dir'], \"lstm_energy_initial_best_val.pth\") # Path for best model\n",
    "\n",
    "    if not os.path.exists(initial_best_model_path): # Check for best model first\n",
    "        print(\"\\n--- Initial Training ---\")\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate_initial'])\n",
    "        model, _, _ = train_model_regression(\n",
    "            model=model, train_loader=train_loader, criterion=nn.MSELoss().to(device),\n",
    "            optimizer=optimizer, device=device, num_epochs=config['train_epochs'],\n",
    "            val_loader=val_loader,\n",
    "            model_path_prefix=os.path.join(config['output_dir'], \"lstm_energy_initial\"), # Saves best model\n",
    "            grad_clip=config['grad_clip']\n",
    "        )\n",
    "        # Save the *last* epoch model state as well, although we'll prune the best\n",
    "        torch.save(model.state_dict(), initial_model_path)\n",
    "        if not os.path.exists(initial_best_model_path): # If validation didn't improve/save\n",
    "             print(f\"Warning: Best validation model was not saved during initial training. Using last epoch model from {initial_model_path}\")\n",
    "             initial_best_model_path = initial_model_path # Fallback to last epoch\n",
    "        else:\n",
    "            print(f\"Initial best model saved to {initial_best_model_path}\")\n",
    "            # Load the best model back for evaluation/pruning base\n",
    "            print(f\"Loading best initial model ({initial_best_model_path}) for pruning base.\")\n",
    "            model.load_state_dict(torch.load(initial_best_model_path, map_location=device))\n",
    "    else:\n",
    "        print(f\"\\nLoading best initial model from {initial_best_model_path}\")\n",
    "        model.load_state_dict(torch.load(initial_best_model_path, map_location=device))\n",
    "\n",
    "\n",
    "    # --- Evaluate Initial Model ---\n",
    "    results = {}\n",
    "    print(\"\\n--- Evaluating Initial Model ---\")\n",
    "    results['initial'] = evaluate_model_regression(model, test_loader, example_input_bs1, device, scalers)\n",
    "    initial_macs = results['initial']['macs']\n",
    "    initial_params = results['initial']['params']\n",
    "    print(f\"Initial Model Performance: MACs={initial_macs:,.0f}, Params={initial_params:,.0f}, MSE={results['initial']['mse']:.4f}\")\n",
    "\n",
    "\n",
    "    # --- Calculate Targets ---\n",
    "    target_macs_value = initial_macs * (1 - config['target_macs_sparsity'])\n",
    "    target_params_value = initial_params * (1 - config['target_params_sparsity'])\n",
    "    print(f\"\\nTargeting MACs <= {target_macs_value:,.0f}, Params <= {target_params_value:,.0f}\")\n",
    "\n",
    "\n",
    "    # --- Pruning and Fine-tuning Loop ---\n",
    "    for strategy_name, strategy_config in config['strategies'].items():\n",
    "        print(f\"\\n===== Processing Strategy: {strategy_name} =====\")\n",
    "        model_to_prune = TimeSeriesLSTM( # Create a fresh instance\n",
    "             input_size=input_size, hidden_size=config['lstm_hidden_size'],\n",
    "             num_layers=config['lstm_num_layers'], output_size=1,\n",
    "             dropout_prob=config['lstm_dropout']\n",
    "         ).to(device)\n",
    "        # Load the *best initial* weights into the fresh model\n",
    "        model_to_prune.load_state_dict(torch.load(initial_best_model_path, map_location=device))\n",
    "        model_to_prune.eval()\n",
    "\n",
    "        # Gradient batch check\n",
    "        needs_grad = isinstance(strategy_config['importance'], tp.importance.TaylorImportance)\n",
    "        grad_batch = example_gradient_batch if needs_grad else None\n",
    "        if needs_grad and not grad_batch:\n",
    "            print(f\"Skipping {strategy_name}: requires gradient batch, but none available.\")\n",
    "            continue\n",
    "\n",
    "        # Perform Pruning\n",
    "        try:\n",
    "            pruned_model = prune_lstm_model_by_threshold(\n",
    "                model=model_to_prune, example_input_bs1=example_input_bs1,\n",
    "                target_macs=target_macs_value, target_params=target_params_value,\n",
    "                strategy=strategy_config,\n",
    "                max_iterations=config['pruning_max_iterations'],\n",
    "                step_pruning_ratio=config['pruning_step_ratio'],\n",
    "                gradient_batch=grad_batch,\n",
    "                prunable_modules=[model_to_prune.fc] # Explicitly prune only FC\n",
    "            )\n",
    "            pruned_path = os.path.join(config['output_dir'], f\"lstm_{strategy_name}_pruned.pth\")\n",
    "            torch.save(pruned_model.state_dict(), pruned_path)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nERROR during pruning ({strategy_name}): {e}\")\n",
    "            import traceback; traceback.print_exc()\n",
    "            results[strategy_name] = {'error': str(e)} # Mark failure\n",
    "            continue # Skip to next strategy\n",
    "\n",
    "        # Fine-tune\n",
    "        print(f\"\\n--- Fine-tuning ({strategy_name}) ---\")\n",
    "        ft_prefix = os.path.join(config['output_dir'], f\"lstm_{strategy_name}_ft\")\n",
    "        ft_best_path = ft_prefix + \"_best_val.pth\"\n",
    "        optimizer_ft = optim.Adam(pruned_model.parameters(), lr=config['learning_rate_finetune'])\n",
    "        fine_tuned_model, _, _ = train_model_regression(\n",
    "            model=pruned_model, train_loader=train_loader, criterion=nn.MSELoss().to(device),\n",
    "            optimizer=optimizer_ft, device=device, num_epochs=config['fine_tune_epochs'],\n",
    "            val_loader=val_loader, model_path_prefix=ft_prefix, grad_clip=config['grad_clip']\n",
    "        )\n",
    "\n",
    "        # Load the best fine-tuned model\n",
    "        if os.path.exists(ft_best_path):\n",
    "            print(f\"Loading best fine-tuned model from {ft_best_path}\")\n",
    "            fine_tuned_model.load_state_dict(torch.load(ft_best_path, map_location=device))\n",
    "        else:\n",
    "            print(f\"Warning: Best fine-tuned model path not found ({ft_best_path}). Using last epoch.\")\n",
    "\n",
    "        # Evaluate Final\n",
    "        print(f\"\\n--- Evaluating Fine-tuned Model ({strategy_name}) ---\")\n",
    "        results[strategy_name] = evaluate_model_regression(\n",
    "            fine_tuned_model, test_loader, example_input_bs1, device, scalers\n",
    "        )\n",
    "\n",
    "        # Save final model (best validated state)\n",
    "        final_path = os.path.join(config['output_dir'], f\"lstm_{strategy_name}_final.pth\")\n",
    "        torch.save(fine_tuned_model.state_dict(), final_path) # Saves the loaded best model\n",
    "\n",
    "        # --- (Optional) Intermediate Comparison Plot ---\n",
    "        compare_results_and_plot_regression(\n",
    "            results, metric_key=config['pruning_primary_metric'],\n",
    "            lower_is_better=True, # True for MSE/MAE\n",
    "            output_dir=config['output_dir']\n",
    "        )\n",
    "\n",
    "    # --- Final Comparison ---\n",
    "    print(\"\\n===== Final Comparison =====\")\n",
    "    compare_results_and_plot_regression(\n",
    "         results, metric_key=config['pruning_primary_metric'],\n",
    "         lower_is_better=True, output_dir=config['output_dir']\n",
    "     )\n",
    "    print(\"\\nWorkflow completed!\")"
   ],
   "id": "b5ee0fc054f06f78",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Run the main function",
   "id": "ee8690ed3c7ce541"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T05:03:28.312135Z",
     "start_time": "2025-05-04T05:02:38.764425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main_lstm()"
   ],
   "id": "7b4ac278517951f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading and processing data...\n",
      "Loading dataset from: ./data/energydata_complete.csv\n",
      "Original data shape: (19735, 29)\n",
      "Data shape after selecting columns & dropping NaNs: (19735, 26)\n",
      "Data split: Train=14210, Val=1578, Test=3947\n",
      "Data loaded successfully:\n",
      "  Input size (features): 25\n",
      "  Sequence length: 72\n",
      "  Train sequences: 14138\n",
      "  Validation sequences: 1506\n",
      "  Test sequences: 3875\n",
      "\n",
      "Model Architecture:\n",
      "TimeSeriesLSTM(\n",
      "  (lstm): LSTM(25, 64, batch_first=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Obtained gradient batch with BS=128\n",
      "\n",
      "Loading best initial model from ./output/lstm_energy_pruning/lstm_energy_initial_best_val.pth\n",
      "\n",
      "--- Evaluating Initial Model ---\n",
      "Evaluating on test set...\n",
      "Metrics calculated on original scale.\n",
      "Evaluation Metrics: MSE=9016.2822, MAE=70.0812, RMSE=94.9541, R2=-0.2028, MAPE=93.92%\n",
      "Initial Model Performance: MACs=43,084,865, Params=23,361, MSE=9016.2822\n",
      "\n",
      "Targeting MACs <= 30,159,405, Params <= 14,017\n",
      "\n",
      "===== Processing Strategy: Magnitude_L1 =====\n",
      "--- Starting Pruning (MagnitudeImportance) ---\n",
      "Target MACs: <= 30,159,405, Target Params: <= 14,017\n",
      "Step Ratio: 0.20, Max Iter: 50\n",
      "Targeting specific modules for pruning: ['Linear']\n",
      "Initial State | MACs: 43,084,865, Params: 23,361\n",
      "Iter 1: No more candidates found by pruner. Stopping.\n",
      "--- Finished Pruning (MagnitudeImportance) ---\n",
      "Initial | MACs: 43,084,865, Params: 23,361\n",
      "Final   | MACs: 43,084,865 (Reduction: 0.00%)\n",
      "        | Params: 23,361 (Reduction: 0.00%)\n",
      "Target  | MACs <= 30,159,405, Params <= 14,017\n",
      "Warning: Pruning finished, but target threshold(s) were not fully met.\n",
      "\n",
      "--- Fine-tuning (Magnitude_L1) ---\n",
      "Starting training for 40 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
      "  result = _VF.lstm(\n",
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
      "  result = _VF.lstm(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40: Train Loss=0.007947, Time: 0.38s, Val Loss=0.005967 (Best model saved)\n",
      "Epoch 2/40: Train Loss=0.007836, Time: 0.37s, Val Loss=0.006300\n",
      "Epoch 3/40: Train Loss=0.007810, Time: 0.36s, Val Loss=0.006193\n",
      "Epoch 4/40: Train Loss=0.007624, Time: 0.38s, Val Loss=0.006296\n",
      "Epoch 5/40: Train Loss=0.007486, Time: 0.39s, Val Loss=0.006488\n",
      "Epoch 6/40: Train Loss=0.007436, Time: 0.38s, Val Loss=0.007220\n",
      "Epoch 7/40: Train Loss=0.007396, Time: 0.37s, Val Loss=0.006859\n",
      "Epoch 8/40: Train Loss=0.007347, Time: 0.35s, Val Loss=0.007789\n",
      "Epoch 9/40: Train Loss=0.007301, Time: 0.21s, Val Loss=0.006720\n",
      "Epoch 10/40: Train Loss=0.007210, Time: 0.20s, Val Loss=0.008798\n",
      "Epoch 11/40: Train Loss=0.007085, Time: 0.21s, Val Loss=0.010427\n",
      "Epoch 12/40: Train Loss=0.007079, Time: 0.23s, Val Loss=0.007465\n",
      "Epoch 13/40: Train Loss=0.006987, Time: 0.22s, Val Loss=0.007462\n",
      "Epoch 14/40: Train Loss=0.006906, Time: 0.21s, Val Loss=0.007876\n",
      "Epoch 15/40: Train Loss=0.006986, Time: 0.22s, Val Loss=0.007736\n",
      "Epoch 16/40: Train Loss=0.006829, Time: 0.21s, Val Loss=0.009604\n",
      "Epoch 17/40: Train Loss=0.006835, Time: 0.20s, Val Loss=0.007339\n",
      "Epoch 18/40: Train Loss=0.006736, Time: 0.20s, Val Loss=0.008909\n",
      "Epoch 19/40: Train Loss=0.006695, Time: 0.20s, Val Loss=0.009358\n",
      "Epoch 20/40: Train Loss=0.006644, Time: 0.21s, Val Loss=0.010499\n",
      "Epoch 21/40: Train Loss=0.006641, Time: 0.20s, Val Loss=0.013594\n",
      "Epoch 22/40: Train Loss=0.006656, Time: 0.20s, Val Loss=0.007986\n",
      "Epoch 23/40: Train Loss=0.006561, Time: 0.20s, Val Loss=0.007250\n",
      "Epoch 24/40: Train Loss=0.006526, Time: 0.20s, Val Loss=0.011284\n",
      "Epoch 25/40: Train Loss=0.006471, Time: 0.21s, Val Loss=0.008117\n",
      "Epoch 26/40: Train Loss=0.006418, Time: 0.24s, Val Loss=0.008273\n",
      "Epoch 27/40: Train Loss=0.006441, Time: 0.21s, Val Loss=0.009594\n",
      "Epoch 28/40: Train Loss=0.006332, Time: 0.20s, Val Loss=0.012405\n",
      "Epoch 29/40: Train Loss=0.006371, Time: 0.20s, Val Loss=0.006551\n",
      "Epoch 30/40: Train Loss=0.006235, Time: 0.20s, Val Loss=0.008134\n",
      "Epoch 31/40: Train Loss=0.006198, Time: 0.20s, Val Loss=0.012731\n",
      "Epoch 32/40: Train Loss=0.006399, Time: 0.20s, Val Loss=0.007735\n",
      "Epoch 33/40: Train Loss=0.006155, Time: 0.20s, Val Loss=0.009187\n",
      "Epoch 34/40: Train Loss=0.006078, Time: 0.20s, Val Loss=0.010188\n",
      "Epoch 35/40: Train Loss=0.006195, Time: 0.28s, Val Loss=0.009645\n",
      "Epoch 36/40: Train Loss=0.006049, Time: 0.32s, Val Loss=0.007154\n",
      "Epoch 37/40: Train Loss=0.006150, Time: 0.20s, Val Loss=0.009269\n",
      "Epoch 38/40: Train Loss=0.006042, Time: 0.28s, Val Loss=0.011023\n",
      "Epoch 39/40: Train Loss=0.005960, Time: 0.35s, Val Loss=0.011489\n",
      "Epoch 40/40: Train Loss=0.005940, Time: 0.34s, Val Loss=0.008545\n",
      "Training finished.\n",
      "Loading best fine-tuned model from ./output/lstm_energy_pruning/lstm_Magnitude_L1_ft_best_val.pth\n",
      "\n",
      "--- Evaluating Fine-tuned Model (Magnitude_L1) ---\n",
      "Evaluating on test set...\n",
      "Metrics calculated on original scale.\n",
      "Evaluation Metrics: MSE=6967.0127, MAE=46.7419, RMSE=83.4686, R2=0.0706, MAPE=50.51%\n",
      "\n",
      "=== Pruning Strategy Comparison (Metric: MSE) ===\n",
      "Strategy        | MACs (M)   | Params (K) | Size (MB)  | MSE         \n",
      "---------------------------------------------------------------------\n",
      "initial         | 43.08      | 23.4       |       0.09 |    9016.2822\n",
      "Magnitude_L1    | 43.08      | 23.4       |       0.09 |    6967.0127\n",
      "Comparison plots saved to ./output/lstm_energy_pruning\n",
      "\n",
      "===== Processing Strategy: Magnitude_L2 =====\n",
      "--- Starting Pruning (MagnitudeImportance) ---\n",
      "Target MACs: <= 30,159,405, Target Params: <= 14,017\n",
      "Step Ratio: 0.20, Max Iter: 50\n",
      "Targeting specific modules for pruning: ['Linear']\n",
      "Initial State | MACs: 43,084,865, Params: 23,361\n",
      "Iter 1: No more candidates found by pruner. Stopping.\n",
      "--- Finished Pruning (MagnitudeImportance) ---\n",
      "Initial | MACs: 43,084,865, Params: 23,361\n",
      "Final   | MACs: 43,084,865 (Reduction: 0.00%)\n",
      "        | Params: 23,361 (Reduction: 0.00%)\n",
      "Target  | MACs <= 30,159,405, Params <= 14,017\n",
      "Warning: Pruning finished, but target threshold(s) were not fully met.\n",
      "\n",
      "--- Fine-tuning (Magnitude_L2) ---\n",
      "Starting training for 40 epochs...\n",
      "Epoch 1/40: Train Loss=0.007947, Time: 0.19s, Val Loss=0.005856 (Best model saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
      "  result = _VF.lstm(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40: Train Loss=0.007802, Time: 0.19s, Val Loss=0.006331\n",
      "Epoch 3/40: Train Loss=0.007596, Time: 0.20s, Val Loss=0.005954\n",
      "Epoch 4/40: Train Loss=0.007619, Time: 0.20s, Val Loss=0.006760\n",
      "Epoch 5/40: Train Loss=0.007480, Time: 0.20s, Val Loss=0.008160\n",
      "Epoch 6/40: Train Loss=0.007443, Time: 0.20s, Val Loss=0.006972\n",
      "Epoch 7/40: Train Loss=0.007314, Time: 0.20s, Val Loss=0.006821\n",
      "Epoch 8/40: Train Loss=0.007257, Time: 0.20s, Val Loss=0.007125\n",
      "Epoch 9/40: Train Loss=0.007237, Time: 0.20s, Val Loss=0.007075\n",
      "Epoch 10/40: Train Loss=0.007254, Time: 0.18s, Val Loss=0.006748\n",
      "Epoch 11/40: Train Loss=0.007081, Time: 0.18s, Val Loss=0.007145\n",
      "Epoch 12/40: Train Loss=0.007086, Time: 0.19s, Val Loss=0.009703\n",
      "Epoch 13/40: Train Loss=0.006992, Time: 0.20s, Val Loss=0.009102\n",
      "Epoch 14/40: Train Loss=0.007037, Time: 0.20s, Val Loss=0.009319\n",
      "Epoch 15/40: Train Loss=0.006975, Time: 0.20s, Val Loss=0.011160\n",
      "Epoch 16/40: Train Loss=0.006859, Time: 0.19s, Val Loss=0.011737\n",
      "Epoch 17/40: Train Loss=0.006826, Time: 0.20s, Val Loss=0.009415\n",
      "Epoch 18/40: Train Loss=0.006755, Time: 0.20s, Val Loss=0.009780\n",
      "Epoch 19/40: Train Loss=0.006763, Time: 0.20s, Val Loss=0.008906\n",
      "Epoch 20/40: Train Loss=0.006769, Time: 0.20s, Val Loss=0.007452\n",
      "Epoch 21/40: Train Loss=0.006630, Time: 0.18s, Val Loss=0.008379\n",
      "Epoch 22/40: Train Loss=0.006664, Time: 0.18s, Val Loss=0.008421\n",
      "Epoch 23/40: Train Loss=0.006539, Time: 0.18s, Val Loss=0.010214\n",
      "Epoch 24/40: Train Loss=0.006463, Time: 0.19s, Val Loss=0.008041\n",
      "Epoch 25/40: Train Loss=0.006436, Time: 0.20s, Val Loss=0.009566\n",
      "Epoch 26/40: Train Loss=0.006440, Time: 0.20s, Val Loss=0.008900\n",
      "Epoch 27/40: Train Loss=0.006389, Time: 0.20s, Val Loss=0.010424\n",
      "Epoch 28/40: Train Loss=0.006327, Time: 0.18s, Val Loss=0.008870\n",
      "Epoch 29/40: Train Loss=0.006236, Time: 0.20s, Val Loss=0.009128\n",
      "Epoch 30/40: Train Loss=0.006214, Time: 0.20s, Val Loss=0.009362\n",
      "Epoch 31/40: Train Loss=0.006193, Time: 0.20s, Val Loss=0.007946\n",
      "Epoch 32/40: Train Loss=0.006169, Time: 0.20s, Val Loss=0.008618\n",
      "Epoch 33/40: Train Loss=0.006199, Time: 0.20s, Val Loss=0.010001\n",
      "Epoch 34/40: Train Loss=0.006047, Time: 0.20s, Val Loss=0.008687\n",
      "Epoch 35/40: Train Loss=0.006499, Time: 0.19s, Val Loss=0.009587\n",
      "Epoch 36/40: Train Loss=0.006139, Time: 0.20s, Val Loss=0.008420\n",
      "Epoch 37/40: Train Loss=0.006120, Time: 0.20s, Val Loss=0.008792\n",
      "Epoch 38/40: Train Loss=0.006036, Time: 0.20s, Val Loss=0.008775\n",
      "Epoch 39/40: Train Loss=0.005996, Time: 0.20s, Val Loss=0.008951\n",
      "Epoch 40/40: Train Loss=0.006052, Time: 0.20s, Val Loss=0.010335\n",
      "Training finished.\n",
      "Loading best fine-tuned model from ./output/lstm_energy_pruning/lstm_Magnitude_L2_ft_best_val.pth\n",
      "\n",
      "--- Evaluating Fine-tuned Model (Magnitude_L2) ---\n",
      "Evaluating on test set...\n",
      "Metrics calculated on original scale.\n",
      "Evaluation Metrics: MSE=13800.6045, MAE=78.2966, RMSE=117.4760, R2=-0.8411, MAPE=102.29%\n",
      "\n",
      "=== Pruning Strategy Comparison (Metric: MSE) ===\n",
      "Strategy        | MACs (M)   | Params (K) | Size (MB)  | MSE         \n",
      "---------------------------------------------------------------------\n",
      "initial         | 43.08      | 23.4       |       0.09 |    9016.2822\n",
      "Magnitude_L1    | 43.08      | 23.4       |       0.09 |    6967.0127\n",
      "Magnitude_L2    | 43.08      | 23.4       |       0.09 |   13800.6045\n",
      "Comparison plots saved to ./output/lstm_energy_pruning\n",
      "\n",
      "===== Processing Strategy: Random =====\n",
      "--- Starting Pruning (RandomImportance) ---\n",
      "Target MACs: <= 30,159,405, Target Params: <= 14,017\n",
      "Step Ratio: 0.20, Max Iter: 50\n",
      "Targeting specific modules for pruning: ['Linear']\n",
      "Initial State | MACs: 43,084,865, Params: 23,361\n",
      "Iter 1: No more candidates found by pruner. Stopping.\n",
      "--- Finished Pruning (RandomImportance) ---\n",
      "Initial | MACs: 43,084,865, Params: 23,361\n",
      "Final   | MACs: 43,084,865 (Reduction: 0.00%)\n",
      "        | Params: 23,361 (Reduction: 0.00%)\n",
      "Target  | MACs <= 30,159,405, Params <= 14,017\n",
      "Warning: Pruning finished, but target threshold(s) were not fully met.\n",
      "\n",
      "--- Fine-tuning (Random) ---\n",
      "Starting training for 40 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
      "  result = _VF.lstm(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40: Train Loss=0.007965, Time: 0.20s, Val Loss=0.005900 (Best model saved)\n",
      "Epoch 2/40: Train Loss=0.007827, Time: 0.20s, Val Loss=0.006081\n",
      "Epoch 3/40: Train Loss=0.007735, Time: 0.20s, Val Loss=0.006170\n",
      "Epoch 4/40: Train Loss=0.007570, Time: 0.20s, Val Loss=0.006233\n",
      "Epoch 5/40: Train Loss=0.007584, Time: 0.20s, Val Loss=0.006082\n",
      "Epoch 6/40: Train Loss=0.007474, Time: 0.20s, Val Loss=0.006136\n",
      "Epoch 7/40: Train Loss=0.007400, Time: 0.20s, Val Loss=0.007067\n",
      "Epoch 8/40: Train Loss=0.007327, Time: 0.20s, Val Loss=0.007676\n",
      "Epoch 9/40: Train Loss=0.007215, Time: 0.20s, Val Loss=0.008256\n",
      "Epoch 10/40: Train Loss=0.007275, Time: 0.20s, Val Loss=0.008668\n",
      "Epoch 11/40: Train Loss=0.007168, Time: 0.21s, Val Loss=0.008084\n",
      "Epoch 12/40: Train Loss=0.007092, Time: 0.20s, Val Loss=0.008117\n",
      "Epoch 13/40: Train Loss=0.007017, Time: 0.20s, Val Loss=0.006981\n",
      "Epoch 14/40: Train Loss=0.006875, Time: 0.21s, Val Loss=0.006871\n",
      "Epoch 15/40: Train Loss=0.006922, Time: 0.21s, Val Loss=0.006659\n",
      "Epoch 16/40: Train Loss=0.006883, Time: 0.23s, Val Loss=0.008516\n",
      "Epoch 17/40: Train Loss=0.006871, Time: 0.28s, Val Loss=0.008672\n",
      "Epoch 18/40: Train Loss=0.006769, Time: 0.29s, Val Loss=0.009792\n",
      "Epoch 19/40: Train Loss=0.006672, Time: 0.20s, Val Loss=0.007631\n",
      "Epoch 20/40: Train Loss=0.006763, Time: 0.24s, Val Loss=0.007641\n",
      "Epoch 21/40: Train Loss=0.006703, Time: 0.30s, Val Loss=0.009109\n",
      "Epoch 22/40: Train Loss=0.006574, Time: 0.21s, Val Loss=0.008983\n",
      "Epoch 23/40: Train Loss=0.006467, Time: 0.20s, Val Loss=0.009875\n",
      "Epoch 24/40: Train Loss=0.006593, Time: 0.20s, Val Loss=0.007794\n",
      "Epoch 25/40: Train Loss=0.006444, Time: 0.20s, Val Loss=0.009365\n",
      "Epoch 26/40: Train Loss=0.006338, Time: 0.20s, Val Loss=0.009209\n",
      "Epoch 27/40: Train Loss=0.006260, Time: 0.24s, Val Loss=0.009203\n",
      "Epoch 28/40: Train Loss=0.006304, Time: 0.20s, Val Loss=0.011955\n",
      "Epoch 29/40: Train Loss=0.006186, Time: 0.20s, Val Loss=0.009857\n",
      "Epoch 30/40: Train Loss=0.006208, Time: 0.20s, Val Loss=0.008139\n",
      "Epoch 31/40: Train Loss=0.006205, Time: 0.25s, Val Loss=0.009721\n",
      "Epoch 32/40: Train Loss=0.006129, Time: 0.22s, Val Loss=0.009620\n",
      "Epoch 33/40: Train Loss=0.006217, Time: 0.22s, Val Loss=0.010117\n",
      "Epoch 34/40: Train Loss=0.006080, Time: 0.20s, Val Loss=0.009408\n",
      "Epoch 35/40: Train Loss=0.006051, Time: 0.19s, Val Loss=0.008982\n",
      "Epoch 36/40: Train Loss=0.006139, Time: 0.21s, Val Loss=0.008380\n",
      "Epoch 37/40: Train Loss=0.006001, Time: 0.20s, Val Loss=0.009639\n",
      "Epoch 38/40: Train Loss=0.005928, Time: 0.24s, Val Loss=0.011387\n",
      "Epoch 39/40: Train Loss=0.006004, Time: 0.22s, Val Loss=0.010219\n",
      "Epoch 40/40: Train Loss=0.005950, Time: 0.21s, Val Loss=0.009580\n",
      "Training finished.\n",
      "Loading best fine-tuned model from ./output/lstm_energy_pruning/lstm_Random_ft_best_val.pth\n",
      "\n",
      "--- Evaluating Fine-tuned Model (Random) ---\n",
      "Evaluating on test set...\n",
      "Metrics calculated on original scale.\n",
      "Evaluation Metrics: MSE=7646.4121, MAE=54.0824, RMSE=87.4438, R2=-0.0201, MAPE=64.98%\n",
      "\n",
      "=== Pruning Strategy Comparison (Metric: MSE) ===\n",
      "Strategy        | MACs (M)   | Params (K) | Size (MB)  | MSE         \n",
      "---------------------------------------------------------------------\n",
      "initial         | 43.08      | 23.4       |       0.09 |    9016.2822\n",
      "Magnitude_L1    | 43.08      | 23.4       |       0.09 |    6967.0127\n",
      "Magnitude_L2    | 43.08      | 23.4       |       0.09 |   13800.6045\n",
      "Random          | 43.08      | 23.4       |       0.09 |    7646.4121\n",
      "Comparison plots saved to ./output/lstm_energy_pruning\n",
      "\n",
      "===== Processing Strategy: Taylor =====\n",
      "--- Starting Pruning (TaylorImportance) ---\n",
      "Target MACs: <= 30,159,405, Target Params: <= 14,017\n",
      "Step Ratio: 0.20, Max Iter: 50\n",
      "Targeting specific modules for pruning: ['Linear']\n",
      "Initial State | MACs: 43,084,865, Params: 23,361\n",
      "Iter 1: No more candidates found by pruner. Stopping.\n",
      "--- Finished Pruning (TaylorImportance) ---\n",
      "Initial | MACs: 43,084,865, Params: 23,361\n",
      "Final   | MACs: 43,084,865 (Reduction: 0.00%)\n",
      "        | Params: 23,361 (Reduction: 0.00%)\n",
      "Target  | MACs <= 30,159,405, Params <= 14,017\n",
      "Warning: Pruning finished, but target threshold(s) were not fully met.\n",
      "\n",
      "--- Fine-tuning (Taylor) ---\n",
      "Starting training for 40 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
      "  result = _VF.lstm(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40: Train Loss=0.007969, Time: 0.26s, Val Loss=0.005905 (Best model saved)\n",
      "Epoch 2/40: Train Loss=0.007775, Time: 0.25s, Val Loss=0.005931\n",
      "Epoch 3/40: Train Loss=0.007671, Time: 0.33s, Val Loss=0.006410\n",
      "Epoch 4/40: Train Loss=0.007506, Time: 0.23s, Val Loss=0.006343\n",
      "Epoch 5/40: Train Loss=0.007533, Time: 0.20s, Val Loss=0.006567\n",
      "Epoch 6/40: Train Loss=0.007488, Time: 0.20s, Val Loss=0.007059\n",
      "Epoch 7/40: Train Loss=0.007291, Time: 0.20s, Val Loss=0.006709\n",
      "Epoch 8/40: Train Loss=0.007266, Time: 0.20s, Val Loss=0.006833\n",
      "Epoch 9/40: Train Loss=0.007264, Time: 0.21s, Val Loss=0.009523\n",
      "Epoch 10/40: Train Loss=0.007171, Time: 0.22s, Val Loss=0.010406\n",
      "Epoch 11/40: Train Loss=0.007173, Time: 0.21s, Val Loss=0.007195\n",
      "Epoch 12/40: Train Loss=0.006998, Time: 0.23s, Val Loss=0.009595\n",
      "Epoch 13/40: Train Loss=0.006945, Time: 0.23s, Val Loss=0.008427\n",
      "Epoch 14/40: Train Loss=0.006924, Time: 0.25s, Val Loss=0.009222\n",
      "Epoch 15/40: Train Loss=0.006862, Time: 0.26s, Val Loss=0.008426\n",
      "Epoch 16/40: Train Loss=0.006884, Time: 0.32s, Val Loss=0.007578\n",
      "Epoch 17/40: Train Loss=0.006805, Time: 0.35s, Val Loss=0.007977\n",
      "Epoch 18/40: Train Loss=0.006737, Time: 0.31s, Val Loss=0.008136\n",
      "Epoch 19/40: Train Loss=0.006744, Time: 0.21s, Val Loss=0.009894\n",
      "Epoch 20/40: Train Loss=0.006663, Time: 0.20s, Val Loss=0.009171\n",
      "Epoch 21/40: Train Loss=0.006578, Time: 0.20s, Val Loss=0.012615\n",
      "Epoch 22/40: Train Loss=0.006676, Time: 0.20s, Val Loss=0.006766\n",
      "Epoch 23/40: Train Loss=0.006454, Time: 0.23s, Val Loss=0.009880\n",
      "Epoch 24/40: Train Loss=0.006437, Time: 0.22s, Val Loss=0.007791\n",
      "Epoch 25/40: Train Loss=0.006476, Time: 0.20s, Val Loss=0.008311\n",
      "Epoch 26/40: Train Loss=0.006375, Time: 0.20s, Val Loss=0.007641\n",
      "Epoch 27/40: Train Loss=0.006360, Time: 0.20s, Val Loss=0.008475\n",
      "Epoch 28/40: Train Loss=0.006292, Time: 0.19s, Val Loss=0.007308\n",
      "Epoch 29/40: Train Loss=0.006238, Time: 0.20s, Val Loss=0.011153\n",
      "Epoch 30/40: Train Loss=0.006220, Time: 0.20s, Val Loss=0.008658\n",
      "Epoch 31/40: Train Loss=0.006324, Time: 0.20s, Val Loss=0.008780\n",
      "Epoch 32/40: Train Loss=0.006111, Time: 0.21s, Val Loss=0.009856\n",
      "Epoch 33/40: Train Loss=0.006140, Time: 0.23s, Val Loss=0.010175\n",
      "Epoch 34/40: Train Loss=0.006088, Time: 0.20s, Val Loss=0.009803\n",
      "Epoch 35/40: Train Loss=0.006036, Time: 0.20s, Val Loss=0.009473\n",
      "Epoch 36/40: Train Loss=0.005939, Time: 0.20s, Val Loss=0.009209\n",
      "Epoch 37/40: Train Loss=0.005996, Time: 0.22s, Val Loss=0.011468\n",
      "Epoch 38/40: Train Loss=0.005914, Time: 0.19s, Val Loss=0.009527\n",
      "Epoch 39/40: Train Loss=0.005815, Time: 0.20s, Val Loss=0.010489\n",
      "Epoch 40/40: Train Loss=0.005879, Time: 0.20s, Val Loss=0.010280\n",
      "Training finished.\n",
      "Loading best fine-tuned model from ./output/lstm_energy_pruning/lstm_Taylor_ft_best_val.pth\n",
      "\n",
      "--- Evaluating Fine-tuned Model (Taylor) ---\n",
      "Evaluating on test set...\n",
      "Metrics calculated on original scale.\n",
      "Evaluation Metrics: MSE=8266.3916, MAE=57.5755, RMSE=90.9197, R2=-0.1028, MAPE=69.80%\n",
      "\n",
      "=== Pruning Strategy Comparison (Metric: MSE) ===\n",
      "Strategy        | MACs (M)   | Params (K) | Size (MB)  | MSE         \n",
      "---------------------------------------------------------------------\n",
      "initial         | 43.08      | 23.4       |       0.09 |    9016.2822\n",
      "Magnitude_L1    | 43.08      | 23.4       |       0.09 |    6967.0127\n",
      "Magnitude_L2    | 43.08      | 23.4       |       0.09 |   13800.6045\n",
      "Random          | 43.08      | 23.4       |       0.09 |    7646.4121\n",
      "Taylor          | 43.08      | 23.4       |       0.09 |    8266.3916\n",
      "Comparison plots saved to ./output/lstm_energy_pruning\n",
      "\n",
      "===== Processing Strategy: FPGM =====\n",
      "--- Starting Pruning (FPGMImportance) ---\n",
      "Target MACs: <= 30,159,405, Target Params: <= 14,017\n",
      "Step Ratio: 0.20, Max Iter: 50\n",
      "Targeting specific modules for pruning: ['Linear']\n",
      "Initial State | MACs: 43,084,865, Params: 23,361\n",
      "Iter 1: No more candidates found by pruner. Stopping.\n",
      "--- Finished Pruning (FPGMImportance) ---\n",
      "Initial | MACs: 43,084,865, Params: 23,361\n",
      "Final   | MACs: 43,084,865 (Reduction: 0.00%)\n",
      "        | Params: 23,361 (Reduction: 0.00%)\n",
      "Target  | MACs <= 30,159,405, Params <= 14,017\n",
      "Warning: Pruning finished, but target threshold(s) were not fully met.\n",
      "\n",
      "--- Fine-tuning (FPGM) ---\n",
      "Starting training for 40 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
      "  result = _VF.lstm(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40: Train Loss=0.007964, Time: 0.21s, Val Loss=0.005874 (Best model saved)\n",
      "Epoch 2/40: Train Loss=0.007758, Time: 0.21s, Val Loss=0.005922\n",
      "Epoch 3/40: Train Loss=0.007659, Time: 0.22s, Val Loss=0.006019\n",
      "Epoch 4/40: Train Loss=0.007608, Time: 0.20s, Val Loss=0.006181\n",
      "Epoch 5/40: Train Loss=0.007531, Time: 0.20s, Val Loss=0.007812\n",
      "Epoch 6/40: Train Loss=0.007456, Time: 0.20s, Val Loss=0.006604\n",
      "Epoch 7/40: Train Loss=0.007418, Time: 0.22s, Val Loss=0.007004\n",
      "Epoch 8/40: Train Loss=0.007326, Time: 0.20s, Val Loss=0.006539\n",
      "Epoch 9/40: Train Loss=0.007217, Time: 0.26s, Val Loss=0.007824\n",
      "Epoch 10/40: Train Loss=0.007172, Time: 0.21s, Val Loss=0.009934\n",
      "Epoch 11/40: Train Loss=0.007118, Time: 0.20s, Val Loss=0.008519\n",
      "Epoch 12/40: Train Loss=0.007030, Time: 0.22s, Val Loss=0.007812\n",
      "Epoch 13/40: Train Loss=0.007042, Time: 0.26s, Val Loss=0.010235\n",
      "Epoch 14/40: Train Loss=0.006958, Time: 0.21s, Val Loss=0.007293\n",
      "Epoch 15/40: Train Loss=0.006913, Time: 0.22s, Val Loss=0.010008\n",
      "Epoch 16/40: Train Loss=0.006930, Time: 0.23s, Val Loss=0.008445\n",
      "Epoch 17/40: Train Loss=0.006894, Time: 0.24s, Val Loss=0.007540\n",
      "Epoch 18/40: Train Loss=0.006805, Time: 0.25s, Val Loss=0.008219\n",
      "Epoch 19/40: Train Loss=0.006875, Time: 0.24s, Val Loss=0.006476\n",
      "Epoch 20/40: Train Loss=0.006827, Time: 0.24s, Val Loss=0.007209\n",
      "Epoch 21/40: Train Loss=0.006608, Time: 0.27s, Val Loss=0.008499\n",
      "Epoch 22/40: Train Loss=0.007015, Time: 0.23s, Val Loss=0.008638\n",
      "Epoch 23/40: Train Loss=0.006626, Time: 0.20s, Val Loss=0.007883\n",
      "Epoch 24/40: Train Loss=0.006560, Time: 0.26s, Val Loss=0.007875\n",
      "Epoch 25/40: Train Loss=0.006479, Time: 0.25s, Val Loss=0.011279\n",
      "Epoch 26/40: Train Loss=0.006531, Time: 0.37s, Val Loss=0.007384\n",
      "Epoch 27/40: Train Loss=0.006463, Time: 0.37s, Val Loss=0.008424\n",
      "Epoch 28/40: Train Loss=0.006361, Time: 0.33s, Val Loss=0.008014\n",
      "Epoch 29/40: Train Loss=0.006364, Time: 0.30s, Val Loss=0.008905\n",
      "Epoch 30/40: Train Loss=0.006305, Time: 0.34s, Val Loss=0.009003\n",
      "Epoch 31/40: Train Loss=0.006270, Time: 0.36s, Val Loss=0.007954\n",
      "Epoch 32/40: Train Loss=0.006211, Time: 0.24s, Val Loss=0.008990\n",
      "Epoch 33/40: Train Loss=0.006198, Time: 0.20s, Val Loss=0.008328\n",
      "Epoch 34/40: Train Loss=0.006200, Time: 0.19s, Val Loss=0.008735\n",
      "Epoch 35/40: Train Loss=0.006201, Time: 0.20s, Val Loss=0.009328\n",
      "Epoch 36/40: Train Loss=0.006229, Time: 0.19s, Val Loss=0.008773\n",
      "Epoch 37/40: Train Loss=0.006151, Time: 0.21s, Val Loss=0.009853\n",
      "Epoch 38/40: Train Loss=0.006012, Time: 0.20s, Val Loss=0.009546\n",
      "Epoch 39/40: Train Loss=0.005916, Time: 0.20s, Val Loss=0.009567\n",
      "Epoch 40/40: Train Loss=0.006020, Time: 0.20s, Val Loss=0.009169\n",
      "Training finished.\n",
      "Loading best fine-tuned model from ./output/lstm_energy_pruning/lstm_FPGM_ft_best_val.pth\n",
      "\n",
      "--- Evaluating Fine-tuned Model (FPGM) ---\n",
      "Evaluating on test set...\n",
      "Metrics calculated on original scale.\n",
      "Evaluation Metrics: MSE=7431.3584, MAE=55.1230, RMSE=86.2053, R2=0.0086, MAPE=67.59%\n",
      "\n",
      "=== Pruning Strategy Comparison (Metric: MSE) ===\n",
      "Strategy        | MACs (M)   | Params (K) | Size (MB)  | MSE         \n",
      "---------------------------------------------------------------------\n",
      "initial         | 43.08      | 23.4       |       0.09 |    9016.2822\n",
      "Magnitude_L1    | 43.08      | 23.4       |       0.09 |    6967.0127\n",
      "Magnitude_L2    | 43.08      | 23.4       |       0.09 |   13800.6045\n",
      "Random          | 43.08      | 23.4       |       0.09 |    7646.4121\n",
      "Taylor          | 43.08      | 23.4       |       0.09 |    8266.3916\n",
      "FPGM            | 43.08      | 23.4       |       0.09 |    7431.3584\n",
      "Comparison plots saved to ./output/lstm_energy_pruning\n",
      "\n",
      "===== Final Comparison =====\n",
      "\n",
      "=== Pruning Strategy Comparison (Metric: MSE) ===\n",
      "Strategy        | MACs (M)   | Params (K) | Size (MB)  | MSE         \n",
      "---------------------------------------------------------------------\n",
      "initial         | 43.08      | 23.4       |       0.09 |    9016.2822\n",
      "Magnitude_L1    | 43.08      | 23.4       |       0.09 |    6967.0127\n",
      "Magnitude_L2    | 43.08      | 23.4       |       0.09 |   13800.6045\n",
      "Random          | 43.08      | 23.4       |       0.09 |    7646.4121\n",
      "Taylor          | 43.08      | 23.4       |       0.09 |    8266.3916\n",
      "FPGM            | 43.08      | 23.4       |       0.09 |    7431.3584\n",
      "Comparison plots saved to ./output/lstm_energy_pruning\n",
      "\n",
      "Workflow completed!\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
