{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pruning Experiment for LSTM on Energy Prediction Dataset",
   "id": "2d8c0a03b0b4f593"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T08:12:24.593323Z",
     "start_time": "2025-05-07T08:12:22.702837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_pruning as tp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "import os\n",
    "import time\n",
    "import copy"
   ],
   "id": "a294b7a752b233c0",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. LSTM Model Definition",
   "id": "570052bd8dc7c73e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TimeSeriesLSTM(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int, output_size: int, dropout_rate: float = 0.5):\n",
    "        super(TimeSeriesLSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        self.lstm.flatten_parameters()\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = out[:, -1, :]\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ],
   "id": "84d1a3db10b7ba1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TimeSeriesLSTM_MoreLayers(nn.Module): # Renamed for clarity\n",
    "    def __init__(self, input_size, hidden_size, num_layers,\n",
    "                 intermediate_size_1=32, # Size for the first intermediate layer\n",
    "                 intermediate_size_2=24, # Size for the second\n",
    "                 intermediate_size_3=16, # Size for the third\n",
    "                 output_size=1,\n",
    "                 dropout_prob=0.2):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                            batch_first=True, dropout=dropout_prob if num_layers > 1 else 0)\n",
    "\n",
    "        # Define the intermediate Linear layers\n",
    "        # Layer 1\n",
    "        self.intermediate_fc1 = nn.Linear(hidden_size, intermediate_size_1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Layer 2\n",
    "        self.intermediate_fc2 = nn.Linear(intermediate_size_1, intermediate_size_2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Layer 3\n",
    "        self.intermediate_fc3 = nn.Linear(intermediate_size_2, intermediate_size_3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Final output layer\n",
    "        self.fc_final = nn.Linear(intermediate_size_3, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_time_step_out = lstm_out[:, -1, :] # Shape: (batch, hidden_size)\n",
    "\n",
    "        # Pass through intermediate layers\n",
    "        x = self.intermediate_fc1(last_time_step_out)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x) # Shape: (batch, intermediate_size_1)\n",
    "\n",
    "        x = self.intermediate_fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x) # Shape: (batch, intermediate_size_2)\n",
    "\n",
    "        x = self.intermediate_fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x) # Shape: (batch, intermediate_size_3)\n",
    "\n",
    "        final_out = self.fc_final(x) # Shape: (batch, output_size)\n",
    "        return final_out"
   ],
   "id": "c24f5a3ce675fa51"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Seed block",
   "id": "d4b84a5a8884261e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T09:45:18.552307Z",
     "start_time": "2025-05-07T09:45:18.548173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IntermediateBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout_prob=0.2):\n",
    "        \"\"\"\n",
    "        A block consisting of a Linear layer, ReLU activation, and Dropout.\n",
    "        Args:\n",
    "            in_features (int): Number of input features to the Linear layer.\n",
    "            out_features (int): Number of output features from the Linear layer.\n",
    "            dropout_prob (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super(IntermediateBlock, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ],
   "id": "8adc8a568416a27f",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### LSTM with Intermidiate Blocks",
   "id": "c4686cdc2e09414c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T09:45:24.237328Z",
     "start_time": "2025-05-07T09:45:24.229883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TimeSeriesLSTM_WithBlocks(nn.Module):\n",
    "    def __init__(self, input_size, lstm_hidden_size, num_lstm_layers,\n",
    "                 block_configs, # List of output sizes for each intermediate block\n",
    "                 output_size=1,\n",
    "                 lstm_dropout_prob=0.2, # Dropout for LSTM layers if num_layers > 1\n",
    "                 block_dropout_prob=0.2): # Dropout within each IntermediateBlock\n",
    "        \"\"\"\n",
    "        LSTM model followed by a sequence of IntermediateBlocks.\n",
    "        Args:\n",
    "            input_size (int): Number of features per time step for LSTM.\n",
    "            lstm_hidden_size (int): Hidden size of the LSTM layer(s).\n",
    "            num_lstm_layers (int): Number of stacked LSTM layers.\n",
    "            block_configs (list of int): A list where each integer is the\n",
    "                                         'out_features' for an IntermediateBlock.\n",
    "                                         e.g., [48, 32, 24] for three blocks.\n",
    "            output_size (int): Final output dimension (e.g., 1 for regression).\n",
    "            lstm_dropout_prob (float): Dropout probability for LSTM (if num_layers > 1).\n",
    "            block_dropout_prob (float): Dropout probability for each IntermediateBlock.\n",
    "        \"\"\"\n",
    "        super(TimeSeriesLSTM_WithBlocks, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, lstm_hidden_size, num_lstm_layers,\n",
    "                            batch_first=True,\n",
    "                            dropout=lstm_dropout_prob if num_lstm_layers > 1 else 0)\n",
    "\n",
    "        # Create intermediate blocks dynamically\n",
    "        self.intermediate_blocks = nn.ModuleList()\n",
    "        current_in_features = lstm_hidden_size # Input to the first block is LSTM's output\n",
    "\n",
    "        for block_out_features in block_configs:\n",
    "            self.intermediate_blocks.append(\n",
    "                IntermediateBlock(current_in_features, block_out_features, block_dropout_prob)\n",
    "            )\n",
    "            current_in_features = block_out_features # Output of this block is input to the next\n",
    "\n",
    "        # Final output layer takes input from the last intermediate block\n",
    "        self.fc_final = nn.Linear(current_in_features, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM part\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Use the output from the last time step of the LSTM\n",
    "        x = lstm_out[:, -1, :] # Shape: (batch, lstm_hidden_size)\n",
    "\n",
    "        # Pass through intermediate blocks\n",
    "        for block in self.intermediate_blocks:\n",
    "            x = block(x)\n",
    "        # After loop, x shape: (batch, block_configs[-1]) if block_configs is not empty\n",
    "        # or (batch, lstm_hidden_size) if block_configs is empty\n",
    "\n",
    "        # Final output\n",
    "        final_out = self.fc_final(x)\n",
    "        return final_out"
   ],
   "id": "613b6cb9fc18b2ce",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. Data Handling for Appliances Energy Dataset",
   "id": "fa5bfde9f572ff1c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### --- Data Configuration ---",
   "id": "1f8bdebfbacf4610"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T08:12:49.614571Z",
     "start_time": "2025-05-07T08:12:49.609687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATASET_PATH = './data/energydata_complete.csv' # ADJUST PATH AS NEEDED\n",
    "SEQUENCE_LENGTH = 6 * 12 # Use 12 hours of past data (12 hours * 6 samples/hour)\n",
    "TARGET_COLUMN = 'Appliances'\n",
    "# Features to use (excluding target, date, and others)\n",
    "FEATURE_COLUMNS = [\n",
    "    'lights', 'T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4', 'RH_4', 'T5', 'RH_5',\n",
    "    'T6', 'RH_6', 'T7', 'RH_7', 'T8', 'RH_8', 'T9', 'RH_9', 'T_out',\n",
    "    'Press_mm_hg', 'RH_out', 'Windspeed', 'Visibility', 'Tdewpoint'\n",
    "]"
   ],
   "id": "61c81d8ddfcfb036",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### --- Helper function to create sequences ---",
   "id": "2ce171fdcc1ab648"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T09:45:29.396343Z",
     "start_time": "2025-05-07T09:45:29.391179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_sequences(input_data, target_data, seq_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    # Stop seq_length steps early to ensure target data is available\n",
    "    for i in range(len(input_data) - seq_length):\n",
    "        sequences.append(input_data[i:i + seq_length])\n",
    "        targets.append(target_data[i + seq_length])\n",
    "    return np.array(sequences), np.array(targets)"
   ],
   "id": "58bf5c6972852395",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### --- Custom Dataset ---",
   "id": "772ca3cbf9a3db8c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T09:45:31.920156Z",
     "start_time": "2025-05-07T09:45:31.915732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EnergyDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32).unsqueeze(1) # Target shape [N, 1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.targets[idx]"
   ],
   "id": "b6f431e56a4fa1f6",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### --- Main Data Loading Function ---",
   "id": "55d077a66ae3818b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T09:45:34.059152Z",
     "start_time": "2025-05-07T09:45:34.044625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_energy_data_loaders(\n",
    "    file_path, # Pass these from config\n",
    "    feature_cols,\n",
    "    target_col,\n",
    "    seq_length,\n",
    "    batch_size=64,\n",
    "    test_size_ratio=0.2, # Ratio for test set\n",
    "    val_size_ratio=0.125 # Ratio for validation set FROM THE REMAINING (0.125 * 0.8 = 0.1 of total)\n",
    "    ):\n",
    "    \"\"\"Loads, preprocesses, scales, and creates sequences for the energy dataset.\"\"\"\n",
    "    print(f\"Loading dataset from: {file_path}\")\n",
    "    try:\n",
    "        # 1. Load Data & Initial Processing\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Original data shape: {df.shape}\")\n",
    "\n",
    "        # --- Convert 'date' to datetime and sort ---\n",
    "        if 'date' not in df.columns:\n",
    "            raise ValueError(\"'date' column not found in the dataset.\")\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df = df.sort_values('date').reset_index(drop=True) # Ensure sorted and clean index\n",
    "\n",
    "        # --- Select relevant columns ---\n",
    "        all_cols_to_keep = list(set(feature_cols + [target_col])) # Use set to avoid duplicates\n",
    "        missing_cols = [col for col in all_cols_to_keep if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Following columns are missing from the dataset: {missing_cols}\")\n",
    "        df_selected = df[all_cols_to_keep]\n",
    "\n",
    "        # --- Ensure all features are numeric (important before scaling) ---\n",
    "        for col in feature_cols:\n",
    "            if not pd.api.types.is_numeric_dtype(df_selected[col]):\n",
    "                try:\n",
    "                    df_selected[col] = pd.to_numeric(df_selected[col], errors='coerce') # Try to convert\n",
    "                    print(f\"Warning: Column '{col}' was non-numeric, converted to numeric. Check for NaNs.\")\n",
    "                except Exception as e:\n",
    "                    raise ValueError(f\"Column '{col}' is non-numeric and could not be converted. Error: {e}\")\n",
    "\n",
    "        # --- Handle NaNs AFTER potential conversion and selection ---\n",
    "        initial_rows = len(df_selected)\n",
    "        df_selected = df_selected.dropna()\n",
    "        if len(df_selected) < initial_rows:\n",
    "            print(f\"Dropped {initial_rows - len(df_selected)} rows due to NaNs.\")\n",
    "        if df_selected.empty:\n",
    "            raise ValueError(\"DataFrame is empty after selecting columns and dropping NaNs.\")\n",
    "        print(f\"Data shape after selecting columns & cleaning NaNs: {df_selected.shape}\")\n",
    "\n",
    "\n",
    "        # 2. Separate Features and Target\n",
    "        X = df_selected[feature_cols].values\n",
    "        y = df_selected[[target_col]].values # Keep as 2D: [N, 1]\n",
    "\n",
    "        # 3. Splitting (Chronological)\n",
    "        n_total = len(X)\n",
    "        n_test = int(n_total * test_size_ratio)\n",
    "        n_val = int((n_total - n_test) * val_size_ratio) # Val size from remaining train_val data\n",
    "        n_train = n_total - n_test - n_val\n",
    "\n",
    "        # Ensure each split has enough data for at least one sequence + target\n",
    "        min_data_needed = seq_length + 1\n",
    "        if n_train < min_data_needed:\n",
    "             raise ValueError(f\"Not enough data for training sequences ({n_train} rows < {min_data_needed} needed). Try decreasing seq_length or test/val ratios.\")\n",
    "        if n_val < min_data_needed:\n",
    "             raise ValueError(f\"Not enough data for validation sequences ({n_val} rows < {min_data_needed} needed).\")\n",
    "        if n_test < min_data_needed:\n",
    "             raise ValueError(f\"Not enough data for test sequences ({n_test} rows < {min_data_needed} needed).\")\n",
    "\n",
    "        X_train, y_train = X[:n_train], y[:n_train]\n",
    "        X_val, y_val = X[n_train : n_train + n_val], y[n_train : n_train + n_val]\n",
    "        X_test, y_test = X[n_train + n_val:], y[n_train + n_val:]\n",
    "        print(f\"Data split: Train={X_train.shape[0]}, Val={X_val.shape[0]}, Test={X_test.shape[0]}\")\n",
    "\n",
    "        # 4. Scaling\n",
    "        scaler_features = MinMaxScaler()\n",
    "        scaler_target = MinMaxScaler()\n",
    "\n",
    "        # Fit on training data ONLY\n",
    "        X_train_scaled = scaler_features.fit_transform(X_train)\n",
    "        y_train_scaled = scaler_target.fit_transform(y_train)\n",
    "\n",
    "        # Transform validation and test data\n",
    "        X_val_scaled = scaler_features.transform(X_val)\n",
    "        y_val_scaled = scaler_target.transform(y_val)\n",
    "        X_test_scaled = scaler_features.transform(X_test)\n",
    "        y_test_scaled = scaler_target.transform(y_test)\n",
    "        print(\"Features and target scaled using MinMaxScaler (fit on train only).\")\n",
    "\n",
    "        # 5. Create Sequences\n",
    "        print(\"Creating sequences...\")\n",
    "        X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled.flatten(), seq_length)\n",
    "        X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled.flatten(), seq_length)\n",
    "        X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled.flatten(), seq_length)\n",
    "\n",
    "        if len(X_train_seq) == 0 or len(X_val_seq) == 0 or len(X_test_seq) == 0:\n",
    "            raise ValueError(\"Sequence creation resulted in one or more empty datasets. Check split sizes and seq_length.\")\n",
    "\n",
    "        # 6. Create Datasets and DataLoaders\n",
    "        train_dataset = EnergyDataset(X_train_seq, y_train_seq)\n",
    "        val_dataset = EnergyDataset(X_val_seq, y_val_seq)\n",
    "        test_dataset = EnergyDataset(X_test_seq, y_test_seq)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True) # Shuffle train\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        input_size = X_train_scaled.shape[1]\n",
    "        scalers = {'features': scaler_features, 'target': scaler_target}\n",
    "\n",
    "        print(f\"Data loaded successfully:\")\n",
    "        print(f\"  Input features per step: {input_size}\")\n",
    "        print(f\"  Sequence length: {seq_length}\")\n",
    "        print(f\"  Train sequences/batches: {len(train_dataset)} / {len(train_loader)}\")\n",
    "        print(f\"  Val sequences/batches: {len(val_dataset)} / {len(val_loader)}\")\n",
    "        print(f\"  Test sequences/batches: {len(test_dataset)} / {len(test_loader)}\")\n",
    "\n",
    "        return train_loader, val_loader, test_loader, input_size, seq_length, scalers\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Dataset file not found at {file_path}\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError during data processing: {ve}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during data loading: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    return None, None, None, 0, 0, None\n"
   ],
   "id": "3ed931b7a77f91d",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Training Function (Regression)",
   "id": "4f2744ef06e4c62a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T09:45:53.406371Z",
     "start_time": "2025-05-07T09:45:53.394061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim # Explicitly import optim\n",
    "import time\n",
    "import os # For saving models\n",
    "import numpy as np # For inf\n",
    "\n",
    "# Assume other necessary imports like model definition are present\n",
    "\n",
    "def train_model_regression(\n",
    "    model,\n",
    "    train_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs,\n",
    "    val_loader=None,\n",
    "    model_path_prefix=\"best_model\", # Prefix for saving model files\n",
    "    grad_clip=None,\n",
    "    lr_scheduler_config=None, # Dict: {'type': 'ReduceLROnPlateau', 'patience': 5, 'factor': 0.5} or None\n",
    "    early_stopping_patience=None # Int: e.g., 10, or None to disable\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Trains a model for a regression task with optional LR scheduling and early stopping.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to train.\n",
    "        train_loader (DataLoader): DataLoader for the training data.\n",
    "        criterion (nn.Module): The loss function (e.g., nn.MSELoss).\n",
    "        optimizer (optim.Optimizer): The optimizer (e.g., Adam).\n",
    "        device (torch.device): The device to train on ('cuda' or 'cpu').\n",
    "        num_epochs (int): The total number of epochs to train for.\n",
    "        val_loader (DataLoader, optional): DataLoader for validation data. If None,\n",
    "                                           validation and best model saving are skipped.\n",
    "        model_path_prefix (str, optional): Prefix for saving the best model's state_dict.\n",
    "                                           e.g., \"output_dir/model_name\" -> \"output_dir/model_name_best_val.pth\"\n",
    "        grad_clip (float, optional): Maximum norm for gradient clipping.\n",
    "        lr_scheduler_config (dict, optional): Configuration for learning rate scheduler.\n",
    "            Example: {'type': 'ReduceLROnPlateau', 'patience': 5, 'factor': 0.5, 'min_lr': 1e-7}\n",
    "                     {'type': 'StepLR', 'step_size': 10, 'gamma': 0.1}\n",
    "        early_stopping_patience (int, optional): Number of epochs to wait for improvement\n",
    "                                                 before stopping. If None, disabled.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (trained_model, train_losses_history, val_losses_history)\n",
    "               The model after training (could be last epoch or best if loaded back).\n",
    "               Lists of training and validation losses per epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    train_losses_history = []\n",
    "    val_losses_history = []\n",
    "\n",
    "    # --- Initialize Learning Rate Scheduler ---\n",
    "    scheduler = None\n",
    "    if lr_scheduler_config and val_loader: # Scheduler often relies on validation metrics\n",
    "        scheduler_type = lr_scheduler_config.get('type', 'ReduceLROnPlateau').lower()\n",
    "        if scheduler_type == 'reducelronplateau':\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                mode='min', # For loss\n",
    "                factor=lr_scheduler_config.get('factor', 0.1),\n",
    "                patience=lr_scheduler_config.get('patience', 10),\n",
    "                verbose=True,\n",
    "                min_lr=lr_scheduler_config.get('min_lr', 0)\n",
    "            )\n",
    "            print(f\"Using ReduceLROnPlateau scheduler (factor={scheduler.factor}, patience={scheduler.patience})\")\n",
    "        elif scheduler_type == 'steplr':\n",
    "            scheduler = optim.lr_scheduler.StepLR(\n",
    "                optimizer,\n",
    "                step_size=lr_scheduler_config.get('step_size', 10),\n",
    "                gamma=lr_scheduler_config.get('gamma', 0.1),\n",
    "                verbose=True\n",
    "            )\n",
    "            print(f\"Using StepLR scheduler (step_size={scheduler.step_size}, gamma={scheduler.gamma})\")\n",
    "        # Add other schedulers like CosineAnnealingLR if needed\n",
    "        else:\n",
    "            print(f\"Warning: Unknown scheduler type '{scheduler_type}'. No scheduler will be used.\")\n",
    "\n",
    "\n",
    "    print(f\"Starting training for up to {num_epochs} epochs...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Set model to training mode\n",
    "        running_train_loss = 0.0\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            if grad_clip:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
    "\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item() * inputs.size(0) # Accumulate loss scaled by batch size\n",
    "\n",
    "        epoch_train_loss = running_train_loss / len(train_loader.dataset) # Average loss per sample\n",
    "        train_losses_history.append(epoch_train_loss)\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "        log_msg = f\"Epoch {epoch+1}/{num_epochs}: Train Loss={epoch_train_loss:.6f}, Time: {epoch_time:.2f}s\"\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr'] # Get current LR\n",
    "        log_msg += f\", LR: {current_lr:.2e}\"\n",
    "\n",
    "\n",
    "        # --- Validation Step (if val_loader is provided) ---\n",
    "        epoch_val_loss = None\n",
    "        if val_loader:\n",
    "            model.eval() # Set model to evaluation mode\n",
    "            running_val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for inputs_val, labels_val in val_loader:\n",
    "                    inputs_val, labels_val = inputs_val.to(device), labels_val.to(device)\n",
    "                    outputs_val = model(inputs_val)\n",
    "                    val_loss_batch = criterion(outputs_val, labels_val)\n",
    "                    running_val_loss += val_loss_batch.item() * inputs_val.size(0)\n",
    "\n",
    "            epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "            val_losses_history.append(epoch_val_loss)\n",
    "            log_msg += f\", Val Loss={epoch_val_loss:.6f}\"\n",
    "\n",
    "            # --- Learning Rate Scheduler Step (if applicable) ---\n",
    "            if scheduler:\n",
    "                if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step(epoch_val_loss) # ReduceLROnPlateau needs the metric\n",
    "                # For schedulers like StepLR, CosineAnnealingLR, call step() without metric\n",
    "                # elif not isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                #     scheduler.step() # This placement is common for epoch-based schedulers other than ReduceLROnPlateau\n",
    "\n",
    "            # --- Best Model Saving ---\n",
    "            if epoch_val_loss < best_val_loss:\n",
    "                best_val_loss = epoch_val_loss\n",
    "                epochs_no_improve = 0 # Reset counter\n",
    "                best_model_path = f\"{model_path_prefix}_best_val.pth\"\n",
    "                try:\n",
    "                    # Ensure the directory exists\n",
    "                    os.makedirs(os.path.dirname(best_model_path), exist_ok=True)\n",
    "                    torch.save(model.state_dict(), best_model_path)\n",
    "                    log_msg += f\" (Best Val Loss: {best_val_loss:.6f} -> Model Saved)\"\n",
    "                except Exception as e:\n",
    "                    log_msg += f\" (Error saving best model: {e})\"\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                log_msg += f\" (Val Loss did not improve for {epochs_no_improve} epoch(s))\"\n",
    "\n",
    "            # --- Early Stopping Check ---\n",
    "            if early_stopping_patience and epochs_no_improve >= early_stopping_patience:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch+1} epochs due to no improvement in validation loss for {early_stopping_patience} consecutive epochs.\")\n",
    "                break # Exit the training loop\n",
    "\n",
    "            model.train() # Switch back to training mode for the next epoch\n",
    "        else: # No validation loader\n",
    "            val_losses_history.append(np.nan) # Placeholder if no validation\n",
    "            # For schedulers that don't need validation metric (like StepLR)\n",
    "            if scheduler and not isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step()\n",
    "\n",
    "\n",
    "        print(log_msg)\n",
    "        # End of epoch loop\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "    # Optional: Load the best model weights back into the model if validation was performed\n",
    "    if val_loader and os.path.exists(f\"{model_path_prefix}_best_val.pth\"):\n",
    "        print(f\"Loading best model weights from {model_path_prefix}_best_val.pth\")\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(f\"{model_path_prefix}_best_val.pth\", map_location=device))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading best model weights: {e}. Returning model from last epoch.\")\n",
    "\n",
    "    return model, train_losses_history, val_losses_history"
   ],
   "id": "9f2ddab93007347c",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Evaluation Function (Regression)",
   "id": "18af4162adace131"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T09:45:58.566943Z",
     "start_time": "2025-05-07T09:45:58.558223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_macs_params(model, example_input):\n",
    "    # Ensure example_input is on the right device\n",
    "    device = next(model.parameters()).device\n",
    "    example_input = example_input.to(device)\n",
    "    # tp.utils.count_ops_and_params can fail with LSTMs sometimes. Use torchinfo as fallback.\n",
    "    try:\n",
    "         macs, params = tp.utils.count_ops_and_params(model, example_input)\n",
    "         return macs, params\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: torch_pruning MACs calculation failed ({e}). Falling back to torchinfo estimate.\")\n",
    "        try:\n",
    "             from torchinfo import summary\n",
    "             # Correct input format for torchinfo might be needed depending on version\n",
    "             # Try with tuple (common format) or just the tensor\n",
    "             input_data_shape = example_input.shape\n",
    "             model_summary = summary(model, input_size=input_data_shape, verbose=0)\n",
    "             params = model_summary.total_params\n",
    "             macs = model_summary.total_mult_adds\n",
    "             print(f\"torchinfo estimate: Params={params}, MACs={macs}\")\n",
    "             return macs, params\n",
    "        except Exception as e2:\n",
    "            print(f\"Warning: torchinfo calculation also failed ({e2}). Returning 0 for MACs/Params.\")\n",
    "            return 0, sum(p.numel() for p in model.parameters()) # Return at least params\n",
    "\n",
    "def evaluate_model_regression(model, test_loader, example_input, device, scalers=None):\n",
    "    model.eval()\n",
    "    macs, params = calculate_macs_params(model, example_input) # Handles potential LSTM issues\n",
    "    size_mb = params * 4 / 1e6 # Assumes float32\n",
    "\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    print(\"Evaluating on test set...\")\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            all_outputs.append(outputs.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    all_outputs = np.concatenate(all_outputs, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    outputs_eval = all_outputs\n",
    "    labels_eval = all_labels\n",
    "\n",
    "    # Inverse transform for interpretable metrics\n",
    "    if scalers and 'target' in scalers:\n",
    "        try:\n",
    "            outputs_eval = scalers['target'].inverse_transform(all_outputs)\n",
    "            labels_eval = scalers['target'].inverse_transform(all_labels)\n",
    "            print(\"Metrics calculated on original scale.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not inverse transform. Metrics on scaled data. Error: {e}\")\n",
    "    else:\n",
    "         print(\"Warning: Target scaler not provided. Metrics calculated on scaled data.\")\n",
    "\n",
    "\n",
    "    mse = mean_squared_error(labels_eval, outputs_eval)\n",
    "    mae = mean_absolute_error(labels_eval, outputs_eval)\n",
    "    r2 = r2_score(labels_eval, outputs_eval)\n",
    "    rmse = np.sqrt(mse)\n",
    "    # MAPE calculation - handle potential zeros in labels_eval\n",
    "    epsilon = 1e-8 # Small value to avoid division by zero\n",
    "    mape = np.mean(np.abs((labels_eval - outputs_eval) / (labels_eval + epsilon))) * 100\n",
    "\n",
    "\n",
    "    print(f\"Evaluation Metrics: MSE={mse:.4f}, MAE={mae:.4f}, RMSE={rmse:.4f}, R2={r2:.4f}, MAPE={mape:.2f}%\")\n",
    "\n",
    "    return {\n",
    "        'macs': macs,\n",
    "        'params': params,\n",
    "        'size_mb': size_mb,\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2,\n",
    "        'mape': mape,\n",
    "        'performance': mse # Use MSE as the primary performance metric (lower is better)\n",
    "    }"
   ],
   "id": "9fad1e927646aabe",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5. Pruning Function (Adapted for LSTM)",
   "id": "620681a5782b7ef4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T09:46:02.855304Z",
     "start_time": "2025-05-07T09:46:02.842594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prune_lstm_model_by_threshold(\n",
    "    model,\n",
    "    example_input_bs1, # BS=1 for MACs/Params calc & non-grad strategies\n",
    "    target_macs,\n",
    "    target_params,\n",
    "    strategy,\n",
    "    max_iterations=50,\n",
    "    step_pruning_ratio=0.1,\n",
    "    gradient_batch=None, # Dict {'inputs': T, 'labels': T} with BS > 1\n",
    "    prunable_modules=None # List of specific layers (e.g., [model.fc])\n",
    "    ):\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    needs_gradient = isinstance(strategy['importance'], (\n",
    "        tp.importance.TaylorImportance,\n",
    "        tp.importance.GroupHessianImportance\n",
    "    ))\n",
    "    if needs_gradient:\n",
    "        if gradient_batch is None: raise ValueError(f\"Strategy needs 'gradient_batch'.\")\n",
    "        if gradient_batch['inputs'].shape[0] <= 1: raise ValueError(f\"Need BS > 1 in gradient_batch\")\n",
    "        gradient_inputs = gradient_batch['inputs'].to(device)\n",
    "        gradient_labels = gradient_batch['labels'].to(device)\n",
    "\n",
    "    print(f\"--- Starting Pruning ({strategy['importance'].__class__.__name__}) ---\")\n",
    "    print(f\"Target MACs: <= {target_macs:,.0f}, Target Params: <= {target_params:,.0f}\")\n",
    "    print(f\"Step Ratio: {step_pruning_ratio:.2f}, Max Iter: {max_iterations}\")\n",
    "\n",
    "    # Determine layers to prune/ignore\n",
    "    if not prunable_modules:\n",
    "         prunable_modules = [m for m in model.modules() if isinstance(m, nn.Linear)]\n",
    "         print(f\"Defaulting to pruning nn.Linear layers: {[m.__class__.__name__ for m in prunable_modules]}\")\n",
    "    else:\n",
    "         print(f\"Targeting specific modules for pruning: {[m.__class__.__name__ for m in prunable_modules]}\")\n",
    "\n",
    "    modules_to_ignore = [m for m in model.modules() if isinstance(m, (nn.Linear, nn.Conv2d, nn.LSTM)) and m not in prunable_modules]\n",
    "    root_types = list(set(type(m) for m in prunable_modules))\n",
    "    if not root_types:\n",
    "        print(\"Warning: No prunable module types identified. Pruning may fail.\")\n",
    "        root_types = [nn.Linear] # Fallback guess\n",
    "\n",
    "\n",
    "    pruner = strategy['pruner'](\n",
    "        model,\n",
    "        example_input_bs1.to(device),\n",
    "        importance=strategy['importance'],\n",
    "        pruning_ratio=step_pruning_ratio,\n",
    "        root_module_types=root_types,\n",
    "        ignored_layers=modules_to_ignore,\n",
    "    )\n",
    "\n",
    "    initial_macs, initial_params = calculate_macs_params(model, example_input_bs1.to(device))\n",
    "    current_macs, current_params = initial_macs, initial_params\n",
    "    print(f\"Initial State | MACs: {current_macs:,.0f}, Params: {current_params:,.0f}\")\n",
    "    # --- Remove or comment out these two lines ---\n",
    "    # prunable_layer_names = [layer.__class__.__name__ for layer in pruner.get_pruning_layers()] # <--- ERROR HERE\n",
    "    # print(f\"Detected Prunable Layers by tp: {prunable_layer_names}\")\n",
    "    # --------------------------------------------\n",
    "    if initial_macs == 0 and initial_params == 0: # Check if initial calc failed\n",
    "         print(\"Warning: Initial MACs/Params calculation failed or returned zero. Cannot proceed.\")\n",
    "         return model # Or raise error\n",
    "\n",
    "    # Existing check (adjust slightly): If no prunable layers are implicitly handled by pruner.step, it will just return empty groups.\n",
    "    # We don't need the explicit check here anymore. The loop checking `if not pruning_groups:` will handle it.\n",
    "    # if not prunable_layer_names and initial_macs > 0:\n",
    "    #     print(\"Warning: torch-pruning did not detect any prunable layers matching criteria.\")\n",
    "\n",
    "\n",
    "    iteration = 0\n",
    "    model.eval()\n",
    "    criterion = nn.MSELoss().to(device) # Loss for gradient calculation\n",
    "\n",
    "    while (current_macs > target_macs or current_params > target_params) and iteration < max_iterations:\n",
    "        iteration += 1\n",
    "        macs_before_step = current_macs\n",
    "        params_before_step = current_params\n",
    "\n",
    "        if needs_gradient:\n",
    "            model.train()\n",
    "            input_for_grad = gradient_inputs.detach().clone()\n",
    "            labels_for_grad = gradient_labels.detach().clone()\n",
    "            try:\n",
    "                for param in model.parameters(): param.requires_grad_(True)\n",
    "                outputs = model(input_for_grad)\n",
    "                loss = criterion(outputs, labels_for_grad)\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError during backward (Iter {iteration}): {e}. Stopping.\")\n",
    "                model.eval() ; break\n",
    "            finally:\n",
    "                model.eval() ; model.zero_grad(set_to_none=True)\n",
    "\n",
    "        try:\n",
    "            pruning_groups = list(pruner.step(interactive=True))\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during pruner.step() (Iter {iteration}): {e}. Stopping.\")\n",
    "            break\n",
    "\n",
    "        if not pruning_groups:\n",
    "            print(f\"Iter {iteration}: No more candidates found by pruner. Stopping.\")\n",
    "            break\n",
    "\n",
    "        for group in pruning_groups: group.prune()\n",
    "\n",
    "        current_macs, current_params = calculate_macs_params(model, example_input_bs1.to(device))\n",
    "        macs_reduced_pct = (macs_before_step - current_macs) / initial_macs * 100 if initial_macs > 0 else 0\n",
    "        params_reduced_pct = (params_before_step - current_params) / initial_params * 100 if initial_params > 0 else 0\n",
    "\n",
    "        print(\n",
    "            f\"Iter {iteration: >3}/{max_iterations} | \"\n",
    "            f\"MACs: {current_macs:,.0f} ({macs_reduced_pct:+6.1f}% R) | \"\n",
    "            f\"Params: {current_params:,.0f} ({params_reduced_pct:+6.1f}% R)\"\n",
    "        )\n",
    "\n",
    "        if current_macs >= macs_before_step and current_params >= params_before_step:\n",
    "            if not (current_macs <= target_macs and current_params <= target_params):\n",
    "                print(f\"Iter {iteration}: No reduction. Stopping.\") ; break\n",
    "            else: break # Targets met\n",
    "\n",
    "    # --- Final Report ---\n",
    "    print(f\"--- Finished Pruning ({strategy['importance'].__class__.__name__}) ---\")\n",
    "    if iteration >= max_iterations: print(f\"Warning: Reached max iterations ({max_iterations}).\")\n",
    "    final_macs, final_params = calculate_macs_params(model, example_input_bs1.to(device))\n",
    "    macs_reduction = (initial_macs - final_macs) / initial_macs * 100 if initial_macs > 0 else 0\n",
    "    params_reduction = (initial_params - final_params) / initial_params * 100 if initial_params > 0 else 0\n",
    "    print(f\"Initial | MACs: {initial_macs:,.0f}, Params: {initial_params:,.0f}\")\n",
    "    print(f\"Final   | MACs: {final_macs:,.0f} (Reduction: {macs_reduction:.2f}%)\")\n",
    "    print(f\"        | Params: {final_params:,.0f} (Reduction: {params_reduction:.2f}%)\")\n",
    "    print(f\"Target  | MACs <= {target_macs:,.0f}, Params <= {target_params:,.0f}\")\n",
    "    if final_macs > target_macs or final_params > target_params:\n",
    "         print(\"Warning: Pruning finished, but target threshold(s) were not fully met.\")\n",
    "\n",
    "    model.eval()\n",
    "    return model"
   ],
   "id": "1ff3f850848b3dfb",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6. Comparison and Plotting Function (Regression)",
   "id": "c83797575d40ca71"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T09:46:06.960860Z",
     "start_time": "2025-05-07T09:46:06.947874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compare_results_and_plot_regression(results, metric_key='mse', lower_is_better=True, output_dir='output'):\n",
    "    if not results: print(\"No results to plot.\") ; return\n",
    "    valid_results = {k: v for k, v in results.items() if isinstance(v, dict) and all(m in v for m in ['macs', 'params', metric_key])}\n",
    "    if not valid_results: print(\"No valid results entries found for plotting.\") ; return\n",
    "\n",
    "    strategy_order = []\n",
    "    if 'initial' in valid_results: strategy_order.append('initial')\n",
    "    strategy_order.extend([s for s in valid_results if s != 'initial'])\n",
    "    if not strategy_order: print(\"No strategies to plot.\"); return\n",
    "\n",
    "    # --- Print Table ---\n",
    "    metric_name = metric_key.upper()\n",
    "    print(f\"\\n=== Pruning Strategy Comparison (Metric: {metric_name}) ===\")\n",
    "    header = f\"{'Strategy':<15} | {'MACs (M)':<10} | {'Params (K)':<10} | {'Size (MB)':<10} | {metric_name:<12}\"\n",
    "    print(header); print(\"-\" * len(header))\n",
    "    for strategy in strategy_order:\n",
    "        metrics = valid_results[strategy]\n",
    "        macs_m = metrics['macs']/1e6 if metrics['macs'] is not None else 0\n",
    "        params_k = metrics['params']/1e3 if metrics['params'] is not None else 0\n",
    "        print(f\"{strategy:<15} | {macs_m:<10.2f} | {params_k:<10.1f} | {metrics.get('size_mb', 0):>10.2f} | {metrics[metric_key]:>12.4f}\")\n",
    "\n",
    "    # --- Generate Bar Charts ---\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    metrics_to_plot = ['macs', 'params', 'size_mb', metric_key]\n",
    "    base_titles = {'macs': 'MACs', 'params': 'Parameters', 'size_mb': 'Model Size (MB)', metric_key: metric_name}\n",
    "    plot_titles = {k: f'{v} Comparison (Lower is Better)' for k, v in base_titles.items()}\n",
    "    if not lower_is_better:\n",
    "        plot_titles[metric_key] = f'{base_titles[metric_key]} Comparison (Higher is Better)'\n",
    "\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(strategy_order)))\n",
    "    initial_metrics = valid_results.get('initial', None)\n",
    "\n",
    "    for plot_metric in metrics_to_plot:\n",
    "        if not all(plot_metric in valid_results[s] for s in strategy_order):\n",
    "             print(f\"Skipping plot for {plot_metric} as it's missing from some results.\")\n",
    "             continue\n",
    "        values = [valid_results[strategy][plot_metric] for strategy in strategy_order]\n",
    "\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        bars = plt.bar(strategy_order, values, color=colors)\n",
    "        plt.ylabel(base_titles[plot_metric])\n",
    "        plt.title(plot_titles[plot_metric])\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "        # Add value labels\n",
    "        max_val = max(values) if values else 0\n",
    "        for i, bar in enumerate(bars):\n",
    "            yval = bar.get_height()\n",
    "            label = \"\"\n",
    "            # ... (Use the formatting logic from previous `compare_results_and_plot_regression` function) ...\n",
    "            if plot_metric == 'macs': label = f'{yval/1e6:.2f}M' if yval > 1e5 else f'{yval:,.0f}' # Adjust format\n",
    "            elif plot_metric == 'params': label = f'{yval/1e3:.1f}K' if yval > 100 else f'{yval:,.0f}' # Adjust format\n",
    "            elif plot_metric == 'size_mb': label = f'{yval:.2f}'\n",
    "            else: label = f'{yval:.4f}' # Regression metric\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., yval + 0.01 * max_val, label, ha='center', va='bottom', fontsize=8, rotation=0)\n",
    "\n",
    "        if initial_metrics and plot_metric in initial_metrics:\n",
    "             initial_value = initial_metrics[plot_metric]\n",
    "             plt.axhline(y=initial_value, color='r', linestyle='--', label=f'Initial Value')\n",
    "             plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(output_dir, f'lstm_energy_{plot_metric}_comparison.png')\n",
    "        try:\n",
    "            plt.savefig(save_path)\n",
    "        except Exception as e:\n",
    "             print(f\"Error saving plot {save_path}: {e}\")\n",
    "        plt.close()\n",
    "    print(f\"Comparison plots saved to {output_dir}\")"
   ],
   "id": "cb7b0daf03377f6e",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T09:46:11.236659Z",
     "start_time": "2025-05-07T09:46:11.230551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "\n",
    "# [...] other imports\n",
    "\n",
    "def save_model_as_onnx(model, example_input, output_path, opset_version=13):\n",
    "    \"\"\"Saves the PyTorch model as ONNX.\"\"\"\n",
    "    # Ensure model is on the same device as the example input for export\n",
    "    device = example_input.device\n",
    "    model.to(device)\n",
    "    model.eval() # Ensure model is in evaluation mode\n",
    "\n",
    "    print(f\"Attempting to save model to ONNX: {output_path}\")\n",
    "    print(f\"Using example input shape: {example_input.shape}\")\n",
    "\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            example_input, # Must have the correct shape (batch, seq_len, features)\n",
    "            output_path,\n",
    "            export_params=True,       # Store the trained parameter weights inside the model file\n",
    "            opset_version=opset_version,    # The ONNX version to export the model to\n",
    "            do_constant_folding=True, # Optional: optimizes the model\n",
    "            input_names=['input'],    # Specify names for input nodes\n",
    "            output_names=['output'],  # Specify names for output nodes\n",
    "            dynamic_axes={            # Allow variable batch size\n",
    "                'input': {0: 'batch_size'},\n",
    "                'output': {0: 'batch_size'}\n",
    "            }\n",
    "        )\n",
    "        print(f\"✅ Model successfully saved as ONNX to {output_path}\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to save model as ONNX: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False"
   ],
   "id": "6548bc5f0a219c6f",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7. Main Workflow (LSTM Energy Prediction)",
   "id": "3ca5c56f382a9628"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T10:16:33.916528Z",
     "start_time": "2025-05-07T10:16:33.885625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main_lstm():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    lstm_hidden_size_val = 256\n",
    "    # --- Configuration ---\n",
    "    config = {\n",
    "        'dataset_path': './data/energydata_complete.csv', # ADJUST AS NEEDED\n",
    "        'target_column_name': 'Appliances',\n",
    "        'feature_column_names': [\n",
    "            'lights', 'T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4', 'RH_4', 'T5', 'RH_5',\n",
    "            'T6', 'RH_6', 'T7', 'RH_7', 'T8', 'RH_8', 'T9', 'RH_9', 'T_out',\n",
    "            'Press_mm_hg', 'RH_out', 'Windspeed', 'Visibility', 'Tdewpoint'\n",
    "        ], # Your list of features\n",
    "        'sequence_length': 6 * 12, # 12 hours\n",
    "\n",
    "        'strategies': {\n",
    "            'Magnitude_L1': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.MagnitudeImportance(p=1)},\n",
    "            'Magnitude_L2': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.MagnitudeImportance(p=2)},\n",
    "            'Random': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.RandomImportance()},\n",
    "            'Taylor': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.TaylorImportance()},\n",
    "            'FPGM': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.FPGMImportance()},\n",
    "        },\n",
    "        'learning_rate_scheduler': {\n",
    "        'type': 'ReduceLROnPlateau', # or 'StepLR'\n",
    "        'patience': 10,             # For ReduceLROnPlateau\n",
    "        'factor': 0.5,              # For ReduceLROnPlateau\n",
    "        'min_lr': 1e-7,             # For ReduceLROnPlateau\n",
    "        # 'step_size': 15,          # For StepLR\n",
    "        # 'gamma': 0.1,             # For StepLR\n",
    "        },\n",
    "        'early_stopping_patience': 20,\n",
    "        # --- LSTM Hyperparameters ---\n",
    "        'lstm_hidden_size': lstm_hidden_size_val,    # Example, adjust as needed\n",
    "        'num_lstm_layers': 5,\n",
    "        'lstm_dropout': 0.5,\n",
    "\n",
    "        # --- Configuration for 7 Intermediate Blocks ---\n",
    "        # Defines the output size of each block. The number of elements defines the number of blocks.\n",
    "        # Example: Tapering down, starting relatively large after LSTM\n",
    "        'block_output_features': [\n",
    "            lstm_hidden_size_val // 2,\n",
    "            lstm_hidden_size_val // 2,\n",
    "            lstm_hidden_size_val // 3,\n",
    "            lstm_hidden_size_val // 3,\n",
    "            lstm_hidden_size_val // 4,\n",
    "            lstm_hidden_size_val // 4,\n",
    "            lstm_hidden_size_val // 5\n",
    "        ],\n",
    "        # 'block_output_features': [96, 80, 64, 48, 32, 24, 16], # Alternative: More explicit tapering\n",
    "\n",
    "        'block_dropout': 0.6,       # Dropout for the intermediate blocks\n",
    "\n",
    "        # --- Training ---\n",
    "        'train_epochs': 50,         # May need more for a deeper model\n",
    "        'fine_tune_epochs': 60,     # Also potentially more\n",
    "        'batch_size': 64,\n",
    "        'learning_rate_initial': 0.0005, # May need adjustment for deeper model\n",
    "        'learning_rate_finetune': 0.0002,\n",
    "        'grad_clip': 1.0,\n",
    "\n",
    "        # --- Paths & Pruning ---\n",
    "        'output_dir': './output/lstm_energy_7blocks', # New output directory\n",
    "        'pruning_max_iterations': 100, # More iterations might be needed for deeper models\n",
    "        'pruning_step_ratio': 0.15,    # Adjust step ratio\n",
    "        'pruning_primary_metric': 'mse',\n",
    "\n",
    "        # --- TARGETS FOR PRUNING ---\n",
    "        'target_macs_absolute': 3_000_000_000, # 3 Billion MACs\n",
    "        'target_params_absolute': None,      # Use size_mb for params target, effectively\n",
    "        'target_size_mb_absolute': 5.0,    # 5 MB target size\n",
    "\n",
    "        # Sparsity targets (will be overridden if absolute targets are set and initial model is larger)\n",
    "        'target_macs_sparsity': 0.5,   # Target 50% MAC reduction\n",
    "        'target_params_sparsity': 0.5, # Target 50% Params reduction\n",
    "    }\n",
    "    os.makedirs(config['output_dir'], exist_ok=True)\n",
    "\n",
    "    # --- Initialize Data ---\n",
    "    print(\"Loading and processing data...\")\n",
    "    train_loader, val_loader, test_loader, input_size, actual_seq_length_used, scalers = get_energy_data_loaders(\n",
    "        file_path=config['dataset_path'],\n",
    "        feature_cols=config['feature_column_names'],\n",
    "        target_col=config['target_column_name'],\n",
    "        seq_length=config['sequence_length'],\n",
    "        batch_size=config['batch_size']\n",
    "        # Add test_size_ratio, val_size_ratio here if you want to override defaults in get_energy_data_loaders\n",
    "    )\n",
    "    if train_loader is None:\n",
    "        print(\"Failed to load data. Exiting.\")\n",
    "        return\n",
    "    # Update sequence_length in config if it was adjusted by data loader (e.g. due to data size)\n",
    "    config['sequence_length'] = actual_seq_length_used\n",
    "\n",
    "\n",
    "    # --- Initialize Model ---\n",
    "    model = TimeSeriesLSTM_WithBlocks(\n",
    "        input_size=input_size,\n",
    "        lstm_hidden_size=config['lstm_hidden_size'],\n",
    "        num_lstm_layers=config['num_lstm_layers'],\n",
    "        block_configs=config['block_output_features'],\n",
    "        output_size=1,\n",
    "        lstm_dropout_prob=config['lstm_dropout'],\n",
    "        block_dropout_prob=config['block_dropout']\n",
    "    )\n",
    "    model.to(device)\n",
    "    print(\"\\nModel Architecture:\")\n",
    "    # Using torchinfo for a more detailed summary if available\n",
    "    try:\n",
    "        from torchinfo import summary as torchinfo_summary\n",
    "        torchinfo_summary(model, input_size=(config['batch_size'], config['sequence_length'], input_size), verbose=0)\n",
    "    except ImportError:\n",
    "        print(model) # Fallback\n",
    "    try:\n",
    "        print(f\"Initial model parameter device: {next(model.parameters()).device}\")\n",
    "    except StopIteration: print(\"Initial model has no parameters.\")\n",
    "\n",
    "\n",
    "    # --- Create Example Inputs ---\n",
    "    example_input_bs1 = torch.randn(1, config['sequence_length'], input_size).to(device)\n",
    "    example_gradient_batch = None\n",
    "    try:\n",
    "        grad_batch_data_peek = next(iter(train_loader))\n",
    "        if grad_batch_data_peek[0].shape[0] > 1:\n",
    "            example_gradient_batch = {'inputs': grad_batch_data_peek[0], 'labels': grad_batch_data_peek[1]}\n",
    "            print(f\"Obtained gradient batch with BS={example_gradient_batch['inputs'].shape[0]}\")\n",
    "        else: print(\"Warning: First train batch has BS=1 or less. Cannot use for Taylor gradient batch initially if Taylor is the first strategy.\")\n",
    "    except Exception as e: print(f\"Could not get gradient batch from DataLoader: {e}\")\n",
    "\n",
    "\n",
    "    # --- Initial Training ---\n",
    "    initial_model_base_path = os.path.join(config['output_dir'], \"lstm_energy_initial\")\n",
    "    initial_model_pth_path = initial_model_base_path + \".pth\" # Last epoch\n",
    "    initial_best_model_pth_path = initial_model_base_path + \"_best_val.pth\"\n",
    "    initial_model_onnx_path = initial_model_base_path + \"_best_val.onnx\"\n",
    "\n",
    "    if not os.path.exists(initial_best_model_pth_path):\n",
    "        print(\"\\n--- Initial Training ---\")\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate_initial'], weight_decay=config.get('weight_decay_initial', 0))\n",
    "        model, train_hist, val_hist = train_model_regression(\n",
    "            model=model, train_loader=train_loader, criterion=nn.MSELoss().to(device),\n",
    "            optimizer=optimizer, device=device, num_epochs=config['train_epochs'],\n",
    "            val_loader=val_loader, model_path_prefix=initial_model_base_path,\n",
    "            grad_clip=config.get('grad_clip', None),\n",
    "            lr_scheduler_config=config.get('learning_rate_scheduler', None),\n",
    "            early_stopping_patience=config.get('early_stopping_patience', None)\n",
    "        )\n",
    "        torch.save(model.state_dict(), initial_model_pth_path) # Save last epoch state\n",
    "\n",
    "        # Determine which model to use for ONNX & as base for pruning\n",
    "        path_for_onnx_and_pruning_base = initial_model_pth_path # Default to last epoch\n",
    "        if os.path.exists(initial_best_model_pth_path):\n",
    "            print(f\"Best initial model saved during training: {initial_best_model_pth_path}\")\n",
    "            model.load_state_dict(torch.load(initial_best_model_pth_path, map_location=device))\n",
    "            path_for_onnx_and_pruning_base = initial_best_model_pth_path\n",
    "        else:\n",
    "            print(f\"Warning: Best validation model file not found. Using last epoch from {initial_model_pth_path} for ONNX/pruning base.\")\n",
    "\n",
    "        print(f\"\\nSaving best available initial model (from {os.path.basename(path_for_onnx_and_pruning_base)}) as ONNX...\")\n",
    "        save_model_as_onnx(model, example_input_bs1, initial_model_onnx_path)\n",
    "    else:\n",
    "        print(f\"\\nLoading best initial model from {initial_best_model_pth_path}\")\n",
    "        model.load_state_dict(torch.load(initial_best_model_pth_path, map_location=device))\n",
    "        if not os.path.exists(initial_model_onnx_path):\n",
    "            print(f\"Saving loaded best initial model ({os.path.basename(initial_best_model_pth_path)}) as ONNX...\")\n",
    "            save_model_as_onnx(model, example_input_bs1, initial_model_onnx_path)\n",
    "        else:\n",
    "            print(f\"ONNX for initial model already exists: {initial_model_onnx_path}\")\n",
    "\n",
    "\n",
    "    # --- Evaluate Initial Model ---\n",
    "    results = {}\n",
    "    print(\"\\n--- Evaluating Initial Model (Post loading best/last initial weights) ---\")\n",
    "    results['initial'] = evaluate_model_regression(model, test_loader, example_input_bs1, device, scalers)\n",
    "    initial_macs = results['initial']['macs']\n",
    "    initial_params = results['initial']['params']\n",
    "    initial_size_mb = results['initial']['size_mb']\n",
    "    print(f\"Initial Model Eval: MACs={initial_macs:,.0f}, Params={initial_params:,.0f}, SizeMB={initial_size_mb:.2f}, MSE={results['initial']['mse']:.4f}\")\n",
    "\n",
    "    # --- Calculate Targets for Pruning (More Robust) ---\n",
    "    target_macs_value = config['target_macs_absolute']\n",
    "    if initial_macs is not None and initial_macs < target_macs_value: # If initial is already smaller\n",
    "        print(f\"Initial MACs ({initial_macs:,.0f}) is already less than target_macs_absolute ({target_macs_value:,.0f}). Adjusting target.\")\n",
    "        target_macs_value = initial_macs * (1 - config['target_macs_sparsity']) # Fallback to sparsity reduction\n",
    "\n",
    "    target_size_mb_config = config.get('target_size_mb_absolute', None)\n",
    "    if target_size_mb_config is not None:\n",
    "        target_params_from_size_mb = int((target_size_mb_config * 1024 * 1024) / 4)\n",
    "    else: # Fallback to sparsity if no size target\n",
    "        target_params_from_size_mb = initial_params * (1 - config['target_params_sparsity']) if initial_params is not None else float('inf')\n",
    "\n",
    "    target_params_value = target_params_from_size_mb # Primarily use size_mb target\n",
    "    if initial_params is not None and initial_params < target_params_value: # If initial is already smaller\n",
    "         print(f\"Initial Params ({initial_params:,.0f}) is already less than target based on size_mb. Adjusting target.\")\n",
    "         target_params_value = initial_params * (1- config['target_params_sparsity']) # Fallback\n",
    "\n",
    "\n",
    "    print(f\"\\nFinal Pruning Targets:\")\n",
    "    print(f\"  Targeting MACs <= {target_macs_value:,.0f} (Initial: {initial_macs:,.0f})\")\n",
    "    if target_size_mb_config is not None:\n",
    "        print(f\"  Targeting Size <= {target_size_mb_config:.2f} MB (Derived Params Target: {target_params_from_size_mb:,.0f})\")\n",
    "    print(f\"  Using Effective Params Target <= {target_params_value:,.0f} (Initial: {initial_params:,.0f})\")\n",
    "\n",
    "\n",
    "    # --- Pruning and Fine-tuning Loop ---\n",
    "    for strategy_name, strategy_config in config['strategies'].items():\n",
    "        print(f\"\\n===== Processing Strategy: {strategy_name} =====\")\n",
    "        # --- Initialize results for this strategy ---\n",
    "        results[strategy_name] = {\n",
    "            'macs': initial_macs if initial_macs is not None else 0, # Default to initial if available\n",
    "            'params': initial_params if initial_params is not None else 0,\n",
    "            'size_mb': initial_size_mb if initial_size_mb is not None else 0,\n",
    "            config['pruning_primary_metric']: results.get('initial', {}).get(config['pruning_primary_metric'], float('inf')),\n",
    "            'error': 'Not run or error occurred'\n",
    "        }\n",
    "\n",
    "        model_to_prune = TimeSeriesLSTM_WithBlocks(\n",
    "             input_size=input_size, lstm_hidden_size=config['lstm_hidden_size'],\n",
    "             num_lstm_layers=config['num_lstm_layers'], block_configs=config['block_output_features'],\n",
    "             output_size=1, lstm_dropout_prob=config['lstm_dropout'],\n",
    "             block_dropout_prob=config['block_dropout']\n",
    "        ).to(device)\n",
    "\n",
    "        # Load the *best initial* weights (from initial_best_model_pth_path)\n",
    "        path_to_load_initial = initial_best_model_pth_path if os.path.exists(initial_best_model_pth_path) else initial_model_pth_path\n",
    "        try:\n",
    "            model_to_prune.load_state_dict(torch.load(path_to_load_initial, map_location=device))\n",
    "            print(f\"Loaded initial weights from {os.path.basename(path_to_load_initial)} for strategy {strategy_name}\")\n",
    "        except Exception as e_load:\n",
    "            print(f\"Error loading state dict for strategy {strategy_name} from {path_to_load_initial}: {e_load}\")\n",
    "            results[strategy_name]['error'] = f\"Load Initial Error: {e_load}\"\n",
    "            continue\n",
    "        model_to_prune.eval()\n",
    "\n",
    "        # Determine gradient batch\n",
    "        needs_grad = isinstance(strategy_config['importance'], tp.importance.TaylorImportance)\n",
    "        grad_batch_for_prune = example_gradient_batch if needs_grad else None\n",
    "        if needs_grad and not grad_batch_for_prune:\n",
    "            print(f\"Skipping {strategy_name}: Taylor/Hessian requires gradient_batch, but none was successfully obtained.\")\n",
    "            results[strategy_name]['error'] = \"Taylor requires grad_batch, unavailable\"\n",
    "            continue\n",
    "\n",
    "        # Define prunable layers from the current model_to_prune instance\n",
    "        prunable_layers_list = []\n",
    "        if hasattr(model_to_prune, 'intermediate_blocks') and model_to_prune.intermediate_blocks:\n",
    "            for i_block in range(len(model_to_prune.intermediate_blocks)):\n",
    "                # Assuming each block in ModuleList is a Sequential and its first element is Linear\n",
    "                if isinstance(model_to_prune.intermediate_blocks[i_block], nn.Sequential) and \\\n",
    "                   len(model_to_prune.intermediate_blocks[i_block]) > 0 and \\\n",
    "                   isinstance(model_to_prune.intermediate_blocks[i_block][0], nn.Linear):\n",
    "                    prunable_layers_list.append(model_to_prune.intermediate_blocks[i_block][0]) # Get the Linear layer\n",
    "                elif isinstance(model_to_prune.intermediate_blocks[i_block], nn.Linear): # If block itself is Linear\n",
    "                    prunable_layers_list.append(model_to_prune.intermediate_blocks[i_block])\n",
    "\n",
    "        if not prunable_layers_list:\n",
    "            print(f\"No prunable layers selected for strategy {strategy_name}. Copying initial results and skipping pruning+finetuning.\")\n",
    "            results[strategy_name] = results['initial'].copy()\n",
    "            results[strategy_name]['notes'] = \"Pruning skipped: No prunable intermediate layers found/selected.\"\n",
    "            continue\n",
    "        print(f\"Targeting for pruning in {strategy_name}: {[str(layer) for layer in prunable_layers_list]}\")\n",
    "\n",
    "        # Perform Pruning\n",
    "        try:\n",
    "            pruned_model = prune_lstm_model_by_threshold(\n",
    "                model=model_to_prune, example_input_bs1=example_input_bs1,\n",
    "                target_macs=target_macs_value, target_params=target_params_value,\n",
    "                strategy=strategy_config,\n",
    "                max_iterations=config['pruning_max_iterations'],\n",
    "                step_pruning_ratio=config['pruning_step_ratio'],\n",
    "                gradient_batch=grad_batch_for_prune,\n",
    "                prunable_modules=prunable_layers_list\n",
    "            )\n",
    "            pruned_pth_path = os.path.join(config['output_dir'], f\"lstm_{strategy_name}_pruned.pth\")\n",
    "            torch.save(pruned_model.state_dict(), pruned_pth_path)\n",
    "            print(f\"Pruned model state saved to {pruned_pth_path}\")\n",
    "        except Exception as e_prune:\n",
    "            print(f\"\\nCRITICAL ERROR during PRUNING for strategy {strategy_name}: {e_prune}\")\n",
    "            import traceback; traceback.print_exc()\n",
    "            results[strategy_name]['error'] = f\"Pruning Error: {e_prune}\"\n",
    "            continue\n",
    "\n",
    "        # Fine-tune\n",
    "        print(f\"\\n--- Fine-tuning ({strategy_name}) ---\")\n",
    "        ft_base_path = os.path.join(config['output_dir'], f\"lstm_{strategy_name}_ft\")\n",
    "        ft_best_pth_path = ft_base_path + \"_best_val.pth\"\n",
    "        optimizer_ft = optim.Adam(pruned_model.parameters(), lr=config['learning_rate_finetune'], weight_decay=config.get('weight_decay_finetune',0))\n",
    "        try:\n",
    "            fine_tuned_model, _, _ = train_model_regression(\n",
    "                model=pruned_model, train_loader=train_loader, criterion=nn.MSELoss().to(device),\n",
    "                optimizer=optimizer_ft, device=device, num_epochs=config['fine_tune_epochs'],\n",
    "                val_loader=val_loader, model_path_prefix=ft_base_path,\n",
    "                grad_clip=config.get('grad_clip', None),\n",
    "                lr_scheduler_config=config.get('learning_rate_scheduler', None),\n",
    "                early_stopping_patience=config.get('early_stopping_patience', None)\n",
    "            )\n",
    "            # After train_model_regression, fine_tuned_model holds the best val weights if val_loader was used and best model saved/loaded\n",
    "        except Exception as e_ft:\n",
    "            print(f\"\\nCRITICAL ERROR during FINE-TUNING for strategy {strategy_name}: {e_ft}\")\n",
    "            import traceback; traceback.print_exc()\n",
    "            results[strategy_name]['error'] = f\"Fine-tuning Error: {e_ft}\"\n",
    "            continue\n",
    "\n",
    "        # Evaluate Final Model\n",
    "        print(f\"\\n--- Evaluating Fine-tuned Model ({strategy_name}) ---\")\n",
    "        try:\n",
    "            final_metrics = evaluate_model_regression(fine_tuned_model, test_loader, example_input_bs1, device, scalers)\n",
    "            results[strategy_name] = final_metrics\n",
    "        except Exception as e_eval:\n",
    "            print(f\"\\nCRITICAL ERROR during EVALUATION for strategy {strategy_name}: {e_eval}\")\n",
    "            import traceback; traceback.print_exc()\n",
    "            results[strategy_name]['error'] = f\"Evaluation Error: {e_eval}\"\n",
    "            continue\n",
    "\n",
    "        # Save final model .pth and .onnx\n",
    "        final_model_pth_to_save = ft_best_pth_path if os.path.exists(ft_best_pth_path) else (ft_base_path + \".pth\") # Prefer best, fallback to last\n",
    "        # If fine_tuned_model holds the best already due to loading in train_fn, save it directly:\n",
    "        # torch.save(fine_tuned_model.state_dict(), final_model_pth_to_save)\n",
    "        # print(f\"Final PyTorch model for strategy {strategy_name} saved based on fine-tuning to {final_model_pth_to_save}\")\n",
    "        # Simpler: always save the state of fine_tuned_model as it should be the best one loaded by train_model_regression\n",
    "        torch.save(fine_tuned_model.state_dict(), ft_best_pth_path) # Always save best as _best_val.pth\n",
    "        print(f\"Best fine-tuned PyTorch model for {strategy_name} saved to {ft_best_pth_path}\")\n",
    "\n",
    "\n",
    "        strategy_onnx_path = ft_best_pth_path.replace('.pth', '.onnx')\n",
    "        save_model_as_onnx(fine_tuned_model, example_input_bs1, strategy_onnx_path)\n",
    "\n",
    "    # --- Final Comparison (Called only ONCE after all strategies attempted) ---\n",
    "    print(\"\\n===== Final Comparison of All Successfully Processed Strategies =====\")\n",
    "    final_processed_results = {}\n",
    "    if 'initial' in results and isinstance(results['initial'], dict) and config['pruning_primary_metric'] in results['initial']:\n",
    "        final_processed_results['initial'] = results['initial']\n",
    "\n",
    "    for strategy_name_res, metrics_res in results.items():\n",
    "        if strategy_name_res != 'initial' and isinstance(metrics_res, dict):\n",
    "            if 'error' not in metrics_res or metrics_res.get('error') == 'Not run or error occurred':\n",
    "                if all(k in metrics_res for k in ['macs', 'params', config['pruning_primary_metric']]):\n",
    "                    final_processed_results[strategy_name_res] = metrics_res\n",
    "                else:\n",
    "                    print(f\"Strategy '{strategy_name_res}' metrics incomplete. Excluding from final plot.\")\n",
    "            else:\n",
    "                print(f\"Strategy '{strategy_name_res}' failed: {metrics_res['error']}. Excluding from final plot.\")\n",
    "\n",
    "    if len(final_processed_results) > 0 : # Check if there's anything to plot\n",
    "        compare_results_and_plot_regression(\n",
    "             final_processed_results,\n",
    "             metric_key=config['pruning_primary_metric'],\n",
    "             lower_is_better=config.get('pruning_primary_metric_lower_is_better', True),\n",
    "             output_dir=config['output_dir']\n",
    "         )\n",
    "    else:\n",
    "        print(\"No successful strategies (nor initial results) to compare for plotting.\")\n",
    "\n",
    "    print(\"\\nWorkflow completed!\")"
   ],
   "id": "c6ff5be5a75cf5ab",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Run the main function",
   "id": "3e53a223f8508b46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T10:17:53.074945Z",
     "start_time": "2025-05-07T10:16:45.229511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main_lstm()"
   ],
   "id": "a9f155971e487e80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading and processing data...\n",
      "Loading dataset from: ./data/energydata_complete.csv\n",
      "Original data shape: (19735, 29)\n",
      "Data shape after selecting columns & cleaning NaNs: (19735, 26)\n",
      "Data split: Train=13815, Val=1973, Test=3947\n",
      "Features and target scaled using MinMaxScaler (fit on train only).\n",
      "Creating sequences...\n",
      "Data loaded successfully:\n",
      "  Input features per step: 25\n",
      "  Sequence length: 72\n",
      "  Train sequences/batches: 13743 / 214\n",
      "  Val sequences/batches: 1901 / 30\n",
      "  Test sequences/batches: 3875 / 61\n",
      "\n",
      "Model Architecture:\n",
      "Initial model parameter device: cuda:0\n",
      "Obtained gradient batch with BS=64\n",
      "\n",
      "--- Initial Training ---\n",
      "Using ReduceLROnPlateau scheduler (factor=0.5, patience=10)\n",
      "Starting training for up to 50 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Train Loss=0.011269, Time: 3.17s, LR: 5.00e-04, Val Loss=0.007128 (Best Val Loss: 0.007128 -> Model Saved)\n",
      "Epoch 2/50: Train Loss=0.010201, Time: 3.58s, LR: 5.00e-04, Val Loss=0.007119 (Best Val Loss: 0.007119 -> Model Saved)\n",
      "Epoch 3/50: Train Loss=0.010056, Time: 3.54s, LR: 5.00e-04, Val Loss=0.007113 (Best Val Loss: 0.007113 -> Model Saved)\n",
      "Epoch 4/50: Train Loss=0.009956, Time: 3.54s, LR: 5.00e-04, Val Loss=0.007114 (Val Loss did not improve for 1 epoch(s))\n",
      "Epoch 5/50: Train Loss=0.009961, Time: 3.50s, LR: 5.00e-04, Val Loss=0.007149 (Val Loss did not improve for 2 epoch(s))\n",
      "Epoch 6/50: Train Loss=0.009947, Time: 3.56s, LR: 5.00e-04, Val Loss=0.007131 (Val Loss did not improve for 3 epoch(s))\n",
      "Epoch 7/50: Train Loss=0.009941, Time: 3.56s, LR: 5.00e-04, Val Loss=0.007147 (Val Loss did not improve for 4 epoch(s))\n",
      "Epoch 8/50: Train Loss=0.009931, Time: 3.58s, LR: 5.00e-04, Val Loss=0.007130 (Val Loss did not improve for 5 epoch(s))\n",
      "Epoch 9/50: Train Loss=0.009939, Time: 3.31s, LR: 5.00e-04, Val Loss=0.007168 (Val Loss did not improve for 6 epoch(s))\n",
      "Epoch 10/50: Train Loss=0.009949, Time: 2.14s, LR: 5.00e-04, Val Loss=0.007153 (Val Loss did not improve for 7 epoch(s))\n",
      "Epoch 11/50: Train Loss=0.009918, Time: 2.61s, LR: 5.00e-04, Val Loss=0.007167 (Val Loss did not improve for 8 epoch(s))\n",
      "Epoch 12/50: Train Loss=0.009945, Time: 3.06s, LR: 5.00e-04, Val Loss=0.007155 (Val Loss did not improve for 9 epoch(s))\n",
      "Epoch 13/50: Train Loss=0.009959, Time: 2.41s, LR: 5.00e-04, Val Loss=0.007141 (Val Loss did not improve for 10 epoch(s))\n",
      "Epoch 14/50: Train Loss=0.009890, Time: 1.84s, LR: 5.00e-04, Val Loss=0.007147 (Val Loss did not improve for 11 epoch(s))\n",
      "Epoch 15/50: Train Loss=0.009952, Time: 1.94s, LR: 2.50e-04, Val Loss=0.007154 (Val Loss did not improve for 12 epoch(s))\n",
      "Epoch 16/50: Train Loss=0.009920, Time: 1.52s, LR: 2.50e-04, Val Loss=0.007166 (Val Loss did not improve for 13 epoch(s))\n",
      "Epoch 17/50: Train Loss=0.009938, Time: 1.79s, LR: 2.50e-04, Val Loss=0.007145 (Val Loss did not improve for 14 epoch(s))\n",
      "Epoch 18/50: Train Loss=0.009923, Time: 2.69s, LR: 2.50e-04, Val Loss=0.007146 (Val Loss did not improve for 15 epoch(s))\n",
      "Epoch 19/50: Train Loss=0.009929, Time: 2.99s, LR: 2.50e-04, Val Loss=0.007130 (Val Loss did not improve for 16 epoch(s))\n",
      "Epoch 20/50: Train Loss=0.009938, Time: 1.90s, LR: 2.50e-04, Val Loss=0.007170 (Val Loss did not improve for 17 epoch(s))\n",
      "Epoch 21/50: Train Loss=0.009938, Time: 2.04s, LR: 2.50e-04, Val Loss=0.007155 (Val Loss did not improve for 18 epoch(s))\n",
      "Epoch 22/50: Train Loss=0.009951, Time: 2.93s, LR: 2.50e-04, Val Loss=0.007154 (Val Loss did not improve for 19 epoch(s))\n",
      "\n",
      "Early stopping triggered after 23 epochs due to no improvement in validation loss for 20 consecutive epochs.\n",
      "Training finished.\n",
      "Loading best model weights from ./output/lstm_energy_7blocks/lstm_energy_initial_best_val.pth\n",
      "Best initial model saved during training: ./output/lstm_energy_7blocks/lstm_energy_initial_best_val.pth\n",
      "\n",
      "Saving best available initial model (from lstm_energy_initial_best_val.pth) as ONNX...\n",
      "Attempting to save model to ONNX: ./output/lstm_energy_7blocks/lstm_energy_initial_best_val.onnx\n",
      "Using example input shape: torch.Size([1, 72, 25])\n",
      "✅ Model successfully saved as ONNX to ./output/lstm_energy_7blocks/lstm_energy_initial_best_val.onnx\n",
      "\n",
      "--- Evaluating Initial Model (Post loading best/last initial weights) ---\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/onnx/symbolic_opset9.py:4277: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n",
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
      "  result = _VF.lstm(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics calculated on original scale.\n",
      "Evaluation Metrics: MSE=7496.0786, MAE=48.9322, RMSE=86.5799, R2=-0.0000, MAPE=55.41%\n",
      "Initial Model Eval: MACs=4,334,366,119, Params=2,475,850, SizeMB=9.90, MSE=7496.0786\n",
      "\n",
      "Final Pruning Targets:\n",
      "  Targeting MACs <= 3,000,000,000 (Initial: 4,334,366,119)\n",
      "  Targeting Size <= 5.00 MB (Derived Params Target: 1,310,720)\n",
      "  Using Effective Params Target <= 1,310,720 (Initial: 2,475,850)\n",
      "\n",
      "===== Processing Strategy: Magnitude_L1 =====\n",
      "Loaded initial weights from lstm_energy_initial_best_val.pth for strategy Magnitude_L1\n",
      "No prunable layers selected for strategy Magnitude_L1. Copying initial results and skipping pruning+finetuning.\n",
      "\n",
      "===== Processing Strategy: Magnitude_L2 =====\n",
      "Loaded initial weights from lstm_energy_initial_best_val.pth for strategy Magnitude_L2\n",
      "No prunable layers selected for strategy Magnitude_L2. Copying initial results and skipping pruning+finetuning.\n",
      "\n",
      "===== Processing Strategy: Random =====\n",
      "Loaded initial weights from lstm_energy_initial_best_val.pth for strategy Random\n",
      "No prunable layers selected for strategy Random. Copying initial results and skipping pruning+finetuning.\n",
      "\n",
      "===== Processing Strategy: Taylor =====\n",
      "Loaded initial weights from lstm_energy_initial_best_val.pth for strategy Taylor\n",
      "No prunable layers selected for strategy Taylor. Copying initial results and skipping pruning+finetuning.\n",
      "\n",
      "===== Processing Strategy: FPGM =====\n",
      "Loaded initial weights from lstm_energy_initial_best_val.pth for strategy FPGM\n",
      "No prunable layers selected for strategy FPGM. Copying initial results and skipping pruning+finetuning.\n",
      "\n",
      "===== Final Comparison of All Successfully Processed Strategies =====\n",
      "\n",
      "=== Pruning Strategy Comparison (Metric: MSE) ===\n",
      "Strategy        | MACs (M)   | Params (K) | Size (MB)  | MSE         \n",
      "---------------------------------------------------------------------\n",
      "initial         | 4334.37    | 2475.8     |       9.90 |    7496.0786\n",
      "Magnitude_L1    | 4334.37    | 2475.8     |       9.90 |    7496.0786\n",
      "Magnitude_L2    | 4334.37    | 2475.8     |       9.90 |    7496.0786\n",
      "Random          | 4334.37    | 2475.8     |       9.90 |    7496.0786\n",
      "Taylor          | 4334.37    | 2475.8     |       9.90 |    7496.0786\n",
      "FPGM            | 4334.37    | 2475.8     |       9.90 |    7496.0786\n",
      "Comparison plots saved to ./output/lstm_energy_7blocks\n",
      "\n",
      "Workflow completed!\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path is defined in your code\n",
    "dataset_path = './data/energydata_complete.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Print the head of the original dataframe\n",
    "print(\"Original dataframe shape:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "# If you want to see the target column statistics\n",
    "print(\"\\nTarget column summary statistics:\")\n",
    "print(df['Appliances'].describe())\n",
    "\n",
    "# To see the feature columns\n",
    "print(\"\\nFeatures in the dataset:\")\n",
    "print(df.columns.tolist())"
   ],
   "id": "f864f8d031e430da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "inputs, targets = next(iter(train_loader))\n",
    "\n",
    "print(f\"\\nBatch shape - Inputs: {inputs.shape}, Targets: {targets.shape}\")\n",
    "print(f\"Input features sample (first sequence):\\n{inputs[0]}\")\n",
    "print(f\"Target values sample:\\n{targets[:5]}\")"
   ],
   "id": "6aeaea8fd3142764"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
