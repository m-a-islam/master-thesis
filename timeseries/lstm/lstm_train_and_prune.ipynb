{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a09f8662",
   "metadata": {},
   "source": [
    "# Pruning Experiment for LSTM on Energy Prediction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-05T13:15:16.482463Z",
     "start_time": "2025-05-05T13:15:14.488642Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_pruning as tp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "import os\n",
    "import time\n",
    "import copy"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. LSTM Model Definition",
   "id": "d34501d960e6ef22"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T20:31:43.200116Z",
     "start_time": "2025-05-04T20:31:43.195118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TimeSeriesLSTM(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int, output_size: int, dropout_rate: float = 0.5):\n",
    "        super(TimeSeriesLSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        self.lstm.flatten_parameters()\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = out[:, -1, :]\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ],
   "id": "32c8b58d641d5cc9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T13:34:10.901924Z",
     "start_time": "2025-05-05T13:34:10.895120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TimeSeriesLSTM_MoreLayers(nn.Module): # Renamed for clarity\n",
    "    def __init__(self, input_size, hidden_size, num_layers,\n",
    "                 intermediate_size_1=32, # Size for the first intermediate layer\n",
    "                 intermediate_size_2=24, # Size for the second\n",
    "                 intermediate_size_3=16, # Size for the third\n",
    "                 output_size=1,\n",
    "                 dropout_prob=0.2):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                            batch_first=True, dropout=dropout_prob if num_layers > 1 else 0)\n",
    "\n",
    "        # Define the intermediate Linear layers\n",
    "        # Layer 1\n",
    "        self.intermediate_fc1 = nn.Linear(hidden_size, intermediate_size_1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Layer 2\n",
    "        self.intermediate_fc2 = nn.Linear(intermediate_size_1, intermediate_size_2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Layer 3\n",
    "        self.intermediate_fc3 = nn.Linear(intermediate_size_2, intermediate_size_3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Final output layer\n",
    "        self.fc_final = nn.Linear(intermediate_size_3, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_time_step_out = lstm_out[:, -1, :] # Shape: (batch, hidden_size)\n",
    "\n",
    "        # Pass through intermediate layers\n",
    "        x = self.intermediate_fc1(last_time_step_out)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x) # Shape: (batch, intermediate_size_1)\n",
    "\n",
    "        x = self.intermediate_fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x) # Shape: (batch, intermediate_size_2)\n",
    "\n",
    "        x = self.intermediate_fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x) # Shape: (batch, intermediate_size_3)\n",
    "\n",
    "        final_out = self.fc_final(x) # Shape: (batch, output_size)\n",
    "        return final_out"
   ],
   "id": "f2dc214be8bac7b5",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Seed block",
   "id": "b3e082e0b6ad8438"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T14:49:15.946774Z",
     "start_time": "2025-05-05T14:49:15.942012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IntermediateBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout_prob=0.2):\n",
    "        \"\"\"\n",
    "        A block consisting of a Linear layer, ReLU activation, and Dropout.\n",
    "        Args:\n",
    "            in_features (int): Number of input features to the Linear layer.\n",
    "            out_features (int): Number of output features from the Linear layer.\n",
    "            dropout_prob (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super(IntermediateBlock, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ],
   "id": "4c0afcf4dde47796",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### LSTM with Intermidiate Blocks",
   "id": "b20c07b4f817040"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T14:49:18.236238Z",
     "start_time": "2025-05-05T14:49:18.229339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TimeSeriesLSTM_WithBlocks(nn.Module):\n",
    "    def __init__(self, input_size, lstm_hidden_size, num_lstm_layers,\n",
    "                 block_configs, # List of output sizes for each intermediate block\n",
    "                 output_size=1,\n",
    "                 lstm_dropout_prob=0.2, # Dropout for LSTM layers if num_layers > 1\n",
    "                 block_dropout_prob=0.2): # Dropout within each IntermediateBlock\n",
    "        \"\"\"\n",
    "        LSTM model followed by a sequence of IntermediateBlocks.\n",
    "        Args:\n",
    "            input_size (int): Number of features per time step for LSTM.\n",
    "            lstm_hidden_size (int): Hidden size of the LSTM layer(s).\n",
    "            num_lstm_layers (int): Number of stacked LSTM layers.\n",
    "            block_configs (list of int): A list where each integer is the\n",
    "                                         'out_features' for an IntermediateBlock.\n",
    "                                         e.g., [48, 32, 24] for three blocks.\n",
    "            output_size (int): Final output dimension (e.g., 1 for regression).\n",
    "            lstm_dropout_prob (float): Dropout probability for LSTM (if num_layers > 1).\n",
    "            block_dropout_prob (float): Dropout probability for each IntermediateBlock.\n",
    "        \"\"\"\n",
    "        super(TimeSeriesLSTM_WithBlocks, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, lstm_hidden_size, num_lstm_layers,\n",
    "                            batch_first=True,\n",
    "                            dropout=lstm_dropout_prob if num_lstm_layers > 1 else 0)\n",
    "\n",
    "        # Create intermediate blocks dynamically\n",
    "        self.intermediate_blocks = nn.ModuleList()\n",
    "        current_in_features = lstm_hidden_size # Input to the first block is LSTM's output\n",
    "\n",
    "        for block_out_features in block_configs:\n",
    "            self.intermediate_blocks.append(\n",
    "                IntermediateBlock(current_in_features, block_out_features, block_dropout_prob)\n",
    "            )\n",
    "            current_in_features = block_out_features # Output of this block is input to the next\n",
    "\n",
    "        # Final output layer takes input from the last intermediate block\n",
    "        self.fc_final = nn.Linear(current_in_features, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM part\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Use the output from the last time step of the LSTM\n",
    "        x = lstm_out[:, -1, :] # Shape: (batch, lstm_hidden_size)\n",
    "\n",
    "        # Pass through intermediate blocks\n",
    "        for block in self.intermediate_blocks:\n",
    "            x = block(x)\n",
    "        # After loop, x shape: (batch, block_configs[-1]) if block_configs is not empty\n",
    "        # or (batch, lstm_hidden_size) if block_configs is empty\n",
    "\n",
    "        # Final output\n",
    "        final_out = self.fc_final(x)\n",
    "        return final_out"
   ],
   "id": "68d8a6b4399b0af8",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. Data Handling for Appliances Energy Dataset",
   "id": "dcf0ae5256b38f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### --- Data Configuration ---",
   "id": "b0b7dfa7ecfa623b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T14:54:34.280392Z",
     "start_time": "2025-05-05T14:54:34.275172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATASET_PATH = './data/energydata_complete.csv' # ADJUST PATH AS NEEDED\n",
    "SEQUENCE_LENGTH = 6 * 12 # Use 12 hours of past data (12 hours * 6 samples/hour)\n",
    "TARGET_COLUMN = 'Appliances'\n",
    "# Features to use (excluding target, date, and others)\n",
    "FEATURE_COLUMNS = [\n",
    "    'lights', 'T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4', 'RH_4', 'T5', 'RH_5',\n",
    "    'T6', 'RH_6', 'T7', 'RH_7', 'T8', 'RH_8', 'T9', 'RH_9', 'T_out',\n",
    "    'Press_mm_hg', 'RH_out', 'Windspeed', 'Visibility', 'Tdewpoint'\n",
    "]"
   ],
   "id": "346c330705147c24",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### --- Helper function to create sequences ---",
   "id": "d474b0a04d72dd4c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T14:54:35.618683Z",
     "start_time": "2025-05-05T14:54:35.613565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_sequences(input_data, target_data, seq_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    # Stop seq_length steps early to ensure target data is available\n",
    "    for i in range(len(input_data) - seq_length):\n",
    "        sequences.append(input_data[i:i + seq_length])\n",
    "        targets.append(target_data[i + seq_length])\n",
    "    return np.array(sequences), np.array(targets)"
   ],
   "id": "a5184c2440abb655",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### --- Custom Dataset ---",
   "id": "3df380163964e5f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T13:15:26.101868Z",
     "start_time": "2025-05-05T13:15:26.096695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EnergyDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32).unsqueeze(1) # Target shape [N, 1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.targets[idx]"
   ],
   "id": "a9e29c58eaace567",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### --- Main Data Loading Function ---",
   "id": "60c367dad6b9ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T13:15:28.300395Z",
     "start_time": "2025-05-05T13:15:28.290349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_energy_data_loaders(\n",
    "    file_path=DATASET_PATH,\n",
    "    feature_cols=FEATURE_COLUMNS,\n",
    "    target_col=TARGET_COLUMN,\n",
    "    seq_length=SEQUENCE_LENGTH,\n",
    "    batch_size=64,\n",
    "    test_size=0.2,\n",
    "    val_size=0.1 # Proportion of the *remaining* data after test split\n",
    "    ):\n",
    "    \"\"\"Loads, preprocesses, scales, and creates sequences for the energy dataset.\"\"\"\n",
    "    print(f\"Loading dataset from: {file_path}\")\n",
    "    try:\n",
    "        # 1. Load Data & Initial Processing\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Original data shape: {df.shape}\")\n",
    "        df['date'] = pd.to_datetime(df['date']) # Parse date\n",
    "        df = df.sort_values('date') # Ensure chronological order\n",
    "        df = df.set_index('date') # Optional: use date index\n",
    "        df = df[feature_cols + [target_col]].dropna() # Select columns and drop NaNs\n",
    "        print(f\"Data shape after selecting columns & dropping NaNs: {df.shape}\")\n",
    "        if df.empty:\n",
    "            raise ValueError(\"DataFrame is empty after selecting columns and dropping NaNs.\")\n",
    "\n",
    "        # 2. Separate Features and Target\n",
    "        X = df[feature_cols].values\n",
    "        y = df[[target_col]].values # Keep as 2D: [N, 1]\n",
    "\n",
    "        # 3. Splitting (Chronological)\n",
    "        n_total = len(X)\n",
    "        n_test = int(n_total * test_size)\n",
    "        n_val = int((n_total - n_test) * val_size)\n",
    "        n_train = n_total - n_test - n_val\n",
    "\n",
    "        if n_train <= seq_length or n_val <= seq_length or n_test <= seq_length:\n",
    "             raise ValueError(f\"Not enough data for sequence length {seq_length} after splitting. \"\n",
    "                              f\"Train={n_train}, Val={n_val}, Test={n_test}\")\n",
    "\n",
    "\n",
    "        X_train, y_train = X[:n_train], y[:n_train]\n",
    "        X_val, y_val = X[n_train:n_train + n_val], y[n_train:n_train + n_val]\n",
    "        X_test, y_test = X[n_train + n_val:], y[n_train + n_val:]\n",
    "\n",
    "        print(f\"Data split: Train={X_train.shape[0]}, Val={X_val.shape[0]}, Test={X_test.shape[0]}\")\n",
    "\n",
    "        # 4. Scaling\n",
    "        scaler_features = MinMaxScaler()\n",
    "        scaler_target = MinMaxScaler()\n",
    "\n",
    "        X_train_scaled = scaler_features.fit_transform(X_train)\n",
    "        X_val_scaled = scaler_features.transform(X_val)\n",
    "        X_test_scaled = scaler_features.transform(X_test)\n",
    "\n",
    "        y_train_scaled = scaler_target.fit_transform(y_train)\n",
    "        # Flatten y for sequence creation, EnergyDataset will unsqueeze later\n",
    "        y_val_scaled = scaler_target.transform(y_val)\n",
    "        y_test_scaled = scaler_target.transform(y_test)\n",
    "\n",
    "        # 5. Create Sequences\n",
    "        X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled.flatten(), seq_length)\n",
    "        X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled.flatten(), seq_length)\n",
    "        X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled.flatten(), seq_length)\n",
    "\n",
    "        # 6. Create Datasets and DataLoaders\n",
    "        train_dataset = EnergyDataset(X_train_seq, y_train_seq)\n",
    "        val_dataset = EnergyDataset(X_val_seq, y_val_seq)\n",
    "        test_dataset = EnergyDataset(X_test_seq, y_test_seq)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        input_size = X_train_scaled.shape[1] # Number of features\n",
    "        scalers = {'features': scaler_features, 'target': scaler_target} # Store scalers\n",
    "\n",
    "        print(f\"Data loaded successfully:\")\n",
    "        print(f\"  Input size (features): {input_size}\")\n",
    "        print(f\"  Sequence length: {seq_length}\")\n",
    "        print(f\"  Train sequences: {len(train_dataset)}\")\n",
    "        print(f\"  Validation sequences: {len(val_dataset)}\")\n",
    "        print(f\"  Test sequences: {len(test_dataset)}\")\n",
    "\n",
    "        return train_loader, val_loader, test_loader, input_size, seq_length, scalers\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Dataset file not found at {file_path}\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError during data processing: {ve}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during data loading: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    return None, None, None, 0, 0, None # Return None on error\n"
   ],
   "id": "26bf2344c6cd369a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Training Function (Regression)",
   "id": "b534f34b2a0a5436"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T13:15:33.685885Z",
     "start_time": "2025-05-05T13:15:33.678803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model_regression(model, train_loader, criterion, optimizer, device, num_epochs, val_loader=None, model_path_prefix=\"best_model\", grad_clip=None):\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    print(f\"Starting training for {num_epochs} epochs...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            if grad_clip:\n",
    "                 torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        log_msg = f\"Epoch {epoch+1}/{num_epochs}: Train Loss={epoch_loss:.6f}, Time: {epoch_time:.2f}s\"\n",
    "\n",
    "        # Validation Step\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for inputs_val, labels_val in val_loader:\n",
    "                    inputs_val, labels_val = inputs_val.to(device), labels_val.to(device)\n",
    "                    outputs_val = model(inputs_val)\n",
    "                    val_loss += criterion(outputs_val, labels_val).item() * inputs_val.size(0)\n",
    "            val_loss /= len(val_loader.dataset)\n",
    "            val_losses.append(val_loss)\n",
    "            log_msg += f\", Val Loss={val_loss:.6f}\"\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                save_path = f\"{model_path_prefix}_best_val.pth\"\n",
    "                try:\n",
    "                    torch.save(model.state_dict(), save_path)\n",
    "                    log_msg += f\" (Best model saved)\"\n",
    "                except Exception as e:\n",
    "                    log_msg += f\" (Error saving model: {e})\"\n",
    "            model.train() # Switch back\n",
    "\n",
    "        print(log_msg)\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "    return model, train_losses, val_losses"
   ],
   "id": "3394e5dd05204081",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Evaluation Function (Regression)",
   "id": "c62ad1d00d793849"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T13:15:38.976666Z",
     "start_time": "2025-05-05T13:15:38.966488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_macs_params(model, example_input):\n",
    "    # Ensure example_input is on the right device\n",
    "    device = next(model.parameters()).device\n",
    "    example_input = example_input.to(device)\n",
    "    # tp.utils.count_ops_and_params can fail with LSTMs sometimes. Use torchinfo as fallback.\n",
    "    try:\n",
    "         macs, params = tp.utils.count_ops_and_params(model, example_input)\n",
    "         return macs, params\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: torch_pruning MACs calculation failed ({e}). Falling back to torchinfo estimate.\")\n",
    "        try:\n",
    "             from torchinfo import summary\n",
    "             # Correct input format for torchinfo might be needed depending on version\n",
    "             # Try with tuple (common format) or just the tensor\n",
    "             input_data_shape = example_input.shape\n",
    "             model_summary = summary(model, input_size=input_data_shape, verbose=0)\n",
    "             params = model_summary.total_params\n",
    "             macs = model_summary.total_mult_adds\n",
    "             print(f\"torchinfo estimate: Params={params}, MACs={macs}\")\n",
    "             return macs, params\n",
    "        except Exception as e2:\n",
    "            print(f\"Warning: torchinfo calculation also failed ({e2}). Returning 0 for MACs/Params.\")\n",
    "            return 0, sum(p.numel() for p in model.parameters()) # Return at least params\n",
    "\n",
    "def evaluate_model_regression(model, test_loader, example_input, device, scalers=None):\n",
    "    model.eval()\n",
    "    macs, params = calculate_macs_params(model, example_input) # Handles potential LSTM issues\n",
    "    size_mb = params * 4 / 1e6 # Assumes float32\n",
    "\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    print(\"Evaluating on test set...\")\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            all_outputs.append(outputs.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    all_outputs = np.concatenate(all_outputs, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    outputs_eval = all_outputs\n",
    "    labels_eval = all_labels\n",
    "\n",
    "    # Inverse transform for interpretable metrics\n",
    "    if scalers and 'target' in scalers:\n",
    "        try:\n",
    "            outputs_eval = scalers['target'].inverse_transform(all_outputs)\n",
    "            labels_eval = scalers['target'].inverse_transform(all_labels)\n",
    "            print(\"Metrics calculated on original scale.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not inverse transform. Metrics on scaled data. Error: {e}\")\n",
    "    else:\n",
    "         print(\"Warning: Target scaler not provided. Metrics calculated on scaled data.\")\n",
    "\n",
    "\n",
    "    mse = mean_squared_error(labels_eval, outputs_eval)\n",
    "    mae = mean_absolute_error(labels_eval, outputs_eval)\n",
    "    r2 = r2_score(labels_eval, outputs_eval)\n",
    "    rmse = np.sqrt(mse)\n",
    "    # MAPE calculation - handle potential zeros in labels_eval\n",
    "    epsilon = 1e-8 # Small value to avoid division by zero\n",
    "    mape = np.mean(np.abs((labels_eval - outputs_eval) / (labels_eval + epsilon))) * 100\n",
    "\n",
    "\n",
    "    print(f\"Evaluation Metrics: MSE={mse:.4f}, MAE={mae:.4f}, RMSE={rmse:.4f}, R2={r2:.4f}, MAPE={mape:.2f}%\")\n",
    "\n",
    "    return {\n",
    "        'macs': macs,\n",
    "        'params': params,\n",
    "        'size_mb': size_mb,\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2,\n",
    "        'mape': mape,\n",
    "        'performance': mse # Use MSE as the primary performance metric (lower is better)\n",
    "    }"
   ],
   "id": "d6de516508982311",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5. Pruning Function (Adapted for LSTM)",
   "id": "212c6003f1c20c89"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T13:15:43.713542Z",
     "start_time": "2025-05-05T13:15:43.559880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prune_lstm_model_by_threshold(\n",
    "    model,\n",
    "    example_input_bs1, # BS=1 for MACs/Params calc & non-grad strategies\n",
    "    target_macs,\n",
    "    target_params,\n",
    "    strategy,\n",
    "    max_iterations=50,\n",
    "    step_pruning_ratio=0.1,\n",
    "    gradient_batch=None, # Dict {'inputs': T, 'labels': T} with BS > 1\n",
    "    prunable_modules=None # List of specific layers (e.g., [model.fc])\n",
    "    ):\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    needs_gradient = isinstance(strategy['importance'], (\n",
    "        tp.importance.TaylorImportance,\n",
    "        tp.importance.GroupHessianImportance\n",
    "    ))\n",
    "    if needs_gradient:\n",
    "        if gradient_batch is None: raise ValueError(f\"Strategy needs 'gradient_batch'.\")\n",
    "        if gradient_batch['inputs'].shape[0] <= 1: raise ValueError(f\"Need BS > 1 in gradient_batch\")\n",
    "        gradient_inputs = gradient_batch['inputs'].to(device)\n",
    "        gradient_labels = gradient_batch['labels'].to(device)\n",
    "\n",
    "    print(f\"--- Starting Pruning ({strategy['importance'].__class__.__name__}) ---\")\n",
    "    print(f\"Target MACs: <= {target_macs:,.0f}, Target Params: <= {target_params:,.0f}\")\n",
    "    print(f\"Step Ratio: {step_pruning_ratio:.2f}, Max Iter: {max_iterations}\")\n",
    "\n",
    "    # Determine layers to prune/ignore\n",
    "    if not prunable_modules:\n",
    "         prunable_modules = [m for m in model.modules() if isinstance(m, nn.Linear)]\n",
    "         print(f\"Defaulting to pruning nn.Linear layers: {[m.__class__.__name__ for m in prunable_modules]}\")\n",
    "    else:\n",
    "         print(f\"Targeting specific modules for pruning: {[m.__class__.__name__ for m in prunable_modules]}\")\n",
    "\n",
    "    modules_to_ignore = [m for m in model.modules() if isinstance(m, (nn.Linear, nn.Conv2d, nn.LSTM)) and m not in prunable_modules]\n",
    "    root_types = list(set(type(m) for m in prunable_modules))\n",
    "    if not root_types:\n",
    "        print(\"Warning: No prunable module types identified. Pruning may fail.\")\n",
    "        root_types = [nn.Linear] # Fallback guess\n",
    "\n",
    "\n",
    "    pruner = strategy['pruner'](\n",
    "        model,\n",
    "        example_input_bs1.to(device),\n",
    "        importance=strategy['importance'],\n",
    "        pruning_ratio=step_pruning_ratio,\n",
    "        root_module_types=root_types,\n",
    "        ignored_layers=modules_to_ignore,\n",
    "    )\n",
    "\n",
    "    initial_macs, initial_params = calculate_macs_params(model, example_input_bs1.to(device))\n",
    "    current_macs, current_params = initial_macs, initial_params\n",
    "    print(f\"Initial State | MACs: {current_macs:,.0f}, Params: {current_params:,.0f}\")\n",
    "    # --- Remove or comment out these two lines ---\n",
    "    # prunable_layer_names = [layer.__class__.__name__ for layer in pruner.get_pruning_layers()] # <--- ERROR HERE\n",
    "    # print(f\"Detected Prunable Layers by tp: {prunable_layer_names}\")\n",
    "    # --------------------------------------------\n",
    "    if initial_macs == 0 and initial_params == 0: # Check if initial calc failed\n",
    "         print(\"Warning: Initial MACs/Params calculation failed or returned zero. Cannot proceed.\")\n",
    "         return model # Or raise error\n",
    "\n",
    "    # Existing check (adjust slightly): If no prunable layers are implicitly handled by pruner.step, it will just return empty groups.\n",
    "    # We don't need the explicit check here anymore. The loop checking `if not pruning_groups:` will handle it.\n",
    "    # if not prunable_layer_names and initial_macs > 0:\n",
    "    #     print(\"Warning: torch-pruning did not detect any prunable layers matching criteria.\")\n",
    "\n",
    "\n",
    "    iteration = 0\n",
    "    model.eval()\n",
    "    criterion = nn.MSELoss().to(device) # Loss for gradient calculation\n",
    "\n",
    "    while (current_macs > target_macs or current_params > target_params) and iteration < max_iterations:\n",
    "        iteration += 1\n",
    "        macs_before_step = current_macs\n",
    "        params_before_step = current_params\n",
    "\n",
    "        if needs_gradient:\n",
    "            model.train()\n",
    "            input_for_grad = gradient_inputs.detach().clone()\n",
    "            labels_for_grad = gradient_labels.detach().clone()\n",
    "            try:\n",
    "                for param in model.parameters(): param.requires_grad_(True)\n",
    "                outputs = model(input_for_grad)\n",
    "                loss = criterion(outputs, labels_for_grad)\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError during backward (Iter {iteration}): {e}. Stopping.\")\n",
    "                model.eval() ; break\n",
    "            finally:\n",
    "                model.eval() ; model.zero_grad(set_to_none=True)\n",
    "\n",
    "        try:\n",
    "            pruning_groups = list(pruner.step(interactive=True))\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during pruner.step() (Iter {iteration}): {e}. Stopping.\")\n",
    "            break\n",
    "\n",
    "        if not pruning_groups:\n",
    "            print(f\"Iter {iteration}: No more candidates found by pruner. Stopping.\")\n",
    "            break\n",
    "\n",
    "        for group in pruning_groups: group.prune()\n",
    "\n",
    "        current_macs, current_params = calculate_macs_params(model, example_input_bs1.to(device))\n",
    "        macs_reduced_pct = (macs_before_step - current_macs) / initial_macs * 100 if initial_macs > 0 else 0\n",
    "        params_reduced_pct = (params_before_step - current_params) / initial_params * 100 if initial_params > 0 else 0\n",
    "\n",
    "        print(\n",
    "            f\"Iter {iteration: >3}/{max_iterations} | \"\n",
    "            f\"MACs: {current_macs:,.0f} ({macs_reduced_pct:+6.1f}% R) | \"\n",
    "            f\"Params: {current_params:,.0f} ({params_reduced_pct:+6.1f}% R)\"\n",
    "        )\n",
    "\n",
    "        if current_macs >= macs_before_step and current_params >= params_before_step:\n",
    "            if not (current_macs <= target_macs and current_params <= target_params):\n",
    "                print(f\"Iter {iteration}: No reduction. Stopping.\") ; break\n",
    "            else: break # Targets met\n",
    "\n",
    "    # --- Final Report ---\n",
    "    print(f\"--- Finished Pruning ({strategy['importance'].__class__.__name__}) ---\")\n",
    "    if iteration >= max_iterations: print(f\"Warning: Reached max iterations ({max_iterations}).\")\n",
    "    final_macs, final_params = calculate_macs_params(model, example_input_bs1.to(device))\n",
    "    macs_reduction = (initial_macs - final_macs) / initial_macs * 100 if initial_macs > 0 else 0\n",
    "    params_reduction = (initial_params - final_params) / initial_params * 100 if initial_params > 0 else 0\n",
    "    print(f\"Initial | MACs: {initial_macs:,.0f}, Params: {initial_params:,.0f}\")\n",
    "    print(f\"Final   | MACs: {final_macs:,.0f} (Reduction: {macs_reduction:.2f}%)\")\n",
    "    print(f\"        | Params: {final_params:,.0f} (Reduction: {params_reduction:.2f}%)\")\n",
    "    print(f\"Target  | MACs <= {target_macs:,.0f}, Params <= {target_params:,.0f}\")\n",
    "    if final_macs > target_macs or final_params > target_params:\n",
    "         print(\"Warning: Pruning finished, but target threshold(s) were not fully met.\")\n",
    "\n",
    "    model.eval()\n",
    "    return model"
   ],
   "id": "9b726ab96d7993b7",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6. Comparison and Plotting Function (Regression)",
   "id": "b067dfeb3c7e56f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T13:15:53.613947Z",
     "start_time": "2025-05-05T13:15:53.603273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compare_results_and_plot_regression(results, metric_key='mse', lower_is_better=True, output_dir='output'):\n",
    "    if not results: print(\"No results to plot.\") ; return\n",
    "    valid_results = {k: v for k, v in results.items() if isinstance(v, dict) and all(m in v for m in ['macs', 'params', metric_key])}\n",
    "    if not valid_results: print(\"No valid results entries found for plotting.\") ; return\n",
    "\n",
    "    strategy_order = []\n",
    "    if 'initial' in valid_results: strategy_order.append('initial')\n",
    "    strategy_order.extend([s for s in valid_results if s != 'initial'])\n",
    "    if not strategy_order: print(\"No strategies to plot.\"); return\n",
    "\n",
    "    # --- Print Table ---\n",
    "    metric_name = metric_key.upper()\n",
    "    print(f\"\\n=== Pruning Strategy Comparison (Metric: {metric_name}) ===\")\n",
    "    header = f\"{'Strategy':<15} | {'MACs (M)':<10} | {'Params (K)':<10} | {'Size (MB)':<10} | {metric_name:<12}\"\n",
    "    print(header); print(\"-\" * len(header))\n",
    "    for strategy in strategy_order:\n",
    "        metrics = valid_results[strategy]\n",
    "        macs_m = metrics['macs']/1e6 if metrics['macs'] is not None else 0\n",
    "        params_k = metrics['params']/1e3 if metrics['params'] is not None else 0\n",
    "        print(f\"{strategy:<15} | {macs_m:<10.2f} | {params_k:<10.1f} | {metrics.get('size_mb', 0):>10.2f} | {metrics[metric_key]:>12.4f}\")\n",
    "\n",
    "    # --- Generate Bar Charts ---\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    metrics_to_plot = ['macs', 'params', 'size_mb', metric_key]\n",
    "    base_titles = {'macs': 'MACs', 'params': 'Parameters', 'size_mb': 'Model Size (MB)', metric_key: metric_name}\n",
    "    plot_titles = {k: f'{v} Comparison (Lower is Better)' for k, v in base_titles.items()}\n",
    "    if not lower_is_better:\n",
    "        plot_titles[metric_key] = f'{base_titles[metric_key]} Comparison (Higher is Better)'\n",
    "\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(strategy_order)))\n",
    "    initial_metrics = valid_results.get('initial', None)\n",
    "\n",
    "    for plot_metric in metrics_to_plot:\n",
    "        if not all(plot_metric in valid_results[s] for s in strategy_order):\n",
    "             print(f\"Skipping plot for {plot_metric} as it's missing from some results.\")\n",
    "             continue\n",
    "        values = [valid_results[strategy][plot_metric] for strategy in strategy_order]\n",
    "\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        bars = plt.bar(strategy_order, values, color=colors)\n",
    "        plt.ylabel(base_titles[plot_metric])\n",
    "        plt.title(plot_titles[plot_metric])\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "        # Add value labels\n",
    "        max_val = max(values) if values else 0\n",
    "        for i, bar in enumerate(bars):\n",
    "            yval = bar.get_height()\n",
    "            label = \"\"\n",
    "            # ... (Use the formatting logic from previous `compare_results_and_plot_regression` function) ...\n",
    "            if plot_metric == 'macs': label = f'{yval/1e6:.2f}M' if yval > 1e5 else f'{yval:,.0f}' # Adjust format\n",
    "            elif plot_metric == 'params': label = f'{yval/1e3:.1f}K' if yval > 100 else f'{yval:,.0f}' # Adjust format\n",
    "            elif plot_metric == 'size_mb': label = f'{yval:.2f}'\n",
    "            else: label = f'{yval:.4f}' # Regression metric\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., yval + 0.01 * max_val, label, ha='center', va='bottom', fontsize=8, rotation=0)\n",
    "\n",
    "        if initial_metrics and plot_metric in initial_metrics:\n",
    "             initial_value = initial_metrics[plot_metric]\n",
    "             plt.axhline(y=initial_value, color='r', linestyle='--', label=f'Initial Value')\n",
    "             plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(output_dir, f'lstm_energy_{plot_metric}_comparison.png')\n",
    "        try:\n",
    "            plt.savefig(save_path)\n",
    "        except Exception as e:\n",
    "             print(f\"Error saving plot {save_path}: {e}\")\n",
    "        plt.close()\n",
    "    print(f\"Comparison plots saved to {output_dir}\")"
   ],
   "id": "6eeda7c2629cea21",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T13:15:57.821885Z",
     "start_time": "2025-05-05T13:15:57.750493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "\n",
    "# [...] other imports\n",
    "\n",
    "def save_model_as_onnx(model, example_input, output_path, opset_version=13):\n",
    "    \"\"\"Saves the PyTorch model as ONNX.\"\"\"\n",
    "    # Ensure model is on the same device as the example input for export\n",
    "    device = example_input.device\n",
    "    model.to(device)\n",
    "    model.eval() # Ensure model is in evaluation mode\n",
    "\n",
    "    print(f\"Attempting to save model to ONNX: {output_path}\")\n",
    "    print(f\"Using example input shape: {example_input.shape}\")\n",
    "\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            example_input, # Must have the correct shape (batch, seq_len, features)\n",
    "            output_path,\n",
    "            export_params=True,       # Store the trained parameter weights inside the model file\n",
    "            opset_version=opset_version,    # The ONNX version to export the model to\n",
    "            do_constant_folding=True, # Optional: optimizes the model\n",
    "            input_names=['input'],    # Specify names for input nodes\n",
    "            output_names=['output'],  # Specify names for output nodes\n",
    "            dynamic_axes={            # Allow variable batch size\n",
    "                'input': {0: 'batch_size'},\n",
    "                'output': {0: 'batch_size'}\n",
    "            }\n",
    "        )\n",
    "        print(f\"✅ Model successfully saved as ONNX to {output_path}\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to save model as ONNX: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False"
   ],
   "id": "509041bd1d9a5873",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7. Main Workflow (LSTM Energy Prediction)",
   "id": "597afc3d99ec2631"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T09:12:50.519611Z",
     "start_time": "2025-05-06T09:12:50.497667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main_lstm():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- Configuration ---\n",
    "    config = {\n",
    "        'strategies': {\n",
    "            'Magnitude_L1': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.MagnitudeImportance(p=1)},\n",
    "            'Magnitude_L2': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.MagnitudeImportance(p=2)},\n",
    "            'Random': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.RandomImportance()},\n",
    "            'Taylor': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.TaylorImportance()},\n",
    "            # 'LAMP': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.LAMPImportance(p=2)}, # LAMP might need careful tuning for regression\n",
    "            'FPGM': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.FPGMImportance()},\n",
    "        },\n",
    "        'block_output_features': [64, 48, 32], # Example: 3 blocks -> FC(lstm_hidden,64), FC(64,48), FC(48,32)\n",
    "\n",
    "        # You'll likely use different dropout probs for LSTM vs blocks\n",
    "               # Dropout for LSTM layer itself (if num_lstm_layers > 1)\n",
    "        'block_dropout': 0.3,        # Dropout for the intermediate blocks\n",
    "        'target_macs_sparsity': 0.3,   # Example: Target 30% MAC reduction\n",
    "        'target_params_sparsity': 0.4, # Example: Target 40% Params reduction\n",
    "        # --- LSTM Hyperparameters ---\n",
    "        'lstm_hidden_size': 64,     # Smaller hidden size to start\n",
    "        'lstm_num_layers': 3,       # Single layer often works for simpler tasks\n",
    "        'lstm_dropout': 0.2,\n",
    "        # --- Training ---\n",
    "        'train_epochs': 30,         # Fewer initial epochs, more for fine-tune\n",
    "        'fine_tune_epochs': 40,\n",
    "        'batch_size': 128,\n",
    "        'learning_rate_initial': 0.002,\n",
    "        'learning_rate_finetune': 0.0005, # Lower LR for fine-tuning\n",
    "        'grad_clip': 1.0, # Use gradient clipping\n",
    "        # --- Paths & Pruning ---\n",
    "        'output_dir': './output/lstm_energy_pruning',\n",
    "        'pruning_max_iterations': 50,\n",
    "        'pruning_step_ratio': 0.2,\n",
    "        'pruning_primary_metric': 'mse', # Metric to compare for pruning plots\n",
    "    }\n",
    "    os.makedirs(config['output_dir'], exist_ok=True)\n",
    "\n",
    "    # --- Initialize Data ---\n",
    "    print(\"Loading and processing data...\")\n",
    "    train_loader, val_loader, test_loader, input_size, seq_length, scalers = get_energy_data_loaders(\n",
    "        batch_size=config['batch_size'], seq_length=SEQUENCE_LENGTH\n",
    "    )\n",
    "    if train_loader is None: return # Exit if data loading failed\n",
    "\n",
    "    # --- Initialize Model ---\n",
    "    \"\"\"\n",
    "    model = TimeSeriesLSTM(\n",
    "        input_size=input_size, hidden_size=config['lstm_hidden_size'],\n",
    "        num_layers=config['lstm_num_layers'], output_size=1,\n",
    "        dropout_rate=config['lstm_dropout']\n",
    "    ).to(device)\n",
    "\n",
    "    model = TimeSeriesLSTM_MoreLayers(\n",
    "        input_size=input_size,\n",
    "        hidden_size=config['lstm_hidden_size'],\n",
    "        num_layers=config['lstm_num_layers'],\n",
    "        output_size=1,\n",
    "        dropout_prob=config['lstm_dropout']\n",
    "    ).to(device)\n",
    "    \"\"\"\n",
    "    model = TimeSeriesLSTM_WithBlocks(\n",
    "        input_size=input_size,\n",
    "        lstm_hidden_size=config['lstm_hidden_size'],\n",
    "        num_lstm_layers=config['lstm_num_layers'], # Assuming this key exists in your config\n",
    "        block_configs=config.get('block_output_features'), # Pass the list of block output sizes\n",
    "        output_size=1,\n",
    "        lstm_dropout_prob=config.get('lstm_dropout'),\n",
    "        block_dropout_prob=config.get('block_dropout')\n",
    "    ).to(device)\n",
    "    print(\"\\nModel Architecture:\")\n",
    "    print(model)\n",
    "\n",
    "    # --- Create Example Inputs ---\n",
    "    example_input_bs1 = torch.randn(1, seq_length, input_size) .to(device) # No need to send to device yet\n",
    "    example_gradient_batch = None\n",
    "    try:\n",
    "        grad_batch_data = next(iter(train_loader))\n",
    "        if grad_batch_data[0].shape[0] > 1:\n",
    "            example_gradient_batch = {'inputs': grad_batch_data[0], 'labels': grad_batch_data[1]}\n",
    "            print(f\"Obtained gradient batch with BS={example_gradient_batch['inputs'].shape[0]}\")\n",
    "        else: print(\"Warning: First train batch has BS=1, cannot use for gradient importance.\")\n",
    "    except Exception as e: print(f\"Could not get gradient batch: {e}\")\n",
    "\n",
    "\n",
    "     # --- Initial Training ---\n",
    "    initial_model_pth_path = os.path.join(config['output_dir'], \"lstm_energy_initial.pth\") # Last epoch\n",
    "    initial_best_model_pth_path = os.path.join(config['output_dir'], \"lstm_energy_initial_best_val.pth\") # Best validation\n",
    "    initial_model_onnx_path = os.path.join(config['output_dir'], \"lstm_energy_initial_best_val.onnx\") # ONNX for the best initial\n",
    "\n",
    "    if not os.path.exists(initial_best_model_pth_path):\n",
    "        print(\"\\n--- Initial Training ---\")\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate_initial'])\n",
    "        # Assuming train_model_regression saves the best model based on val_loader\n",
    "        model, _, _ = train_model_regression(\n",
    "            model=model, train_loader=train_loader, criterion=nn.MSELoss().to(device),\n",
    "            optimizer=optimizer, device=device, num_epochs=config['train_epochs'],\n",
    "            val_loader=val_loader,\n",
    "            model_path_prefix=os.path.join(config['output_dir'], \"lstm_energy_initial\"), # prefix for _best_val.pth\n",
    "            grad_clip=config.get('grad_clip', None)\n",
    "        )\n",
    "        # Save the *last* epoch model state as well\n",
    "        torch.save(model.state_dict(), initial_model_pth_path)\n",
    "        print(f\"Last epoch of initial training saved to {initial_model_pth_path}\")\n",
    "\n",
    "        # Determine which model to use for ONNX (best validated if available, else last epoch)\n",
    "        if os.path.exists(initial_best_model_pth_path):\n",
    "            print(f\"Best initial model saved during training: {initial_best_model_pth_path}\")\n",
    "            # Load the best model to ensure 'model' variable holds the best weights\n",
    "            model.load_state_dict(torch.load(initial_best_model_pth_path, map_location=device))\n",
    "            print(f\"Loaded best initial model ({initial_best_model_pth_path}) for ONNX export and pruning base.\")\n",
    "            onnx_base_model_pth = initial_best_model_pth_path\n",
    "        else:\n",
    "            print(f\"Warning: Best validation model not saved. Using last epoch model from {initial_model_pth_path} for ONNX.\")\n",
    "            # 'model' already holds the last epoch weights here\n",
    "            onnx_base_model_pth = initial_model_pth_path # Should actually be model, already holding last epoch\n",
    "\n",
    "        # ---> SAVE INITIAL MODEL AS ONNX <---\n",
    "        print(f\"\\nSaving best initial model (from {os.path.basename(onnx_base_model_pth)}) as ONNX...\")\n",
    "        # Ensure 'model' currently holds the weights we want to export (i.e., the best initial)\n",
    "        save_model_as_onnx(model, example_input_bs1, initial_model_onnx_path)\n",
    "        # ----------------------------------\n",
    "\n",
    "    else: # If initial_best_model_pth_path already exists\n",
    "        print(f\"\\nLoading best initial model from {initial_best_model_pth_path}\")\n",
    "        model.load_state_dict(torch.load(initial_best_model_pth_path, map_location=device))\n",
    "        # ---> SAVE INITIAL MODEL AS ONNX (if not already done, or overwrite if preferred) <---\n",
    "        if not os.path.exists(initial_model_onnx_path): # Only save if it doesn't exist\n",
    "            print(f\"Saving loaded best initial model ({os.path.basename(initial_best_model_pth_path)}) as ONNX...\")\n",
    "            save_model_as_onnx(model, example_input_bs1, initial_model_onnx_path)\n",
    "        else:\n",
    "            print(f\"ONNX for initial model already exists at {initial_model_onnx_path}. Skipping.\")\n",
    "        # ----------------------------------\n",
    "\n",
    "\n",
    "    # --- Evaluate Initial Model (ensure 'model' has best initial weights) ---\n",
    "    results = {}\n",
    "    print(\"\\n--- Evaluating Initial Model ---\")\n",
    "    results['initial'] = evaluate_model_regression(model, test_loader, example_input_bs1, device, scalers)\n",
    "    initial_macs = results['initial']['macs']\n",
    "    initial_params = results['initial']['params']\n",
    "    print(f\"Initial Model Performance: MACs={initial_macs:,.0f}, Params={initial_params:,.0f}, MSE={results['initial']['mse']:.4f}\")\n",
    "\n",
    "\n",
    "    # --- Calculate Targets ---\n",
    "    target_macs_value = initial_macs * (1 - config['target_macs_sparsity'])\n",
    "    target_params_value = initial_params * (1 - config['target_params_sparsity'])\n",
    "    print(f\"\\nTargeting MACs <= {target_macs_value:,.0f}, Params <= {target_params_value:,.0f}\")\n",
    "\n",
    "\n",
    "    # --- Pruning and Fine-tuning Loop ---\n",
    "    for strategy_name, strategy_config in config['strategies'].items():\n",
    "        print(f\"\\n===== Processing Strategy: {strategy_name} =====\")\n",
    "        model_to_prune = TimeSeriesLSTM_WithBlocks(\n",
    "             input_size=input_size,\n",
    "             lstm_hidden_size=config['lstm_hidden_size'],\n",
    "             num_lstm_layers=config['lstm_num_layers'],\n",
    "             block_configs=config.get('block_output_features'),\n",
    "             output_size=1,\n",
    "             lstm_dropout_prob=config.get('lstm_dropout'),\n",
    "             block_dropout_prob=config.get('block_dropout')\n",
    "        ).to(device)\n",
    "        # Load the *best initial* weights into the fresh model\n",
    "        model_to_prune.load_state_dict(torch.load(initial_best_model_pth_path, map_location=device))\n",
    "        model_to_prune.eval()\n",
    "\n",
    "        # Gradient batch check\n",
    "        needs_grad = isinstance(strategy_config['importance'], tp.importance.TaylorImportance)\n",
    "        grad_batch = example_gradient_batch if needs_grad else None\n",
    "        if needs_grad and not grad_batch:\n",
    "            print(f\"Skipping {strategy_name}: requires gradient batch, but none available.\")\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # --- > SELECT PRUNABLE LAYERS FROM BLOCKS < ---\n",
    "        # To target layers *inside* the blocks, you need to access them via intermediate_blocks\n",
    "        # e.g., model_to_prune.intermediate_blocks[0].fc is the Linear layer of the first block\n",
    "        prunable_layers_list = []\n",
    "        if hasattr(model_to_prune, 'intermediate_blocks') and len(model_to_prune.intermediate_blocks) > 0:\n",
    "            # Example: Prune the Linear layers within all intermediate blocks\n",
    "            for i in range(len(model_to_prune.intermediate_blocks)):\n",
    "                prunable_layers_list.append(model_to_prune.intermediate_blocks[i].fc)\n",
    "        else:\n",
    "            # Fallback if no blocks (or if you also want to prune a global fc after blocks, if any)\n",
    "            # prunable_layers_list.append(model_to_prune.some_other_fc_if_exists)\n",
    "            print(\"Warning: No intermediate blocks found to prune. Check model architecture and 'block_output_features' config.\")\n",
    "\n",
    "\n",
    "        # Check if any prunable layers were actually added\n",
    "        if not prunable_layers_list:\n",
    "            print(f\"No prunable layers selected for strategy {strategy_name}. Skipping pruning for this strategy.\")\n",
    "            # Optionally evaluate the unpruned model here or mark as failed\n",
    "            results[strategy_name] = results['initial'].copy() # Copy initial if not pruning\n",
    "            results[strategy_name]['notes'] = \"Pruning skipped: No prunable layers found/selected.\"\n",
    "            continue\n",
    "\n",
    "\n",
    "        print(f\"Targeting for pruning: {[type(layer).__name__ + ' with out_features=' + str(layer.out_features) for layer in prunable_layers_list]}\")\n",
    "\n",
    "\n",
    "\n",
    "        # Perform Pruning\n",
    "        try:\n",
    "            pruned_model = prune_lstm_model_by_threshold(\n",
    "                model=model_to_prune,\n",
    "                example_input_bs1=example_input_bs1,\n",
    "                target_macs=target_macs_value,\n",
    "                target_params=target_params_value,\n",
    "                strategy=strategy_config,\n",
    "                max_iterations=config['pruning_max_iterations'],\n",
    "                step_pruning_ratio=config['pruning_step_ratio'],\n",
    "                gradient_batch=grad_batch,\n",
    "                prunable_modules=prunable_layers_list  # Updated to include LSTM # Explicitly prune only FC\n",
    "            )\n",
    "            pruned_path = os.path.join(config['output_dir'], f\"lstm_{strategy_name}_pruned.pth\")\n",
    "            torch.save(pruned_model.state_dict(), pruned_path)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nERROR during pruning ({strategy_name}): {e}\")\n",
    "            import traceback; traceback.print_exc()\n",
    "            results[strategy_name] = {'error': str(e)} # Mark failure\n",
    "            continue # Skip to next strategy\n",
    "\n",
    "        # Fine-tune\n",
    "        print(f\"\\n--- Fine-tuning ({strategy_name}) ---\")\n",
    "        ft_prefix = os.path.join(config['output_dir'], f\"lstm_{strategy_name}_ft\")\n",
    "        ft_best_path = ft_prefix + \"_best_val.pth\"\n",
    "        optimizer_ft = optim.Adam(pruned_model.parameters(), lr=config['learning_rate_finetune'])\n",
    "        fine_tuned_model, _, _ = train_model_regression(\n",
    "            model=pruned_model, train_loader=train_loader, criterion=nn.MSELoss().to(device),\n",
    "            optimizer=optimizer_ft, device=device, num_epochs=config['fine_tune_epochs'],\n",
    "            val_loader=val_loader, model_path_prefix=ft_prefix, grad_clip=config['grad_clip']\n",
    "        )\n",
    "\n",
    "        # Load the best fine-tuned model\n",
    "        if os.path.exists(ft_best_path):\n",
    "            print(f\"Loading best fine-tuned model from {ft_best_path}\")\n",
    "            fine_tuned_model.load_state_dict(torch.load(ft_best_path, map_location=device))\n",
    "        else:\n",
    "            print(f\"Warning: Best fine-tuned model path not found ({ft_best_path}). Using last epoch.\")\n",
    "\n",
    "        # Evaluate Final\n",
    "        print(f\"\\n--- Evaluating Fine-tuned Model ({strategy_name}) ---\")\n",
    "        results[strategy_name] = evaluate_model_regression(\n",
    "            fine_tuned_model, test_loader, example_input_bs1, device, scalers\n",
    "        )\n",
    "\n",
    "        # Save final model state dict (.pth)\n",
    "        final_path = os.path.join(config['output_dir'], f\"lstm_{strategy_name}_final.pth\")\n",
    "        torch.save(fine_tuned_model.state_dict(), final_path)\n",
    "        print(f\"Final PyTorch model saved to {final_path}\")\n",
    "\n",
    "        # ---> ADD ONNX SAVING HERE <---\n",
    "        onnx_path = final_path.replace('.pth', '.onnx')\n",
    "        # Ensure the fine_tuned_model is used, and example_input_bs1 has the correct shape/device\n",
    "        save_model_as_onnx(fine_tuned_model, example_input_bs1, onnx_path)\n",
    "\n",
    "        # --- (Optional) Intermediate Comparison Plot ---\n",
    "        compare_results_and_plot_regression(\n",
    "            results, metric_key=config['pruning_primary_metric'],\n",
    "            lower_is_better=True, # True for MSE/MAE\n",
    "            output_dir=config['output_dir']\n",
    "        )\n",
    "\n",
    "    # --- Final Comparison ---\n",
    "    print(\"\\n===== Final Comparison =====\")\n",
    "    compare_results_and_plot_regression(\n",
    "         results, metric_key=config['pruning_primary_metric'],\n",
    "         lower_is_better=True, output_dir=config['output_dir']\n",
    "     )\n",
    "    print(\"\\nWorkflow completed!\")"
   ],
   "id": "b5ee0fc054f06f78",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Run the main function",
   "id": "ee8690ed3c7ce541"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T09:14:08.021060Z",
     "start_time": "2025-05-06T09:12:57.063783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main_lstm()"
   ],
   "id": "7b4ac278517951f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading and processing data...\n",
      "Loading dataset from: ./data/energydata_complete.csv\n",
      "Original data shape: (19735, 29)\n",
      "Data shape after selecting columns & dropping NaNs: (19735, 26)\n",
      "Data split: Train=14210, Val=1578, Test=3947\n",
      "Data loaded successfully:\n",
      "  Input size (features): 25\n",
      "  Sequence length: 72\n",
      "  Train sequences: 14138\n",
      "  Validation sequences: 1506\n",
      "  Test sequences: 3875\n",
      "\n",
      "Model Architecture:\n",
      "TimeSeriesLSTM_WithBlocks(\n",
      "  (lstm): LSTM(25, 64, num_layers=3, batch_first=True, dropout=0.2)\n",
      "  (intermediate_blocks): ModuleList(\n",
      "    (0): IntermediateBlock(\n",
      "      (fc): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (1): IntermediateBlock(\n",
      "      (fc): Linear(in_features=64, out_features=48, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (2): IntermediateBlock(\n",
      "      (fc): Linear(in_features=48, out_features=32, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc_final): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "Obtained gradient batch with BS=128\n",
      "\n",
      "--- Initial Training ---\n",
      "Starting training for 30 epochs...\n",
      "Epoch 1/30: Train Loss=0.012609, Time: 0.56s, Val Loss=0.007176 (Best model saved)\n",
      "Epoch 2/30: Train Loss=0.010426, Time: 0.53s, Val Loss=0.006971 (Best model saved)\n",
      "Epoch 3/30: Train Loss=0.009676, Time: 0.55s, Val Loss=0.007073\n",
      "Epoch 4/30: Train Loss=0.009261, Time: 0.53s, Val Loss=0.006491 (Best model saved)\n",
      "Epoch 5/30: Train Loss=0.008848, Time: 0.52s, Val Loss=0.006536\n",
      "Epoch 6/30: Train Loss=0.008527, Time: 0.52s, Val Loss=0.006293 (Best model saved)\n",
      "Epoch 7/30: Train Loss=0.008165, Time: 0.52s, Val Loss=0.006266 (Best model saved)\n",
      "Epoch 8/30: Train Loss=0.008086, Time: 0.36s, Val Loss=0.006290\n",
      "Epoch 9/30: Train Loss=0.008159, Time: 0.28s, Val Loss=0.006303\n",
      "Epoch 10/30: Train Loss=0.007865, Time: 0.32s, Val Loss=0.006038 (Best model saved)\n",
      "Epoch 11/30: Train Loss=0.007763, Time: 0.33s, Val Loss=0.006216\n",
      "Epoch 12/30: Train Loss=0.007747, Time: 0.33s, Val Loss=0.006086\n",
      "Epoch 13/30: Train Loss=0.007638, Time: 0.28s, Val Loss=0.006522\n",
      "Epoch 14/30: Train Loss=0.007639, Time: 0.28s, Val Loss=0.005811 (Best model saved)\n",
      "Epoch 15/30: Train Loss=0.007527, Time: 0.28s, Val Loss=0.006500\n",
      "Epoch 16/30: Train Loss=0.007383, Time: 0.28s, Val Loss=0.006594\n",
      "Epoch 17/30: Train Loss=0.007284, Time: 0.27s, Val Loss=0.006702\n",
      "Epoch 18/30: Train Loss=0.007374, Time: 0.29s, Val Loss=0.006156\n",
      "Epoch 19/30: Train Loss=0.007160, Time: 0.28s, Val Loss=0.005951\n",
      "Epoch 20/30: Train Loss=0.007163, Time: 0.29s, Val Loss=0.006397\n",
      "Epoch 21/30: Train Loss=0.007056, Time: 0.30s, Val Loss=0.006125\n",
      "Epoch 22/30: Train Loss=0.007053, Time: 0.30s, Val Loss=0.005838\n",
      "Epoch 23/30: Train Loss=0.006937, Time: 0.28s, Val Loss=0.007030\n",
      "Epoch 24/30: Train Loss=0.006820, Time: 0.28s, Val Loss=0.006636\n",
      "Epoch 25/30: Train Loss=0.006702, Time: 0.27s, Val Loss=0.006160\n",
      "Epoch 26/30: Train Loss=0.006561, Time: 0.30s, Val Loss=0.006210\n",
      "Epoch 27/30: Train Loss=0.006785, Time: 0.33s, Val Loss=0.006307\n",
      "Epoch 28/30: Train Loss=0.006689, Time: 0.31s, Val Loss=0.005937\n",
      "Epoch 29/30: Train Loss=0.006487, Time: 0.30s, Val Loss=0.006351\n",
      "Epoch 30/30: Train Loss=0.006518, Time: 0.30s, Val Loss=0.005648 (Best model saved)\n",
      "Training finished.\n",
      "Last epoch of initial training saved to ./output/lstm_energy_pruning/lstm_energy_initial.pth\n",
      "Best initial model saved during training: ./output/lstm_energy_pruning/lstm_energy_initial_best_val.pth\n",
      "Loaded best initial model (./output/lstm_energy_pruning/lstm_energy_initial_best_val.pth) for ONNX export and pruning base.\n",
      "\n",
      "Saving best initial model (from lstm_energy_initial_best_val.pth) as ONNX...\n",
      "Attempting to save model to ONNX: ./output/lstm_energy_pruning/lstm_energy_initial_best_val.onnx\n",
      "Using example input shape: torch.Size([1, 72, 25])\n",
      "✅ Model successfully saved as ONNX to ./output/lstm_energy_pruning/lstm_energy_initial_best_val.onnx\n",
      "\n",
      "--- Evaluating Initial Model ---\n",
      "Evaluating on test set...\n",
      "Metrics calculated on original scale.\n",
      "Evaluation Metrics: MSE=8873.1475, MAE=63.8705, RMSE=94.1974, R2=-0.1837, MAPE=80.80%\n",
      "Initial Model Performance: MACs=165,205,825, Params=98,737, MSE=8873.1475\n",
      "\n",
      "Targeting MACs <= 115,644,078, Params <= 59,242\n",
      "\n",
      "===== Processing Strategy: Magnitude_L1 =====\n",
      "Targeting for pruning: ['Linear with out_features=64', 'Linear with out_features=48', 'Linear with out_features=32']\n",
      "--- Starting Pruning (MagnitudeImportance) ---\n",
      "Target MACs: <= 115,644,078, Target Params: <= 59,242\n",
      "Step Ratio: 0.20, Max Iter: 50\n",
      "Targeting specific modules for pruning: ['Linear', 'Linear', 'Linear']\n",
      "Initial State | MACs: 165,205,825, Params: 98,737\n",
      "Iter   1/50 | MACs: 165,203,206 (  +0.0% R) | Params: 96,148 (  +2.6% R)\n",
      "Iter 2: No more candidates found by pruner. Stopping.\n",
      "--- Finished Pruning (MagnitudeImportance) ---\n",
      "Initial | MACs: 165,205,825, Params: 98,737\n",
      "Final   | MACs: 165,203,206 (Reduction: 0.00%)\n",
      "        | Params: 96,148 (Reduction: 2.62%)\n",
      "Target  | MACs <= 115,644,078, Params <= 59,242\n",
      "Warning: Pruning finished, but target threshold(s) were not fully met.\n",
      "\n",
      "--- Fine-tuning (Magnitude_L1) ---\n",
      "Starting training for 40 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/onnx/symbolic_opset9.py:4277: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n",
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
      "  result = _VF.lstm(\n",
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
      "  result = _VF.lstm(\n",
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch_pruning/pruner/algorithms/base_pruner.py:407: UserWarning: Pruning exceed the maximum iterative steps, no pruning will be performed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40: Train Loss=0.006179, Time: 0.39s, Val Loss=0.006120 (Best model saved)\n",
      "Epoch 2/40: Train Loss=0.005872, Time: 0.41s, Val Loss=0.006013 (Best model saved)\n",
      "Epoch 3/40: Train Loss=0.005742, Time: 0.29s, Val Loss=0.005870 (Best model saved)\n",
      "Epoch 4/40: Train Loss=0.005749, Time: 0.29s, Val Loss=0.006326\n",
      "Epoch 5/40: Train Loss=0.005413, Time: 0.31s, Val Loss=0.006356\n",
      "Epoch 6/40: Train Loss=0.005507, Time: 0.31s, Val Loss=0.006552\n",
      "Epoch 7/40: Train Loss=0.005497, Time: 0.28s, Val Loss=0.006169\n",
      "Epoch 8/40: Train Loss=0.005416, Time: 0.31s, Val Loss=0.006187\n",
      "Epoch 9/40: Train Loss=0.005436, Time: 0.33s, Val Loss=0.006556\n",
      "Epoch 10/40: Train Loss=0.005271, Time: 0.30s, Val Loss=0.006880\n",
      "Epoch 11/40: Train Loss=0.005215, Time: 0.32s, Val Loss=0.006489\n",
      "Epoch 12/40: Train Loss=0.005182, Time: 0.33s, Val Loss=0.006880\n",
      "Epoch 13/40: Train Loss=0.005078, Time: 0.37s, Val Loss=0.006619\n",
      "Epoch 14/40: Train Loss=0.005148, Time: 0.47s, Val Loss=0.006754\n",
      "Epoch 15/40: Train Loss=0.005227, Time: 0.52s, Val Loss=0.007400\n",
      "Epoch 16/40: Train Loss=0.005071, Time: 0.30s, Val Loss=0.006758\n",
      "Epoch 17/40: Train Loss=0.004842, Time: 0.27s, Val Loss=0.006835\n",
      "Epoch 18/40: Train Loss=0.005022, Time: 0.28s, Val Loss=0.006806\n",
      "Epoch 19/40: Train Loss=0.005096, Time: 0.27s, Val Loss=0.006541\n",
      "Epoch 20/40: Train Loss=0.004838, Time: 0.27s, Val Loss=0.007477\n",
      "Epoch 21/40: Train Loss=0.004785, Time: 0.27s, Val Loss=0.006776\n",
      "Epoch 22/40: Train Loss=0.004881, Time: 0.26s, Val Loss=0.006292\n",
      "Epoch 23/40: Train Loss=0.004780, Time: 0.27s, Val Loss=0.007310\n",
      "Epoch 24/40: Train Loss=0.004879, Time: 0.28s, Val Loss=0.006956\n",
      "Epoch 25/40: Train Loss=0.004668, Time: 0.27s, Val Loss=0.007694\n",
      "Epoch 26/40: Train Loss=0.004699, Time: 0.28s, Val Loss=0.007487\n",
      "Epoch 27/40: Train Loss=0.004646, Time: 0.27s, Val Loss=0.007255\n",
      "Epoch 28/40: Train Loss=0.004685, Time: 0.27s, Val Loss=0.006705\n",
      "Epoch 29/40: Train Loss=0.004541, Time: 0.27s, Val Loss=0.006939\n",
      "Epoch 30/40: Train Loss=0.004630, Time: 0.28s, Val Loss=0.007423\n",
      "Epoch 31/40: Train Loss=0.004547, Time: 0.28s, Val Loss=0.007351\n",
      "Epoch 32/40: Train Loss=0.004805, Time: 0.27s, Val Loss=0.007046\n",
      "Epoch 33/40: Train Loss=0.004386, Time: 0.26s, Val Loss=0.006690\n",
      "Epoch 34/40: Train Loss=0.004406, Time: 0.27s, Val Loss=0.007577\n",
      "Epoch 35/40: Train Loss=0.004362, Time: 0.27s, Val Loss=0.007698\n",
      "Epoch 36/40: Train Loss=0.004435, Time: 0.27s, Val Loss=0.007318\n",
      "Epoch 37/40: Train Loss=0.004380, Time: 0.27s, Val Loss=0.007867\n",
      "Epoch 38/40: Train Loss=0.004369, Time: 0.27s, Val Loss=0.007414\n",
      "Epoch 39/40: Train Loss=0.004330, Time: 0.28s, Val Loss=0.006661\n",
      "Epoch 40/40: Train Loss=0.004246, Time: 0.28s, Val Loss=0.007620\n",
      "Training finished.\n",
      "Loading best fine-tuned model from ./output/lstm_energy_pruning/lstm_Magnitude_L1_ft_best_val.pth\n",
      "\n",
      "--- Evaluating Fine-tuned Model (Magnitude_L1) ---\n",
      "Evaluating on test set...\n",
      "Metrics calculated on original scale.\n",
      "Evaluation Metrics: MSE=11991.1709, MAE=62.2816, RMSE=109.5042, R2=-0.5997, MAPE=71.39%\n",
      "Final PyTorch model saved to ./output/lstm_energy_pruning/lstm_Magnitude_L1_final.pth\n",
      "Attempting to save model to ONNX: ./output/lstm_energy_pruning/lstm_Magnitude_L1_final.onnx\n",
      "Using example input shape: torch.Size([1, 72, 25])\n",
      "✅ Model successfully saved as ONNX to ./output/lstm_energy_pruning/lstm_Magnitude_L1_final.onnx\n",
      "\n",
      "=== Pruning Strategy Comparison (Metric: MSE) ===\n",
      "Strategy        | MACs (M)   | Params (K) | Size (MB)  | MSE         \n",
      "---------------------------------------------------------------------\n",
      "initial         | 165.21     | 98.7       |       0.39 |    8873.1475\n",
      "Magnitude_L1    | 165.20     | 96.1       |       0.38 |   11991.1709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/onnx/symbolic_opset9.py:4277: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison plots saved to ./output/lstm_energy_pruning\n",
      "\n",
      "===== Processing Strategy: Magnitude_L2 =====\n",
      "Targeting for pruning: ['Linear with out_features=64', 'Linear with out_features=48', 'Linear with out_features=32']\n",
      "--- Starting Pruning (MagnitudeImportance) ---\n",
      "Target MACs: <= 115,644,078, Target Params: <= 59,242\n",
      "Step Ratio: 0.20, Max Iter: 50\n",
      "Targeting specific modules for pruning: ['Linear', 'Linear', 'Linear']\n",
      "Initial State | MACs: 165,205,825, Params: 98,737\n",
      "Iter   1/50 | MACs: 165,203,206 (  +0.0% R) | Params: 96,148 (  +2.6% R)\n",
      "Iter 2: No more candidates found by pruner. Stopping.\n",
      "--- Finished Pruning (MagnitudeImportance) ---\n",
      "Initial | MACs: 165,205,825, Params: 98,737\n",
      "Final   | MACs: 165,203,206 (Reduction: 0.00%)\n",
      "        | Params: 96,148 (Reduction: 2.62%)\n",
      "Target  | MACs <= 115,644,078, Params <= 59,242\n",
      "Warning: Pruning finished, but target threshold(s) were not fully met.\n",
      "\n",
      "--- Fine-tuning (Magnitude_L2) ---\n",
      "Starting training for 40 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
      "  result = _VF.lstm(\n",
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch_pruning/pruner/algorithms/base_pruner.py:407: UserWarning: Pruning exceed the maximum iterative steps, no pruning will be performed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40: Train Loss=0.006159, Time: 0.28s, Val Loss=0.005905 (Best model saved)\n",
      "Epoch 2/40: Train Loss=0.005747, Time: 0.28s, Val Loss=0.005950\n",
      "Epoch 3/40: Train Loss=0.005675, Time: 0.28s, Val Loss=0.006266\n",
      "Epoch 4/40: Train Loss=0.005643, Time: 0.27s, Val Loss=0.006300\n",
      "Epoch 5/40: Train Loss=0.005433, Time: 0.27s, Val Loss=0.006325\n",
      "Epoch 6/40: Train Loss=0.005472, Time: 0.28s, Val Loss=0.006416\n",
      "Epoch 7/40: Train Loss=0.005507, Time: 0.28s, Val Loss=0.006511\n",
      "Epoch 8/40: Train Loss=0.005325, Time: 0.26s, Val Loss=0.006120\n",
      "Epoch 9/40: Train Loss=0.005365, Time: 0.26s, Val Loss=0.006020\n",
      "Epoch 10/40: Train Loss=0.005261, Time: 0.27s, Val Loss=0.006262\n",
      "Epoch 11/40: Train Loss=0.005149, Time: 0.27s, Val Loss=0.006148\n",
      "Epoch 12/40: Train Loss=0.005105, Time: 0.26s, Val Loss=0.006172\n",
      "Epoch 13/40: Train Loss=0.005102, Time: 0.26s, Val Loss=0.006512\n",
      "Epoch 14/40: Train Loss=0.005190, Time: 0.28s, Val Loss=0.006575\n",
      "Epoch 15/40: Train Loss=0.005115, Time: 0.27s, Val Loss=0.006855\n",
      "Epoch 16/40: Train Loss=0.005187, Time: 0.26s, Val Loss=0.006621\n",
      "Epoch 17/40: Train Loss=0.004999, Time: 0.29s, Val Loss=0.006648\n",
      "Epoch 18/40: Train Loss=0.004957, Time: 0.27s, Val Loss=0.006407\n",
      "Epoch 19/40: Train Loss=0.004950, Time: 0.27s, Val Loss=0.006571\n",
      "Epoch 20/40: Train Loss=0.004965, Time: 0.28s, Val Loss=0.006515\n",
      "Epoch 21/40: Train Loss=0.004831, Time: 0.28s, Val Loss=0.006581\n",
      "Epoch 22/40: Train Loss=0.004818, Time: 0.28s, Val Loss=0.006703\n",
      "Epoch 23/40: Train Loss=0.004712, Time: 0.27s, Val Loss=0.006747\n",
      "Epoch 24/40: Train Loss=0.004752, Time: 0.27s, Val Loss=0.006492\n",
      "Epoch 25/40: Train Loss=0.004740, Time: 0.27s, Val Loss=0.006635\n",
      "Epoch 26/40: Train Loss=0.004642, Time: 0.26s, Val Loss=0.006816\n",
      "Epoch 27/40: Train Loss=0.004647, Time: 0.27s, Val Loss=0.006587\n",
      "Epoch 28/40: Train Loss=0.004603, Time: 0.27s, Val Loss=0.007014\n",
      "Epoch 29/40: Train Loss=0.004639, Time: 0.28s, Val Loss=0.006964\n",
      "Epoch 30/40: Train Loss=0.004532, Time: 0.28s, Val Loss=0.007105\n",
      "Epoch 31/40: Train Loss=0.004645, Time: 0.26s, Val Loss=0.007402\n",
      "Epoch 32/40: Train Loss=0.004493, Time: 0.28s, Val Loss=0.007283\n",
      "Epoch 33/40: Train Loss=0.004594, Time: 0.28s, Val Loss=0.006516\n",
      "Epoch 34/40: Train Loss=0.004393, Time: 0.28s, Val Loss=0.007714\n",
      "Epoch 35/40: Train Loss=0.004401, Time: 0.27s, Val Loss=0.007011\n",
      "Epoch 36/40: Train Loss=0.004457, Time: 0.27s, Val Loss=0.006919\n",
      "Epoch 37/40: Train Loss=0.004467, Time: 0.26s, Val Loss=0.007008\n",
      "Epoch 38/40: Train Loss=0.004400, Time: 0.28s, Val Loss=0.007279\n",
      "Epoch 39/40: Train Loss=0.004414, Time: 0.28s, Val Loss=0.007216\n",
      "Epoch 40/40: Train Loss=0.004199, Time: 0.27s, Val Loss=0.007446\n",
      "Training finished.\n",
      "Loading best fine-tuned model from ./output/lstm_energy_pruning/lstm_Magnitude_L2_ft_best_val.pth\n",
      "\n",
      "--- Evaluating Fine-tuned Model (Magnitude_L2) ---\n",
      "Evaluating on test set...\n",
      "Metrics calculated on original scale.\n",
      "Evaluation Metrics: MSE=11006.4932, MAE=61.1071, RMSE=104.9118, R2=-0.4683, MAPE=71.05%\n",
      "Final PyTorch model saved to ./output/lstm_energy_pruning/lstm_Magnitude_L2_final.pth\n",
      "Attempting to save model to ONNX: ./output/lstm_energy_pruning/lstm_Magnitude_L2_final.onnx\n",
      "Using example input shape: torch.Size([1, 72, 25])\n",
      "✅ Model successfully saved as ONNX to ./output/lstm_energy_pruning/lstm_Magnitude_L2_final.onnx\n",
      "\n",
      "=== Pruning Strategy Comparison (Metric: MSE) ===\n",
      "Strategy        | MACs (M)   | Params (K) | Size (MB)  | MSE         \n",
      "---------------------------------------------------------------------\n",
      "initial         | 165.21     | 98.7       |       0.39 |    8873.1475\n",
      "Magnitude_L1    | 165.20     | 96.1       |       0.38 |   11991.1709\n",
      "Magnitude_L2    | 165.20     | 96.1       |       0.38 |   11006.4932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/onnx/symbolic_opset9.py:4277: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison plots saved to ./output/lstm_energy_pruning\n",
      "\n",
      "===== Processing Strategy: Random =====\n",
      "Targeting for pruning: ['Linear with out_features=64', 'Linear with out_features=48', 'Linear with out_features=32']\n",
      "--- Starting Pruning (RandomImportance) ---\n",
      "Target MACs: <= 115,644,078, Target Params: <= 59,242\n",
      "Step Ratio: 0.20, Max Iter: 50\n",
      "Targeting specific modules for pruning: ['Linear', 'Linear', 'Linear']\n",
      "Initial State | MACs: 165,205,825, Params: 98,737\n",
      "Iter   1/50 | MACs: 165,203,206 (  +0.0% R) | Params: 96,148 (  +2.6% R)\n",
      "Iter 2: No more candidates found by pruner. Stopping.\n",
      "--- Finished Pruning (RandomImportance) ---\n",
      "Initial | MACs: 165,205,825, Params: 98,737\n",
      "Final   | MACs: 165,203,206 (Reduction: 0.00%)\n",
      "        | Params: 96,148 (Reduction: 2.62%)\n",
      "Target  | MACs <= 115,644,078, Params <= 59,242\n",
      "Warning: Pruning finished, but target threshold(s) were not fully met.\n",
      "\n",
      "--- Fine-tuning (Random) ---\n",
      "Starting training for 40 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
      "  result = _VF.lstm(\n",
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch_pruning/pruner/algorithms/base_pruner.py:407: UserWarning: Pruning exceed the maximum iterative steps, no pruning will be performed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40: Train Loss=0.006178, Time: 0.28s, Val Loss=0.006071 (Best model saved)\n",
      "Epoch 2/40: Train Loss=0.005849, Time: 0.28s, Val Loss=0.005968 (Best model saved)\n",
      "Epoch 3/40: Train Loss=0.005727, Time: 0.26s, Val Loss=0.006268\n",
      "Epoch 4/40: Train Loss=0.005551, Time: 0.26s, Val Loss=0.006180\n",
      "Epoch 5/40: Train Loss=0.005490, Time: 0.27s, Val Loss=0.006127\n",
      "Epoch 6/40: Train Loss=0.005551, Time: 0.27s, Val Loss=0.005937 (Best model saved)\n",
      "Epoch 7/40: Train Loss=0.005408, Time: 0.26s, Val Loss=0.005976\n",
      "Epoch 8/40: Train Loss=0.005386, Time: 0.26s, Val Loss=0.005996\n",
      "Epoch 9/40: Train Loss=0.005310, Time: 0.26s, Val Loss=0.006131\n",
      "Epoch 10/40: Train Loss=0.005260, Time: 0.26s, Val Loss=0.006315\n",
      "Epoch 11/40: Train Loss=0.005299, Time: 0.26s, Val Loss=0.006310\n",
      "Epoch 12/40: Train Loss=0.005302, Time: 0.26s, Val Loss=0.006527\n",
      "Epoch 13/40: Train Loss=0.005098, Time: 0.26s, Val Loss=0.006301\n",
      "Epoch 14/40: Train Loss=0.005081, Time: 0.26s, Val Loss=0.006518\n",
      "Epoch 15/40: Train Loss=0.005133, Time: 0.26s, Val Loss=0.006401\n",
      "Epoch 16/40: Train Loss=0.005009, Time: 0.26s, Val Loss=0.006623\n",
      "Epoch 17/40: Train Loss=0.005045, Time: 0.29s, Val Loss=0.006800\n",
      "Epoch 18/40: Train Loss=0.005044, Time: 0.27s, Val Loss=0.006832\n",
      "Epoch 19/40: Train Loss=0.004851, Time: 0.27s, Val Loss=0.007503\n",
      "Epoch 20/40: Train Loss=0.004921, Time: 0.27s, Val Loss=0.007295\n",
      "Epoch 21/40: Train Loss=0.004771, Time: 0.26s, Val Loss=0.006915\n",
      "Epoch 22/40: Train Loss=0.004886, Time: 0.27s, Val Loss=0.006606\n",
      "Epoch 23/40: Train Loss=0.004806, Time: 0.26s, Val Loss=0.006957\n",
      "Epoch 24/40: Train Loss=0.004801, Time: 0.26s, Val Loss=0.007002\n",
      "Epoch 25/40: Train Loss=0.004793, Time: 0.27s, Val Loss=0.007323\n",
      "Epoch 26/40: Train Loss=0.004742, Time: 0.27s, Val Loss=0.007228\n",
      "Epoch 27/40: Train Loss=0.004675, Time: 0.26s, Val Loss=0.007263\n",
      "Epoch 28/40: Train Loss=0.004652, Time: 0.26s, Val Loss=0.006815\n",
      "Epoch 29/40: Train Loss=0.004621, Time: 0.26s, Val Loss=0.007204\n",
      "Epoch 30/40: Train Loss=0.004545, Time: 0.27s, Val Loss=0.007974\n",
      "Epoch 31/40: Train Loss=0.004548, Time: 0.26s, Val Loss=0.007138\n",
      "Epoch 32/40: Train Loss=0.004584, Time: 0.26s, Val Loss=0.007636\n",
      "Epoch 33/40: Train Loss=0.004670, Time: 0.26s, Val Loss=0.007453\n",
      "Epoch 34/40: Train Loss=0.004602, Time: 0.27s, Val Loss=0.006839\n",
      "Epoch 35/40: Train Loss=0.004442, Time: 0.26s, Val Loss=0.007000\n",
      "Epoch 36/40: Train Loss=0.004445, Time: 0.27s, Val Loss=0.006825\n",
      "Epoch 37/40: Train Loss=0.004474, Time: 0.26s, Val Loss=0.007524\n",
      "Epoch 38/40: Train Loss=0.004405, Time: 0.26s, Val Loss=0.007371\n",
      "Epoch 39/40: Train Loss=0.004393, Time: 0.26s, Val Loss=0.007211\n",
      "Epoch 40/40: Train Loss=0.004332, Time: 0.26s, Val Loss=0.006950\n",
      "Training finished.\n",
      "Loading best fine-tuned model from ./output/lstm_energy_pruning/lstm_Random_ft_best_val.pth\n",
      "\n",
      "--- Evaluating Fine-tuned Model (Random) ---\n",
      "Evaluating on test set...\n",
      "Metrics calculated on original scale.\n",
      "Evaluation Metrics: MSE=13546.1172, MAE=65.3089, RMSE=116.3878, R2=-0.8071, MAPE=75.56%\n",
      "Final PyTorch model saved to ./output/lstm_energy_pruning/lstm_Random_final.pth\n",
      "Attempting to save model to ONNX: ./output/lstm_energy_pruning/lstm_Random_final.onnx\n",
      "Using example input shape: torch.Size([1, 72, 25])\n",
      "✅ Model successfully saved as ONNX to ./output/lstm_energy_pruning/lstm_Random_final.onnx\n",
      "\n",
      "=== Pruning Strategy Comparison (Metric: MSE) ===\n",
      "Strategy        | MACs (M)   | Params (K) | Size (MB)  | MSE         \n",
      "---------------------------------------------------------------------\n",
      "initial         | 165.21     | 98.7       |       0.39 |    8873.1475\n",
      "Magnitude_L1    | 165.20     | 96.1       |       0.38 |   11991.1709\n",
      "Magnitude_L2    | 165.20     | 96.1       |       0.38 |   11006.4932\n",
      "Random          | 165.20     | 96.1       |       0.38 |   13546.1172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/onnx/symbolic_opset9.py:4277: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison plots saved to ./output/lstm_energy_pruning\n",
      "\n",
      "===== Processing Strategy: Taylor =====\n",
      "Targeting for pruning: ['Linear with out_features=64', 'Linear with out_features=48', 'Linear with out_features=32']\n",
      "--- Starting Pruning (TaylorImportance) ---\n",
      "Target MACs: <= 115,644,078, Target Params: <= 59,242\n",
      "Step Ratio: 0.20, Max Iter: 50\n",
      "Targeting specific modules for pruning: ['Linear', 'Linear', 'Linear']\n",
      "Initial State | MACs: 165,205,825, Params: 98,737\n",
      "\n",
      "Error during pruner.step() (Iter 1): 'NoneType' object has no attribute 'data'. Stopping.\n",
      "--- Finished Pruning (TaylorImportance) ---\n",
      "Initial | MACs: 165,205,825, Params: 98,737\n",
      "Final   | MACs: 165,205,825 (Reduction: 0.00%)\n",
      "        | Params: 98,737 (Reduction: 0.00%)\n",
      "Target  | MACs <= 115,644,078, Params <= 59,242\n",
      "Warning: Pruning finished, but target threshold(s) were not fully met.\n",
      "\n",
      "--- Fine-tuning (Taylor) ---\n",
      "Starting training for 40 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
      "  result = _VF.lstm(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40: Train Loss=0.006044, Time: 0.38s, Val Loss=0.005807 (Best model saved)\n",
      "Epoch 2/40: Train Loss=0.005761, Time: 0.32s, Val Loss=0.006180\n",
      "Epoch 3/40: Train Loss=0.005694, Time: 0.28s, Val Loss=0.006087\n",
      "Epoch 4/40: Train Loss=0.005532, Time: 0.27s, Val Loss=0.006055\n",
      "Epoch 5/40: Train Loss=0.005495, Time: 0.26s, Val Loss=0.006166\n",
      "Epoch 6/40: Train Loss=0.005353, Time: 0.27s, Val Loss=0.006282\n",
      "Epoch 7/40: Train Loss=0.005347, Time: 0.26s, Val Loss=0.006249\n",
      "Epoch 8/40: Train Loss=0.005311, Time: 0.29s, Val Loss=0.006592\n",
      "Epoch 9/40: Train Loss=0.005265, Time: 0.27s, Val Loss=0.006359\n",
      "Epoch 10/40: Train Loss=0.005240, Time: 0.26s, Val Loss=0.006048\n",
      "Epoch 11/40: Train Loss=0.005182, Time: 0.26s, Val Loss=0.006532\n",
      "Epoch 12/40: Train Loss=0.005108, Time: 0.27s, Val Loss=0.006511\n",
      "Epoch 13/40: Train Loss=0.005064, Time: 0.26s, Val Loss=0.006703\n",
      "Epoch 14/40: Train Loss=0.005013, Time: 0.27s, Val Loss=0.006357\n",
      "Epoch 15/40: Train Loss=0.005126, Time: 0.26s, Val Loss=0.006634\n",
      "Epoch 16/40: Train Loss=0.004870, Time: 0.26s, Val Loss=0.006744\n",
      "Epoch 17/40: Train Loss=0.005018, Time: 0.28s, Val Loss=0.006081\n",
      "Epoch 18/40: Train Loss=0.004946, Time: 0.27s, Val Loss=0.006225\n",
      "Epoch 19/40: Train Loss=0.004904, Time: 0.27s, Val Loss=0.006337\n",
      "Epoch 20/40: Train Loss=0.004799, Time: 0.28s, Val Loss=0.006645\n",
      "Epoch 21/40: Train Loss=0.004825, Time: 0.27s, Val Loss=0.006714\n",
      "Epoch 22/40: Train Loss=0.004698, Time: 0.27s, Val Loss=0.006745\n",
      "Epoch 23/40: Train Loss=0.004809, Time: 0.27s, Val Loss=0.006772\n",
      "Epoch 24/40: Train Loss=0.004662, Time: 0.27s, Val Loss=0.006835\n",
      "Epoch 25/40: Train Loss=0.004588, Time: 0.26s, Val Loss=0.006630\n",
      "Epoch 26/40: Train Loss=0.004648, Time: 0.27s, Val Loss=0.007314\n",
      "Epoch 27/40: Train Loss=0.004623, Time: 0.27s, Val Loss=0.006780\n",
      "Epoch 28/40: Train Loss=0.004496, Time: 0.26s, Val Loss=0.007497\n",
      "Epoch 29/40: Train Loss=0.004523, Time: 0.27s, Val Loss=0.006663\n",
      "Epoch 30/40: Train Loss=0.004488, Time: 0.26s, Val Loss=0.006690\n",
      "Epoch 31/40: Train Loss=0.004400, Time: 0.26s, Val Loss=0.007426\n",
      "Epoch 32/40: Train Loss=0.004412, Time: 0.27s, Val Loss=0.006742\n",
      "Epoch 33/40: Train Loss=0.004526, Time: 0.25s, Val Loss=0.007093\n",
      "Epoch 34/40: Train Loss=0.004464, Time: 0.26s, Val Loss=0.007016\n",
      "Epoch 35/40: Train Loss=0.004316, Time: 0.26s, Val Loss=0.006767\n",
      "Epoch 36/40: Train Loss=0.004416, Time: 0.26s, Val Loss=0.007660\n",
      "Epoch 37/40: Train Loss=0.004420, Time: 0.27s, Val Loss=0.007445\n",
      "Epoch 38/40: Train Loss=0.004218, Time: 0.27s, Val Loss=0.006959\n",
      "Epoch 39/40: Train Loss=0.004317, Time: 0.28s, Val Loss=0.007352\n",
      "Epoch 40/40: Train Loss=0.004307, Time: 0.27s, Val Loss=0.006752\n",
      "Training finished.\n",
      "Loading best fine-tuned model from ./output/lstm_energy_pruning/lstm_Taylor_ft_best_val.pth\n",
      "\n",
      "--- Evaluating Fine-tuned Model (Taylor) ---\n",
      "Evaluating on test set...\n",
      "Metrics calculated on original scale.\n",
      "Evaluation Metrics: MSE=9709.2285, MAE=57.5777, RMSE=98.5354, R2=-0.2953, MAPE=66.73%\n",
      "Final PyTorch model saved to ./output/lstm_energy_pruning/lstm_Taylor_final.pth\n",
      "Attempting to save model to ONNX: ./output/lstm_energy_pruning/lstm_Taylor_final.onnx\n",
      "Using example input shape: torch.Size([1, 72, 25])\n",
      "✅ Model successfully saved as ONNX to ./output/lstm_energy_pruning/lstm_Taylor_final.onnx\n",
      "\n",
      "=== Pruning Strategy Comparison (Metric: MSE) ===\n",
      "Strategy        | MACs (M)   | Params (K) | Size (MB)  | MSE         \n",
      "---------------------------------------------------------------------\n",
      "initial         | 165.21     | 98.7       |       0.39 |    8873.1475\n",
      "Magnitude_L1    | 165.20     | 96.1       |       0.38 |   11991.1709\n",
      "Magnitude_L2    | 165.20     | 96.1       |       0.38 |   11006.4932\n",
      "Random          | 165.20     | 96.1       |       0.38 |   13546.1172\n",
      "Taylor          | 165.21     | 98.7       |       0.39 |    9709.2285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/onnx/symbolic_opset9.py:4277: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison plots saved to ./output/lstm_energy_pruning\n",
      "\n",
      "===== Processing Strategy: FPGM =====\n",
      "Targeting for pruning: ['Linear with out_features=64', 'Linear with out_features=48', 'Linear with out_features=32']\n",
      "--- Starting Pruning (FPGMImportance) ---\n",
      "Target MACs: <= 115,644,078, Target Params: <= 59,242\n",
      "Step Ratio: 0.20, Max Iter: 50\n",
      "Targeting specific modules for pruning: ['Linear', 'Linear', 'Linear']\n",
      "Initial State | MACs: 165,205,825, Params: 98,737\n",
      "Iter   1/50 | MACs: 165,203,206 (  +0.0% R) | Params: 96,148 (  +2.6% R)\n",
      "Iter 2: No more candidates found by pruner. Stopping.\n",
      "--- Finished Pruning (FPGMImportance) ---\n",
      "Initial | MACs: 165,205,825, Params: 98,737\n",
      "Final   | MACs: 165,203,206 (Reduction: 0.00%)\n",
      "        | Params: 96,148 (Reduction: 2.62%)\n",
      "Target  | MACs <= 115,644,078, Params <= 59,242\n",
      "Warning: Pruning finished, but target threshold(s) were not fully met.\n",
      "\n",
      "--- Fine-tuning (FPGM) ---\n",
      "Starting training for 40 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
      "  result = _VF.lstm(\n",
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch_pruning/pruner/algorithms/base_pruner.py:407: UserWarning: Pruning exceed the maximum iterative steps, no pruning will be performed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40: Train Loss=0.006101, Time: 0.28s, Val Loss=0.006226 (Best model saved)\n",
      "Epoch 2/40: Train Loss=0.005848, Time: 0.28s, Val Loss=0.006199 (Best model saved)\n",
      "Epoch 3/40: Train Loss=0.005700, Time: 0.27s, Val Loss=0.006232\n",
      "Epoch 4/40: Train Loss=0.005658, Time: 0.26s, Val Loss=0.006111 (Best model saved)\n",
      "Epoch 5/40: Train Loss=0.005548, Time: 0.26s, Val Loss=0.006057 (Best model saved)\n",
      "Epoch 6/40: Train Loss=0.005416, Time: 0.26s, Val Loss=0.006257\n",
      "Epoch 7/40: Train Loss=0.005407, Time: 0.27s, Val Loss=0.006187\n",
      "Epoch 8/40: Train Loss=0.005338, Time: 0.27s, Val Loss=0.006296\n",
      "Epoch 9/40: Train Loss=0.005324, Time: 0.26s, Val Loss=0.006251\n",
      "Epoch 10/40: Train Loss=0.005212, Time: 0.27s, Val Loss=0.006337\n",
      "Epoch 11/40: Train Loss=0.005195, Time: 0.26s, Val Loss=0.006335\n",
      "Epoch 12/40: Train Loss=0.005205, Time: 0.34s, Val Loss=0.006655\n",
      "Epoch 13/40: Train Loss=0.004898, Time: 0.33s, Val Loss=0.006317\n",
      "Epoch 14/40: Train Loss=0.005068, Time: 0.31s, Val Loss=0.006501\n",
      "Epoch 15/40: Train Loss=0.005160, Time: 0.28s, Val Loss=0.006173\n",
      "Epoch 16/40: Train Loss=0.004960, Time: 0.28s, Val Loss=0.006409\n",
      "Epoch 17/40: Train Loss=0.004923, Time: 0.26s, Val Loss=0.006273\n",
      "Epoch 18/40: Train Loss=0.004880, Time: 0.27s, Val Loss=0.006283\n",
      "Epoch 19/40: Train Loss=0.004859, Time: 0.27s, Val Loss=0.006481\n",
      "Epoch 20/40: Train Loss=0.004977, Time: 0.29s, Val Loss=0.006499\n",
      "Epoch 21/40: Train Loss=0.004764, Time: 0.27s, Val Loss=0.006563\n",
      "Epoch 22/40: Train Loss=0.004777, Time: 0.26s, Val Loss=0.006530\n",
      "Epoch 23/40: Train Loss=0.004693, Time: 0.27s, Val Loss=0.006480\n",
      "Epoch 24/40: Train Loss=0.004864, Time: 0.27s, Val Loss=0.006671\n",
      "Epoch 25/40: Train Loss=0.004589, Time: 0.26s, Val Loss=0.006838\n",
      "Epoch 26/40: Train Loss=0.004730, Time: 0.27s, Val Loss=0.006917\n",
      "Epoch 27/40: Train Loss=0.004571, Time: 0.26s, Val Loss=0.006681\n",
      "Epoch 28/40: Train Loss=0.004580, Time: 0.26s, Val Loss=0.006619\n",
      "Epoch 29/40: Train Loss=0.004632, Time: 0.26s, Val Loss=0.007288\n",
      "Epoch 30/40: Train Loss=0.004569, Time: 0.26s, Val Loss=0.007221\n",
      "Epoch 31/40: Train Loss=0.004415, Time: 0.25s, Val Loss=0.007232\n",
      "Epoch 32/40: Train Loss=0.004497, Time: 0.26s, Val Loss=0.006552\n",
      "Epoch 33/40: Train Loss=0.004434, Time: 0.26s, Val Loss=0.006481\n",
      "Epoch 34/40: Train Loss=0.004419, Time: 0.26s, Val Loss=0.007107\n",
      "Epoch 35/40: Train Loss=0.004373, Time: 0.26s, Val Loss=0.006990\n",
      "Epoch 36/40: Train Loss=0.004459, Time: 0.26s, Val Loss=0.006757\n",
      "Epoch 37/40: Train Loss=0.004397, Time: 0.26s, Val Loss=0.006876\n",
      "Epoch 38/40: Train Loss=0.004245, Time: 0.27s, Val Loss=0.006754\n",
      "Epoch 39/40: Train Loss=0.004355, Time: 0.27s, Val Loss=0.006490\n",
      "Epoch 40/40: Train Loss=0.004317, Time: 0.26s, Val Loss=0.007127\n",
      "Training finished.\n",
      "Loading best fine-tuned model from ./output/lstm_energy_pruning/lstm_FPGM_ft_best_val.pth\n",
      "\n",
      "--- Evaluating Fine-tuned Model (FPGM) ---\n",
      "Evaluating on test set...\n",
      "Metrics calculated on original scale.\n",
      "Evaluation Metrics: MSE=13498.6602, MAE=64.6358, RMSE=116.1837, R2=-0.8008, MAPE=74.64%\n",
      "Final PyTorch model saved to ./output/lstm_energy_pruning/lstm_FPGM_final.pth\n",
      "Attempting to save model to ONNX: ./output/lstm_energy_pruning/lstm_FPGM_final.onnx\n",
      "Using example input shape: torch.Size([1, 72, 25])\n",
      "✅ Model successfully saved as ONNX to ./output/lstm_energy_pruning/lstm_FPGM_final.onnx\n",
      "\n",
      "=== Pruning Strategy Comparison (Metric: MSE) ===\n",
      "Strategy        | MACs (M)   | Params (K) | Size (MB)  | MSE         \n",
      "---------------------------------------------------------------------\n",
      "initial         | 165.21     | 98.7       |       0.39 |    8873.1475\n",
      "Magnitude_L1    | 165.20     | 96.1       |       0.38 |   11991.1709\n",
      "Magnitude_L2    | 165.20     | 96.1       |       0.38 |   11006.4932\n",
      "Random          | 165.20     | 96.1       |       0.38 |   13546.1172\n",
      "Taylor          | 165.21     | 98.7       |       0.39 |    9709.2285\n",
      "FPGM            | 165.20     | 96.1       |       0.38 |   13498.6602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/onnx/symbolic_opset9.py:4277: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison plots saved to ./output/lstm_energy_pruning\n",
      "\n",
      "===== Final Comparison =====\n",
      "\n",
      "=== Pruning Strategy Comparison (Metric: MSE) ===\n",
      "Strategy        | MACs (M)   | Params (K) | Size (MB)  | MSE         \n",
      "---------------------------------------------------------------------\n",
      "initial         | 165.21     | 98.7       |       0.39 |    8873.1475\n",
      "Magnitude_L1    | 165.20     | 96.1       |       0.38 |   11991.1709\n",
      "Magnitude_L2    | 165.20     | 96.1       |       0.38 |   11006.4932\n",
      "Random          | 165.20     | 96.1       |       0.38 |   13546.1172\n",
      "Taylor          | 165.21     | 98.7       |       0.39 |    9709.2285\n",
      "FPGM            | 165.20     | 96.1       |       0.38 |   13498.6602\n",
      "Comparison plots saved to ./output/lstm_energy_pruning\n",
      "\n",
      "Workflow completed!\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T17:31:02.528904Z",
     "start_time": "2025-05-04T17:31:02.437066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path is defined in your code\n",
    "dataset_path = './data/energydata_complete.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Print the head of the original dataframe\n",
    "print(\"Original dataframe shape:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "# If you want to see the target column statistics\n",
    "print(\"\\nTarget column summary statistics:\")\n",
    "print(df['Appliances'].describe())\n",
    "\n",
    "# To see the feature columns\n",
    "print(\"\\nFeatures in the dataset:\")\n",
    "print(df.columns.tolist())"
   ],
   "id": "8cbdb18424a04c8a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataframe shape: (19735, 29)\n",
      "                  date  Appliances  lights     T1       RH_1    T2       RH_2  \\\n",
      "0  2016-01-11 17:00:00          60      30  19.89  47.596667  19.2  44.790000   \n",
      "1  2016-01-11 17:10:00          60      30  19.89  46.693333  19.2  44.722500   \n",
      "2  2016-01-11 17:20:00          50      30  19.89  46.300000  19.2  44.626667   \n",
      "3  2016-01-11 17:30:00          50      40  19.89  46.066667  19.2  44.590000   \n",
      "4  2016-01-11 17:40:00          60      40  19.89  46.333333  19.2  44.530000   \n",
      "\n",
      "      T3       RH_3         T4  ...         T9   RH_9     T_out  Press_mm_hg  \\\n",
      "0  19.79  44.730000  19.000000  ...  17.033333  45.53  6.600000        733.5   \n",
      "1  19.79  44.790000  19.000000  ...  17.066667  45.56  6.483333        733.6   \n",
      "2  19.79  44.933333  18.926667  ...  17.000000  45.50  6.366667        733.7   \n",
      "3  19.79  45.000000  18.890000  ...  17.000000  45.40  6.250000        733.8   \n",
      "4  19.79  45.000000  18.890000  ...  17.000000  45.40  6.133333        733.9   \n",
      "\n",
      "   RH_out  Windspeed  Visibility  Tdewpoint        rv1        rv2  \n",
      "0    92.0   7.000000   63.000000        5.3  13.275433  13.275433  \n",
      "1    92.0   6.666667   59.166667        5.2  18.606195  18.606195  \n",
      "2    92.0   6.333333   55.333333        5.1  28.642668  28.642668  \n",
      "3    92.0   6.000000   51.500000        5.0  45.410389  45.410389  \n",
      "4    92.0   5.666667   47.666667        4.9  10.084097  10.084097  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "\n",
      "Target column summary statistics:\n",
      "count    19735.000000\n",
      "mean        97.694958\n",
      "std        102.524891\n",
      "min         10.000000\n",
      "25%         50.000000\n",
      "50%         60.000000\n",
      "75%        100.000000\n",
      "max       1080.000000\n",
      "Name: Appliances, dtype: float64\n",
      "\n",
      "Features in the dataset:\n",
      "['date', 'Appliances', 'lights', 'T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4', 'RH_4', 'T5', 'RH_5', 'T6', 'RH_6', 'T7', 'RH_7', 'T8', 'RH_8', 'T9', 'RH_9', 'T_out', 'Press_mm_hg', 'RH_out', 'Windspeed', 'Visibility', 'Tdewpoint', 'rv1', 'rv2']\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T17:31:20.101686Z",
     "start_time": "2025-05-04T17:31:20.092080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs, targets = next(iter(train_loader))\n",
    "\n",
    "print(f\"\\nBatch shape - Inputs: {inputs.shape}, Targets: {targets.shape}\")\n",
    "print(f\"Input features sample (first sequence):\\n{inputs[0]}\")\n",
    "print(f\"Target values sample:\\n{targets[:5]}\")"
   ],
   "id": "8da5ca570c06ed75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch shape - Inputs: torch.Size([64, 72, 25]), Targets: torch.Size([64, 1])\n",
      "Input features sample (first sequence):\n",
      "tensor([[0.1429, 0.6156, 0.3790,  ..., 0.5833, 0.6000, 0.4898],\n",
      "        [0.2857, 0.6156, 0.3738,  ..., 0.5714, 0.6000, 0.4889],\n",
      "        [0.1429, 0.6156, 0.3715,  ..., 0.5714, 0.6000, 0.4833],\n",
      "        ...,\n",
      "        [0.0000, 0.6156, 0.3727,  ..., 0.7143, 0.6821, 0.7389],\n",
      "        [0.0000, 0.6074, 0.3727,  ..., 0.6786, 0.5923, 0.7306],\n",
      "        [0.0000, 0.6033, 0.3769,  ..., 0.6429, 0.5026, 0.7222]])\n",
      "Target values sample:\n",
      "tensor([[0.0374],\n",
      "        [0.0374],\n",
      "        [0.0374],\n",
      "        [0.0654],\n",
      "        [0.0654]])\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
