{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-14T03:22:15.192022Z",
     "start_time": "2025-05-14T03:22:15.140984Z"
    }
   },
   "source": [
    "# Cell 1: Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_pruning as tp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split # Though not used for CMaps chronological split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import pickle # For saving/loading history\n",
    "\n",
    "# Type Hinting\n",
    "from typing import Tuple, List, Dict, Union, Optional\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu124\n",
      "CUDA Available: True\n",
      "CUDA Device Name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cell 2: IntermediateBlock Definition (Helper for LSTM)",
   "id": "5c5cfb627b999f98"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T03:22:22.366406Z",
     "start_time": "2025-05-14T03:22:22.361010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IntermediateBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout_prob=0.2):\n",
    "        \"\"\"\n",
    "        A block consisting of a Linear layer, ReLU activation, and Dropout.\n",
    "        \"\"\"\n",
    "        super(IntermediateBlock, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ],
   "id": "ea0ba1915e908c4",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cell 3: LSTM Model Definition (Using IntermediateBlocks)",
   "id": "512e419857db7ab2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T03:22:24.267476Z",
     "start_time": "2025-05-14T03:22:24.257424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TimeSeriesLSTM_WithBlocks(nn.Module):\n",
    "    def __init__(self, input_size: int, lstm_hidden_size: int, num_lstm_layers: int,\n",
    "                 block_configs: List[int], # List of output sizes for each intermediate block\n",
    "                 output_size: int = 1,\n",
    "                 lstm_dropout_prob: float = 0.0, # Dropout for LSTM layers if num_layers > 1\n",
    "                 block_dropout_prob: float = 0.2, # Dropout within each IntermediateBlock\n",
    "                 bidirectional_lstm: bool = False\n",
    "                ):\n",
    "        super(TimeSeriesLSTM_WithBlocks, self).__init__()\n",
    "        self.model_type = 'LSTM_WithBlocks'\n",
    "        self.input_size = input_size\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.output_size = output_size\n",
    "        self.bidirectional_lstm = bidirectional_lstm\n",
    "        num_directions = 2 if bidirectional_lstm else 1\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, lstm_hidden_size, num_lstm_layers,\n",
    "                            batch_first=True,\n",
    "                            dropout=lstm_dropout_prob if num_lstm_layers > 1 else 0,\n",
    "                            bidirectional=bidirectional_lstm)\n",
    "\n",
    "        self.intermediate_blocks = nn.ModuleList()\n",
    "        current_in_features = lstm_hidden_size * num_directions # Output of LSTM\n",
    "\n",
    "        for block_out_features in block_configs:\n",
    "            self.intermediate_blocks.append(\n",
    "                IntermediateBlock(current_in_features, block_out_features, block_dropout_prob)\n",
    "            )\n",
    "            current_in_features = block_out_features\n",
    "\n",
    "        self.fc_final = nn.Linear(current_in_features, output_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight_ih' in name: nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name: nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name: param.data.fill_(0)\n",
    "        for block in self.intermediate_blocks:\n",
    "            if hasattr(block, 'fc') and isinstance(block.fc, nn.Linear):\n",
    "                 nn.init.kaiming_uniform_(block.fc.weight, nonlinearity='relu')\n",
    "                 if block.fc.bias is not None: nn.init.constant_(block.fc.bias, 0)\n",
    "        if hasattr(self, 'fc_final') and isinstance(self.fc_final, nn.Linear):\n",
    "             nn.init.kaiming_uniform_(self.fc_final.weight, nonlinearity='relu') # Or xavier for final linear\n",
    "             if self.fc_final.bias is not None: nn.init.constant_(self.fc_final.bias, 0)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = x.size(0)\n",
    "        num_directions = 2 if self.bidirectional_lstm else 1\n",
    "        h0 = torch.zeros(self.num_lstm_layers * num_directions, batch_size, self.lstm_hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_lstm_layers * num_directions, batch_size, self.lstm_hidden_size).to(x.device)\n",
    "\n",
    "        lstm_out, _ = self.lstm(x, (h0,c0))\n",
    "        # lstm_out is (batch, seq_len, hidden_size * num_directions)\n",
    "        # We typically take the output of the last time step\n",
    "        processed_lstm_out = lstm_out[:, -1, :]\n",
    "\n",
    "        # Pass through intermediate blocks\n",
    "        intermediate_out = processed_lstm_out\n",
    "        for block in self.intermediate_blocks:\n",
    "            intermediate_out = block(intermediate_out)\n",
    "\n",
    "        final_out = self.fc_final(intermediate_out)\n",
    "        return final_out\n",
    "\n",
    "    def get_prunable_layers(self) -> List[nn.Linear]:\n",
    "        \"\"\"Returns a list of Linear layers within intermediate blocks and the final FC layer.\"\"\"\n",
    "        prunable = []\n",
    "        if hasattr(self, 'intermediate_blocks'):\n",
    "            for block in self.intermediate_blocks:\n",
    "                if isinstance(block, IntermediateBlock) and hasattr(block, 'fc'):\n",
    "                    prunable.append(block.fc)\n",
    "        if hasattr(self, 'fc_final') and isinstance(self.fc_final, nn.Linear):\n",
    "            prunable.append(self.fc_final)\n",
    "        return prunable"
   ],
   "id": "7e738367135d075f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cell 4: NASA CMaps Data Handling Functions",
   "id": "7f8314e74e208289"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T03:22:40.321373Z",
     "start_time": "2025-05-14T03:22:40.301512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- CMaps Data Constants ---\n",
    "CMAPS_COLUMN_NAMES = ['unit_number', 'time_in_cycles'] + \\\n",
    "                     [f'op_setting_{i}' for i in range(1, 4)] + \\\n",
    "                     [f'sensor_{i}' for i in range(1, 22)] # FD001 has 21 sensors typically loaded\n",
    "\n",
    "# --- CMaps Loading and Basic Preprocessing ---\n",
    "def load_cmapss_dataframe(file_path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=' ', header=None, usecols=range(len(CMAPS_COLUMN_NAMES))) # Limit columns\n",
    "        df.columns = CMAPS_COLUMN_NAMES\n",
    "        df.dropna(axis=1, how='all', inplace=True) # Drop fully empty columns if any due to parsing\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CMaps file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def add_cmapss_rul(df: pd.DataFrame, clip_at: int = 125) -> pd.DataFrame:\n",
    "    if df is None: return None\n",
    "    max_cycles = df.groupby('unit_number')['time_in_cycles'].max().reset_index()\n",
    "    max_cycles.columns = ['unit_number', 'max_cycle']\n",
    "    df = df.merge(max_cycles, on='unit_number', how='left')\n",
    "    df['RUL'] = df['max_cycle'] - df['time_in_cycles']\n",
    "    df.drop(columns=['max_cycle'], inplace=True)\n",
    "    if clip_at is not None:\n",
    "        df['RUL'] = df['RUL'].clip(upper=clip_at)\n",
    "    return df\n",
    "\n",
    "def normalize_cmapss_features(df: pd.DataFrame, feature_cols: List[str], scaler: MinMaxScaler = None) -> Tuple[pd.DataFrame, MinMaxScaler]:\n",
    "    if df is None: return None, None\n",
    "    data_to_scale = df[feature_cols]\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler()\n",
    "        df[feature_cols] = scaler.fit_transform(data_to_scale)\n",
    "    else:\n",
    "        df[feature_cols] = scaler.transform(data_to_scale)\n",
    "    return df, scaler\n",
    "\n",
    "# --- CMaps Sequence Creation Functions ---\n",
    "def create_cmapss_sequences(df_scaled: pd.DataFrame, feature_cols: List[str], target_col: str, sequence_length: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    data_x, data_y = [], []\n",
    "    for unit_id, group in df_scaled.groupby('unit_number'):\n",
    "        features = group[feature_cols].values\n",
    "        rul = group[target_col].values\n",
    "        for i in range(len(features) - sequence_length + 1): # From 0 to len-seq_len\n",
    "            data_x.append(features[i:(i + sequence_length)])\n",
    "            data_y.append(rul[i + sequence_length - 1]) # RUL at the end of the sequence window\n",
    "    return np.array(data_x), np.array(data_y)\n",
    "\n",
    "def create_cmapss_test_sequences(df_scaled: pd.DataFrame, feature_cols: List[str], sequence_length: int, true_rul_df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    data_x, data_y = [], []\n",
    "    for unit_id in df_scaled['unit_number'].unique():\n",
    "        unit_data_features = df_scaled[df_scaled['unit_number'] == unit_id][feature_cols].values\n",
    "\n",
    "        # Take the last 'sequence_length' records for features\n",
    "        if len(unit_data_features) >= sequence_length:\n",
    "            current_seq_x = unit_data_features[-sequence_length:]\n",
    "        else: # Pad if shorter\n",
    "            padding = np.zeros((sequence_length - len(unit_data_features), unit_data_features.shape[1]))\n",
    "            current_seq_x = np.vstack((padding, unit_data_features))\n",
    "        data_x.append(current_seq_x)\n",
    "\n",
    "        # RUL_FD00x.txt contains true RUL for each test unit at its last cycle\n",
    "        # unit_id is 1-indexed, DataFrame iloc is 0-indexed\n",
    "        if unit_id - 1 < len(true_rul_df):\n",
    "            data_y.append(true_rul_df.iloc[unit_id - 1, 0])\n",
    "        else:\n",
    "            data_y.append(np.nan) # Should not happen if RUL file is correct\n",
    "\n",
    "    data_x_array = np.array(data_x)\n",
    "    data_y_array = np.array(data_y)\n",
    "\n",
    "    valid_indices = ~np.isnan(data_y_array) # Remove any test samples if RUL was NaN\n",
    "    return data_x_array[valid_indices], data_y_array[valid_indices]\n",
    "\n",
    "# --- Main CMaps Data Loading Function ---\n",
    "def get_cmapss_data_loaders(\n",
    "    data_dir: str, train_file: str, test_file: str, test_rul_file: str,\n",
    "    sequence_length: int,\n",
    "    batch_size: int = 64,\n",
    "    val_split_ratio: float = 0.2, # % of training units for validation\n",
    "    cols_to_drop_manually: Optional[List[str]] = None # Optional list of sensor/op_settings to drop\n",
    "    ) -> Tuple[Optional[DataLoader], Optional[DataLoader], Optional[DataLoader], int, Dict[str, MinMaxScaler]]:\n",
    "\n",
    "    print(f\"--- Loading and Preparing CMaps Data from: {data_dir} ---\")\n",
    "    try:\n",
    "        train_df = load_cmapss_dataframe(os.path.join(data_dir, train_file))\n",
    "        test_df_raw = load_cmapss_dataframe(os.path.join(data_dir, test_file)) # Keep a raw copy for true test sequence end\n",
    "        test_rul_df = pd.read_csv(os.path.join(data_dir, test_rul_file), header=None, names=['RUL'])\n",
    "\n",
    "        if train_df is None or test_df_raw is None or test_rul_df is None:\n",
    "            raise FileNotFoundError(\"One or more CMaps data files failed to load.\")\n",
    "\n",
    "        train_df = add_cmapss_rul(train_df)\n",
    "\n",
    "        # Identify features (all sensors and op_settings, excluding unit, time, RUL)\n",
    "        all_potential_features = [col for col in train_df.columns if 'sensor' in col or 'op_setting' in col]\n",
    "\n",
    "        # Remove low-variance columns based on TRAINING data\n",
    "        # A common heuristic for CMaps: columns with std almost zero or constant.\n",
    "        # Let's use a threshold, e.g., std < 0.001\n",
    "        stable_cols = [col for col in all_potential_features if train_df[col].std() < 0.001]\n",
    "        print(f\"Identified stable columns (std < 0.001) from training data: {stable_cols}\")\n",
    "\n",
    "        # Also drop manually specified columns if any\n",
    "        if cols_to_drop_manually is None:\n",
    "            cols_to_drop_manually = []\n",
    "\n",
    "        cols_to_drop = list(set(stable_cols + cols_to_drop_manually))\n",
    "        print(f\"Columns to drop: {cols_to_drop}\")\n",
    "\n",
    "        feature_cols_final = [col for col in all_potential_features if col not in cols_to_drop]\n",
    "        print(f\"Using {len(feature_cols_final)} features: {feature_cols_final}\")\n",
    "        num_features = len(feature_cols_final)\n",
    "\n",
    "        # Normalize features\n",
    "        train_df_norm, scaler_features = normalize_cmapss_features(train_df.copy(), feature_cols_final, scaler=None)\n",
    "        test_df_norm, _ = normalize_cmapss_features(test_df_raw.copy(), feature_cols_final, scaler=scaler_features) # Use same scaler\n",
    "\n",
    "        # Split training units for validation\n",
    "        train_units = train_df_norm['unit_number'].unique()\n",
    "        np.random.seed(42) # For reproducible split\n",
    "        np.random.shuffle(train_units)\n",
    "        val_count = int(len(train_units) * val_split_ratio)\n",
    "        val_unit_ids = train_units[:val_count]\n",
    "        train_unit_ids = train_units[val_count:]\n",
    "\n",
    "        df_train_split = train_df_norm[train_df_norm['unit_number'].isin(train_unit_ids)]\n",
    "        df_val_split = train_df_norm[train_df_norm['unit_number'].isin(val_unit_ids)]\n",
    "        print(f\"Train/Val unit split: {len(train_unit_ids)} train units, {len(val_unit_ids)} val units.\")\n",
    "\n",
    "        # Create sequences\n",
    "        print(\"Creating train/val sequences...\")\n",
    "        X_train_seq, y_train_seq = create_cmapss_sequences(df_train_split, feature_cols_final, 'RUL', sequence_length)\n",
    "        X_val_seq, y_val_seq = create_cmapss_sequences(df_val_split, feature_cols_final, 'RUL', sequence_length)\n",
    "\n",
    "        print(\"Creating test sequences...\")\n",
    "        X_test_seq, y_test_seq = create_cmapss_test_sequences(test_df_norm, feature_cols_final, sequence_length, test_rul_df)\n",
    "\n",
    "        print(f\"Generated sequences: Train X:{X_train_seq.shape}, y:{y_train_seq.shape} | Val X:{X_val_seq.shape}, y:{y_val_seq.shape} | Test X:{X_test_seq.shape}, y:{y_test_seq.shape}\")\n",
    "\n",
    "        if X_train_seq.shape[0] == 0 or X_val_seq.shape[0] == 0 or X_test_seq.shape[0] == 0:\n",
    "            print(\"Error: Sequence creation resulted in empty datasets. Check sequence_length and data splits.\")\n",
    "            return None, None, None, 0, {}\n",
    "\n",
    "        # Create Datasets and DataLoaders\n",
    "        train_dataset = CMAPSS_LSTM_Dataset(X_train_seq, y_train_seq) # Use specific LSTM dataset class\n",
    "        val_dataset = CMAPSS_LSTM_Dataset(X_val_seq, y_val_seq)\n",
    "        test_dataset = CMAPSS_LSTM_Dataset(X_test_seq, y_test_seq)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        scalers = {'features': scaler_features} # We are not scaling target RUL here as it's usually directly predicted\n",
    "\n",
    "        print(f\"CMaps data loaders created. Input features per step: {num_features}\")\n",
    "        return train_loader, val_loader, test_loader, num_features, {'features': scaler_features}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in get_cmapss_data_loaders: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None, 0, {}\n",
    "\n",
    "# --- CMaps LSTM Dataset Class ---\n",
    "class CMAPSS_LSTM_Dataset(Dataset):\n",
    "    def __init__(self, sequences_x: np.ndarray, sequences_y: np.ndarray):\n",
    "        self.sequences_x = torch.tensor(sequences_x, dtype=torch.float32)\n",
    "        self.sequences_y = torch.tensor(sequences_y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.sequences_x)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.sequences_x[idx], self.sequences_y[idx]"
   ],
   "id": "894cce9e73d2f4eb",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cell 5: Utility, Training, Evaluation, and Pruning Functions",
   "id": "113a5fe7757c0d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T03:22:48.675928Z",
     "start_time": "2025-05-14T03:22:48.652177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Utility: calculate_flops_params, save_model_state (with ONNX), load_model_state ---\n",
    "def calculate_flops_params(model, example_input):\n",
    "    device = next(model.parameters()).device\n",
    "    example_input_on_device = example_input.to(device)\n",
    "    # LSTM layers might cause issues with torch-pruning's default counter on some versions\n",
    "    # We can try, and if it fails, fall back or just report params\n",
    "    try:\n",
    "        macs, params = tp.utils.count_ops_and_params(model, example_input_on_device)\n",
    "        # For LSTMs, MACs can be very high. tp.utils.count_ops_and_params might primarily count Linear/Conv ops\n",
    "        # This might under-report true LSTM MACs/FLOPs if not specifically handled.\n",
    "        # If macs seems too low, consider it an estimate of non-recurrent ops, or research LSTM-specific FLOP counters.\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: tp.utils.count_ops_and_params failed ({e}). Reporting parameters only for now.\")\n",
    "        params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        macs = 0 # Indicate MACs calculation failed or is unreliable for this model with this tool\n",
    "    return macs, params\n",
    "\n",
    "\n",
    "def save_model_as_onnx_generic(model: nn.Module, example_input: torch.Tensor, output_path: str, device: torch.device):\n",
    "    model.eval().to(device)\n",
    "    example_input_on_device = example_input.to(device)\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            model, example_input_on_device, output_path, export_params=True, opset_version=11,\n",
    "            input_names=['input'], output_names=['output'],\n",
    "            dynamic_axes={'input': {0: 'batch_size', 1: 'sequence_length'}, 'output': {0: 'batch_size'}}\n",
    "        ) # Added dynamic axis for sequence_length\n",
    "        print(f\"✅ Model successfully saved as ONNX to {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Loglevel: Error - Failed to export model to ONNX at {output_path}: {e}\")\n",
    "        import traceback; traceback.print_exc()\n",
    "\n",
    "def save_model_state_generic(model: nn.Module, path_prefix: str, example_input_for_onnx: Optional[torch.Tensor] = None, device_for_onnx: Optional[torch.device] = None):\n",
    "    if not path_prefix.endswith(\".pth\"): pth_path = path_prefix + \".pth\"\n",
    "    else: pth_path = path_prefix; path_prefix = path_prefix[:-4]\n",
    "    os.makedirs(os.path.dirname(pth_path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), pth_path)\n",
    "    print(f\"Model state (.pth) saved to {pth_path}\")\n",
    "    if example_input_for_onnx is not None and device_for_onnx is not None:\n",
    "        onnx_path = path_prefix + \".onnx\"\n",
    "        save_model_as_onnx_generic(model, example_input_for_onnx, onnx_path, device_for_onnx)\n",
    "\n",
    "def load_model_state_generic(model: nn.Module, path: str, device: torch.device):\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    print(f\"Model state loaded from {path}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# --- Training Function for Recurrent/Sequential Models (like train_recurrent_model) ---\n",
    "def train_sequential_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader,\n",
    "                         criterion: nn.Module, optimizer: torch.optim.Optimizer, scheduler: Optional[torch.optim.lr_scheduler._LRScheduler],\n",
    "                         device: torch.device, num_epochs: int, patience: int = 10,\n",
    "                         grad_clip: Optional[float] = None,\n",
    "                         model_name_for_log: str = \"Model\") -> Tuple[nn.Module, List[float], List[float]]:\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "    train_loss_history, val_loss_history = [], []\n",
    "\n",
    "    print(f\"Starting training for {model_name_for_log} on {device} with patience={patience}\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss_train = 0.0\n",
    "        epoch_start_time = time.time()\n",
    "        for features, targets in train_loader:\n",
    "            features, targets = features.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            if grad_clip: torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "            running_loss_train += loss.item() * features.size(0)\n",
    "\n",
    "        epoch_train_loss = running_loss_train / len(train_loader.dataset)\n",
    "        train_loss_history.append(epoch_train_loss)\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "        running_loss_val = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for features, targets in val_loader:\n",
    "                features, targets = features.to(device), targets.to(device)\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, targets)\n",
    "                running_loss_val += loss.item() * features.size(0)\n",
    "        epoch_val_loss = running_loss_val / len(val_loader.dataset)\n",
    "        val_loss_history.append(epoch_val_loss)\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"{model_name_for_log} - Epoch {epoch+1}/{num_epochs}: Train Loss={epoch_train_loss:.6f}, Val Loss={epoch_val_loss:.6f}, LR={current_lr:.1e}, Time={epoch_time:.2f}s\")\n",
    "\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            print(f\"  *** New best validation loss: {best_val_loss:.6f} (Epoch {epoch+1}) ***\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"  Early stopping for {model_name_for_log} triggered after {patience} epochs without val_loss improvement.\")\n",
    "            break\n",
    "\n",
    "        if scheduler:\n",
    "            if isinstance(scheduler, ReduceLROnPlateau): scheduler.step(epoch_val_loss)\n",
    "            else: scheduler.step()\n",
    "\n",
    "    print(f\"Training finished for {model_name_for_log}. Best validation loss: {best_val_loss:.6f}\")\n",
    "    if best_model_state: model.load_state_dict(best_model_state); print(\"  Loaded best model state based on validation loss.\")\n",
    "    else: print(\"  Warning: No improvement in validation loss recorded or training ended early.\")\n",
    "    return model, train_loss_history, val_loss_history\n",
    "\n",
    "# --- Evaluation Function (RMSE, etc.) ---\n",
    "def evaluate_sequential_model(model: nn.Module, data_loader: DataLoader, device: torch.device,\n",
    "                            example_input: torch.Tensor, scalers: Optional[Dict] = None,\n",
    "                            criterion_for_loss: Optional[nn.Module]=None) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    # Recalculate MACs/Params for the current model state\n",
    "    # This is important because evaluate might be called on initial, pruned, or fine-tuned model\n",
    "    macs, params = calculate_flops_params(model, example_input) # Ensure example_input is correct shape\n",
    "    size_mb = params * 4 / 1e6\n",
    "\n",
    "    all_predictions_scaled, all_targets_scaled = [], []\n",
    "    total_loss_scaled = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, targets in data_loader:\n",
    "            features, targets = features.to(device), targets.to(device)\n",
    "            predictions = model(features)\n",
    "            if criterion_for_loss: # Calculate loss on scaled data if criterion provided\n",
    "                loss = criterion_for_loss(predictions, targets)\n",
    "                total_loss_scaled += loss.item() * features.size(0)\n",
    "            all_predictions_scaled.append(predictions.cpu().numpy())\n",
    "            all_targets_scaled.append(targets.cpu().numpy())\n",
    "\n",
    "    all_predictions_scaled_np = np.concatenate(all_predictions_scaled).squeeze()\n",
    "    all_targets_scaled_np = np.concatenate(all_targets_scaled).squeeze()\n",
    "\n",
    "    # Inverse transform IF a target scaler is provided (for interpretable metrics)\n",
    "    # For CMaps RUL, we did not scale the RUL target.\n",
    "    # If you *did* scale the RUL for CMaps (or other target vars for other datasets), use scaler['target']\n",
    "    predictions_eval = all_predictions_scaled_np\n",
    "    targets_eval = all_targets_scaled_np\n",
    "\n",
    "    if scalers and 'target' in scalers and scalers['target'] is not None: # If target was scaled\n",
    "        print(\"Inverse transforming targets and predictions for metrics using 'target' scaler.\")\n",
    "        try:\n",
    "            # Scaler expects 2D array [n_samples, n_features=1]\n",
    "            predictions_eval = scalers['target'].inverse_transform(all_predictions_scaled_np.reshape(-1, 1)).squeeze()\n",
    "            targets_eval = scalers['target'].inverse_transform(all_targets_scaled_np.reshape(-1, 1)).squeeze()\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to inverse transform with 'target' scaler. Metrics on scaled values. Error: {e}\")\n",
    "    elif scalers and 'features' in scalers and not ('target' in scalers and scalers['target'] is not None):\n",
    "        # This condition implies target was not scaled, which is often true for CMaps RUL\n",
    "        print(\"Targets were not scaled or 'target' scaler not provided. Metrics are on the direct output scale (RUL units).\")\n",
    "        pass # predictions_eval and targets_eval are already the direct model outputs/targets\n",
    "\n",
    "\n",
    "    mse = mean_squared_error(targets_eval, predictions_eval)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(targets_eval, predictions_eval)\n",
    "    r2 = r2_score(targets_eval, predictions_eval)\n",
    "\n",
    "    avg_loss_scaled = total_loss_scaled / len(data_loader.dataset) if criterion_for_loss and len(data_loader.dataset) > 0 else np.nan\n",
    "\n",
    "    print(f\"Evaluation: RMSE={rmse:.4f}, MAE={mae:.4f}, R2={r2:.4f}, Scaled MSE (if criterion given)={avg_loss_scaled:.6f}\")\n",
    "\n",
    "    return {\n",
    "        'rmse': rmse, 'mae': mae, 'r2': r2, 'mse': mse, 'scaled_loss': avg_loss_scaled,\n",
    "        'flops': macs, 'params': params, 'size_mb': size_mb\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Pruning Function (Adapted for generic sequential models like LSTM with specified prunable linears) ---\n",
    "def prune_sequential_model(\n",
    "    model_to_prune: nn.Module, example_input: torch.Tensor,\n",
    "    strategy_name_for_debug: str, strategy_config: Dict,\n",
    "    target_sparsity: float, iterative_steps: int,\n",
    "    prunable_linear_layers: List[nn.Linear], # Specific Linear layers to target\n",
    "    importance_metric_instance: Optional[tp.importance.Importance] = None\n",
    "    ) -> nn.Module:\n",
    "\n",
    "    device = example_input.device\n",
    "    model_to_prune.eval().to(device)\n",
    "\n",
    "    print(f\"\\n--- Model Structure BEFORE Pruning ({strategy_name_for_debug}) ---\")\n",
    "    print(model_to_prune)\n",
    "    flops_before, params_before = calculate_flops_params(model_to_prune, example_input)\n",
    "    print(f\"State Before Pruning ({strategy_name_for_debug}): FLOPs={flops_before/1e6:.4f}M, Params={params_before/1e6:.4f}M\")\n",
    "\n",
    "    if not prunable_linear_layers:\n",
    "        print(f\"Warning ({strategy_name_for_debug}): No prunable linear layers provided. Skipping pruning.\")\n",
    "        return model_to_prune\n",
    "\n",
    "    # Determine layers to IGNORE: all other Linear/LSTM layers NOT in prunable_linear_layers\n",
    "    actual_ignored_layers = []\n",
    "    for module in model_to_prune.modules():\n",
    "        if isinstance(module, (nn.Linear, nn.LSTM)):\n",
    "            is_prunable_target = False\n",
    "            for target_layer in prunable_linear_layers:\n",
    "                if module is target_layer: # Check for object identity\n",
    "                    is_prunable_target = True\n",
    "                    break\n",
    "            if not is_prunable_target:\n",
    "                actual_ignored_layers.append(module)\n",
    "\n",
    "    print(f\"Targeting {len(prunable_linear_layers)} specific Linear layers for pruning in {strategy_name_for_debug}.\")\n",
    "    print(f\"Ignoring {len(actual_ignored_layers)} other Linear/LSTM layers.\")\n",
    "\n",
    "\n",
    "    if importance_metric_instance is None:\n",
    "        current_importance_metric = strategy_config['importance']\n",
    "    else:\n",
    "        current_importance_metric = importance_metric_instance # Use pre-configured (e.g. Taylor)\n",
    "    print(f\"Using Importance Metric ({strategy_name_for_debug}): {type(current_importance_metric).__name__}\")\n",
    "\n",
    "    pruner_class = strategy_config['pruner']\n",
    "    print(f\"Using Pruner Class ({strategy_name_for_debug}): {pruner_class.__name__}\")\n",
    "\n",
    "    try:\n",
    "        pruner = pruner_class(\n",
    "            model=model_to_prune,\n",
    "            example_inputs=example_input.to(device), # For graph construction\n",
    "            importance=current_importance_metric,\n",
    "            iterative_steps=iterative_steps, # Pruner handles internal iteration\n",
    "            ch_sparsity=target_sparsity,     # Target overall structured sparsity for the chosen layers\n",
    "            root_module_types=[nn.Linear], # We are targeting nn.Linear components\n",
    "            ignored_layers=actual_ignored_layers,\n",
    "        )\n",
    "    except Exception as E_init:\n",
    "        print(f\"ERROR ({strategy_name_for_debug}): Could not initialize pruner {pruner_class.__name__}: {E_init}\")\n",
    "        raise E_init\n",
    "\n",
    "\n",
    "    print(f\"Starting pruning with {strategy_name_for_debug}, Target Sparsity: {target_sparsity:.2f}\")\n",
    "    if isinstance(current_importance_metric, tp.importance.TaylorImportance):\n",
    "        # ... (Taylor gradient calculation logic as before - ensure example_input has batch dimension) ...\n",
    "        print(f\"Calculating gradients for TaylorImportance ({strategy_name_for_debug})...\")\n",
    "        model_to_prune.train()\n",
    "        # Need a batch for Taylor. If example_input is BS=1, try to get one from a loader.\n",
    "        # This part needs robust handling of example_input for Taylor or passing a gradient_batch.\n",
    "        # For simplicity, assume example_input might work or that Taylor requires pre-calculated grads on importance object.\n",
    "        if example_input.size(0) <=1 : print(\"Warning: TaylorImportance might need BS>1 in example_input for robust grad calc.\")\n",
    "        output_taylor = model_to_prune(example_input.to(device))\n",
    "        loss_taylor = torch.sum(output_taylor**2) # Dummy regression loss\n",
    "        model_to_prune.zero_grad()\n",
    "        try: loss_taylor.backward(); print(\" Taylor Grads OK.\")\n",
    "        except Exception as e_taylor_bw: print(f\"ERROR Taylor BW ({strategy_name_for_debug}): {e_taylor_bw}\"); raise e_taylor_bw\n",
    "        finally: model_to_prune.eval()\n",
    "\n",
    "\n",
    "    # Perform pruning step(s)\n",
    "    try:\n",
    "        # With `iterative_steps` set in pruner init, pruner.step() does all internal steps.\n",
    "        # If specific layers in prunable_linear_layers were not picked up by `root_module_types` & `ignored_layers` logic,\n",
    "        # `pruner.step()` might not create groups for them. The pruner identifies prunable groups.\n",
    "        # We hope that by specifying `root_module_types=[nn.Linear]` and correctly ignoring others,\n",
    "        # the pruner only creates groups related to the desired `prunable_linear_layers`.\n",
    "        pruner.step()\n",
    "    except Exception as e_step:\n",
    "        print(f\"ERROR ({strategy_name_for_debug}): Pruner.step() failed: {e_step}\")\n",
    "        import traceback; traceback.print_exc()\n",
    "        raise e_step\n",
    "\n",
    "    print(f\"\\n--- Model Structure AFTER Pruning ({strategy_name_for_debug}) ---\")\n",
    "    print(model_to_prune)\n",
    "    flops_after, params_after = calculate_flops_params(model_to_prune, example_input)\n",
    "    print(f\"Pruning finished for {strategy_name_for_debug}. Final FLOPs={flops_after/1e6:.4f}M, Params={params_after/1e6:.4f}M\")\n",
    "    if flops_before > 0 and flops_after is not None: print(f\"FLOPs Reduction: {(flops_before-flops_after)/flops_before*100:.2f}%\")\n",
    "    if params_before > 0 and params_after is not None: print(f\"Params Reduction: {(params_before-params_after)/params_before*100:.2f}%\")\n",
    "\n",
    "    print(f\"\\n--- Named Parameters AFTER Pruning ({strategy_name_for_debug}) ---\")\n",
    "    for name, param in model_to_prune.named_parameters():\n",
    "        print(f\"  {name}: shape={param.shape}, num_elements={param.numel()}, requires_grad={param.requires_grad}\")\n",
    "\n",
    "    for param in model_to_prune.parameters(): param.requires_grad = True # Ensure fine-tunable\n",
    "    return model_to_prune"
   ],
   "id": "480857c421de0b02",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cell 6: Comparison Plotting Functions (Can reuse compare_results_and_plot_rmse and plot_finetuning_curves)",
   "id": "7a9a8133dd6b1a3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T03:23:09.063317Z",
     "start_time": "2025-05-14T03:23:09.046409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Comparison Plotting for Regression Results ---\n",
    "def compare_results_and_plot_regression_final(results: Dict[str, Dict[str, float]],\n",
    "                                       primary_metric_key: str, # e.g., 'rmse' or 'mse'\n",
    "                                       output_dir: str,\n",
    "                                       lower_is_better: bool = True):\n",
    "    # ... (Paste the body of your `compare_results_and_plot_rmse` function here)\n",
    "    # Make sure it uses `primary_metric_key` dynamically for labels and sorting.\n",
    "    if not results: print(\"No results to plot.\"); return\n",
    "    valid_results = {k: v for k, v in results.items() if isinstance(v, dict) and all(m in v for m in ['flops', 'params', primary_metric_key])}\n",
    "    if not valid_results: print(f\"No valid results with key '{primary_metric_key}' found for plotting.\"); return\n",
    "\n",
    "    sort_key_func = lambda s: valid_results[s].get(primary_metric_key, float('inf') if lower_is_better else float('-inf'))\n",
    "    sorted_strategies = sorted(valid_results.keys(), key=sort_key_func, reverse=not lower_is_better)\n",
    "\n",
    "    plot_strategies = []\n",
    "    if 'initial' in sorted_strategies: plot_strategies.append('initial'); sorted_strategies.remove('initial')\n",
    "    plot_strategies.extend(sorted_strategies)\n",
    "\n",
    "\n",
    "    print(f\"\\n=== Pruning Strategy Comparison (Metric: {primary_metric_key.upper()}) ===\")\n",
    "    header = f\"{'Strategy':<15} | {'FLOPs (M)':<10} | {'Params (M)':<10} | {'Size (MB)':<10} | {primary_metric_key.upper():<12}\"\n",
    "    print(header); print(\"-\" * len(header))\n",
    "    for strategy in plot_strategies:\n",
    "        metrics = valid_results[strategy]\n",
    "        flops_m = metrics.get('flops', 0) / 1e6\n",
    "        params_m = metrics.get('params', 0) / 1e6\n",
    "        size_mb_val = metrics.get('size_mb', 0)\n",
    "        metric_val = metrics.get(primary_metric_key, float('nan'))\n",
    "        print(f\"{strategy:<15} | {flops_m:<10.2f} | {params_m:<10.2f} | {size_mb_val:>10.2f} | {metric_val:>12.4f}\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    metrics_to_plot = ['flops', 'params', 'size_mb', primary_metric_key]\n",
    "    base_titles = {'flops': 'FLOPs', 'params': 'Parameters', 'size_mb': 'Model Size (MB)', primary_metric_key: primary_metric_key.upper()}\n",
    "\n",
    "    for plot_metric in metrics_to_plot:\n",
    "        if not any(plot_metric in valid_results.get(s, {}) for s in plot_strategies):\n",
    "             print(f\"Skipping plot for '{plot_metric}', data not found in results.\"); continue\n",
    "        values = []\n",
    "        current_plot_labels = []\n",
    "        for strategy in plot_strategies:\n",
    "            if strategy in valid_results and plot_metric in valid_results[strategy]:\n",
    "                 metric_val = valid_results[strategy][plot_metric]\n",
    "                 if plot_metric in ['flops', 'params']: metric_val /= 1e6\n",
    "                 values.append(metric_val)\n",
    "                 current_plot_labels.append(strategy)\n",
    "\n",
    "        if not values: print(f\"No values to plot for {plot_metric}\"); continue\n",
    "\n",
    "        plt.figure(figsize=(max(8, len(current_plot_labels) * 1.5), 7)) # Dynamic width\n",
    "        bars = plt.bar(current_plot_labels, values, color=plt.cm.viridis(np.linspace(0, 1, len(current_plot_labels))))\n",
    "        plt.ylabel(base_titles[plot_metric] + (' (Millions)' if plot_metric in ['flops', 'params'] else ''))\n",
    "        title_suffix = '(Lower is Better)' if (plot_metric == primary_metric_key and lower_is_better) or \\\n",
    "                                             (plot_metric != primary_metric_key) else '(Higher is Better)' # Flops/params always lower is better\n",
    "        plt.title(f'{base_titles[plot_metric]} Comparison {title_suffix}')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "        label_format_str = '.4f' if plot_metric == primary_metric_key else '.2f'\n",
    "        max_val_for_text = max(values) if values else 0\n",
    "        for bar in bars:\n",
    "            yval = bar.get_height()\n",
    "            if not np.isnan(yval):\n",
    "                plt.text(bar.get_x() + bar.get_width()/2., yval + 0.01 * max_val_for_text if max_val_for_text > 0 else yval + 0.01,\n",
    "                         f'{yval:{label_format_str}}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "        if 'initial' in valid_results and plot_metric in valid_results['initial'] and not np.isnan(valid_results['initial'][plot_metric]):\n",
    "            initial_val_plot = valid_results['initial'][plot_metric]\n",
    "            if plot_metric in ['flops', 'params']: initial_val_plot /= 1e6\n",
    "            plt.axhline(y=initial_val_plot, color='r', linestyle='--',\n",
    "                        label=f'Initial ({initial_val_plot:{label_format_str}})')\n",
    "            plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(output_dir, f'cmapss_lstm_{plot_metric}_comparison.png')\n",
    "        try: plt.savefig(save_path); plt.close()\n",
    "        except Exception as e_plot: print(f\"Error saving plot {save_path}: {e_plot}\")\n",
    "    print(f\"Comparison plots saved to {output_dir}\")\n",
    "\n",
    "\n",
    "# --- Plotting Fine-tuning Loss Curves ---\n",
    "def plot_cmapss_finetuning_curves(history_dict: Dict[str, Dict[str, List[float]]],\n",
    "                           initial_history: Optional[Dict[str, List[float]]] = None,\n",
    "                           output_dir: str = \"./\", model_prefix=\"cmapss_lstm\"):\n",
    "    # ... (Paste the body of your `plot_finetuning_curves` function here) ...\n",
    "    # Change any hardcoded \"mlp_\" in save paths to use `model_prefix`\n",
    "    # Example: plot_path = os.path.join(output_dir, f\"{model_prefix}_finetuning_loss_{strategy_name}.png\")\n",
    "    num_strategies_ft = len(history_dict)\n",
    "    for strategy_name, history in history_dict.items():\n",
    "        if not history or 'train_loss' not in history or 'val_loss' not in history: continue\n",
    "        plt.figure(figsize=(10, 6)); epochs_ft = range(1, len(history['train_loss']) + 1)\n",
    "        plt.plot(epochs_ft, history['train_loss'], label=f'{strategy_name} - Train Loss')\n",
    "        plt.plot(epochs_ft, history['val_loss'], label=f'{strategy_name} - Val Loss')\n",
    "        if initial_history and initial_history.get('train_loss') and initial_history.get('val_loss'):\n",
    "             epochs_initial = range(1, len(initial_history['train_loss']) + 1)\n",
    "             plt.plot(epochs_initial, initial_history['train_loss'], linestyle='--', color='gray', alpha=0.8, label='Initial Model - Train Loss')\n",
    "             plt.plot(epochs_initial, initial_history['val_loss'], linestyle=':', color='darkgray', alpha=0.8, label='Initial Model - Val Loss')\n",
    "        plt.xlabel(\"Epochs\"); plt.ylabel(\"Loss (MSE)\"); plt.title(f\"Fine-tuning Loss: {strategy_name} Strategy\")\n",
    "        plt.legend(); plt.grid(True); plt.yscale('log')\n",
    "        plot_path = os.path.join(output_dir, f\"{model_prefix}_finetuning_loss_{strategy_name}.png\")\n",
    "        try: plt.savefig(plot_path); plt.close()\n",
    "        except Exception as e: print(f\"Error saving {plot_path}: {e}\")\n",
    "        else: print(f\"Saved fine-tuning curve for {strategy_name} to {plot_path}\")\n",
    "\n",
    "    if num_strategies_ft > 0:\n",
    "        plt.figure(figsize=(12, 7)); plotted_something_combined = False\n",
    "        for strategy_name, history in history_dict.items():\n",
    "            if history and history.get('val_loss'):\n",
    "                epochs_ft = range(1, len(history['val_loss']) + 1)\n",
    "                plt.plot(epochs_ft, history['val_loss'], label=f'{strategy_name} - Val Loss'); plotted_something_combined = True\n",
    "        if initial_history and initial_history.get('val_loss'):\n",
    "            epochs_initial = range(1, len(initial_history['val_loss']) + 1)\n",
    "            plt.plot(epochs_initial, initial_history['val_loss'], linestyle=':', color='black', linewidth=2, label='Initial Model - Val Loss'); plotted_something_combined = True\n",
    "        if plotted_something_combined:\n",
    "            plt.xlabel(\"Epochs\"); plt.ylabel(\"Validation Loss (MSE)\"); plt.title(f\"Comparison of Validation Loss ({model_prefix.upper()})\")\n",
    "            plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1.0)); plt.grid(True); plt.yscale('log')\n",
    "            combined_plot_path = os.path.join(output_dir, f\"{model_prefix}_val_loss_comparison.png\")\n",
    "            try: plt.savefig(combined_plot_path, bbox_inches='tight'); plt.close()\n",
    "            except Exception as e: print(f\"Error saving {combined_plot_path}: {e}\")\n",
    "            else: print(f\"Saved combined validation loss curves to {combined_plot_path}\")"
   ],
   "id": "1cec3a9272ba52b2",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cell 7: Main Workflow Configuration (for CMaps LSTM)",
   "id": "757b43e4c3865945"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T03:24:01.764101Z",
     "start_time": "2025-05-14T03:24:01.756175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Configuration for CMaps LSTM ---\n",
    "MODEL_ARCH_NAME = \"LSTM_CMaps\" # For naming output files\n",
    "DATA_DIR_CMAPS = './data/CMaps/' # <<< YOUR PATH TO CMAPS FD001 DATA FOLDER HERE\n",
    "OUTPUT_DIR_CMAPS_LSTM = f'./output_pruning/{MODEL_ARCH_NAME}/fd001/' # Specific output for this run\n",
    "\n",
    "# Ensure data directory exists (basic check)\n",
    "if not os.path.isdir(DATA_DIR_CMAPS):\n",
    "    print(f\"ERROR: Data directory for CMaps not found: {DATA_DIR_CMAPS}\")\n",
    "    # raise FileNotFoundError(f\"Data directory for CMaps not found: {DATA_DIR_CMAPS}\") # Or exit\n",
    "\n",
    "os.makedirs(OUTPUT_DIR_CMAPS_LSTM, exist_ok=True)\n",
    "\n",
    "CMAPS_TRAIN_FILE = 'train_FD001.txt'\n",
    "CMAPS_TEST_FILE = 'test_FD001.txt'\n",
    "CMAPS_TEST_RUL_FILE = 'RUL_FD001.txt'\n",
    "# Optional: Columns to drop beyond low-variance ones (from domain knowledge or EDA)\n",
    "# For FD001, sensors s_1, s_5, s_6, s_10, s_16, s_18, s_19 are often considered constant/low-variance.\n",
    "# The data loader also removes columns with std < 0.001 automatically.\n",
    "CMAPS_COLS_TO_DROP_MANUALLY = ['sensor_1', 'sensor_5', 'sensor_6', 'sensor_10', 'sensor_16', 'sensor_18', 'sensor_19',\n",
    "                               'op_setting_3'] # op_setting_3 is often constant for FD001\n",
    "\n",
    "# --- LSTM Model Config for CMaps ---\n",
    "CMAPS_SEQUENCE_LENGTH = 30\n",
    "CMAPS_LSTM_HIDDEN_SIZE = 100\n",
    "CMAPS_LSTM_NUM_LAYERS = 2\n",
    "# Example: Block configs, define the output features of each intermediate dense block\n",
    "CMAPS_LSTM_BLOCK_CONFIGS = [64, 32] # Two intermediate blocks: LSTM_out -> 64 -> 32 -> final_fc\n",
    "CMAPS_LSTM_DROPOUT_LSTM = 0.0 # No dropout between LSTM layers if num_layers=1 or 0.\n",
    "CMAPS_LSTM_DROPOUT_BLOCK = 0.25\n",
    "CMAPS_LSTM_BIDIRECTIONAL = False\n",
    "\n",
    "# --- Training Config for CMaps LSTM ---\n",
    "CMAPS_INITIAL_TRAIN_EPOCHS = 60 # Adjusted\n",
    "CMAPS_FINETUNE_EPOCHS = 70   # Adjusted\n",
    "CMAPS_BATCH_SIZE = 128\n",
    "CMAPS_INITIAL_LR = 0.001\n",
    "CMAPS_FINETUNE_LR = 0.0002\n",
    "CMAPS_PATIENCE = 15 # For early stopping\n",
    "CMAPS_GRAD_CLIP = 1.0\n",
    "CMAPS_VAL_SPLIT_RATIO = 0.2 # 20% of training *units* for validation\n",
    "\n",
    "# --- Pruning Config for CMaps LSTM ---\n",
    "CMAPS_PRUNING_TARGET_SPARSITY = 0.3 # Start moderately, esp. for layers after LSTM\n",
    "CMAPS_PRUNING_ITERATIVE_STEPS = 1\n",
    "CMAPS_DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Using the same pruning_strategies dictionary as defined for MLP\n",
    "# This dictionary should be defined in an earlier cell or here\n",
    "# For example:\n",
    "pruning_strategies_cmapss = {\n",
    "    'Magnitude_L1': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.MagnitudeImportance(p=1)},\n",
    "    'Magnitude_L2': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.MagnitudeImportance(p=2)},\n",
    "    'Random': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.RandomImportance()},\n",
    "    # Taylor can be slow without batching importance calculation; requires careful handling of example_input\n",
    "    'Taylor': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.TaylorImportance()}, # Ensure Taylor uses a batch\n",
    "}"
   ],
   "id": "fb0171b962982b6",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cell 8: Data Loading and Preparation Execution (CMaps LSTM)",
   "id": "62dfba3ada80316c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T03:24:06.685715Z",
     "start_time": "2025-05-14T03:24:06.250828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"--- Starting Data Preparation for {MODEL_ARCH_NAME} (CMaps) ---\")\n",
    "train_loader_cmapss, val_loader_cmapss, test_loader_cmapss, num_features_cmapss, scalers_cmapss = get_cmapss_data_loaders(\n",
    "    data_dir=DATA_DIR_CMAPS,\n",
    "    train_file=CMAPS_TRAIN_FILE,\n",
    "    test_file=CMAPS_TEST_FILE,\n",
    "    test_rul_file=CMAPS_TEST_RUL_FILE,\n",
    "    sequence_length=CMAPS_SEQUENCE_LENGTH,\n",
    "    batch_size=CMAPS_BATCH_SIZE,\n",
    "    val_split_ratio=CMAPS_VAL_SPLIT_RATIO,\n",
    "    cols_to_drop_manually=CMAPS_COLS_TO_DROP_MANUALLY\n",
    ")\n",
    "\n",
    "if train_loader_cmapss is None:\n",
    "    raise RuntimeError(\"Failed to load CMaps data. Workflow cannot continue.\")\n",
    "\n",
    "print(f\"Number of features for CMaps LSTM input: {num_features_cmapss}\")\n",
    "\n",
    "# Create an example input tensor for CMaps LSTM: (batch_size=1, seq_len, num_features)\n",
    "example_input_tensor_cmapss = torch.randn(1, CMAPS_SEQUENCE_LENGTH, num_features_cmapss).to(CMAPS_DEVICE)\n",
    "# For Taylor importance, a larger batch from the actual data might be better\n",
    "# We can fetch one batch and use its 'features' part if needed for Taylor later\n",
    "try:\n",
    "    example_batch_cmapss_features, _ = next(iter(train_loader_cmapss))\n",
    "    example_batch_cmapss_features = example_batch_cmapss_features.to(CMAPS_DEVICE)\n",
    "    print(f\"Example batch for Taylor (if used): {example_batch_cmapss_features.shape}\")\n",
    "except StopIteration:\n",
    "    print(\"Warning: Train loader is empty, cannot get example batch for Taylor.\")\n",
    "    example_batch_cmapss_features = example_input_tensor_cmapss # Fallback"
   ],
   "id": "ec9ae8b6973b4bc9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Preparation for LSTM_CMaps (CMaps) ---\n",
      "--- Loading and Preparing CMaps Data from: ./data/CMaps/ ---\n",
      "Identified stable columns (std < 0.001) from training data: ['op_setting_2', 'op_setting_3', 'sensor_1', 'sensor_5', 'sensor_10', 'sensor_16', 'sensor_18', 'sensor_19']\n",
      "Columns to drop: ['op_setting_2', 'sensor_18', 'sensor_19', 'sensor_6', 'sensor_5', 'sensor_16', 'sensor_10', 'op_setting_3', 'sensor_1']\n",
      "Using 15 features: ['op_setting_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_7', 'sensor_8', 'sensor_9', 'sensor_11', 'sensor_12', 'sensor_13', 'sensor_14', 'sensor_15', 'sensor_17', 'sensor_20', 'sensor_21']\n",
      "Train/Val unit split: 80 train units, 20 val units.\n",
      "Creating train/val sequences...\n",
      "Creating test sequences...\n",
      "Generated sequences: Train X:(14241, 30, 15), y:(14241,) | Val X:(3490, 30, 15), y:(3490,) | Test X:(100, 30, 15), y:(100,)\n",
      "CMaps data loaders created. Input features per step: 15\n",
      "Number of features for CMaps LSTM input: 15\n",
      "Example batch for Taylor (if used): torch.Size([128, 30, 15])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cell 9: Main Pruning Workflow Execution (CMaps LSTM)",
   "id": "bef67b386d8f9b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T03:24:58.750724Z",
     "start_time": "2025-05-14T03:24:19.012487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Main Workflow for CMaps LSTM ---\n",
    "all_results_cmapss_lstm = {}\n",
    "structural_metrics_after_pruning_cmapss_lstm = {}\n",
    "fine_tuning_history_cmapss_lstm = {}\n",
    "initial_train_history_cmapss_lstm = {} # For storing initial model's training curves\n",
    "\n",
    "initial_model_base_path_clstm = os.path.join(OUTPUT_DIR_CMAPS_LSTM, f\"{MODEL_ARCH_NAME.lower()}_initial\")\n",
    "initial_history_path_clstm = os.path.join(OUTPUT_DIR_CMAPS_LSTM, f\"{MODEL_ARCH_NAME.lower()}_initial_train_history.pkl\")\n",
    "current_cmapss_lstm_model = None\n",
    "\n",
    "# --- 1. Initial CMaps LSTM Training ---\n",
    "if not os.path.exists(initial_model_base_path_clstm + \".pth\"):\n",
    "    print(f\"\\n--- Training Initial {MODEL_ARCH_NAME} Model ---\")\n",
    "    current_cmapss_lstm_model = TimeSeriesLSTM_WithBlocks(\n",
    "        input_size=num_features_cmapss, lstm_hidden_size=CMAPS_LSTM_HIDDEN_SIZE,\n",
    "        num_lstm_layers=CMAPS_LSTM_NUM_LAYERS, block_configs=CMAPS_LSTM_BLOCK_CONFIGS,\n",
    "        lstm_dropout_prob=CMAPS_LSTM_DROPOUT_LSTM, block_dropout_prob=CMAPS_LSTM_DROPOUT_BLOCK,\n",
    "        bidirectional_lstm=CMAPS_LSTM_BIDIRECTIONAL\n",
    "    ).to(CMAPS_DEVICE)\n",
    "\n",
    "    criterion_clstm_initial = nn.MSELoss()\n",
    "    optimizer_clstm_initial = torch.optim.Adam(current_cmapss_lstm_model.parameters(), lr=CMAPS_INITIAL_LR)\n",
    "    scheduler_clstm_initial = ReduceLROnPlateau(optimizer_clstm_initial, mode='min', factor=0.5, patience=max(1, int(CMAPS_PATIENCE/3)), verbose=True) # Shorter patience for scheduler\n",
    "\n",
    "    current_cmapss_lstm_model, train_hist_clstm, val_hist_clstm = train_sequential_model(\n",
    "        model=current_cmapss_lstm_model, train_loader=train_loader_cmapss, val_loader=val_loader_cmapss,\n",
    "        criterion=criterion_clstm_initial, optimizer=optimizer_clstm_initial, scheduler=scheduler_clstm_initial,\n",
    "        device=CMAPS_DEVICE, num_epochs=CMAPS_INITIAL_TRAIN_EPOCHS, patience=CMAPS_PATIENCE,\n",
    "        grad_clip=CMAPS_GRAD_CLIP, model_name_for_log=f\"Initial {MODEL_ARCH_NAME}\"\n",
    "    )\n",
    "    initial_train_history_cmapss_lstm['train_loss'] = train_hist_clstm\n",
    "    initial_train_history_cmapss_lstm['val_loss'] = val_hist_clstm\n",
    "    save_model_state_generic(current_cmapss_lstm_model, initial_model_base_path_clstm,\n",
    "                             example_input_for_onnx=example_input_tensor_cmapss, device_for_onnx=CMAPS_DEVICE)\n",
    "    with open(initial_history_path_clstm, 'wb') as f: pickle.dump(initial_train_history_cmapss_lstm, f)\n",
    "    print(f\"Initial {MODEL_ARCH_NAME} training history saved to {initial_history_path_clstm}\")\n",
    "else:\n",
    "    print(f\"\\n--- Loading Initial {MODEL_ARCH_NAME} Model from {initial_model_base_path_clstm}.pth ---\")\n",
    "    current_cmapss_lstm_model = TimeSeriesLSTM_WithBlocks(\n",
    "         input_size=num_features_cmapss, lstm_hidden_size=CMAPS_LSTM_HIDDEN_SIZE,\n",
    "        num_lstm_layers=CMAPS_LSTM_NUM_LAYERS, block_configs=CMAPS_LSTM_BLOCK_CONFIGS,\n",
    "        lstm_dropout_prob=CMAPS_LSTM_DROPOUT_LSTM, block_dropout_prob=CMAPS_LSTM_DROPOUT_BLOCK,\n",
    "        bidirectional_lstm=CMAPS_LSTM_BIDIRECTIONAL\n",
    "    ).to(CMAPS_DEVICE)\n",
    "    current_cmapss_lstm_model = load_model_state_generic(current_cmapss_lstm_model, initial_model_base_path_clstm + \".pth\", CMAPS_DEVICE)\n",
    "    if os.path.exists(initial_history_path_clstm):\n",
    "        with open(initial_history_path_clstm, 'rb') as f: initial_train_history_cmapss_lstm = pickle.load(f)\n",
    "        print(f\"Initial {MODEL_ARCH_NAME} training history loaded from {initial_history_path_clstm}\")\n",
    "    else: initial_train_history_cmapss_lstm = {}\n",
    "\n",
    "\n",
    "# --- 2. Evaluate Initial CMaps LSTM Model ---\n",
    "print(f\"\\n--- Evaluating Initial {MODEL_ARCH_NAME} Model on Test Set ---\")\n",
    "initial_metrics_clstm = evaluate_sequential_model(\n",
    "    current_cmapss_lstm_model, test_loader_cmapss, CMAPS_DEVICE,\n",
    "    example_input_tensor_cmapss, scalers_cmapss, criterion_for_loss=nn.MSELoss()\n",
    ")\n",
    "all_results_cmapss_lstm['initial'] = initial_metrics_clstm\n",
    "structural_metrics_after_pruning_cmapss_lstm['initial'] = {\n",
    "    'flops': initial_metrics_clstm['flops'], 'params': initial_metrics_clstm['params'], 'size_mb': initial_metrics_clstm['size_mb']\n",
    "}\n",
    "\n",
    "# --- 3. Pruning and Fine-tuning Loop for CMaps LSTM ---\n",
    "for strategy_name, strategy_details in pruning_strategies_cmapss.items():\n",
    "    print(f\"\\n\\n{'='*20} {MODEL_ARCH_NAME} STRATEGY: {strategy_name.upper()} {'='*20}\")\n",
    "\n",
    "    model_for_clstm_strategy = TimeSeriesLSTM_WithBlocks(\n",
    "        input_size=num_features_cmapss, lstm_hidden_size=CMAPS_LSTM_HIDDEN_SIZE,\n",
    "        num_lstm_layers=CMAPS_LSTM_NUM_LAYERS, block_configs=CMAPS_LSTM_BLOCK_CONFIGS,\n",
    "        lstm_dropout_prob=CMAPS_LSTM_DROPOUT_LSTM, block_dropout_prob=CMAPS_LSTM_DROPOUT_BLOCK,\n",
    "        bidirectional_lstm=CMAPS_LSTM_BIDIRECTIONAL\n",
    "    ).to(CMAPS_DEVICE)\n",
    "    model_for_clstm_strategy = load_model_state_generic(model_for_clstm_strategy, initial_model_base_path_clstm + \".pth\", CMAPS_DEVICE)\n",
    "\n",
    "    current_importance_clstm = None\n",
    "    if strategy_name == 'Taylor':\n",
    "        # For Taylor with batch size, importance needs to be re-instantiated or handled carefully\n",
    "        # If importance.TaylorImportance takes sample_batch_size:\n",
    "        current_importance_clstm = tp.importance.TaylorImportance() # This assumes global grads; use sample_batch_size if implemented\n",
    "                                                                   # or use the example_batch_cmapss_features as example_inputs to pruner\n",
    "\n",
    "    clstm_to_actually_prune = copy.deepcopy(model_for_clstm_strategy)\n",
    "    clstm_pruned_successfully = False\n",
    "\n",
    "    # Define prunable layers from the model instance\n",
    "    prunable_linears_in_lstm = []\n",
    "    if hasattr(clstm_to_actually_prune, 'get_prunable_layers'):\n",
    "        prunable_linears_in_lstm = clstm_to_actually_prune.get_prunable_layers()\n",
    "\n",
    "    if not prunable_linears_in_lstm:\n",
    "        print(f\"No prunable linear layers found by get_prunable_layers() for {MODEL_ARCH_NAME} strategy {strategy_name}. Skipping pruning.\")\n",
    "        all_results_cmapss_lstm[strategy_name] = all_results_cmapss_lstm['initial'].copy()\n",
    "        structural_metrics_after_pruning_cmapss_lstm[strategy_name] = structural_metrics_after_pruning_cmapss_lstm['initial'].copy()\n",
    "        fine_tuning_history_cmapss_lstm[strategy_name] = {} # Empty history\n",
    "        continue\n",
    "\n",
    "\n",
    "    print(f\"--- Attempting Pruning {MODEL_ARCH_NAME} with Strategy: {strategy_name} ---\")\n",
    "    # Use example_batch_cmapss_features for Taylor if its importance benefits from a batch\n",
    "    example_for_pruner_strat = example_batch_cmapss_features if strategy_name == 'Taylor' and example_batch_cmapss_features is not None else example_input_tensor_cmapss\n",
    "\n",
    "    try:\n",
    "        pruned_clstm_model = prune_sequential_model(\n",
    "            model_to_prune=clstm_to_actually_prune,\n",
    "            example_input=example_for_pruner_strat, # Use batch for Taylor if configured\n",
    "            strategy_name_for_debug=strategy_name,\n",
    "            strategy_config=strategy_details,\n",
    "            target_sparsity=CMAPS_PRUNING_TARGET_SPARSITY,\n",
    "            iterative_steps=CMAPS_PRUNING_ITERATIVE_STEPS,\n",
    "            prunable_linear_layers=prunable_linears_in_lstm, # Pass the specific list\n",
    "            importance_metric_instance=current_importance_clstm\n",
    "        )\n",
    "        # prune_sequential_model modifies in-place, so clstm_to_actually_prune is the pruned model\n",
    "        pruned_clstm_for_finetuning = clstm_to_actually_prune\n",
    "        clstm_pruned_successfully = True\n",
    "    except Exception as e_prune_clstm:\n",
    "        print(f\"!!!!!! CRITICAL PRUNING FAILURE for {MODEL_ARCH_NAME} strategy {strategy_name}: {e_prune_clstm} !!!!!!\")\n",
    "        pruned_clstm_for_finetuning = model_for_clstm_strategy # Revert to original initial model\n",
    "        clstm_pruned_successfully = False\n",
    "\n",
    "    # Store structural metrics\n",
    "    if clstm_pruned_successfully:\n",
    "        flops_clstm_post_prune, params_clstm_post_prune = calculate_flops_params(pruned_clstm_for_finetuning, example_input_tensor_cmapss)\n",
    "        size_mb_clstm_post_prune = params_clstm_post_prune * 4 / 1e6\n",
    "        structural_metrics_after_pruning_cmapss_lstm[strategy_name] = {\n",
    "            'flops': flops_clstm_post_prune, 'params': params_clstm_post_prune, 'size_mb': size_mb_clstm_post_prune\n",
    "        }\n",
    "    else:\n",
    "        structural_metrics_after_pruning_cmapss_lstm[strategy_name] = structural_metrics_after_pruning_cmapss_lstm['initial']\n",
    "\n",
    "    pruned_clstm_base_path = os.path.join(OUTPUT_DIR_CMAPS_LSTM, f\"{MODEL_ARCH_NAME.lower()}_{strategy_name}_pruned\")\n",
    "    save_model_state_generic(pruned_clstm_for_finetuning, pruned_clstm_base_path)\n",
    "\n",
    "    # Fine-tune\n",
    "    print(f\"\\n--- Fine-tuning {MODEL_ARCH_NAME} for Strategy: {strategy_name} ---\")\n",
    "    for param in pruned_clstm_for_finetuning.parameters(): param.requires_grad = True\n",
    "    optimizer_clstm_ft = torch.optim.Adam(pruned_clstm_for_finetuning.parameters(), lr=CMAPS_FINETUNE_LR)\n",
    "    scheduler_clstm_ft = ReduceLROnPlateau(optimizer_clstm_ft, mode='min', factor=0.5, patience=max(1,int(CMAPS_PATIENCE/3)), verbose=True)\n",
    "    criterion_clstm_ft = nn.MSELoss()\n",
    "\n",
    "    fine_tuned_clstm_instance, ft_train_hist_clstm, ft_val_hist_clstm = train_sequential_model(\n",
    "        model=pruned_clstm_for_finetuning, train_loader=train_loader_cmapss, val_loader=val_loader_cmapss,\n",
    "        criterion=criterion_clstm_ft, optimizer=optimizer_clstm_ft, scheduler=scheduler_clstm_ft,\n",
    "        device=CMAPS_DEVICE, num_epochs=CMAPS_FINETUNE_EPOCHS, patience=CMAPS_PATIENCE,\n",
    "        grad_clip=CMAPS_GRAD_CLIP, model_name_for_log=f\"{MODEL_ARCH_NAME} {strategy_name} FineTune\"\n",
    "    )\n",
    "    fine_tuning_history_cmapss_lstm[strategy_name] = {'train_loss': ft_train_hist_clstm, 'val_loss': ft_val_hist_clstm}\n",
    "\n",
    "    # Evaluate fine-tuned model\n",
    "    print(f\"\\n--- Evaluating Fine-tuned {MODEL_ARCH_NAME} ({strategy_name}) on Test Set ---\")\n",
    "    final_metrics_clstm_test = evaluate_sequential_model(\n",
    "        fine_tuned_clstm_instance, test_loader_cmapss, CMAPS_DEVICE,\n",
    "        example_input_tensor_cmapss, scalers_cmapss, criterion_for_loss=nn.MSELoss()\n",
    "    )\n",
    "    combined_final_clstm_metrics = {'rmse': final_metrics_clstm_test['rmse'], **structural_metrics_after_pruning_cmapss_lstm[strategy_name]}\n",
    "    # Add all other metrics from evaluation\n",
    "    for k_metric, v_metric in final_metrics_clstm_test.items():\n",
    "        if k_metric not in combined_final_clstm_metrics: # Avoid overwriting rmse from structural if it exists by mistake\n",
    "            combined_final_clstm_metrics[k_metric] = v_metric\n",
    "\n",
    "    all_results_cmapss_lstm[strategy_name] = combined_final_clstm_metrics\n",
    "\n",
    "    # Save final model\n",
    "    final_clstm_base_path = os.path.join(OUTPUT_DIR_CMAPS_LSTM, f\"{MODEL_ARCH_NAME.lower()}_{strategy_name}_final\")\n",
    "    save_model_state_generic(fine_tuned_clstm_instance, final_clstm_base_path,\n",
    "                             example_input_for_onnx=example_input_tensor_cmapss, device_for_onnx=CMAPS_DEVICE)\n",
    "\n",
    "# --- 4. Compare LSTM Results ---\n",
    "print(f\"\\n\\n--- Final Test RMSE and Structural Metrics ({MODEL_ARCH_NAME} Post-Pruning) Comparison ---\")\n",
    "# Use 'rmse' as the primary metric for CMaps RUL regression, lower is better\n",
    "compare_results_and_plot_regression_final(all_results_cmapss_lstm,\n",
    "                                          primary_metric_key='rmse',\n",
    "                                          output_dir=OUTPUT_DIR_CMAPS_LSTM,\n",
    "                                          lower_is_better=True)\n",
    "\n",
    "# --- 5. Plot LSTM Fine-tuning History ---\n",
    "print(f\"\\n\\n--- Plotting {MODEL_ARCH_NAME} Fine-tuning Loss Curves ---\")\n",
    "if fine_tuning_history_cmapss_lstm:\n",
    "    plot_cmapss_finetuning_curves(fine_tuning_history_cmapss_lstm,\n",
    "                           initial_history=(initial_train_history_cmapss_lstm if initial_train_history_cmapss_lstm else None),\n",
    "                           output_dir=OUTPUT_DIR_CMAPS_LSTM, model_prefix=MODEL_ARCH_NAME.lower())\n",
    "\n",
    "# --- 6. Plot LSTM Structural Metrics ---\n",
    "print(f\"\\n\\n--- Plotting {MODEL_ARCH_NAME} Structural Metrics (Initial vs. Post-Pruning) ---\")\n",
    "# Reuse the generic structural plot function defined for MLP, it should work if data struct is same.\n",
    "if 'plot_structural_metrics_comparison' in locals() and structural_metrics_after_pruning_cmapss_lstm:\n",
    "    plot_structural_metrics_comparison(structural_metrics_after_pruning_cmapss_lstm, OUTPUT_DIR_CMAPS_LSTM)\n",
    "elif 'compare_results_and_plot_regression_final' in locals() and structural_metrics_after_pruning_cmapss_lstm:\n",
    "     print(\"Using compare_results_and_plot_regression_final to display structural metrics too, but specific plot func preferred.\")\n",
    "     # This plot will be similar to the final results plot, focusing on structure\n",
    "else:\n",
    "    print(\"Structural metrics or plotting function for structural metrics not found.\")\n",
    "\n",
    "print(f\"\\n{MODEL_ARCH_NAME} (CMaps LSTM) Pruning Workflow Completed!\")\n",
    "\n",
    "# --- Call the Main Function for CMaps LSTM ---\n",
    "if __name__ == \"__main__\": # Or if running directly in notebook:\n",
    "    # Ensure all previous cells defining functions and classes have been run\n",
    "    # For CMaps specific run:\n",
    "    try:\n",
    "        # (Instantiate and run main workflow for CMaps LSTM similar to how main_lstm was called for energy)\n",
    "        # This structure is more like a script now.\n",
    "        # To run this in a notebook, you might call parts or refactor into a main_cmapss_lstm() function.\n",
    "        # For direct execution in the cell:\n",
    "        # Step 1: Execute Cell 1-6 to define everything\n",
    "        # Step 2: Execute Cell 7 to set CMAPS_... configs\n",
    "        # Step 3: Execute Cell 8 to load CMAPS data\n",
    "        # Step 4: Execute Cell 9 to run the CMAPS LSTM workflow.\n",
    "        pass # Cell 9 above contains the direct workflow\n",
    "    except NameError as ne:\n",
    "        print(f\"NameError encountered: {ne}. Ensure all definition cells are run before the main workflow cell.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred in the main CMAPS LSTM workflow: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ],
   "id": "34574bf7f3cc76c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Initial LSTM_CMaps Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for Initial LSTM_CMaps on cuda with patience=15\n",
      "Initial LSTM_CMaps - Epoch 1/60: Train Loss=3618.368577, Val Loss=1759.029200, LR=1.0e-03, Time=0.60s\n",
      "  *** New best validation loss: 1759.029200 (Epoch 1) ***\n",
      "Initial LSTM_CMaps - Epoch 2/60: Train Loss=2025.617219, Val Loss=1758.698367, LR=1.0e-03, Time=0.29s\n",
      "  *** New best validation loss: 1758.698367 (Epoch 2) ***\n",
      "Initial LSTM_CMaps - Epoch 3/60: Train Loss=1948.831202, Val Loss=1479.312015, LR=1.0e-03, Time=0.23s\n",
      "  *** New best validation loss: 1479.312015 (Epoch 3) ***\n",
      "Initial LSTM_CMaps - Epoch 4/60: Train Loss=868.394750, Val Loss=339.336848, LR=1.0e-03, Time=0.23s\n",
      "  *** New best validation loss: 339.336848 (Epoch 4) ***\n",
      "Initial LSTM_CMaps - Epoch 5/60: Train Loss=621.781703, Val Loss=299.470727, LR=1.0e-03, Time=0.23s\n",
      "  *** New best validation loss: 299.470727 (Epoch 5) ***\n",
      "Initial LSTM_CMaps - Epoch 6/60: Train Loss=682.562602, Val Loss=242.237535, LR=1.0e-03, Time=0.23s\n",
      "  *** New best validation loss: 242.237535 (Epoch 6) ***\n",
      "Initial LSTM_CMaps - Epoch 7/60: Train Loss=559.288761, Val Loss=190.099028, LR=1.0e-03, Time=0.22s\n",
      "  *** New best validation loss: 190.099028 (Epoch 7) ***\n",
      "Initial LSTM_CMaps - Epoch 8/60: Train Loss=531.151230, Val Loss=235.014546, LR=1.0e-03, Time=0.22s\n",
      "Initial LSTM_CMaps - Epoch 9/60: Train Loss=503.766668, Val Loss=418.619924, LR=1.0e-03, Time=0.26s\n",
      "Initial LSTM_CMaps - Epoch 10/60: Train Loss=490.285245, Val Loss=220.780252, LR=1.0e-03, Time=0.32s\n",
      "Initial LSTM_CMaps - Epoch 11/60: Train Loss=494.645093, Val Loss=245.428703, LR=1.0e-03, Time=0.30s\n",
      "Initial LSTM_CMaps - Epoch 12/60: Train Loss=505.926555, Val Loss=228.103312, LR=1.0e-03, Time=0.36s\n",
      "Initial LSTM_CMaps - Epoch 13/60: Train Loss=477.752195, Val Loss=171.723502, LR=1.0e-03, Time=0.33s\n",
      "  *** New best validation loss: 171.723502 (Epoch 13) ***\n",
      "Initial LSTM_CMaps - Epoch 14/60: Train Loss=464.708687, Val Loss=178.568854, LR=1.0e-03, Time=0.39s\n",
      "Initial LSTM_CMaps - Epoch 15/60: Train Loss=472.187316, Val Loss=180.910761, LR=1.0e-03, Time=0.41s\n",
      "Initial LSTM_CMaps - Epoch 16/60: Train Loss=468.406834, Val Loss=162.212616, LR=1.0e-03, Time=0.32s\n",
      "  *** New best validation loss: 162.212616 (Epoch 16) ***\n",
      "Initial LSTM_CMaps - Epoch 17/60: Train Loss=453.143380, Val Loss=262.096576, LR=1.0e-03, Time=0.23s\n",
      "Initial LSTM_CMaps - Epoch 18/60: Train Loss=446.810898, Val Loss=167.900091, LR=1.0e-03, Time=0.23s\n",
      "Initial LSTM_CMaps - Epoch 19/60: Train Loss=464.395628, Val Loss=175.885852, LR=1.0e-03, Time=0.23s\n",
      "Initial LSTM_CMaps - Epoch 20/60: Train Loss=433.346968, Val Loss=160.503584, LR=1.0e-03, Time=0.22s\n",
      "  *** New best validation loss: 160.503584 (Epoch 20) ***\n",
      "Initial LSTM_CMaps - Epoch 21/60: Train Loss=438.987803, Val Loss=154.290077, LR=1.0e-03, Time=0.23s\n",
      "  *** New best validation loss: 154.290077 (Epoch 21) ***\n",
      "Initial LSTM_CMaps - Epoch 22/60: Train Loss=433.205009, Val Loss=154.651559, LR=1.0e-03, Time=0.22s\n",
      "Initial LSTM_CMaps - Epoch 23/60: Train Loss=445.705357, Val Loss=173.565157, LR=1.0e-03, Time=0.22s\n",
      "Initial LSTM_CMaps - Epoch 24/60: Train Loss=423.601410, Val Loss=173.513539, LR=1.0e-03, Time=0.21s\n",
      "Initial LSTM_CMaps - Epoch 25/60: Train Loss=435.673389, Val Loss=295.302647, LR=1.0e-03, Time=0.21s\n",
      "Initial LSTM_CMaps - Epoch 26/60: Train Loss=426.516911, Val Loss=144.362672, LR=1.0e-03, Time=0.21s\n",
      "  *** New best validation loss: 144.362672 (Epoch 26) ***\n",
      "Initial LSTM_CMaps - Epoch 27/60: Train Loss=434.248952, Val Loss=172.077240, LR=1.0e-03, Time=0.22s\n",
      "Initial LSTM_CMaps - Epoch 28/60: Train Loss=424.401951, Val Loss=214.027508, LR=1.0e-03, Time=0.23s\n",
      "Initial LSTM_CMaps - Epoch 29/60: Train Loss=417.476262, Val Loss=222.239762, LR=1.0e-03, Time=0.22s\n",
      "Initial LSTM_CMaps - Epoch 30/60: Train Loss=414.609807, Val Loss=154.094972, LR=1.0e-03, Time=0.22s\n",
      "Initial LSTM_CMaps - Epoch 31/60: Train Loss=408.657676, Val Loss=152.768577, LR=1.0e-03, Time=0.23s\n",
      "Initial LSTM_CMaps - Epoch 32/60: Train Loss=409.767918, Val Loss=169.135491, LR=1.0e-03, Time=0.21s\n",
      "Initial LSTM_CMaps - Epoch 33/60: Train Loss=403.366082, Val Loss=172.463152, LR=5.0e-04, Time=0.21s\n",
      "Initial LSTM_CMaps - Epoch 34/60: Train Loss=398.731450, Val Loss=142.800013, LR=5.0e-04, Time=0.23s\n",
      "  *** New best validation loss: 142.800013 (Epoch 34) ***\n",
      "Initial LSTM_CMaps - Epoch 35/60: Train Loss=402.178446, Val Loss=156.852481, LR=5.0e-04, Time=0.23s\n",
      "Initial LSTM_CMaps - Epoch 36/60: Train Loss=397.630902, Val Loss=132.130769, LR=5.0e-04, Time=0.23s\n",
      "  *** New best validation loss: 132.130769 (Epoch 36) ***\n",
      "Initial LSTM_CMaps - Epoch 37/60: Train Loss=406.817065, Val Loss=135.301350, LR=5.0e-04, Time=0.22s\n",
      "Initial LSTM_CMaps - Epoch 38/60: Train Loss=391.757069, Val Loss=157.847388, LR=5.0e-04, Time=0.21s\n",
      "Initial LSTM_CMaps - Epoch 39/60: Train Loss=387.603636, Val Loss=152.923970, LR=5.0e-04, Time=0.21s\n",
      "Initial LSTM_CMaps - Epoch 40/60: Train Loss=392.975176, Val Loss=154.794661, LR=5.0e-04, Time=0.22s\n",
      "Initial LSTM_CMaps - Epoch 41/60: Train Loss=389.232474, Val Loss=146.805776, LR=5.0e-04, Time=0.21s\n",
      "Initial LSTM_CMaps - Epoch 42/60: Train Loss=385.631668, Val Loss=149.965377, LR=5.0e-04, Time=0.22s\n",
      "Initial LSTM_CMaps - Epoch 43/60: Train Loss=381.361062, Val Loss=144.206568, LR=2.5e-04, Time=0.23s\n",
      "Initial LSTM_CMaps - Epoch 44/60: Train Loss=378.650828, Val Loss=147.354120, LR=2.5e-04, Time=0.23s\n",
      "Initial LSTM_CMaps - Epoch 45/60: Train Loss=380.813772, Val Loss=153.041001, LR=2.5e-04, Time=0.23s\n",
      "Initial LSTM_CMaps - Epoch 46/60: Train Loss=382.174980, Val Loss=141.809617, LR=2.5e-04, Time=0.22s\n",
      "Initial LSTM_CMaps - Epoch 47/60: Train Loss=384.477684, Val Loss=138.021998, LR=2.5e-04, Time=0.21s\n",
      "Initial LSTM_CMaps - Epoch 48/60: Train Loss=376.659264, Val Loss=149.439131, LR=2.5e-04, Time=0.23s\n",
      "Initial LSTM_CMaps - Epoch 49/60: Train Loss=368.463296, Val Loss=150.371904, LR=1.3e-04, Time=0.21s\n",
      "Initial LSTM_CMaps - Epoch 50/60: Train Loss=375.296639, Val Loss=148.402179, LR=1.3e-04, Time=0.21s\n",
      "Initial LSTM_CMaps - Epoch 51/60: Train Loss=376.158786, Val Loss=142.648649, LR=1.3e-04, Time=0.21s\n",
      "  Early stopping for Initial LSTM_CMaps triggered after 15 epochs without val_loss improvement.\n",
      "Training finished for Initial LSTM_CMaps. Best validation loss: 132.130769\n",
      "  Loaded best model state based on validation loss.\n",
      "Model state (.pth) saved to ./output_pruning/LSTM_CMaps/fd001/lstm_cmaps_initial.pth\n",
      "✅ Model successfully saved as ONNX to ./output_pruning/LSTM_CMaps/fd001/lstm_cmaps_initial.onnx\n",
      "Initial LSTM_CMaps training history saved to ./output_pruning/LSTM_CMaps/fd001/lstm_cmaps_initial_train_history.pkl\n",
      "\n",
      "--- Evaluating Initial LSTM_CMaps Model on Test Set ---\n",
      "Targets were not scaled or 'target' scaler not provided. Metrics are on the direct output scale (RUL units).\n",
      "Evaluation: RMSE=14.6524, MAE=10.7351, R2=0.8757, Scaled MSE (if criterion given)=214.692383\n",
      "\n",
      "\n",
      "==================== LSTM_CMaps STRATEGY: MAGNITUDE_L1 ====================\n",
      "Model state loaded from ./output_pruning/LSTM_CMaps/fd001/lstm_cmaps_initial.pth\n",
      "--- Attempting Pruning LSTM_CMaps with Strategy: Magnitude_L1 ---\n",
      "\n",
      "--- Model Structure BEFORE Pruning (Magnitude_L1) ---\n",
      "TimeSeriesLSTM_WithBlocks(\n",
      "  (lstm): LSTM(15, 100, num_layers=2, batch_first=True)\n",
      "  (intermediate_blocks): ModuleList(\n",
      "    (0): IntermediateBlock(\n",
      "      (fc): Linear(in_features=100, out_features=64, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (1): IntermediateBlock(\n",
      "      (fc): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc_final): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "State Before Pruning (Magnitude_L1): FLOPs=58.3287M, Params=0.1362M\n",
      "Targeting 3 specific Linear layers for pruning in Magnitude_L1.\n",
      "Ignoring 1 other Linear/LSTM layers.\n",
      "Using Importance Metric (Magnitude_L1): MagnitudeImportance\n",
      "Using Pruner Class (Magnitude_L1): BasePruner\n",
      "Starting pruning with Magnitude_L1, Target Sparsity: 0.30\n",
      "\n",
      "--- Model Structure AFTER Pruning (Magnitude_L1) ---\n",
      "TimeSeriesLSTM_WithBlocks(\n",
      "  (lstm): LSTM(15, 100, num_layers=2, batch_first=True)\n",
      "  (intermediate_blocks): ModuleList(\n",
      "    (0): IntermediateBlock(\n",
      "      (fc): Linear(in_features=100, out_features=44, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (1): IntermediateBlock(\n",
      "      (fc): Linear(in_features=44, out_features=22, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc_final): Linear(in_features=22, out_features=1, bias=True)\n",
      ")\n",
      "Pruning finished for Magnitude_L1. Final FLOPs=58.3255M, Params=0.1331M\n",
      "FLOPs Reduction: 0.01%\n",
      "Params Reduction: 2.29%\n",
      "\n",
      "--- Named Parameters AFTER Pruning (Magnitude_L1) ---\n",
      "  lstm.weight_ih_l0: shape=torch.Size([400, 15]), num_elements=6000, requires_grad=True\n",
      "  lstm.weight_hh_l0: shape=torch.Size([400, 100]), num_elements=40000, requires_grad=True\n",
      "  lstm.bias_ih_l0: shape=torch.Size([400]), num_elements=400, requires_grad=True\n",
      "  lstm.bias_hh_l0: shape=torch.Size([400]), num_elements=400, requires_grad=True\n",
      "  lstm.weight_ih_l1: shape=torch.Size([400, 100]), num_elements=40000, requires_grad=True\n",
      "  lstm.weight_hh_l1: shape=torch.Size([400, 100]), num_elements=40000, requires_grad=True\n",
      "  lstm.bias_ih_l1: shape=torch.Size([400]), num_elements=400, requires_grad=True\n",
      "  lstm.bias_hh_l1: shape=torch.Size([400]), num_elements=400, requires_grad=True\n",
      "  intermediate_blocks.0.fc.weight: shape=torch.Size([44, 100]), num_elements=4400, requires_grad=True\n",
      "  intermediate_blocks.0.fc.bias: shape=torch.Size([44]), num_elements=44, requires_grad=True\n",
      "  intermediate_blocks.1.fc.weight: shape=torch.Size([22, 44]), num_elements=968, requires_grad=True\n",
      "  intermediate_blocks.1.fc.bias: shape=torch.Size([22]), num_elements=22, requires_grad=True\n",
      "  fc_final.weight: shape=torch.Size([1, 22]), num_elements=22, requires_grad=True\n",
      "  fc_final.bias: shape=torch.Size([1]), num_elements=1, requires_grad=True\n",
      "Model state (.pth) saved to ./output_pruning/LSTM_CMaps/fd001/lstm_cmaps_Magnitude_L1_pruned.pth\n",
      "\n",
      "--- Fine-tuning LSTM_CMaps for Strategy: Magnitude_L1 ---\n",
      "Starting training for LSTM_CMaps Magnitude_L1 FineTune on cuda with patience=15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/onnx/symbolic_opset9.py:4277: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n",
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
      "  result = _VF.lstm(\n",
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
      "  result = _VF.lstm(\n",
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch_pruning/pruner/algorithms/base_pruner.py:87: UserWarning: ch_sparsity is deprecated in v1.3.0. Please use pruning_ratio.\n",
      "  warnings.warn(\n",
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_CMaps Magnitude_L1 FineTune - Epoch 1/70: Train Loss=424.852654, Val Loss=140.385697, LR=2.0e-04, Time=0.23s\n",
      "  *** New best validation loss: 140.385697 (Epoch 1) ***\n",
      "LSTM_CMaps Magnitude_L1 FineTune - Epoch 2/70: Train Loss=417.412425, Val Loss=149.863599, LR=2.0e-04, Time=0.23s\n",
      "LSTM_CMaps Magnitude_L1 FineTune - Epoch 3/70: Train Loss=418.187986, Val Loss=146.010895, LR=2.0e-04, Time=0.24s\n",
      "LSTM_CMaps Magnitude_L1 FineTune - Epoch 4/70: Train Loss=421.369201, Val Loss=147.503588, LR=2.0e-04, Time=0.22s\n",
      "LSTM_CMaps Magnitude_L1 FineTune - Epoch 5/70: Train Loss=413.279252, Val Loss=144.571693, LR=2.0e-04, Time=0.23s\n",
      "LSTM_CMaps Magnitude_L1 FineTune - Epoch 6/70: Train Loss=423.985341, Val Loss=159.225122, LR=2.0e-04, Time=0.21s\n",
      "LSTM_CMaps Magnitude_L1 FineTune - Epoch 7/70: Train Loss=422.808979, Val Loss=148.898800, LR=2.0e-04, Time=0.21s\n",
      "LSTM_CMaps Magnitude_L1 FineTune - Epoch 8/70: Train Loss=415.623273, Val Loss=149.853627, LR=1.0e-04, Time=0.22s\n",
      "LSTM_CMaps Magnitude_L1 FineTune - Epoch 9/70: Train Loss=406.295909, Val Loss=147.274805, LR=1.0e-04, Time=0.23s\n",
      "LSTM_CMaps Magnitude_L1 FineTune - Epoch 10/70: Train Loss=415.172528, Val Loss=150.821275, LR=1.0e-04, Time=0.21s\n",
      "LSTM_CMaps Magnitude_L1 FineTune - Epoch 11/70: Train Loss=412.076764, Val Loss=148.252493, LR=1.0e-04, Time=0.22s\n",
      "LSTM_CMaps Magnitude_L1 FineTune - Epoch 12/70: Train Loss=407.397177, Val Loss=169.098115, LR=1.0e-04, Time=0.23s\n",
      "LSTM_CMaps Magnitude_L1 FineTune - Epoch 13/70: Train Loss=408.567951, Val Loss=147.994791, LR=1.0e-04, Time=0.23s\n",
      "LSTM_CMaps Magnitude_L1 FineTune - Epoch 14/70: Train Loss=405.170030, Val Loss=155.686704, LR=5.0e-05, Time=0.22s\n",
      "LSTM_CMaps Magnitude_L1 FineTune - Epoch 15/70: Train Loss=414.166060, Val Loss=142.358093, LR=5.0e-05, Time=0.23s\n",
      "LSTM_CMaps Magnitude_L1 FineTune - Epoch 16/70: Train Loss=403.216890, Val Loss=143.757057, LR=5.0e-05, Time=0.22s\n",
      "  Early stopping for LSTM_CMaps Magnitude_L1 FineTune triggered after 15 epochs without val_loss improvement.\n",
      "Training finished for LSTM_CMaps Magnitude_L1 FineTune. Best validation loss: 140.385697\n",
      "  Loaded best model state based on validation loss.\n",
      "\n",
      "--- Evaluating Fine-tuned LSTM_CMaps (Magnitude_L1) on Test Set ---\n",
      "Targets were not scaled or 'target' scaler not provided. Metrics are on the direct output scale (RUL units).\n",
      "Evaluation: RMSE=14.2301, MAE=10.2620, R2=0.8827, Scaled MSE (if criterion given)=202.496201\n",
      "Model state (.pth) saved to ./output_pruning/LSTM_CMaps/fd001/lstm_cmaps_Magnitude_L1_final.pth\n",
      "✅ Model successfully saved as ONNX to ./output_pruning/LSTM_CMaps/fd001/lstm_cmaps_Magnitude_L1_final.onnx\n",
      "\n",
      "\n",
      "==================== LSTM_CMaps STRATEGY: MAGNITUDE_L2 ====================\n",
      "Model state loaded from ./output_pruning/LSTM_CMaps/fd001/lstm_cmaps_initial.pth\n",
      "--- Attempting Pruning LSTM_CMaps with Strategy: Magnitude_L2 ---\n",
      "\n",
      "--- Model Structure BEFORE Pruning (Magnitude_L2) ---\n",
      "TimeSeriesLSTM_WithBlocks(\n",
      "  (lstm): LSTM(15, 100, num_layers=2, batch_first=True)\n",
      "  (intermediate_blocks): ModuleList(\n",
      "    (0): IntermediateBlock(\n",
      "      (fc): Linear(in_features=100, out_features=64, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (1): IntermediateBlock(\n",
      "      (fc): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc_final): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "State Before Pruning (Magnitude_L2): FLOPs=58.3287M, Params=0.1362M\n",
      "Targeting 3 specific Linear layers for pruning in Magnitude_L2.\n",
      "Ignoring 1 other Linear/LSTM layers.\n",
      "Using Importance Metric (Magnitude_L2): MagnitudeImportance\n",
      "Using Pruner Class (Magnitude_L2): BasePruner\n",
      "Starting pruning with Magnitude_L2, Target Sparsity: 0.30\n",
      "\n",
      "--- Model Structure AFTER Pruning (Magnitude_L2) ---\n",
      "TimeSeriesLSTM_WithBlocks(\n",
      "  (lstm): LSTM(15, 100, num_layers=2, batch_first=True)\n",
      "  (intermediate_blocks): ModuleList(\n",
      "    (0): IntermediateBlock(\n",
      "      (fc): Linear(in_features=100, out_features=44, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (1): IntermediateBlock(\n",
      "      (fc): Linear(in_features=44, out_features=22, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc_final): Linear(in_features=22, out_features=1, bias=True)\n",
      ")\n",
      "Pruning finished for Magnitude_L2. Final FLOPs=58.3255M, Params=0.1331M\n",
      "FLOPs Reduction: 0.01%\n",
      "Params Reduction: 2.29%\n",
      "\n",
      "--- Named Parameters AFTER Pruning (Magnitude_L2) ---\n",
      "  lstm.weight_ih_l0: shape=torch.Size([400, 15]), num_elements=6000, requires_grad=True\n",
      "  lstm.weight_hh_l0: shape=torch.Size([400, 100]), num_elements=40000, requires_grad=True\n",
      "  lstm.bias_ih_l0: shape=torch.Size([400]), num_elements=400, requires_grad=True\n",
      "  lstm.bias_hh_l0: shape=torch.Size([400]), num_elements=400, requires_grad=True\n",
      "  lstm.weight_ih_l1: shape=torch.Size([400, 100]), num_elements=40000, requires_grad=True\n",
      "  lstm.weight_hh_l1: shape=torch.Size([400, 100]), num_elements=40000, requires_grad=True\n",
      "  lstm.bias_ih_l1: shape=torch.Size([400]), num_elements=400, requires_grad=True\n",
      "  lstm.bias_hh_l1: shape=torch.Size([400]), num_elements=400, requires_grad=True\n",
      "  intermediate_blocks.0.fc.weight: shape=torch.Size([44, 100]), num_elements=4400, requires_grad=True\n",
      "  intermediate_blocks.0.fc.bias: shape=torch.Size([44]), num_elements=44, requires_grad=True\n",
      "  intermediate_blocks.1.fc.weight: shape=torch.Size([22, 44]), num_elements=968, requires_grad=True\n",
      "  intermediate_blocks.1.fc.bias: shape=torch.Size([22]), num_elements=22, requires_grad=True\n",
      "  fc_final.weight: shape=torch.Size([1, 22]), num_elements=22, requires_grad=True\n",
      "  fc_final.bias: shape=torch.Size([1]), num_elements=1, requires_grad=True\n",
      "Model state (.pth) saved to ./output_pruning/LSTM_CMaps/fd001/lstm_cmaps_Magnitude_L2_pruned.pth\n",
      "\n",
      "--- Fine-tuning LSTM_CMaps for Strategy: Magnitude_L2 ---\n",
      "Starting training for LSTM_CMaps Magnitude_L2 FineTune on cuda with patience=15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/onnx/symbolic_opset9.py:4277: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n",
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
      "  result = _VF.lstm(\n",
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch_pruning/pruner/algorithms/base_pruner.py:87: UserWarning: ch_sparsity is deprecated in v1.3.0. Please use pruning_ratio.\n",
      "  warnings.warn(\n",
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 1/70: Train Loss=403.164663, Val Loss=158.073052, LR=2.0e-04, Time=0.23s\n",
      "  *** New best validation loss: 158.073052 (Epoch 1) ***\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 2/70: Train Loss=404.560628, Val Loss=157.873434, LR=2.0e-04, Time=0.22s\n",
      "  *** New best validation loss: 157.873434 (Epoch 2) ***\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 3/70: Train Loss=407.986518, Val Loss=148.681007, LR=2.0e-04, Time=0.21s\n",
      "  *** New best validation loss: 148.681007 (Epoch 3) ***\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 4/70: Train Loss=410.107881, Val Loss=156.377295, LR=2.0e-04, Time=0.21s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 5/70: Train Loss=402.541529, Val Loss=146.041616, LR=2.0e-04, Time=0.21s\n",
      "  *** New best validation loss: 146.041616 (Epoch 5) ***\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 6/70: Train Loss=400.905902, Val Loss=149.452488, LR=2.0e-04, Time=0.20s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 7/70: Train Loss=401.630538, Val Loss=147.470620, LR=2.0e-04, Time=0.22s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 8/70: Train Loss=404.332957, Val Loss=147.098275, LR=2.0e-04, Time=0.23s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 9/70: Train Loss=401.593862, Val Loss=164.550046, LR=2.0e-04, Time=0.21s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 10/70: Train Loss=402.848119, Val Loss=149.818324, LR=2.0e-04, Time=0.22s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 11/70: Train Loss=405.131074, Val Loss=152.076349, LR=2.0e-04, Time=0.22s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 12/70: Train Loss=404.587840, Val Loss=145.582682, LR=1.0e-04, Time=0.22s\n",
      "  *** New best validation loss: 145.582682 (Epoch 12) ***\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 13/70: Train Loss=395.325802, Val Loss=152.883827, LR=1.0e-04, Time=0.23s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 14/70: Train Loss=393.655678, Val Loss=141.660073, LR=1.0e-04, Time=0.23s\n",
      "  *** New best validation loss: 141.660073 (Epoch 14) ***\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 15/70: Train Loss=397.499853, Val Loss=151.798971, LR=1.0e-04, Time=0.23s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 16/70: Train Loss=395.685235, Val Loss=140.639252, LR=1.0e-04, Time=0.23s\n",
      "  *** New best validation loss: 140.639252 (Epoch 16) ***\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 17/70: Train Loss=402.938996, Val Loss=142.721724, LR=1.0e-04, Time=0.22s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 18/70: Train Loss=392.018881, Val Loss=144.218233, LR=1.0e-04, Time=0.23s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 19/70: Train Loss=401.288911, Val Loss=148.781897, LR=1.0e-04, Time=0.21s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 20/70: Train Loss=392.564314, Val Loss=148.633963, LR=1.0e-04, Time=0.23s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 21/70: Train Loss=401.269058, Val Loss=146.072549, LR=1.0e-04, Time=0.21s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 22/70: Train Loss=400.722675, Val Loss=145.619856, LR=1.0e-04, Time=0.22s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 23/70: Train Loss=391.463355, Val Loss=151.943734, LR=5.0e-05, Time=0.23s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 24/70: Train Loss=401.226149, Val Loss=147.431016, LR=5.0e-05, Time=0.23s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 25/70: Train Loss=395.018538, Val Loss=145.812392, LR=5.0e-05, Time=0.22s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 26/70: Train Loss=404.673477, Val Loss=150.519600, LR=5.0e-05, Time=0.24s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 27/70: Train Loss=396.055689, Val Loss=139.901587, LR=5.0e-05, Time=0.22s\n",
      "  *** New best validation loss: 139.901587 (Epoch 27) ***\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 28/70: Train Loss=392.573347, Val Loss=147.276328, LR=5.0e-05, Time=0.22s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 29/70: Train Loss=399.671329, Val Loss=151.008056, LR=5.0e-05, Time=0.22s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 30/70: Train Loss=397.524523, Val Loss=147.431375, LR=5.0e-05, Time=0.22s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 31/70: Train Loss=389.610099, Val Loss=157.213569, LR=5.0e-05, Time=0.22s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 32/70: Train Loss=393.596932, Val Loss=148.104091, LR=5.0e-05, Time=0.22s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 33/70: Train Loss=403.316108, Val Loss=158.462148, LR=5.0e-05, Time=0.23s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 34/70: Train Loss=398.459718, Val Loss=149.797793, LR=2.5e-05, Time=0.22s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 35/70: Train Loss=397.897569, Val Loss=146.237436, LR=2.5e-05, Time=0.21s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 36/70: Train Loss=393.930656, Val Loss=146.031810, LR=2.5e-05, Time=0.21s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 37/70: Train Loss=394.658286, Val Loss=149.746868, LR=2.5e-05, Time=0.23s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 38/70: Train Loss=400.638405, Val Loss=148.164775, LR=2.5e-05, Time=0.23s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 39/70: Train Loss=400.613101, Val Loss=145.887944, LR=2.5e-05, Time=0.22s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 40/70: Train Loss=394.460887, Val Loss=150.729382, LR=1.3e-05, Time=0.24s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 41/70: Train Loss=392.737209, Val Loss=144.935020, LR=1.3e-05, Time=0.21s\n",
      "LSTM_CMaps Magnitude_L2 FineTune - Epoch 42/70: Train Loss=393.239094, Val Loss=142.075818, LR=1.3e-05, Time=0.23s\n",
      "  Early stopping for LSTM_CMaps Magnitude_L2 FineTune triggered after 15 epochs without val_loss improvement.\n",
      "Training finished for LSTM_CMaps Magnitude_L2 FineTune. Best validation loss: 139.901587\n",
      "  Loaded best model state based on validation loss.\n",
      "\n",
      "--- Evaluating Fine-tuned LSTM_CMaps (Magnitude_L2) on Test Set ---\n",
      "Targets were not scaled or 'target' scaler not provided. Metrics are on the direct output scale (RUL units).\n",
      "Evaluation: RMSE=14.6948, MAE=10.8087, R2=0.8750, Scaled MSE (if criterion given)=215.937500\n",
      "Model state (.pth) saved to ./output_pruning/LSTM_CMaps/fd001/lstm_cmaps_Magnitude_L2_final.pth\n",
      "✅ Model successfully saved as ONNX to ./output_pruning/LSTM_CMaps/fd001/lstm_cmaps_Magnitude_L2_final.onnx\n",
      "\n",
      "\n",
      "==================== LSTM_CMaps STRATEGY: RANDOM ====================\n",
      "Model state loaded from ./output_pruning/LSTM_CMaps/fd001/lstm_cmaps_initial.pth\n",
      "--- Attempting Pruning LSTM_CMaps with Strategy: Random ---\n",
      "\n",
      "--- Model Structure BEFORE Pruning (Random) ---\n",
      "TimeSeriesLSTM_WithBlocks(\n",
      "  (lstm): LSTM(15, 100, num_layers=2, batch_first=True)\n",
      "  (intermediate_blocks): ModuleList(\n",
      "    (0): IntermediateBlock(\n",
      "      (fc): Linear(in_features=100, out_features=64, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (1): IntermediateBlock(\n",
      "      (fc): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc_final): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "State Before Pruning (Random): FLOPs=58.3287M, Params=0.1362M\n",
      "Targeting 3 specific Linear layers for pruning in Random.\n",
      "Ignoring 1 other Linear/LSTM layers.\n",
      "Using Importance Metric (Random): RandomImportance\n",
      "Using Pruner Class (Random): BasePruner\n",
      "Starting pruning with Random, Target Sparsity: 0.30\n",
      "\n",
      "--- Model Structure AFTER Pruning (Random) ---\n",
      "TimeSeriesLSTM_WithBlocks(\n",
      "  (lstm): LSTM(15, 100, num_layers=2, batch_first=True)\n",
      "  (intermediate_blocks): ModuleList(\n",
      "    (0): IntermediateBlock(\n",
      "      (fc): Linear(in_features=100, out_features=44, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (1): IntermediateBlock(\n",
      "      (fc): Linear(in_features=44, out_features=22, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc_final): Linear(in_features=22, out_features=1, bias=True)\n",
      ")\n",
      "Pruning finished for Random. Final FLOPs=58.3255M, Params=0.1331M\n",
      "FLOPs Reduction: 0.01%\n",
      "Params Reduction: 2.29%\n",
      "\n",
      "--- Named Parameters AFTER Pruning (Random) ---\n",
      "  lstm.weight_ih_l0: shape=torch.Size([400, 15]), num_elements=6000, requires_grad=True\n",
      "  lstm.weight_hh_l0: shape=torch.Size([400, 100]), num_elements=40000, requires_grad=True\n",
      "  lstm.bias_ih_l0: shape=torch.Size([400]), num_elements=400, requires_grad=True\n",
      "  lstm.bias_hh_l0: shape=torch.Size([400]), num_elements=400, requires_grad=True\n",
      "  lstm.weight_ih_l1: shape=torch.Size([400, 100]), num_elements=40000, requires_grad=True\n",
      "  lstm.weight_hh_l1: shape=torch.Size([400, 100]), num_elements=40000, requires_grad=True\n",
      "  lstm.bias_ih_l1: shape=torch.Size([400]), num_elements=400, requires_grad=True\n",
      "  lstm.bias_hh_l1: shape=torch.Size([400]), num_elements=400, requires_grad=True\n",
      "  intermediate_blocks.0.fc.weight: shape=torch.Size([44, 100]), num_elements=4400, requires_grad=True\n",
      "  intermediate_blocks.0.fc.bias: shape=torch.Size([44]), num_elements=44, requires_grad=True\n",
      "  intermediate_blocks.1.fc.weight: shape=torch.Size([22, 44]), num_elements=968, requires_grad=True\n",
      "  intermediate_blocks.1.fc.bias: shape=torch.Size([22]), num_elements=22, requires_grad=True\n",
      "  fc_final.weight: shape=torch.Size([1, 22]), num_elements=22, requires_grad=True\n",
      "  fc_final.bias: shape=torch.Size([1]), num_elements=1, requires_grad=True\n",
      "Model state (.pth) saved to ./output_pruning/LSTM_CMaps/fd001/lstm_cmaps_Random_pruned.pth\n",
      "\n",
      "--- Fine-tuning LSTM_CMaps for Strategy: Random ---\n",
      "Starting training for LSTM_CMaps Random FineTune on cuda with patience=15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/onnx/symbolic_opset9.py:4277: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n",
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
      "  result = _VF.lstm(\n",
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch_pruning/pruner/algorithms/base_pruner.py:87: UserWarning: ch_sparsity is deprecated in v1.3.0. Please use pruning_ratio.\n",
      "  warnings.warn(\n",
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_CMaps Random FineTune - Epoch 1/70: Train Loss=965.990082, Val Loss=217.666730, LR=2.0e-04, Time=0.22s\n",
      "  *** New best validation loss: 217.666730 (Epoch 1) ***\n",
      "LSTM_CMaps Random FineTune - Epoch 2/70: Train Loss=516.423215, Val Loss=185.241281, LR=2.0e-04, Time=0.21s\n",
      "  *** New best validation loss: 185.241281 (Epoch 2) ***\n",
      "LSTM_CMaps Random FineTune - Epoch 3/70: Train Loss=500.944238, Val Loss=144.696084, LR=2.0e-04, Time=0.23s\n",
      "  *** New best validation loss: 144.696084 (Epoch 3) ***\n",
      "LSTM_CMaps Random FineTune - Epoch 4/70: Train Loss=496.175316, Val Loss=163.238034, LR=2.0e-04, Time=0.22s\n",
      "LSTM_CMaps Random FineTune - Epoch 5/70: Train Loss=504.194899, Val Loss=158.458811, LR=2.0e-04, Time=0.21s\n",
      "LSTM_CMaps Random FineTune - Epoch 6/70: Train Loss=494.581497, Val Loss=159.372282, LR=2.0e-04, Time=0.21s\n",
      "LSTM_CMaps Random FineTune - Epoch 7/70: Train Loss=493.583302, Val Loss=153.860656, LR=2.0e-04, Time=0.23s\n",
      "LSTM_CMaps Random FineTune - Epoch 8/70: Train Loss=493.151765, Val Loss=152.417275, LR=2.0e-04, Time=0.22s\n",
      "LSTM_CMaps Random FineTune - Epoch 9/70: Train Loss=490.268457, Val Loss=160.215954, LR=2.0e-04, Time=0.25s\n",
      "LSTM_CMaps Random FineTune - Epoch 10/70: Train Loss=488.387538, Val Loss=145.592786, LR=1.0e-04, Time=0.21s\n",
      "LSTM_CMaps Random FineTune - Epoch 11/70: Train Loss=475.117260, Val Loss=156.688436, LR=1.0e-04, Time=0.21s\n",
      "LSTM_CMaps Random FineTune - Epoch 12/70: Train Loss=483.988351, Val Loss=141.488937, LR=1.0e-04, Time=0.20s\n",
      "  *** New best validation loss: 141.488937 (Epoch 12) ***\n",
      "LSTM_CMaps Random FineTune - Epoch 13/70: Train Loss=471.585985, Val Loss=148.812892, LR=1.0e-04, Time=0.23s\n",
      "LSTM_CMaps Random FineTune - Epoch 14/70: Train Loss=498.701802, Val Loss=160.850223, LR=1.0e-04, Time=0.23s\n",
      "LSTM_CMaps Random FineTune - Epoch 15/70: Train Loss=483.850481, Val Loss=147.502788, LR=1.0e-04, Time=0.22s\n",
      "LSTM_CMaps Random FineTune - Epoch 16/70: Train Loss=486.351220, Val Loss=148.812663, LR=1.0e-04, Time=0.21s\n",
      "LSTM_CMaps Random FineTune - Epoch 17/70: Train Loss=478.497890, Val Loss=152.877146, LR=1.0e-04, Time=0.22s\n",
      "LSTM_CMaps Random FineTune - Epoch 18/70: Train Loss=499.256031, Val Loss=153.520658, LR=1.0e-04, Time=0.23s\n",
      "LSTM_CMaps Random FineTune - Epoch 19/70: Train Loss=491.902040, Val Loss=150.241327, LR=5.0e-05, Time=0.22s\n",
      "LSTM_CMaps Random FineTune - Epoch 20/70: Train Loss=487.148067, Val Loss=154.571587, LR=5.0e-05, Time=0.22s\n",
      "LSTM_CMaps Random FineTune - Epoch 21/70: Train Loss=487.568032, Val Loss=155.451541, LR=5.0e-05, Time=0.23s\n",
      "LSTM_CMaps Random FineTune - Epoch 22/70: Train Loss=477.033934, Val Loss=156.518808, LR=5.0e-05, Time=0.23s\n",
      "LSTM_CMaps Random FineTune - Epoch 23/70: Train Loss=479.404088, Val Loss=154.345210, LR=5.0e-05, Time=0.23s\n",
      "LSTM_CMaps Random FineTune - Epoch 24/70: Train Loss=482.077784, Val Loss=150.009621, LR=5.0e-05, Time=0.23s\n",
      "LSTM_CMaps Random FineTune - Epoch 25/70: Train Loss=485.004162, Val Loss=157.742724, LR=2.5e-05, Time=0.21s\n",
      "LSTM_CMaps Random FineTune - Epoch 26/70: Train Loss=482.963147, Val Loss=146.994626, LR=2.5e-05, Time=0.21s\n",
      "LSTM_CMaps Random FineTune - Epoch 27/70: Train Loss=491.416008, Val Loss=155.357857, LR=2.5e-05, Time=0.22s\n",
      "  Early stopping for LSTM_CMaps Random FineTune triggered after 15 epochs without val_loss improvement.\n",
      "Training finished for LSTM_CMaps Random FineTune. Best validation loss: 141.488937\n",
      "  Loaded best model state based on validation loss.\n",
      "\n",
      "--- Evaluating Fine-tuned LSTM_CMaps (Random) on Test Set ---\n",
      "Targets were not scaled or 'target' scaler not provided. Metrics are on the direct output scale (RUL units).\n",
      "Evaluation: RMSE=14.4510, MAE=10.4574, R2=0.8791, Scaled MSE (if criterion given)=208.830261\n",
      "Model state (.pth) saved to ./output_pruning/LSTM_CMaps/fd001/lstm_cmaps_Random_final.pth\n",
      "✅ Model successfully saved as ONNX to ./output_pruning/LSTM_CMaps/fd001/lstm_cmaps_Random_final.onnx\n",
      "\n",
      "\n",
      "==================== LSTM_CMaps STRATEGY: TAYLOR ====================\n",
      "Model state loaded from ./output_pruning/LSTM_CMaps/fd001/lstm_cmaps_initial.pth\n",
      "--- Attempting Pruning LSTM_CMaps with Strategy: Taylor ---\n",
      "\n",
      "--- Model Structure BEFORE Pruning (Taylor) ---\n",
      "TimeSeriesLSTM_WithBlocks(\n",
      "  (lstm): LSTM(15, 100, num_layers=2, batch_first=True)\n",
      "  (intermediate_blocks): ModuleList(\n",
      "    (0): IntermediateBlock(\n",
      "      (fc): Linear(in_features=100, out_features=64, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (1): IntermediateBlock(\n",
      "      (fc): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc_final): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "State Before Pruning (Taylor): FLOPs=0.4642M, Params=0.1362M\n",
      "Targeting 3 specific Linear layers for pruning in Taylor.\n",
      "Ignoring 1 other Linear/LSTM layers.\n",
      "Using Importance Metric (Taylor): TaylorImportance\n",
      "Using Pruner Class (Taylor): BasePruner\n",
      "Starting pruning with Taylor, Target Sparsity: 0.30\n",
      "Calculating gradients for TaylorImportance (Taylor)...\n",
      " Taylor Grads OK.\n",
      "\n",
      "--- Model Structure AFTER Pruning (Taylor) ---\n",
      "TimeSeriesLSTM_WithBlocks(\n",
      "  (lstm): LSTM(15, 100, num_layers=2, batch_first=True)\n",
      "  (intermediate_blocks): ModuleList(\n",
      "    (0): IntermediateBlock(\n",
      "      (fc): Linear(in_features=100, out_features=44, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (1): IntermediateBlock(\n",
      "      (fc): Linear(in_features=44, out_features=22, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc_final): Linear(in_features=22, out_features=1, bias=True)\n",
      ")\n",
      "Pruning finished for Taylor. Final FLOPs=0.4611M, Params=0.1331M\n",
      "FLOPs Reduction: 0.67%\n",
      "Params Reduction: 2.29%\n",
      "\n",
      "--- Named Parameters AFTER Pruning (Taylor) ---\n",
      "  lstm.weight_ih_l0: shape=torch.Size([400, 15]), num_elements=6000, requires_grad=True\n",
      "  lstm.weight_hh_l0: shape=torch.Size([400, 100]), num_elements=40000, requires_grad=True\n",
      "  lstm.bias_ih_l0: shape=torch.Size([400]), num_elements=400, requires_grad=True\n",
      "  lstm.bias_hh_l0: shape=torch.Size([400]), num_elements=400, requires_grad=True\n",
      "  lstm.weight_ih_l1: shape=torch.Size([400, 100]), num_elements=40000, requires_grad=True\n",
      "  lstm.weight_hh_l1: shape=torch.Size([400, 100]), num_elements=40000, requires_grad=True\n",
      "  lstm.bias_ih_l1: shape=torch.Size([400]), num_elements=400, requires_grad=True\n",
      "  lstm.bias_hh_l1: shape=torch.Size([400]), num_elements=400, requires_grad=True\n",
      "  intermediate_blocks.0.fc.weight: shape=torch.Size([44, 100]), num_elements=4400, requires_grad=True\n",
      "  intermediate_blocks.0.fc.bias: shape=torch.Size([44]), num_elements=44, requires_grad=True\n",
      "  intermediate_blocks.1.fc.weight: shape=torch.Size([22, 44]), num_elements=968, requires_grad=True\n",
      "  intermediate_blocks.1.fc.bias: shape=torch.Size([22]), num_elements=22, requires_grad=True\n",
      "  fc_final.weight: shape=torch.Size([1, 22]), num_elements=22, requires_grad=True\n",
      "  fc_final.bias: shape=torch.Size([1]), num_elements=1, requires_grad=True\n",
      "Model state (.pth) saved to ./output_pruning/LSTM_CMaps/fd001/lstm_cmaps_Taylor_pruned.pth\n",
      "\n",
      "--- Fine-tuning LSTM_CMaps for Strategy: Taylor ---\n",
      "Starting training for LSTM_CMaps Taylor FineTune on cuda with patience=15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/onnx/symbolic_opset9.py:4277: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n",
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
      "  result = _VF.lstm(\n",
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch_pruning/pruner/algorithms/base_pruner.py:87: UserWarning: ch_sparsity is deprecated in v1.3.0. Please use pruning_ratio.\n",
      "  warnings.warn(\n",
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_CMaps Taylor FineTune - Epoch 1/70: Train Loss=383.336360, Val Loss=139.331635, LR=2.0e-04, Time=0.23s\n",
      "  *** New best validation loss: 139.331635 (Epoch 1) ***\n",
      "LSTM_CMaps Taylor FineTune - Epoch 2/70: Train Loss=385.694041, Val Loss=155.037706, LR=2.0e-04, Time=0.23s\n",
      "LSTM_CMaps Taylor FineTune - Epoch 3/70: Train Loss=391.095612, Val Loss=167.041369, LR=2.0e-04, Time=0.23s\n",
      "LSTM_CMaps Taylor FineTune - Epoch 4/70: Train Loss=385.801008, Val Loss=143.925228, LR=2.0e-04, Time=0.22s\n",
      "LSTM_CMaps Taylor FineTune - Epoch 5/70: Train Loss=384.902980, Val Loss=149.027090, LR=2.0e-04, Time=0.21s\n",
      "LSTM_CMaps Taylor FineTune - Epoch 6/70: Train Loss=395.067005, Val Loss=157.947111, LR=2.0e-04, Time=0.21s\n",
      "LSTM_CMaps Taylor FineTune - Epoch 7/70: Train Loss=381.966691, Val Loss=142.049115, LR=2.0e-04, Time=0.22s\n",
      "LSTM_CMaps Taylor FineTune - Epoch 8/70: Train Loss=377.747696, Val Loss=141.336361, LR=1.0e-04, Time=0.21s\n",
      "LSTM_CMaps Taylor FineTune - Epoch 9/70: Train Loss=386.590763, Val Loss=146.218114, LR=1.0e-04, Time=0.23s\n",
      "LSTM_CMaps Taylor FineTune - Epoch 10/70: Train Loss=379.488237, Val Loss=150.170739, LR=1.0e-04, Time=0.23s\n",
      "LSTM_CMaps Taylor FineTune - Epoch 11/70: Train Loss=375.369639, Val Loss=145.583272, LR=1.0e-04, Time=0.23s\n",
      "LSTM_CMaps Taylor FineTune - Epoch 12/70: Train Loss=382.780501, Val Loss=144.761826, LR=1.0e-04, Time=0.22s\n",
      "LSTM_CMaps Taylor FineTune - Epoch 13/70: Train Loss=379.820450, Val Loss=145.485636, LR=1.0e-04, Time=0.23s\n",
      "LSTM_CMaps Taylor FineTune - Epoch 14/70: Train Loss=379.952507, Val Loss=142.445415, LR=5.0e-05, Time=0.21s\n",
      "LSTM_CMaps Taylor FineTune - Epoch 15/70: Train Loss=379.606598, Val Loss=146.798121, LR=5.0e-05, Time=0.22s\n",
      "LSTM_CMaps Taylor FineTune - Epoch 16/70: Train Loss=377.849425, Val Loss=144.568662, LR=5.0e-05, Time=0.21s\n",
      "  Early stopping for LSTM_CMaps Taylor FineTune triggered after 15 epochs without val_loss improvement.\n",
      "Training finished for LSTM_CMaps Taylor FineTune. Best validation loss: 139.331635\n",
      "  Loaded best model state based on validation loss.\n",
      "\n",
      "--- Evaluating Fine-tuned LSTM_CMaps (Taylor) on Test Set ---\n",
      "Targets were not scaled or 'target' scaler not provided. Metrics are on the direct output scale (RUL units).\n",
      "Evaluation: RMSE=14.6646, MAE=10.8438, R2=0.8755, Scaled MSE (if criterion given)=215.051575\n",
      "Model state (.pth) saved to ./output_pruning/LSTM_CMaps/fd001/lstm_cmaps_Taylor_final.pth\n",
      "✅ Model successfully saved as ONNX to ./output_pruning/LSTM_CMaps/fd001/lstm_cmaps_Taylor_final.onnx\n",
      "\n",
      "\n",
      "--- Final Test RMSE and Structural Metrics (LSTM_CMaps Post-Pruning) Comparison ---\n",
      "\n",
      "=== Pruning Strategy Comparison (Metric: RMSE) ===\n",
      "Strategy        | FLOPs (M)  | Params (M) | Size (MB)  | RMSE        \n",
      "---------------------------------------------------------------------\n",
      "initial         | 58.33      | 0.14       |       0.54 |      14.6524\n",
      "Magnitude_L1    | 58.33      | 0.13       |       0.53 |      14.2301\n",
      "Random          | 58.33      | 0.13       |       0.53 |      14.4510\n",
      "Taylor          | 58.33      | 0.13       |       0.53 |      14.6646\n",
      "Magnitude_L2    | 58.33      | 0.13       |       0.53 |      14.6948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/onnx/symbolic_opset9.py:4277: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison plots saved to ./output_pruning/LSTM_CMaps/fd001/\n",
      "\n",
      "\n",
      "--- Plotting LSTM_CMaps Fine-tuning Loss Curves ---\n",
      "Saved fine-tuning curve for Magnitude_L1 to ./output_pruning/LSTM_CMaps/fd001/lstm_cmaps_finetuning_loss_Magnitude_L1.png\n",
      "Saved fine-tuning curve for Magnitude_L2 to ./output_pruning/LSTM_CMaps/fd001/lstm_cmaps_finetuning_loss_Magnitude_L2.png\n",
      "Saved fine-tuning curve for Random to ./output_pruning/LSTM_CMaps/fd001/lstm_cmaps_finetuning_loss_Random.png\n",
      "Saved fine-tuning curve for Taylor to ./output_pruning/LSTM_CMaps/fd001/lstm_cmaps_finetuning_loss_Taylor.png\n",
      "Saved combined validation loss curves to ./output_pruning/LSTM_CMaps/fd001/lstm_cmaps_val_loss_comparison.png\n",
      "\n",
      "\n",
      "--- Plotting LSTM_CMaps Structural Metrics (Initial vs. Post-Pruning) ---\n",
      "Using compare_results_and_plot_regression_final to display structural metrics too, but specific plot func preferred.\n",
      "\n",
      "LSTM_CMaps (CMaps LSTM) Pruning Workflow Completed!\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
