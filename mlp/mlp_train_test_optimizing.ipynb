{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-12T08:20:55.230471Z",
     "start_time": "2025-05-12T08:20:53.872270Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error # For evaluation\n",
    "\n",
    "# torch-pruning\n",
    "import torch_pruning as tp\n",
    "\n",
    "# Type Hinting (Optional but good practice)\n",
    "from typing import Tuple, List, Dict, Union, Optional"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " ### Data Loading and Preprocessing Functions (Using your provided code)",
   "id": "bae069a69d096056"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:22:44.220776Z",
     "start_time": "2025-05-12T08:22:44.210222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Functions from your provided code ---\n",
    "\n",
    "column_names = ['unit_number', 'time_in_cycles'] + [f'op_setting_{i}' for i in range(1, 4)] + [f'sensor_{i}' for i in range(1, 24)]\n",
    "\n",
    "def load_dataframe(file_path: str) -> pd.DataFrame | None:\n",
    "    \"\"\"Loads a single CMaps data file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=' ', header=None, names=column_names)\n",
    "        # Drop the last two columns if they are all NaNs (often artifacts of space delimiter)\n",
    "        df.dropna(axis=1, how='all', inplace=True)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def clean_data(df: pd.DataFrame) -> list:\n",
    "    \"\"\"Identifies columns to remove based on low std dev.\"\"\"\n",
    "    if df is None:\n",
    "        return []\n",
    "    # Columns with std dev < 0.02 (potential candidates for removal)\n",
    "    # Avoid removing unit_number or time_in_cycles here.\n",
    "    cols_to_check = [col for col in df.columns if 'sensor' in col or 'op_setting' in col]\n",
    "    low_std_cols = [col for col in cols_to_check if df[col].std() < 0.02]\n",
    "    print(f\"Columns with std < 0.02 (potential removal): {low_std_cols}\")\n",
    "    # You might decide which ones to actually remove based on domain knowledge or experiment\n",
    "    # For this example, let's remove them as identified.\n",
    "    return low_std_cols\n",
    "\n",
    "def add_rul(df: pd.DataFrame) -> pd.DataFrame | None:\n",
    "    \"\"\"Calculates and adds the Remaining Useful Life (RUL) column.\"\"\"\n",
    "    if df is None:\n",
    "        return None\n",
    "    max_cycles = df.groupby('unit_number')['time_in_cycles'].max().reset_index()\n",
    "    max_cycles.columns = ['unit_number', 'max_cycle']\n",
    "    df = df.merge(max_cycles, on='unit_number', how='left')\n",
    "    df['RUL'] = df['max_cycle'] - df['time_in_cycles']\n",
    "    df.drop(columns=['max_cycle'], inplace=True)\n",
    "    # Clip RUL (optional, common practice to limit max RUL)\n",
    "    df['RUL'] = df['RUL'].clip(upper=125)\n",
    "    return df\n",
    "\n",
    "def normalize_data(df: pd.DataFrame,\n",
    "                   columns_to_normalize: List[str], scaler: MinMaxScaler = None) -> Tuple[pd.DataFrame, MinMaxScaler] | Tuple[None, None]:\n",
    "    \"\"\"Normalizes specified columns using MinMaxScaler.\"\"\"\n",
    "    if df is None:\n",
    "        return None, None\n",
    "    data_to_scale = df[columns_to_normalize]\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler()\n",
    "        df[columns_to_normalize] = scaler.fit_transform(data_to_scale)\n",
    "    else:\n",
    "        # Ensure only columns present in the scaler are transformed\n",
    "        valid_cols = [col for col in columns_to_normalize if col in scaler.feature_names_in_]\n",
    "        if len(valid_cols) < len(columns_to_normalize):\n",
    "            print(\"Warning: Some columns not found in the provided scaler. Skipping them.\")\n",
    "        if valid_cols: # Check if there's anything to transform\n",
    "             df[valid_cols] = scaler.transform(df[valid_cols])\n",
    "\n",
    "    return df, scaler\n",
    "\n",
    "# --- Data Preparation Main Function ---\n",
    "def prepare_cmapss_data(data_dir: str, train_file: str, test_file: str, test_rul_file: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, MinMaxScaler, List[str]]:\n",
    "    \"\"\"Loads, cleans, preprocesses train/test data and RUL.\"\"\"\n",
    "    print(\"--- Preparing Training Data ---\")\n",
    "    train_df = load_dataframe(os.path.join(data_dir, train_file))\n",
    "    train_df = add_rul(train_df)\n",
    "\n",
    "    print(\"\\n--- Preparing Test Data ---\")\n",
    "    test_df = load_dataframe(os.path.join(data_dir, test_file))\n",
    "    test_rul_df = pd.read_csv(os.path.join(data_dir, test_rul_file), header=None, names=['RUL'])\n",
    "    # Adjust RUL based on test_rul_df and clipping if needed\n",
    "    # Test RUL is usually the RUL at the *end* of the test sequence\n",
    "    # We'll use this test_rul_df directly later for evaluation targets\n",
    "\n",
    "    # Clean Data - identify columns based on TRAINING data variance\n",
    "    cols_to_remove = clean_data(train_df)\n",
    "    feature_cols = [col for col in train_df.columns if col not in ['unit_number', 'time_in_cycles', 'RUL'] + cols_to_remove]\n",
    "    print(f\"\\nUsing Features: {feature_cols}\")\n",
    "\n",
    "    # Drop removed columns from both train and test\n",
    "    train_df.drop(columns=cols_to_remove, inplace=True, errors='ignore')\n",
    "    test_df.drop(columns=cols_to_remove, inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "    # Normalize features based on TRAINING data\n",
    "    print(\"\\n--- Normalizing Data ---\")\n",
    "    train_df_norm, scaler = normalize_data(train_df.copy(), feature_cols, scaler=None)\n",
    "    # Use the same scaler for test data\n",
    "    test_df_norm, _ = normalize_data(test_df.copy(), feature_cols, scaler=scaler)\n",
    "\n",
    "    return train_df_norm, test_df_norm, test_rul_df, scaler, feature_cols"
   ],
   "id": "8ed4af0ac9ecc063",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Define MLP Model and Dataset Class\n",
   "id": "b27a03f0f1d02e51"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:22:47.798664Z",
     "start_time": "2025-05-12T08:22:47.790087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- MLP Model Definition (Using your provided class) ---\n",
    "class MLPmodel(nn.Module):\n",
    "    def __init__(self, layer_units: list, input_size: int, output_size: int = 1, dropout_rate: float = 0.2): # Default dropout 0.2\n",
    "        super(MLPmodel, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        self.model_type = 'MLP'\n",
    "\n",
    "        current_size = input_size\n",
    "        # Add hidden layers\n",
    "        for units in layer_units:\n",
    "            self.layers.append(nn.Linear(current_size, units))\n",
    "            # Add BatchNorm before Dropout potentially\n",
    "            # self.layers.append(nn.BatchNorm1d(units)) # Optional\n",
    "            self.dropouts.append(nn.Dropout(dropout_rate))\n",
    "            current_size = units\n",
    "\n",
    "        # Add the output layer (no dropout or explicit activation for regression)\n",
    "        self.layers.append(nn.Linear(current_size, output_size))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu') # Kaiming often good default\n",
    "                if layer.bias is not None:\n",
    "                    nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Input tensor shape: (batch_size, num_features)\n",
    "        for i in range(len(self.layers) - 1): # Iterate through hidden layers\n",
    "            x = self.layers[i](x)\n",
    "            x = nn.ReLU()(x)\n",
    "            x = self.dropouts[i](x)\n",
    "\n",
    "        # Output layer (no activation/dropout after last linear layer for regression)\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "# --- PyTorch Dataset for CMaps MLP ---\n",
    "class CMAPSS_MLP_Dataset(Dataset):\n",
    "    def __init__(self, features: np.ndarray, targets: np.ndarray):\n",
    "        if features.ndim == 3: # Handle potential sequence input by flattening or taking last step\n",
    "             print(\"Warning: Input features seem sequential. Taking last step for MLP.\")\n",
    "             # This assumes LSTM-prepared data; better to prepare MLP data correctly upstream\n",
    "             features = features[:, -1, :]\n",
    "        if targets.ndim > 1 and targets.shape[1] > 1:\n",
    "             print(\"Warning: Targets have more than one dimension. Squeezing.\")\n",
    "             targets = targets.squeeze()\n",
    "\n",
    "\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32).unsqueeze(1) # Ensure target shape is [N, 1]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.features[idx], self.targets[idx]"
   ],
   "id": "1d72abf80e56967d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training and Evaluation Functions (Adapted for Regression)",
   "id": "3dac04ff7e938de9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:22:54.184809Z",
     "start_time": "2025-05-12T08:22:54.175287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Utility Functions ---\n",
    "def save_model_state(model, path):\n",
    "    \"\"\"Saves model state dictionary.\"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model state saved to {path}\")\n",
    "\n",
    "def load_model_state(model, path, device):\n",
    "    \"\"\"Loads model state dictionary.\"\"\"\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    print(f\"Model state loaded from {path}\")\n",
    "    return model\n",
    "\n",
    "def calculate_flops_params(model, example_input):\n",
    "    \"\"\"Calculates FLOPs and Parameters using torch-pruning.\"\"\"\n",
    "    # Note: For MLPs, FLOPs are roughly 2*params per linear layer pass (MACs).\n",
    "    # count_ops_and_params focuses on conv/linear typically.\n",
    "    flops, params = tp.utils.count_ops_and_params(model, example_input)\n",
    "    return flops, params\n",
    "\n",
    "# --- Evaluation Function (RMSE) ---\n",
    "def evaluate_model_rmse(model: nn.Module, data_loader: DataLoader, device: torch.device, example_input: torch.Tensor) -> Dict[str, float]:\n",
    "    \"\"\"Evaluates the regression model using RMSE and calculates FLOPs/Params.\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for features, targets in data_loader:\n",
    "            features = features.to(device)\n",
    "            predictions = model(features)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_targets.append(targets.numpy()) # Targets are already [N, 1]\n",
    "\n",
    "    # Concatenate results from all batches\n",
    "    all_predictions = np.concatenate(all_predictions).squeeze()\n",
    "    all_targets = np.concatenate(all_targets).squeeze()\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(all_targets, all_predictions))\n",
    "    print(f\"Evaluation RMSE: {rmse:.4f}\")\n",
    "\n",
    "    # Calculate FLOPs and Params\n",
    "    flops, params = calculate_flops_params(model, example_input.to(device))\n",
    "    size_mb = params * 4 / 1e6 # Approximation\n",
    "\n",
    "    return {\n",
    "        'rmse': rmse,\n",
    "        'flops': flops,\n",
    "        'params': params,\n",
    "        'size_mb': size_mb\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Training Function (Modified from your ResNet version for MLP/Regression) ---\n",
    "def train_mlp_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader,\n",
    "                 criterion: nn.Module, optimizer: torch.optim.Optimizer, scheduler: Optional[torch.optim.lr_scheduler._LRScheduler],\n",
    "                 device: torch.device, num_epochs: int, patience: int = 10,\n",
    "                 model_save_path: str = \"temp_best_model.pth\") -> nn.Module:\n",
    "    \"\"\"Trains an MLP model with validation, early stopping, and returns best model.\"\"\"\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    print(f\"Starting training on {device} with patience={patience}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for features, targets in train_loader:\n",
    "            features, targets = features.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * features.size(0) # Loss per batch item\n",
    "\n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset) # Average loss per sample\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for features, targets in val_loader:\n",
    "                features, targets = features.to(device), targets.to(device)\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() * features.size(0) # Loss per batch item\n",
    "\n",
    "        epoch_val_loss = val_loss / len(val_loader.dataset) # Average loss per sample\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss={epoch_train_loss:.4f}, Val Loss={epoch_val_loss:.4f}, LR={current_lr:.1e}\")\n",
    "\n",
    "        # Early Stopping Check & Save Best Model\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            # Optionally save checkpoint immediately\n",
    "            # save_model_state(model, model_save_path)\n",
    "            print(f\"*** New best validation loss: {best_val_loss:.4f} (Epoch {epoch+1}) ***\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"Val loss did not improve for {epochs_no_improve} epoch(s).\")\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {patience} epochs without improvement.\")\n",
    "            break\n",
    "\n",
    "        # Step the scheduler\n",
    "        if scheduler:\n",
    "            if isinstance(scheduler, ReduceLROnPlateau):\n",
    "                scheduler.step(epoch_val_loss) # Step based on validation loss\n",
    "            else:\n",
    "                scheduler.step() # Step per epoch for other schedulers\n",
    "\n",
    "    print(f\"Training finished. Best validation loss: {best_val_loss:.4f}\")\n",
    "    # Load the best model state found during training\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"Loaded best model state based on validation loss.\")\n",
    "    else:\n",
    "         print(\"Warning: No improvement in validation loss observed. Returning model from last epoch.\")\n",
    "\n",
    "    # Optionally remove the temporary save file if you saved intermediate checkpoints\n",
    "    # if os.path.exists(model_save_path):\n",
    "    #    os.remove(model_save_path)\n",
    "\n",
    "    return model"
   ],
   "id": "f78b71cc98b01c9b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Pruning Function (Adapted for MLP)",
   "id": "d6a95d0fae741f9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:53:19.630011Z",
     "start_time": "2025-05-12T09:53:19.614473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prune_mlp_model(model_to_prune: nn.Module, example_input: torch.Tensor, strategy: Dict,\n",
    "                    target_sparsity: float = 0.5, iterative_steps: int = 1, # Note: iterative_steps > 1 handled below\n",
    "                    importance: Optional[tp.importance.Importance] = None) -> nn.Module:\n",
    "    \"\"\"Prunes an MLP model using the specified strategy (Revised).\"\"\"\n",
    "    device = example_input.device # Get device from input\n",
    "    model_to_prune.eval().to(device) # Ensure model is on correct device and in eval mode\n",
    "\n",
    "    # --- Determine ignored layer ---\n",
    "    ignored_layers = []\n",
    "    if hasattr(model_to_prune, 'layers') and isinstance(model_to_prune.layers, nn.ModuleList) and len(model_to_prune.layers) > 0:\n",
    "        # Assuming the last layer in the ModuleList is the output layer\n",
    "        output_layer = model_to_prune.layers[-1]\n",
    "        if isinstance(output_layer, nn.Linear): # Check if it's actually Linear\n",
    "             ignored_layers.append(output_layer)\n",
    "             print(f\"Ignoring output layer during pruning: {output_layer}\")\n",
    "        else:\n",
    "             print(f\"Warning: Last layer is not nn.Linear ({type(output_layer)}), not ignoring automatically.\")\n",
    "\n",
    "    # --- Select Importance Metric ---\n",
    "    if importance is None:\n",
    "        importance_metric = strategy['importance']\n",
    "    else:\n",
    "        importance_metric = importance # Use provided importance (e.g., for Taylor)\n",
    "    print(f\"Using Importance Metric: {type(importance_metric).__name__}\")\n",
    "\n",
    "    # --- Instantiate Pruner ---\n",
    "    pruner_class = strategy['pruner']\n",
    "    print(f\"Using Pruner Class: {pruner_class.__name__}\")\n",
    "\n",
    "    # Standard arguments for most pruners in torch-pruning >= 1.2.0\n",
    "    # Note: ch_sparsity is often used for channel pruning. For linear layers (neurons),\n",
    "    # it might correspond to pruning whole rows/neurons if interpreted structrually.\n",
    "    # Let's assume ch_sparsity implies pruning rows/neurons here.\n",
    "    try:\n",
    "        pruner = pruner_class(\n",
    "            model=model_to_prune,\n",
    "            example_inputs=example_input.to(device), # Ensure input on correct device\n",
    "            importance=importance_metric,\n",
    "            iterative_steps=iterative_steps, # Let the pruner handle iterations internally if steps > 1\n",
    "            ch_sparsity=target_sparsity,   # Sparsity target\n",
    "            root_module_types=[nn.Linear], # Focus on Linear layers\n",
    "            ignored_layers=ignored_layers,\n",
    "            # Optional: round_to=8 might be useful if you need specific alignment\n",
    "        )\n",
    "    except TypeError as e:\n",
    "        print(f\"Warning: Error initializing pruner {pruner_class.__name__} with standard args: {e}\")\n",
    "        print(\"Attempting initialization with fewer args...\")\n",
    "        # Fallback for potentially simpler pruner constructors\n",
    "        try:\n",
    "            pruner = pruner_class(\n",
    "                model=model_to_prune,\n",
    "                example_inputs=example_input.to(device),\n",
    "                importance=importance_metric,\n",
    "                ch_sparsity=target_sparsity, # Still likely needed\n",
    "                ignored_layers=ignored_layers,\n",
    "             )\n",
    "        except Exception as E:\n",
    "             print(f\"ERROR: Could not initialize pruner {pruner_class.__name__}\")\n",
    "             raise E\n",
    "\n",
    "\n",
    "    # --- Calculate Initial State ---\n",
    "    flops_before, params_before = calculate_flops_params(model_to_prune, example_input.to(device))\n",
    "    print(f\"State Before Pruning: FLOPs={flops_before/1e6:.3f}M, Params={params_before/1e6:.3f}M\")\n",
    "    print(f\"Starting pruning with {strategy['importance'].__class__.__name__}, Target Sparsity: {target_sparsity:.2f}\")\n",
    "\n",
    "    # --- Special handling BEFORE pruner.step() for specific Importance types ---\n",
    "    if isinstance(importance_metric, tp.importance.TaylorImportance):\n",
    "        # Calculate gradients needed by TaylorImportance\n",
    "        model_to_prune.train() # Requires grad mode\n",
    "        # Ensure inputs and model are on the same device\n",
    "        input_on_device = example_input.to(device)\n",
    "        output = model_to_prune(input_on_device)\n",
    "        # Simple regression loss: dummy target is zero\n",
    "        loss = torch.sum(output**2) # L2 norm squared (or .mean()) as dummy loss\n",
    "        # Or: loss = output.mean()\n",
    "        model_to_prune.zero_grad() # Zero gradients before backward\n",
    "        try:\n",
    "            loss.backward()\n",
    "            print(\"Calculated gradients for TaylorImportance.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Could not perform backward pass for TaylorImportance: {e}\")\n",
    "            raise e\n",
    "        finally:\n",
    "             model_to_prune.eval() # Always switch back to eval mode\n",
    "\n",
    "    # --- Execute Pruning ---\n",
    "    # The `pruner.step(interactive=False)` method (default) handles the pruning\n",
    "    # over the specified `iterative_steps` internally for most pruners.\n",
    "    # You typically don't need an external loop unless using interactive=True.\n",
    "    try:\n",
    "        pruner.step() # Execute pruning based on the pruner's internal logic\n",
    "    except AttributeError as e:\n",
    "         # Catch cases where 'step' might not be the correct method or has issues\n",
    "         print(f\"ERROR: During pruner.step() for {pruner_class.__name__}: {e}\")\n",
    "         print(\"This pruner might require interactive=True or have a different API.\")\n",
    "         # If interactive is needed:\n",
    "         # for group in pruner.step(interactive=True):\n",
    "         #    group.prune()\n",
    "         raise e # Re-raise after printing info\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: An unexpected error occurred during pruner.step() for {pruner_class.__name__}: {e}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "    # --- Calculate Final State ---\n",
    "    flops_after, params_after = calculate_flops_params(model_to_prune, example_input.to(device))\n",
    "    print(f\"Pruning finished. Final FLOPs: {flops_after/1e6:.3f}M, Params: {params_after/1e6:.3f}M\")\n",
    "    print(f\"FLOPs Reduction: {(flops_before-flops_after)/flops_before*100:.2f}%\")\n",
    "    print(f\"Params Reduction: {(params_before-params_after)/params_before*100:.2f}%\")\n",
    "\n",
    "    # Ensure parameters require gradients for fine-tuning\n",
    "    # Pruning might have altered the requires_grad flag in some cases\n",
    "    num_params_fixed = 0\n",
    "    for name, param in model_to_prune.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            param.requires_grad = True\n",
    "            num_params_fixed += 1\n",
    "    if num_params_fixed > 0:\n",
    "        print(f\"Set requires_grad=True for {num_params_fixed} parameters.\")\n",
    "\n",
    "\n",
    "    return model_to_prune"
   ],
   "id": "70f8b1a324b7fa4f",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###  Comparison and Plotting (Adapted for Regression)",
   "id": "713571cff53fda73"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:42:02.573319Z",
     "start_time": "2025-05-12T09:42:02.563548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os # Ensure imported\n",
    "from typing import Dict # Ensure imported\n",
    "\n",
    "def compare_results_and_plot_rmse(results: Dict[str, Dict[str, float]], output_dir: str):\n",
    "    \"\"\"Prints comparison table and plots results for regression (RMSE).\"\"\"\n",
    "\n",
    "    print(\"\\n=== Pruning Strategy Comparison (RMSE) ===\")\n",
    "    print(f\"{'Strategy':<12} | {'FLOPs':<12} | {'Params':<10} | {'Size (MB)':<10} | {'RMSE':<10}\")\n",
    "    print(\"-\" * 65)\n",
    "    # Sort strategies by RMSE (lower is better) for better comparison\n",
    "    sorted_strategies = sorted(results.keys(), key=lambda s: results[s].get('rmse', float('inf')))\n",
    "\n",
    "    for strategy in sorted_strategies:\n",
    "        metrics = results[strategy]\n",
    "        # Use .get() with defaults for robustness if a metric is missing\n",
    "        flops_m = metrics.get('flops', 0) / 1e6\n",
    "        params_m = metrics.get('params', 0) / 1e6\n",
    "        size_mb_val = metrics.get('size_mb', 0)\n",
    "        rmse_val = metrics.get('rmse', float('nan'))\n",
    "        print(f\"{strategy:<12} | {flops_m:<11.2f}M | {params_m:<9.2f}M | {size_mb_val:>9.2f} | {rmse_val:<10.4f}\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # Ensure 'initial' is first if it exists for plotting reference\n",
    "    plot_strategies = ['initial'] + [s for s in sorted_strategies if s != 'initial']\n",
    "    metrics_to_plot = ['flops', 'params', 'size_mb', 'rmse']\n",
    "    titles = {'flops': 'FLOPs Comparison', 'params': 'Parameters Comparison',\n",
    "              'size_mb': 'Model Size (MB) Comparison', 'rmse': 'RMSE Comparison (Lower is Better)'}\n",
    "    y_labels = {'flops': 'FLOPs (Millions)', 'params': 'Parameters (Millions)',\n",
    "                'size_mb': 'Size (MB)', 'rmse': 'RMSE'}\n",
    "\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(plot_strategies)))\n",
    "\n",
    "    for metric_name in metrics_to_plot:\n",
    "        if not any(metric_name in results.get(s, {}) for s in plot_strategies):\n",
    "             print(f\"Skipping plot for '{metric_name}', data not found in results.\")\n",
    "             continue\n",
    "\n",
    "        values = []\n",
    "        for strategy in plot_strategies:\n",
    "             metric_val = results.get(strategy, {}).get(metric_name, np.nan)\n",
    "             if metric_name in ['flops', 'params']:\n",
    "                 if not np.isnan(metric_val): # Avoid division by zero or on NaN\n",
    "                     metric_val /= 1e6\n",
    "             values.append(metric_val)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bars = plt.bar(plot_strategies, values, color=colors)\n",
    "        plt.xlabel('Strategy')\n",
    "        plt.ylabel(y_labels[metric_name])\n",
    "        plt.title(titles[metric_name])\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "        # Determine format string for labels OUTSIDE the f-string placeholder\n",
    "        label_format = '.4f' if metric_name == 'rmse' else '.2f'\n",
    "        for bar in bars:\n",
    "            yval = bar.get_height()\n",
    "            if not np.isnan(yval):\n",
    "                # Use the determined label_format inside the f-string placeholder\n",
    "                plt.text(bar.get_x() + bar.get_width()/2., yval, f'{yval:{label_format}}',\n",
    "                         ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "        # Add initial model reference line\n",
    "        if 'initial' in results and metric_name in results['initial'] and not np.isnan(results['initial'][metric_name]):\n",
    "            initial_value = results['initial'][metric_name]\n",
    "            if metric_name in ['flops', 'params']:\n",
    "                initial_value /= 1e6\n",
    "\n",
    "            # --- CORRECTED PART for the label ---\n",
    "            # 1. Determine the format string based on the metric\n",
    "            initial_label_format = '.4f' if metric_name == 'rmse' else '.2f'\n",
    "            # 2. Apply the format string to the value\n",
    "            formatted_initial_value = f\"{initial_value:{initial_label_format}}\"\n",
    "            # 3. Construct the label string\n",
    "            initial_line_label = f\"Initial ({formatted_initial_value})\"\n",
    "\n",
    "            plt.axhline(y=initial_value, color='r', linestyle='--', label=initial_line_label) # Use the constructed label\n",
    "            plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f'mlp_{metric_name}_comparison.png'))\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"Comparison plots saved to {output_dir}\")"
   ],
   "id": "619bf68dbdb48e64",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Main Workflow Configuration",
   "id": "fd7f90a7b2e0b08e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:23:22.894275Z",
     "start_time": "2025-05-12T08:23:22.854606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Configuration ---\n",
    "DATA_DIR = './data/CMaps/' # <<< IMPORTANT: Set path to your NASA CMaps data directory\n",
    "OUTPUT_DIR = './output_mlp_pruning/fd001/'\n",
    "TRAIN_FILE = 'train_FD001.txt'\n",
    "TEST_FILE = 'test_FD001.txt'\n",
    "TEST_RUL_FILE = 'RUL_FD001.txt'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Model Config\n",
    "MLP_HIDDEN_UNITS = [128, 64, 32] # Example MLP structure\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "# Training Config\n",
    "INITIAL_TRAIN_EPOCHS = 100 # Train longer initially\n",
    "FINETUNE_EPOCHS = 100    # Fine-tune potentially as long\n",
    "BATCH_SIZE = 128\n",
    "INITIAL_LR = 0.001\n",
    "FINETUNE_LR = 0.0005\n",
    "PATIENCE = 15 # Patience for early stopping\n",
    "VAL_SPLIT_RATIO = 0.2 # Use 20% of training engines for validation\n",
    "\n",
    "# Pruning Config\n",
    "PRUNING_TARGET_SPARSITY = 0.5 # Target 50% sparsity\n",
    "PRUNING_ITERATIVE_STEPS = 1 # For structured pruning, 1 step is common\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Pruning strategies (can reuse from ResNet example)\n",
    "# Note: BNScalePruner/GroupNormPruner less applicable to MLP without BatchNorm layers\n",
    "pruning_strategies = {\n",
    "    'magnitude': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.MagnitudeImportance(p=2)},\n",
    "    #'bn_scale': {'pruner': tp.pruner.BNScalePruner, 'importance': tp.importance.BNScaleImportance()}, # If you add BN layers\n",
    "    'random': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.RandomImportance()},\n",
    "    'Taylor': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.TaylorImportance()},\n",
    "    #'Hessian': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.GroupHessianImportance()}, # Slow, requires grads\n",
    "    'lamp': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.LAMPImportance(p=2)},\n",
    "    #'geometry': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.FPGMImportance()} # More geometric\n",
    "}"
   ],
   "id": "fdf31bb603c5e8b0",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Loading and Preparation Execution",
   "id": "1692327131960ad2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:23:34.486567Z",
     "start_time": "2025-05-12T08:23:34.143605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Load and Prepare Data ---\n",
    "train_df_norm, test_df_norm, test_rul_df, scaler, feature_cols = prepare_cmapss_data(\n",
    "    DATA_DIR, TRAIN_FILE, TEST_FILE, TEST_RUL_FILE\n",
    ")\n",
    "\n",
    "INPUT_SIZE = len(feature_cols) # Determine input size dynamically\n",
    "print(f\"MLP Input Size determined as: {INPUT_SIZE}\")\n",
    "\n",
    "# --- Split Training Data into Train/Validation (by engine unit) ---\n",
    "train_units = train_df_norm['unit_number'].unique()\n",
    "np.random.seed(42) # For reproducible split\n",
    "np.random.shuffle(train_units)\n",
    "split_idx = int(len(train_units) * (1 - VAL_SPLIT_RATIO))\n",
    "train_unit_ids = train_units[:split_idx]\n",
    "val_unit_ids = train_units[split_idx:]\n",
    "\n",
    "df_train_split = train_df_norm[train_df_norm['unit_number'].isin(train_unit_ids)]\n",
    "df_val_split = train_df_norm[train_df_norm['unit_number'].isin(val_unit_ids)]\n",
    "\n",
    "print(f\"Training data split: {len(df_train_split)} samples ({len(train_unit_ids)} engines)\")\n",
    "print(f\"Validation data split: {len(df_val_split)} samples ({len(val_unit_ids)} engines)\")\n",
    "\n",
    "# --- Prepare MLP Inputs/Outputs ---\n",
    "# Training data: use all time steps\n",
    "X_train = df_train_split[feature_cols].values\n",
    "y_train = df_train_split['RUL'].values\n",
    "\n",
    "# Validation data: use all time steps\n",
    "X_val = df_val_split[feature_cols].values\n",
    "y_val = df_val_split['RUL'].values\n",
    "\n",
    "# Test data: use ONLY the LAST time step for each engine\n",
    "X_test = []\n",
    "test_engine_ids = test_df_norm['unit_number'].unique()\n",
    "for eng_id in test_engine_ids:\n",
    "    eng_data = test_df_norm[test_df_norm['unit_number'] == eng_id]\n",
    "    last_step_features = eng_data[feature_cols].iloc[-1].values # Get last row features\n",
    "    X_test.append(last_step_features)\n",
    "X_test = np.array(X_test)\n",
    "# Target RULs for test set are provided directly in RUL_FD001.txt\n",
    "y_test = test_rul_df['RUL'].values[:len(X_test)] # Ensure lengths match if RUL file has extra lines\n",
    "\n",
    "print(f\"Prepared MLP data shapes:\")\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "\n",
    "# --- Create DataLoaders ---\n",
    "train_dataset = CMAPSS_MLP_Dataset(X_train, y_train)\n",
    "val_dataset = CMAPSS_MLP_Dataset(X_val, y_val)\n",
    "test_dataset = CMAPSS_MLP_Dataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False) # Use same batch size for consistency\n",
    "\n",
    "# Create an example input tensor for pruning/flops calculation\n",
    "example_input_tensor = torch.randn(1, INPUT_SIZE).to(DEVICE)"
   ],
   "id": "6920f457d6b0d906",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preparing Training Data ---\n",
      "\n",
      "--- Preparing Test Data ---\n",
      "Columns with std < 0.02 (potential removal): ['op_setting_1', 'op_setting_2', 'op_setting_3', 'sensor_1', 'sensor_5', 'sensor_6', 'sensor_10', 'sensor_16', 'sensor_18', 'sensor_19']\n",
      "\n",
      "Using Features: ['sensor_2', 'sensor_3', 'sensor_4', 'sensor_7', 'sensor_8', 'sensor_9', 'sensor_11', 'sensor_12', 'sensor_13', 'sensor_14', 'sensor_15', 'sensor_17', 'sensor_20', 'sensor_21']\n",
      "\n",
      "--- Normalizing Data ---\n",
      "MLP Input Size determined as: 14\n",
      "Training data split: 16340 samples (80 engines)\n",
      "Validation data split: 4291 samples (20 engines)\n",
      "Prepared MLP data shapes:\n",
      "X_train: (16340, 14), y_train: (16340,)\n",
      "X_val: (4291, 14), y_val: (4291,)\n",
      "X_test: (100, 14), y_test: (100,)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Main Pruning Workflow Execution",
   "id": "5b4fb1a2e19de774"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:54:17.876165Z",
     "start_time": "2025-05-12T09:53:39.796516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Main Workflow ---\n",
    "all_results = {}\n",
    "initial_model_path = os.path.join(OUTPUT_DIR, \"mlp_initial.pth\")\n",
    "model = None # Define model variable\n",
    "\n",
    "# --- 1. Initial Training ---\n",
    "if not os.path.exists(initial_model_path):\n",
    "    print(\"\\n--- Training Initial MLP Model ---\")\n",
    "    model = MLPmodel(layer_units=MLP_HIDDEN_UNITS, input_size=INPUT_SIZE, dropout_rate=DROPOUT_RATE).to(DEVICE)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=INITIAL_LR)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=int(PATIENCE/2), verbose=True)\n",
    "\n",
    "    model = train_mlp_model(\n",
    "        model=model, train_loader=train_loader, val_loader=val_loader,\n",
    "        criterion=criterion, optimizer=optimizer, scheduler=scheduler,\n",
    "        device=DEVICE, num_epochs=INITIAL_TRAIN_EPOCHS, patience=PATIENCE\n",
    "    )\n",
    "    save_model_state(model, initial_model_path)\n",
    "else:\n",
    "    print(f\"\\n--- Loading Initial MLP Model from {initial_model_path} ---\")\n",
    "    # Instantiate model first, then load state\n",
    "    model = MLPmodel(layer_units=MLP_HIDDEN_UNITS, input_size=INPUT_SIZE, dropout_rate=DROPOUT_RATE).to(DEVICE)\n",
    "    model = load_model_state(model, initial_model_path, DEVICE)\n",
    "\n",
    "# --- 2. Evaluate Initial Model ---\n",
    "print(\"\\n--- Evaluating Initial MLP Model on Test Set ---\")\n",
    "initial_metrics = evaluate_model_rmse(model, test_loader, DEVICE, example_input_tensor)\n",
    "all_results['initial'] = initial_metrics\n",
    "\n",
    "# --- 3. Pruning and Fine-tuning Loop ---\n",
    "for strategy_name, strategy_details in pruning_strategies.items():\n",
    "    print(f\"\\n--- Pruning MLP with Strategy: {strategy_name} ---\")\n",
    "    # Load a fresh copy of the initial model for each strategy\n",
    "    pruned_model = MLPmodel(layer_units=MLP_HIDDEN_UNITS, input_size=INPUT_SIZE, dropout_rate=DROPOUT_RATE).to(DEVICE)\n",
    "    pruned_model = load_model_state(pruned_model, initial_model_path, DEVICE)\n",
    "\n",
    "    # Prepare stateful importance if needed (e.g., Taylor)\n",
    "    current_importance = None\n",
    "    if strategy_name == 'Taylor':\n",
    "         current_importance = tp.importance.TaylorImportance()\n",
    "         # Gradient calculation moved inside prune_mlp_model for simplicity here\n",
    "         pass\n",
    "\n",
    "\n",
    "    # Prune the model\n",
    "    try:\n",
    "        pruned_model = prune_mlp_model(\n",
    "            model_to_prune=pruned_model,\n",
    "            example_input=example_input_tensor,\n",
    "            strategy=strategy_details,\n",
    "            target_sparsity=PRUNING_TARGET_SPARSITY,\n",
    "            iterative_steps=PRUNING_ITERATIVE_STEPS,\n",
    "            importance = current_importance # Pass stateful importance if created\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"!!!!!! Pruning failed for strategy {strategy_name}: {e} !!!!!!\")\n",
    "        continue # Skip to next strategy if pruning fails\n",
    "\n",
    "    pruned_model_path = os.path.join(OUTPUT_DIR, f\"mlp_{strategy_name}_pruned.pth\")\n",
    "    save_model_state(pruned_model, pruned_model_path)\n",
    "\n",
    "    # Fine-tune the pruned model\n",
    "    print(f\"\\n--- Fine-tuning MLP after {strategy_name} Pruning ---\")\n",
    "    # Ensure requires_grad is True before creating optimizer\n",
    "    for param in pruned_model.parameters(): param.requires_grad = True\n",
    "\n",
    "    optimizer_ft = torch.optim.Adam(pruned_model.parameters(), lr=FINETUNE_LR)\n",
    "    scheduler_ft = ReduceLROnPlateau(optimizer_ft, mode='min', factor=0.5, patience=int(PATIENCE/2), verbose=True)\n",
    "    criterion_ft = nn.MSELoss()\n",
    "\n",
    "    fine_tuned_model = train_mlp_model(\n",
    "        model=pruned_model, train_loader=train_loader, val_loader=val_loader,\n",
    "        criterion=criterion_ft, optimizer=optimizer_ft, scheduler=scheduler_ft,\n",
    "        device=DEVICE, num_epochs=FINETUNE_EPOCHS, patience=PATIENCE\n",
    "    )\n",
    "\n",
    "    # Evaluate the fine-tuned model\n",
    "    print(f\"\\n--- Evaluating Fine-tuned MLP ({strategy_name}) on Test Set ---\")\n",
    "    final_metrics = evaluate_model_rmse(fine_tuned_model, test_loader, DEVICE, example_input_tensor)\n",
    "    all_results[strategy_name] = final_metrics\n",
    "\n",
    "    # Save the final fine-tuned model\n",
    "    final_model_path = os.path.join(OUTPUT_DIR, f\"mlp_{strategy_name}_final.pth\")\n",
    "    save_model_state(fine_tuned_model, final_model_path)\n",
    "\n",
    "\n",
    "# --- 4. Compare Results ---\n",
    "print(\"\\n--- Final Results Comparison ---\")\n",
    "compare_results_and_plot_rmse(all_results, OUTPUT_DIR)\n",
    "\n",
    "print(\"\\nMLP Pruning Workflow Completed!\")"
   ],
   "id": "fc791c22499e123d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Initial MLP Model ---\n",
      "Starting training on cuda with patience=15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: Train Loss=4277.9776, Val Loss=831.4228, LR=1.0e-03\n",
      "*** New best validation loss: 831.4228 (Epoch 1) ***\n",
      "Epoch 2/100: Train Loss=836.0778, Val Loss=434.3090, LR=1.0e-03\n",
      "*** New best validation loss: 434.3090 (Epoch 2) ***\n",
      "Epoch 3/100: Train Loss=699.9373, Val Loss=421.8724, LR=1.0e-03\n",
      "*** New best validation loss: 421.8724 (Epoch 3) ***\n",
      "Epoch 4/100: Train Loss=664.5800, Val Loss=395.3857, LR=1.0e-03\n",
      "*** New best validation loss: 395.3857 (Epoch 4) ***\n",
      "Epoch 5/100: Train Loss=646.5040, Val Loss=396.7813, LR=1.0e-03\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 6/100: Train Loss=636.8023, Val Loss=385.8818, LR=1.0e-03\n",
      "*** New best validation loss: 385.8818 (Epoch 6) ***\n",
      "Epoch 7/100: Train Loss=618.2175, Val Loss=378.6449, LR=1.0e-03\n",
      "*** New best validation loss: 378.6449 (Epoch 7) ***\n",
      "Epoch 8/100: Train Loss=618.8014, Val Loss=389.7877, LR=1.0e-03\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 9/100: Train Loss=598.5212, Val Loss=376.3227, LR=1.0e-03\n",
      "*** New best validation loss: 376.3227 (Epoch 9) ***\n",
      "Epoch 10/100: Train Loss=597.4970, Val Loss=375.6344, LR=1.0e-03\n",
      "*** New best validation loss: 375.6344 (Epoch 10) ***\n",
      "Epoch 11/100: Train Loss=594.8528, Val Loss=367.7174, LR=1.0e-03\n",
      "*** New best validation loss: 367.7174 (Epoch 11) ***\n",
      "Epoch 12/100: Train Loss=585.4339, Val Loss=367.4377, LR=1.0e-03\n",
      "*** New best validation loss: 367.4377 (Epoch 12) ***\n",
      "Epoch 13/100: Train Loss=580.2203, Val Loss=362.9722, LR=1.0e-03\n",
      "*** New best validation loss: 362.9722 (Epoch 13) ***\n",
      "Epoch 14/100: Train Loss=577.6565, Val Loss=362.3407, LR=1.0e-03\n",
      "*** New best validation loss: 362.3407 (Epoch 14) ***\n",
      "Epoch 15/100: Train Loss=576.6795, Val Loss=365.5691, LR=1.0e-03\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 16/100: Train Loss=572.8108, Val Loss=373.8873, LR=1.0e-03\n",
      "Val loss did not improve for 2 epoch(s).\n",
      "Epoch 17/100: Train Loss=566.0396, Val Loss=362.3781, LR=1.0e-03\n",
      "Val loss did not improve for 3 epoch(s).\n",
      "Epoch 18/100: Train Loss=567.2455, Val Loss=361.2303, LR=1.0e-03\n",
      "*** New best validation loss: 361.2303 (Epoch 18) ***\n",
      "Epoch 19/100: Train Loss=565.5960, Val Loss=363.8243, LR=1.0e-03\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 20/100: Train Loss=556.9908, Val Loss=362.7423, LR=1.0e-03\n",
      "Val loss did not improve for 2 epoch(s).\n",
      "Epoch 21/100: Train Loss=547.9497, Val Loss=357.5357, LR=1.0e-03\n",
      "*** New best validation loss: 357.5357 (Epoch 21) ***\n",
      "Epoch 22/100: Train Loss=567.5090, Val Loss=359.3516, LR=1.0e-03\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 23/100: Train Loss=540.6326, Val Loss=361.0937, LR=1.0e-03\n",
      "Val loss did not improve for 2 epoch(s).\n",
      "Epoch 24/100: Train Loss=546.1557, Val Loss=358.5951, LR=1.0e-03\n",
      "Val loss did not improve for 3 epoch(s).\n",
      "Epoch 25/100: Train Loss=544.4192, Val Loss=364.3420, LR=1.0e-03\n",
      "Val loss did not improve for 4 epoch(s).\n",
      "Epoch 26/100: Train Loss=529.5627, Val Loss=356.1261, LR=1.0e-03\n",
      "*** New best validation loss: 356.1261 (Epoch 26) ***\n",
      "Epoch 27/100: Train Loss=535.9941, Val Loss=358.0937, LR=1.0e-03\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 28/100: Train Loss=542.0153, Val Loss=357.3348, LR=1.0e-03\n",
      "Val loss did not improve for 2 epoch(s).\n",
      "Epoch 29/100: Train Loss=525.2949, Val Loss=361.2923, LR=1.0e-03\n",
      "Val loss did not improve for 3 epoch(s).\n",
      "Epoch 30/100: Train Loss=527.5216, Val Loss=356.3390, LR=1.0e-03\n",
      "Val loss did not improve for 4 epoch(s).\n",
      "Epoch 31/100: Train Loss=522.8097, Val Loss=377.4947, LR=1.0e-03\n",
      "Val loss did not improve for 5 epoch(s).\n",
      "Epoch 32/100: Train Loss=526.7094, Val Loss=360.3164, LR=1.0e-03\n",
      "Val loss did not improve for 6 epoch(s).\n",
      "Epoch 33/100: Train Loss=525.9587, Val Loss=352.3821, LR=1.0e-03\n",
      "*** New best validation loss: 352.3821 (Epoch 33) ***\n",
      "Epoch 34/100: Train Loss=509.7782, Val Loss=358.1297, LR=1.0e-03\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 35/100: Train Loss=515.7215, Val Loss=355.1268, LR=1.0e-03\n",
      "Val loss did not improve for 2 epoch(s).\n",
      "Epoch 36/100: Train Loss=519.4121, Val Loss=357.9577, LR=1.0e-03\n",
      "Val loss did not improve for 3 epoch(s).\n",
      "Epoch 37/100: Train Loss=519.3728, Val Loss=356.8002, LR=1.0e-03\n",
      "Val loss did not improve for 4 epoch(s).\n",
      "Epoch 38/100: Train Loss=513.0649, Val Loss=355.0331, LR=1.0e-03\n",
      "Val loss did not improve for 5 epoch(s).\n",
      "Epoch 39/100: Train Loss=504.8905, Val Loss=356.1286, LR=1.0e-03\n",
      "Val loss did not improve for 6 epoch(s).\n",
      "Epoch 40/100: Train Loss=506.8540, Val Loss=358.7570, LR=1.0e-03\n",
      "Val loss did not improve for 7 epoch(s).\n",
      "Epoch 41/100: Train Loss=501.9128, Val Loss=363.1211, LR=1.0e-03\n",
      "Val loss did not improve for 8 epoch(s).\n",
      "Epoch 42/100: Train Loss=500.0213, Val Loss=356.9853, LR=5.0e-04\n",
      "Val loss did not improve for 9 epoch(s).\n",
      "Epoch 43/100: Train Loss=503.8514, Val Loss=363.9078, LR=5.0e-04\n",
      "Val loss did not improve for 10 epoch(s).\n",
      "Epoch 44/100: Train Loss=501.9231, Val Loss=355.5679, LR=5.0e-04\n",
      "Val loss did not improve for 11 epoch(s).\n",
      "Epoch 45/100: Train Loss=498.4441, Val Loss=358.1486, LR=5.0e-04\n",
      "Val loss did not improve for 12 epoch(s).\n",
      "Epoch 46/100: Train Loss=499.2413, Val Loss=356.6586, LR=5.0e-04\n",
      "Val loss did not improve for 13 epoch(s).\n",
      "Epoch 47/100: Train Loss=498.1829, Val Loss=357.2770, LR=5.0e-04\n",
      "Val loss did not improve for 14 epoch(s).\n",
      "Epoch 48/100: Train Loss=489.3498, Val Loss=358.1590, LR=5.0e-04\n",
      "Val loss did not improve for 15 epoch(s).\n",
      "Early stopping triggered after 15 epochs without improvement.\n",
      "Training finished. Best validation loss: 352.3821\n",
      "Loaded best model state based on validation loss.\n",
      "Model state saved to ./output_mlp_pruning/fd001/mlp_initial.pth\n",
      "\n",
      "--- Evaluating Initial MLP Model on Test Set ---\n",
      "Evaluation RMSE: 18.4583\n",
      "\n",
      "--- Pruning MLP with Strategy: magnitude ---\n",
      "Model state loaded from ./output_mlp_pruning/fd001/mlp_initial.pth\n",
      "Ignoring output layer during pruning: Linear(in_features=32, out_features=1, bias=True)\n",
      "Using Importance Metric: MagnitudeImportance\n",
      "Using Pruner Class: BasePruner\n",
      "State Before Pruning: FLOPs=0.012M, Params=0.012M\n",
      "Starting pruning with MagnitudeImportance, Target Sparsity: 0.50\n",
      "Pruning finished. Final FLOPs: 0.004M, Params: 0.004M\n",
      "FLOPs Reduction: 70.83%\n",
      "Params Reduction: 70.83%\n",
      "Model state saved to ./output_mlp_pruning/fd001/mlp_magnitude_pruned.pth\n",
      "\n",
      "--- Fine-tuning MLP after magnitude Pruning ---\n",
      "Starting training on cuda with patience=15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch_pruning/pruner/algorithms/base_pruner.py:87: UserWarning: ch_sparsity is deprecated in v1.3.0. Please use pruning_ratio.\n",
      "  warnings.warn(\n",
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: Train Loss=1438.0322, Val Loss=396.6622, LR=5.0e-04\n",
      "*** New best validation loss: 396.6622 (Epoch 1) ***\n",
      "Epoch 2/100: Train Loss=594.2083, Val Loss=365.4766, LR=5.0e-04\n",
      "*** New best validation loss: 365.4766 (Epoch 2) ***\n",
      "Epoch 3/100: Train Loss=581.5642, Val Loss=362.1640, LR=5.0e-04\n",
      "*** New best validation loss: 362.1640 (Epoch 3) ***\n",
      "Epoch 4/100: Train Loss=586.5283, Val Loss=358.3324, LR=5.0e-04\n",
      "*** New best validation loss: 358.3324 (Epoch 4) ***\n",
      "Epoch 5/100: Train Loss=591.5512, Val Loss=356.9907, LR=5.0e-04\n",
      "*** New best validation loss: 356.9907 (Epoch 5) ***\n",
      "Epoch 6/100: Train Loss=587.9350, Val Loss=357.0493, LR=5.0e-04\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 7/100: Train Loss=598.0355, Val Loss=356.1322, LR=5.0e-04\n",
      "*** New best validation loss: 356.1322 (Epoch 7) ***\n",
      "Epoch 8/100: Train Loss=582.0053, Val Loss=358.2220, LR=5.0e-04\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 9/100: Train Loss=573.7702, Val Loss=356.3579, LR=5.0e-04\n",
      "Val loss did not improve for 2 epoch(s).\n",
      "Epoch 10/100: Train Loss=582.8108, Val Loss=354.7176, LR=5.0e-04\n",
      "*** New best validation loss: 354.7176 (Epoch 10) ***\n",
      "Epoch 11/100: Train Loss=588.4450, Val Loss=357.3923, LR=5.0e-04\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 12/100: Train Loss=566.5460, Val Loss=356.4515, LR=5.0e-04\n",
      "Val loss did not improve for 2 epoch(s).\n",
      "Epoch 13/100: Train Loss=575.7009, Val Loss=356.4832, LR=5.0e-04\n",
      "Val loss did not improve for 3 epoch(s).\n",
      "Epoch 14/100: Train Loss=567.3341, Val Loss=358.0183, LR=5.0e-04\n",
      "Val loss did not improve for 4 epoch(s).\n",
      "Epoch 15/100: Train Loss=554.9713, Val Loss=359.0438, LR=5.0e-04\n",
      "Val loss did not improve for 5 epoch(s).\n",
      "Epoch 16/100: Train Loss=562.9023, Val Loss=357.6171, LR=5.0e-04\n",
      "Val loss did not improve for 6 epoch(s).\n",
      "Epoch 17/100: Train Loss=565.2169, Val Loss=356.3105, LR=5.0e-04\n",
      "Val loss did not improve for 7 epoch(s).\n",
      "Epoch 18/100: Train Loss=565.8373, Val Loss=357.2494, LR=5.0e-04\n",
      "Val loss did not improve for 8 epoch(s).\n",
      "Epoch 19/100: Train Loss=557.4376, Val Loss=356.0928, LR=2.5e-04\n",
      "Val loss did not improve for 9 epoch(s).\n",
      "Epoch 20/100: Train Loss=565.2236, Val Loss=357.0220, LR=2.5e-04\n",
      "Val loss did not improve for 10 epoch(s).\n",
      "Epoch 21/100: Train Loss=566.1592, Val Loss=355.6323, LR=2.5e-04\n",
      "Val loss did not improve for 11 epoch(s).\n",
      "Epoch 22/100: Train Loss=564.3475, Val Loss=356.0890, LR=2.5e-04\n",
      "Val loss did not improve for 12 epoch(s).\n",
      "Epoch 23/100: Train Loss=558.6331, Val Loss=354.5562, LR=2.5e-04\n",
      "*** New best validation loss: 354.5562 (Epoch 23) ***\n",
      "Epoch 24/100: Train Loss=552.9703, Val Loss=356.2124, LR=2.5e-04\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 25/100: Train Loss=551.7347, Val Loss=356.1852, LR=2.5e-04\n",
      "Val loss did not improve for 2 epoch(s).\n",
      "Epoch 26/100: Train Loss=543.2777, Val Loss=356.8082, LR=2.5e-04\n",
      "Val loss did not improve for 3 epoch(s).\n",
      "Epoch 27/100: Train Loss=548.0032, Val Loss=356.2229, LR=2.5e-04\n",
      "Val loss did not improve for 4 epoch(s).\n",
      "Epoch 28/100: Train Loss=551.9446, Val Loss=357.6012, LR=2.5e-04\n",
      "Val loss did not improve for 5 epoch(s).\n",
      "Epoch 29/100: Train Loss=550.6732, Val Loss=358.9356, LR=2.5e-04\n",
      "Val loss did not improve for 6 epoch(s).\n",
      "Epoch 30/100: Train Loss=552.5540, Val Loss=356.0956, LR=2.5e-04\n",
      "Val loss did not improve for 7 epoch(s).\n",
      "Epoch 31/100: Train Loss=542.2123, Val Loss=356.0468, LR=2.5e-04\n",
      "Val loss did not improve for 8 epoch(s).\n",
      "Epoch 32/100: Train Loss=548.7198, Val Loss=356.5102, LR=1.3e-04\n",
      "Val loss did not improve for 9 epoch(s).\n",
      "Epoch 33/100: Train Loss=556.3674, Val Loss=356.3132, LR=1.3e-04\n",
      "Val loss did not improve for 10 epoch(s).\n",
      "Epoch 34/100: Train Loss=543.2008, Val Loss=355.8928, LR=1.3e-04\n",
      "Val loss did not improve for 11 epoch(s).\n",
      "Epoch 35/100: Train Loss=548.7510, Val Loss=357.4337, LR=1.3e-04\n",
      "Val loss did not improve for 12 epoch(s).\n",
      "Epoch 36/100: Train Loss=552.9055, Val Loss=356.8094, LR=1.3e-04\n",
      "Val loss did not improve for 13 epoch(s).\n",
      "Epoch 37/100: Train Loss=544.8888, Val Loss=356.2876, LR=1.3e-04\n",
      "Val loss did not improve for 14 epoch(s).\n",
      "Epoch 38/100: Train Loss=543.6942, Val Loss=356.5054, LR=1.3e-04\n",
      "Val loss did not improve for 15 epoch(s).\n",
      "Early stopping triggered after 15 epochs without improvement.\n",
      "Training finished. Best validation loss: 354.5562\n",
      "Loaded best model state based on validation loss.\n",
      "\n",
      "--- Evaluating Fine-tuned MLP (magnitude) on Test Set ---\n",
      "Evaluation RMSE: 18.5907\n",
      "Model state saved to ./output_mlp_pruning/fd001/mlp_magnitude_final.pth\n",
      "\n",
      "--- Pruning MLP with Strategy: random ---\n",
      "Model state loaded from ./output_mlp_pruning/fd001/mlp_initial.pth\n",
      "Ignoring output layer during pruning: Linear(in_features=32, out_features=1, bias=True)\n",
      "Using Importance Metric: RandomImportance\n",
      "Using Pruner Class: BasePruner\n",
      "State Before Pruning: FLOPs=0.012M, Params=0.012M\n",
      "Starting pruning with RandomImportance, Target Sparsity: 0.50\n",
      "Pruning finished. Final FLOPs: 0.004M, Params: 0.004M\n",
      "FLOPs Reduction: 70.83%\n",
      "Params Reduction: 70.83%\n",
      "Model state saved to ./output_mlp_pruning/fd001/mlp_random_pruned.pth\n",
      "\n",
      "--- Fine-tuning MLP after random Pruning ---\n",
      "Starting training on cuda with patience=15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch_pruning/pruner/algorithms/base_pruner.py:87: UserWarning: ch_sparsity is deprecated in v1.3.0. Please use pruning_ratio.\n",
      "  warnings.warn(\n",
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: Train Loss=3234.1270, Val Loss=913.2397, LR=5.0e-04\n",
      "*** New best validation loss: 913.2397 (Epoch 1) ***\n",
      "Epoch 2/100: Train Loss=988.9687, Val Loss=438.1702, LR=5.0e-04\n",
      "*** New best validation loss: 438.1702 (Epoch 2) ***\n",
      "Epoch 3/100: Train Loss=858.1054, Val Loss=403.5121, LR=5.0e-04\n",
      "*** New best validation loss: 403.5121 (Epoch 3) ***\n",
      "Epoch 4/100: Train Loss=848.1945, Val Loss=402.7371, LR=5.0e-04\n",
      "*** New best validation loss: 402.7371 (Epoch 4) ***\n",
      "Epoch 5/100: Train Loss=829.8641, Val Loss=388.6739, LR=5.0e-04\n",
      "*** New best validation loss: 388.6739 (Epoch 5) ***\n",
      "Epoch 6/100: Train Loss=812.6633, Val Loss=388.0064, LR=5.0e-04\n",
      "*** New best validation loss: 388.0064 (Epoch 6) ***\n",
      "Epoch 7/100: Train Loss=798.8382, Val Loss=389.9772, LR=5.0e-04\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 8/100: Train Loss=807.3219, Val Loss=382.3556, LR=5.0e-04\n",
      "*** New best validation loss: 382.3556 (Epoch 8) ***\n",
      "Epoch 9/100: Train Loss=800.3400, Val Loss=382.4948, LR=5.0e-04\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 10/100: Train Loss=784.8807, Val Loss=384.2870, LR=5.0e-04\n",
      "Val loss did not improve for 2 epoch(s).\n",
      "Epoch 11/100: Train Loss=791.0060, Val Loss=379.1247, LR=5.0e-04\n",
      "*** New best validation loss: 379.1247 (Epoch 11) ***\n",
      "Epoch 12/100: Train Loss=763.5373, Val Loss=375.7640, LR=5.0e-04\n",
      "*** New best validation loss: 375.7640 (Epoch 12) ***\n",
      "Epoch 13/100: Train Loss=779.9904, Val Loss=372.8279, LR=5.0e-04\n",
      "*** New best validation loss: 372.8279 (Epoch 13) ***\n",
      "Epoch 14/100: Train Loss=774.0580, Val Loss=371.3455, LR=5.0e-04\n",
      "*** New best validation loss: 371.3455 (Epoch 14) ***\n",
      "Epoch 15/100: Train Loss=778.5773, Val Loss=369.0196, LR=5.0e-04\n",
      "*** New best validation loss: 369.0196 (Epoch 15) ***\n",
      "Epoch 16/100: Train Loss=761.3327, Val Loss=365.5529, LR=5.0e-04\n",
      "*** New best validation loss: 365.5529 (Epoch 16) ***\n",
      "Epoch 17/100: Train Loss=757.6861, Val Loss=367.9882, LR=5.0e-04\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 18/100: Train Loss=756.0297, Val Loss=363.9743, LR=5.0e-04\n",
      "*** New best validation loss: 363.9743 (Epoch 18) ***\n",
      "Epoch 19/100: Train Loss=752.1715, Val Loss=364.8348, LR=5.0e-04\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 20/100: Train Loss=762.0868, Val Loss=362.4917, LR=5.0e-04\n",
      "*** New best validation loss: 362.4917 (Epoch 20) ***\n",
      "Epoch 21/100: Train Loss=752.2234, Val Loss=362.1977, LR=5.0e-04\n",
      "*** New best validation loss: 362.1977 (Epoch 21) ***\n",
      "Epoch 22/100: Train Loss=755.2709, Val Loss=357.6698, LR=5.0e-04\n",
      "*** New best validation loss: 357.6698 (Epoch 22) ***\n",
      "Epoch 23/100: Train Loss=748.6381, Val Loss=367.5179, LR=5.0e-04\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 24/100: Train Loss=746.8189, Val Loss=361.0083, LR=5.0e-04\n",
      "Val loss did not improve for 2 epoch(s).\n",
      "Epoch 25/100: Train Loss=747.9800, Val Loss=360.0059, LR=5.0e-04\n",
      "Val loss did not improve for 3 epoch(s).\n",
      "Epoch 26/100: Train Loss=758.9265, Val Loss=364.6955, LR=5.0e-04\n",
      "Val loss did not improve for 4 epoch(s).\n",
      "Epoch 27/100: Train Loss=743.2316, Val Loss=360.6648, LR=5.0e-04\n",
      "Val loss did not improve for 5 epoch(s).\n",
      "Epoch 28/100: Train Loss=742.1428, Val Loss=356.3759, LR=5.0e-04\n",
      "*** New best validation loss: 356.3759 (Epoch 28) ***\n",
      "Epoch 29/100: Train Loss=733.8772, Val Loss=356.6187, LR=5.0e-04\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 30/100: Train Loss=728.5410, Val Loss=363.4093, LR=5.0e-04\n",
      "Val loss did not improve for 2 epoch(s).\n",
      "Epoch 31/100: Train Loss=725.7887, Val Loss=359.6857, LR=5.0e-04\n",
      "Val loss did not improve for 3 epoch(s).\n",
      "Epoch 32/100: Train Loss=736.3481, Val Loss=356.5869, LR=5.0e-04\n",
      "Val loss did not improve for 4 epoch(s).\n",
      "Epoch 33/100: Train Loss=734.2870, Val Loss=351.7289, LR=5.0e-04\n",
      "*** New best validation loss: 351.7289 (Epoch 33) ***\n",
      "Epoch 34/100: Train Loss=721.6049, Val Loss=352.8838, LR=5.0e-04\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 35/100: Train Loss=727.7099, Val Loss=351.3576, LR=5.0e-04\n",
      "*** New best validation loss: 351.3576 (Epoch 35) ***\n",
      "Epoch 36/100: Train Loss=734.1513, Val Loss=352.0880, LR=5.0e-04\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 37/100: Train Loss=719.1880, Val Loss=355.9534, LR=5.0e-04\n",
      "Val loss did not improve for 2 epoch(s).\n",
      "Epoch 38/100: Train Loss=727.9761, Val Loss=352.8229, LR=5.0e-04\n",
      "Val loss did not improve for 3 epoch(s).\n",
      "Epoch 39/100: Train Loss=725.8557, Val Loss=352.6648, LR=5.0e-04\n",
      "Val loss did not improve for 4 epoch(s).\n",
      "Epoch 40/100: Train Loss=709.1797, Val Loss=349.2511, LR=5.0e-04\n",
      "*** New best validation loss: 349.2511 (Epoch 40) ***\n",
      "Epoch 41/100: Train Loss=706.1303, Val Loss=350.1345, LR=5.0e-04\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 42/100: Train Loss=713.7184, Val Loss=356.6300, LR=5.0e-04\n",
      "Val loss did not improve for 2 epoch(s).\n",
      "Epoch 43/100: Train Loss=706.4490, Val Loss=347.6278, LR=5.0e-04\n",
      "*** New best validation loss: 347.6278 (Epoch 43) ***\n",
      "Epoch 44/100: Train Loss=698.4682, Val Loss=351.7486, LR=5.0e-04\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 45/100: Train Loss=712.6253, Val Loss=351.6102, LR=5.0e-04\n",
      "Val loss did not improve for 2 epoch(s).\n",
      "Epoch 46/100: Train Loss=722.8501, Val Loss=357.3353, LR=5.0e-04\n",
      "Val loss did not improve for 3 epoch(s).\n",
      "Epoch 47/100: Train Loss=715.7353, Val Loss=353.6051, LR=5.0e-04\n",
      "Val loss did not improve for 4 epoch(s).\n",
      "Epoch 48/100: Train Loss=714.2783, Val Loss=350.0916, LR=5.0e-04\n",
      "Val loss did not improve for 5 epoch(s).\n",
      "Epoch 49/100: Train Loss=695.9137, Val Loss=350.2228, LR=5.0e-04\n",
      "Val loss did not improve for 6 epoch(s).\n",
      "Epoch 50/100: Train Loss=714.1246, Val Loss=348.6190, LR=5.0e-04\n",
      "Val loss did not improve for 7 epoch(s).\n",
      "Epoch 51/100: Train Loss=704.8413, Val Loss=350.0583, LR=5.0e-04\n",
      "Val loss did not improve for 8 epoch(s).\n",
      "Epoch 52/100: Train Loss=705.9288, Val Loss=348.7239, LR=2.5e-04\n",
      "Val loss did not improve for 9 epoch(s).\n",
      "Epoch 53/100: Train Loss=696.2396, Val Loss=355.7481, LR=2.5e-04\n",
      "Val loss did not improve for 10 epoch(s).\n",
      "Epoch 54/100: Train Loss=698.1648, Val Loss=348.7876, LR=2.5e-04\n",
      "Val loss did not improve for 11 epoch(s).\n",
      "Epoch 55/100: Train Loss=690.3169, Val Loss=349.4124, LR=2.5e-04\n",
      "Val loss did not improve for 12 epoch(s).\n",
      "Epoch 56/100: Train Loss=704.6483, Val Loss=350.3589, LR=2.5e-04\n",
      "Val loss did not improve for 13 epoch(s).\n",
      "Epoch 57/100: Train Loss=707.7634, Val Loss=350.4500, LR=2.5e-04\n",
      "Val loss did not improve for 14 epoch(s).\n",
      "Epoch 58/100: Train Loss=696.5190, Val Loss=352.2078, LR=2.5e-04\n",
      "Val loss did not improve for 15 epoch(s).\n",
      "Early stopping triggered after 15 epochs without improvement.\n",
      "Training finished. Best validation loss: 347.6278\n",
      "Loaded best model state based on validation loss.\n",
      "\n",
      "--- Evaluating Fine-tuned MLP (random) on Test Set ---\n",
      "Evaluation RMSE: 18.9070\n",
      "Model state saved to ./output_mlp_pruning/fd001/mlp_random_final.pth\n",
      "\n",
      "--- Pruning MLP with Strategy: Taylor ---\n",
      "Model state loaded from ./output_mlp_pruning/fd001/mlp_initial.pth\n",
      "Ignoring output layer during pruning: Linear(in_features=32, out_features=1, bias=True)\n",
      "Using Importance Metric: TaylorImportance\n",
      "Using Pruner Class: BasePruner\n",
      "State Before Pruning: FLOPs=0.012M, Params=0.012M\n",
      "Starting pruning with TaylorImportance, Target Sparsity: 0.50\n",
      "Calculated gradients for TaylorImportance.\n",
      "Pruning finished. Final FLOPs: 0.004M, Params: 0.004M\n",
      "FLOPs Reduction: 70.83%\n",
      "Params Reduction: 70.83%\n",
      "Model state saved to ./output_mlp_pruning/fd001/mlp_Taylor_pruned.pth\n",
      "\n",
      "--- Fine-tuning MLP after Taylor Pruning ---\n",
      "Starting training on cuda with patience=15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch_pruning/pruner/algorithms/base_pruner.py:87: UserWarning: ch_sparsity is deprecated in v1.3.0. Please use pruning_ratio.\n",
      "  warnings.warn(\n",
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: Train Loss=1924.6356, Val Loss=510.5144, LR=5.0e-04\n",
      "*** New best validation loss: 510.5144 (Epoch 1) ***\n",
      "Epoch 2/100: Train Loss=726.7381, Val Loss=381.5013, LR=5.0e-04\n",
      "*** New best validation loss: 381.5013 (Epoch 2) ***\n",
      "Epoch 3/100: Train Loss=672.2033, Val Loss=370.7648, LR=5.0e-04\n",
      "*** New best validation loss: 370.7648 (Epoch 3) ***\n",
      "Epoch 4/100: Train Loss=652.6088, Val Loss=368.9616, LR=5.0e-04\n",
      "*** New best validation loss: 368.9616 (Epoch 4) ***\n",
      "Epoch 5/100: Train Loss=641.9424, Val Loss=369.1669, LR=5.0e-04\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 6/100: Train Loss=646.9728, Val Loss=370.4775, LR=5.0e-04\n",
      "Val loss did not improve for 2 epoch(s).\n",
      "Epoch 7/100: Train Loss=637.8505, Val Loss=373.9305, LR=5.0e-04\n",
      "Val loss did not improve for 3 epoch(s).\n",
      "Epoch 8/100: Train Loss=631.7592, Val Loss=367.1711, LR=5.0e-04\n",
      "*** New best validation loss: 367.1711 (Epoch 8) ***\n",
      "Epoch 9/100: Train Loss=619.2146, Val Loss=367.6930, LR=5.0e-04\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 10/100: Train Loss=623.9516, Val Loss=363.4873, LR=5.0e-04\n",
      "*** New best validation loss: 363.4873 (Epoch 10) ***\n",
      "Epoch 11/100: Train Loss=631.7307, Val Loss=363.3204, LR=5.0e-04\n",
      "*** New best validation loss: 363.3204 (Epoch 11) ***\n",
      "Epoch 12/100: Train Loss=612.4644, Val Loss=360.5444, LR=5.0e-04\n",
      "*** New best validation loss: 360.5444 (Epoch 12) ***\n",
      "Epoch 13/100: Train Loss=613.3287, Val Loss=359.5661, LR=5.0e-04\n",
      "*** New best validation loss: 359.5661 (Epoch 13) ***\n",
      "Epoch 14/100: Train Loss=610.2078, Val Loss=367.9998, LR=5.0e-04\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 15/100: Train Loss=606.5012, Val Loss=360.3857, LR=5.0e-04\n",
      "Val loss did not improve for 2 epoch(s).\n",
      "Epoch 16/100: Train Loss=604.4092, Val Loss=360.2624, LR=5.0e-04\n",
      "Val loss did not improve for 3 epoch(s).\n",
      "Epoch 17/100: Train Loss=604.4104, Val Loss=357.7686, LR=5.0e-04\n",
      "*** New best validation loss: 357.7686 (Epoch 17) ***\n",
      "Epoch 18/100: Train Loss=599.7223, Val Loss=356.4403, LR=5.0e-04\n",
      "*** New best validation loss: 356.4403 (Epoch 18) ***\n",
      "Epoch 19/100: Train Loss=589.4675, Val Loss=357.2466, LR=5.0e-04\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 20/100: Train Loss=603.9569, Val Loss=356.4829, LR=5.0e-04\n",
      "Val loss did not improve for 2 epoch(s).\n",
      "Epoch 21/100: Train Loss=597.8270, Val Loss=355.6410, LR=5.0e-04\n",
      "*** New best validation loss: 355.6410 (Epoch 21) ***\n",
      "Epoch 22/100: Train Loss=599.5204, Val Loss=359.3213, LR=5.0e-04\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 23/100: Train Loss=593.4714, Val Loss=353.5278, LR=5.0e-04\n",
      "*** New best validation loss: 353.5278 (Epoch 23) ***\n",
      "Epoch 24/100: Train Loss=576.8525, Val Loss=355.7457, LR=5.0e-04\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 25/100: Train Loss=577.8003, Val Loss=355.5328, LR=5.0e-04\n",
      "Val loss did not improve for 2 epoch(s).\n",
      "Epoch 26/100: Train Loss=576.5218, Val Loss=354.1267, LR=5.0e-04\n",
      "Val loss did not improve for 3 epoch(s).\n",
      "Epoch 27/100: Train Loss=577.8645, Val Loss=353.0985, LR=5.0e-04\n",
      "*** New best validation loss: 353.0985 (Epoch 27) ***\n",
      "Epoch 28/100: Train Loss=584.1258, Val Loss=353.8141, LR=5.0e-04\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 29/100: Train Loss=571.2257, Val Loss=351.9283, LR=5.0e-04\n",
      "*** New best validation loss: 351.9283 (Epoch 29) ***\n",
      "Epoch 30/100: Train Loss=573.0282, Val Loss=351.4887, LR=5.0e-04\n",
      "*** New best validation loss: 351.4887 (Epoch 30) ***\n",
      "Epoch 31/100: Train Loss=576.5794, Val Loss=355.1368, LR=5.0e-04\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 32/100: Train Loss=568.6852, Val Loss=353.5670, LR=5.0e-04\n",
      "Val loss did not improve for 2 epoch(s).\n",
      "Epoch 33/100: Train Loss=574.2202, Val Loss=353.8442, LR=5.0e-04\n",
      "Val loss did not improve for 3 epoch(s).\n",
      "Epoch 34/100: Train Loss=568.0085, Val Loss=352.1647, LR=5.0e-04\n",
      "Val loss did not improve for 4 epoch(s).\n",
      "Epoch 35/100: Train Loss=567.4325, Val Loss=351.9323, LR=5.0e-04\n",
      "Val loss did not improve for 5 epoch(s).\n",
      "Epoch 36/100: Train Loss=566.0049, Val Loss=352.5094, LR=5.0e-04\n",
      "Val loss did not improve for 6 epoch(s).\n",
      "Epoch 37/100: Train Loss=564.7143, Val Loss=353.6903, LR=5.0e-04\n",
      "Val loss did not improve for 7 epoch(s).\n",
      "Epoch 38/100: Train Loss=561.5949, Val Loss=353.1031, LR=5.0e-04\n",
      "Val loss did not improve for 8 epoch(s).\n",
      "Epoch 39/100: Train Loss=563.8608, Val Loss=352.8370, LR=2.5e-04\n",
      "Val loss did not improve for 9 epoch(s).\n",
      "Epoch 40/100: Train Loss=557.0431, Val Loss=354.3513, LR=2.5e-04\n",
      "Val loss did not improve for 10 epoch(s).\n",
      "Epoch 41/100: Train Loss=557.3173, Val Loss=353.0804, LR=2.5e-04\n",
      "Val loss did not improve for 11 epoch(s).\n",
      "Epoch 42/100: Train Loss=569.3043, Val Loss=352.4736, LR=2.5e-04\n",
      "Val loss did not improve for 12 epoch(s).\n",
      "Epoch 43/100: Train Loss=569.3066, Val Loss=353.5470, LR=2.5e-04\n",
      "Val loss did not improve for 13 epoch(s).\n",
      "Epoch 44/100: Train Loss=557.7514, Val Loss=354.8268, LR=2.5e-04\n",
      "Val loss did not improve for 14 epoch(s).\n",
      "Epoch 45/100: Train Loss=551.9011, Val Loss=354.2147, LR=2.5e-04\n",
      "Val loss did not improve for 15 epoch(s).\n",
      "Early stopping triggered after 15 epochs without improvement.\n",
      "Training finished. Best validation loss: 351.4887\n",
      "Loaded best model state based on validation loss.\n",
      "\n",
      "--- Evaluating Fine-tuned MLP (Taylor) on Test Set ---\n",
      "Evaluation RMSE: 18.6223\n",
      "Model state saved to ./output_mlp_pruning/fd001/mlp_Taylor_final.pth\n",
      "\n",
      "--- Pruning MLP with Strategy: lamp ---\n",
      "Model state loaded from ./output_mlp_pruning/fd001/mlp_initial.pth\n",
      "Ignoring output layer during pruning: Linear(in_features=32, out_features=1, bias=True)\n",
      "Using Importance Metric: LAMPImportance\n",
      "Using Pruner Class: BasePruner\n",
      "State Before Pruning: FLOPs=0.012M, Params=0.012M\n",
      "Starting pruning with LAMPImportance, Target Sparsity: 0.50\n",
      "Pruning finished. Final FLOPs: 0.004M, Params: 0.004M\n",
      "FLOPs Reduction: 70.83%\n",
      "Params Reduction: 70.83%\n",
      "Model state saved to ./output_mlp_pruning/fd001/mlp_lamp_pruned.pth\n",
      "\n",
      "--- Fine-tuning MLP after lamp Pruning ---\n",
      "Starting training on cuda with patience=15\n",
      "Epoch 1/100: Train Loss=1458.6153, Val Loss=399.2599, LR=5.0e-04\n",
      "*** New best validation loss: 399.2599 (Epoch 1) ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch_pruning/pruner/algorithms/base_pruner.py:87: UserWarning: ch_sparsity is deprecated in v1.3.0. Please use pruning_ratio.\n",
      "  warnings.warn(\n",
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: Train Loss=594.4565, Val Loss=365.9210, LR=5.0e-04\n",
      "*** New best validation loss: 365.9210 (Epoch 2) ***\n",
      "Epoch 3/100: Train Loss=590.9945, Val Loss=360.4965, LR=5.0e-04\n",
      "*** New best validation loss: 360.4965 (Epoch 3) ***\n",
      "Epoch 4/100: Train Loss=585.2041, Val Loss=360.5169, LR=5.0e-04\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 5/100: Train Loss=589.7443, Val Loss=357.6806, LR=5.0e-04\n",
      "*** New best validation loss: 357.6806 (Epoch 5) ***\n",
      "Epoch 6/100: Train Loss=584.9153, Val Loss=356.8022, LR=5.0e-04\n",
      "*** New best validation loss: 356.8022 (Epoch 6) ***\n",
      "Epoch 7/100: Train Loss=588.0763, Val Loss=357.6678, LR=5.0e-04\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 8/100: Train Loss=589.5545, Val Loss=355.8854, LR=5.0e-04\n",
      "*** New best validation loss: 355.8854 (Epoch 8) ***\n",
      "Epoch 9/100: Train Loss=576.8014, Val Loss=360.3429, LR=5.0e-04\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 10/100: Train Loss=581.2550, Val Loss=356.3161, LR=5.0e-04\n",
      "Val loss did not improve for 2 epoch(s).\n",
      "Epoch 11/100: Train Loss=571.5440, Val Loss=358.7799, LR=5.0e-04\n",
      "Val loss did not improve for 3 epoch(s).\n",
      "Epoch 12/100: Train Loss=575.6131, Val Loss=357.4922, LR=5.0e-04\n",
      "Val loss did not improve for 4 epoch(s).\n",
      "Epoch 13/100: Train Loss=582.3107, Val Loss=356.4347, LR=5.0e-04\n",
      "Val loss did not improve for 5 epoch(s).\n",
      "Epoch 14/100: Train Loss=565.8260, Val Loss=357.6284, LR=5.0e-04\n",
      "Val loss did not improve for 6 epoch(s).\n",
      "Epoch 15/100: Train Loss=577.0435, Val Loss=357.2031, LR=5.0e-04\n",
      "Val loss did not improve for 7 epoch(s).\n",
      "Epoch 16/100: Train Loss=564.5762, Val Loss=357.8530, LR=5.0e-04\n",
      "Val loss did not improve for 8 epoch(s).\n",
      "Epoch 17/100: Train Loss=572.5214, Val Loss=356.0500, LR=2.5e-04\n",
      "Val loss did not improve for 9 epoch(s).\n",
      "Epoch 18/100: Train Loss=572.5955, Val Loss=356.3494, LR=2.5e-04\n",
      "Val loss did not improve for 10 epoch(s).\n",
      "Epoch 19/100: Train Loss=565.5277, Val Loss=356.5647, LR=2.5e-04\n",
      "Val loss did not improve for 11 epoch(s).\n",
      "Epoch 20/100: Train Loss=556.6785, Val Loss=356.2952, LR=2.5e-04\n",
      "Val loss did not improve for 12 epoch(s).\n",
      "Epoch 21/100: Train Loss=561.3145, Val Loss=355.8676, LR=2.5e-04\n",
      "*** New best validation loss: 355.8676 (Epoch 21) ***\n",
      "Epoch 22/100: Train Loss=558.7308, Val Loss=356.3275, LR=2.5e-04\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 23/100: Train Loss=558.0748, Val Loss=355.8926, LR=2.5e-04\n",
      "Val loss did not improve for 2 epoch(s).\n",
      "Epoch 24/100: Train Loss=560.4428, Val Loss=355.3859, LR=2.5e-04\n",
      "*** New best validation loss: 355.3859 (Epoch 24) ***\n",
      "Epoch 25/100: Train Loss=558.4225, Val Loss=355.0866, LR=2.5e-04\n",
      "*** New best validation loss: 355.0866 (Epoch 25) ***\n",
      "Epoch 26/100: Train Loss=548.1709, Val Loss=355.2965, LR=2.5e-04\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 27/100: Train Loss=562.0173, Val Loss=355.6336, LR=2.5e-04\n",
      "Val loss did not improve for 2 epoch(s).\n",
      "Epoch 28/100: Train Loss=559.9361, Val Loss=356.0297, LR=2.5e-04\n",
      "Val loss did not improve for 3 epoch(s).\n",
      "Epoch 29/100: Train Loss=551.4774, Val Loss=355.3536, LR=2.5e-04\n",
      "Val loss did not improve for 4 epoch(s).\n",
      "Epoch 30/100: Train Loss=548.3019, Val Loss=355.0012, LR=2.5e-04\n",
      "*** New best validation loss: 355.0012 (Epoch 30) ***\n",
      "Epoch 31/100: Train Loss=550.4726, Val Loss=354.6072, LR=2.5e-04\n",
      "*** New best validation loss: 354.6072 (Epoch 31) ***\n",
      "Epoch 32/100: Train Loss=552.2526, Val Loss=357.5706, LR=2.5e-04\n",
      "Val loss did not improve for 1 epoch(s).\n",
      "Epoch 33/100: Train Loss=547.5073, Val Loss=356.0245, LR=2.5e-04\n",
      "Val loss did not improve for 2 epoch(s).\n",
      "Epoch 34/100: Train Loss=544.0810, Val Loss=355.6790, LR=2.5e-04\n",
      "Val loss did not improve for 3 epoch(s).\n",
      "Epoch 35/100: Train Loss=549.5972, Val Loss=356.7042, LR=2.5e-04\n",
      "Val loss did not improve for 4 epoch(s).\n",
      "Epoch 36/100: Train Loss=546.4533, Val Loss=357.2055, LR=2.5e-04\n",
      "Val loss did not improve for 5 epoch(s).\n",
      "Epoch 37/100: Train Loss=538.0772, Val Loss=355.6722, LR=2.5e-04\n",
      "Val loss did not improve for 6 epoch(s).\n",
      "Epoch 38/100: Train Loss=545.0830, Val Loss=355.7346, LR=2.5e-04\n",
      "Val loss did not improve for 7 epoch(s).\n",
      "Epoch 39/100: Train Loss=542.4992, Val Loss=355.6963, LR=2.5e-04\n",
      "Val loss did not improve for 8 epoch(s).\n",
      "Epoch 40/100: Train Loss=549.7072, Val Loss=355.2489, LR=1.3e-04\n",
      "Val loss did not improve for 9 epoch(s).\n",
      "Epoch 41/100: Train Loss=541.9390, Val Loss=355.8229, LR=1.3e-04\n",
      "Val loss did not improve for 10 epoch(s).\n",
      "Epoch 42/100: Train Loss=548.0830, Val Loss=355.4283, LR=1.3e-04\n",
      "Val loss did not improve for 11 epoch(s).\n",
      "Epoch 43/100: Train Loss=547.9437, Val Loss=355.6300, LR=1.3e-04\n",
      "Val loss did not improve for 12 epoch(s).\n",
      "Epoch 44/100: Train Loss=537.4173, Val Loss=355.5063, LR=1.3e-04\n",
      "Val loss did not improve for 13 epoch(s).\n",
      "Epoch 45/100: Train Loss=540.7973, Val Loss=355.4852, LR=1.3e-04\n",
      "Val loss did not improve for 14 epoch(s).\n",
      "Epoch 46/100: Train Loss=544.2530, Val Loss=356.9137, LR=1.3e-04\n",
      "Val loss did not improve for 15 epoch(s).\n",
      "Early stopping triggered after 15 epochs without improvement.\n",
      "Training finished. Best validation loss: 354.6072\n",
      "Loaded best model state based on validation loss.\n",
      "\n",
      "--- Evaluating Fine-tuned MLP (lamp) on Test Set ---\n",
      "Evaluation RMSE: 18.4824\n",
      "Model state saved to ./output_mlp_pruning/fd001/mlp_lamp_final.pth\n",
      "\n",
      "--- Final Results Comparison ---\n",
      "\n",
      "=== Pruning Strategy Comparison (RMSE) ===\n",
      "Strategy     | FLOPs        | Params     | Size (MB)  | RMSE      \n",
      "-----------------------------------------------------------------\n",
      "initial      | 0.01       M | 0.01     M |      0.05 | 18.4583   \n",
      "lamp         | 0.00       M | 0.00     M |      0.01 | 18.4824   \n",
      "magnitude    | 0.00       M | 0.00     M |      0.01 | 18.5907   \n",
      "Taylor       | 0.00       M | 0.00     M |      0.01 | 18.6223   \n",
      "random       | 0.00       M | 0.00     M |      0.01 | 18.9070   \n",
      "Comparison plots saved to ./output_mlp_pruning/fd001/\n",
      "\n",
      "MLP Pruning Workflow Completed!\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
