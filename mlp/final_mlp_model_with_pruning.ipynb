{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T09:38:22.642141Z",
     "start_time": "2025-05-29T09:36:33.426059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_pruning as tp\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_BASE_NAME = \"mlp_nasa\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# NASA Dataset preprocessing functions from your provided code\n",
    "column_names = ['unit_number', 'time_in_cycles'] + [f'op_setting_{i}' for i in range(1, 4)] + [f'sensor_{i}' for i in\n",
    "                                                                                               range(1, 24)]\n",
    "\n",
    "\n",
    "def load_dataframe(file_path: str) -> pd.DataFrame | None:\n",
    "    \"\"\"Loads a single CMaps data file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=' ', header=None, names=column_names)\n",
    "        # Drop the last two columns if they are all NaNs (often artifacts of space delimiter)\n",
    "        df.dropna(axis=1, how='all', inplace=True)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def clean_data(df: pd.DataFrame) -> list:\n",
    "    \"\"\"Identifies columns to remove based on low std dev.\"\"\"\n",
    "    if df is None:\n",
    "        return []\n",
    "    # Columns with std dev < 0.02 (potential candidates for removal)\n",
    "    cols_to_check = [col for col in df.columns if 'sensor' in col or 'op_setting' in col]\n",
    "    low_std_cols = [col for col in cols_to_check if df[col].std() < 0.02]\n",
    "    print(f\"Columns with std < 0.02 (potential removal): {low_std_cols}\")\n",
    "    return low_std_cols\n",
    "\n",
    "\n",
    "def add_rul(df: pd.DataFrame) -> pd.DataFrame | None:\n",
    "    \"\"\"Calculates and adds the Remaining Useful Life (RUL) column.\"\"\"\n",
    "    if df is None:\n",
    "        return None\n",
    "    max_cycles = df.groupby('unit_number')['time_in_cycles'].max().reset_index()\n",
    "    max_cycles.columns = ['unit_number', 'max_cycle']\n",
    "    df = df.merge(max_cycles, on='unit_number', how='left')\n",
    "    df['RUL'] = df['max_cycle'] - df['time_in_cycles']\n",
    "    df.drop(columns=['max_cycle'], inplace=True)\n",
    "    # Clip RUL (optional, common practice to limit max RUL)\n",
    "    df['RUL'] = df['RUL'].clip(upper=125)\n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize_data(df: pd.DataFrame,\n",
    "                   columns_to_normalize: List[str], scaler: MinMaxScaler = None) -> Tuple[pd.DataFrame, MinMaxScaler] | \\\n",
    "                                                                                    Tuple[None, None]:\n",
    "    \"\"\"Normalizes specified columns using MinMaxScaler.\"\"\"\n",
    "    if df is None:\n",
    "        return None, None\n",
    "    data_to_scale = df[columns_to_normalize]\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler()\n",
    "        df[columns_to_normalize] = scaler.fit_transform(data_to_scale)\n",
    "    else:\n",
    "        # Ensure only columns present in the scaler are transformed\n",
    "        valid_cols = [col for col in columns_to_normalize if col in scaler.feature_names_in_]\n",
    "        if len(valid_cols) < len(columns_to_normalize):\n",
    "            print(\"Warning: Some columns not found in the provided scaler. Skipping them.\")\n",
    "        if valid_cols:\n",
    "            df[valid_cols] = scaler.transform(df[valid_cols])\n",
    "    return df, scaler\n",
    "\n",
    "\n",
    "def prepare_cmapss_data(data_dir: str, train_file: str, test_file: str, test_rul_file: str) -> Tuple[\n",
    "    pd.DataFrame, pd.DataFrame, pd.DataFrame, MinMaxScaler, List[str]]:\n",
    "    \"\"\"Loads, cleans, preprocesses train/test data and RUL.\"\"\"\n",
    "    print(\"--- Preparing Training Data ---\")\n",
    "    train_df = load_dataframe(os.path.join(data_dir, train_file))\n",
    "    train_df = add_rul(train_df)\n",
    "\n",
    "    print(\"\\n--- Preparing Test Data ---\")\n",
    "    test_df = load_dataframe(os.path.join(data_dir, test_file))\n",
    "    test_rul_df = pd.read_csv(os.path.join(data_dir, test_rul_file), header=None, names=['RUL'])\n",
    "\n",
    "    # Clean Data - identify columns based on TRAINING data variance\n",
    "    cols_to_remove = clean_data(train_df)\n",
    "    feature_cols = [col for col in train_df.columns if\n",
    "                    col not in ['unit_number', 'time_in_cycles', 'RUL'] + cols_to_remove]\n",
    "    print(f\"\\nUsing Features: {feature_cols}\")\n",
    "\n",
    "    # Drop removed columns from both train and test\n",
    "    train_df.drop(columns=cols_to_remove, inplace=True, errors='ignore')\n",
    "    test_df.drop(columns=cols_to_remove, inplace=True, errors='ignore')\n",
    "\n",
    "    # Normalize features based on TRAINING data\n",
    "    print(\"\\n--- Normalizing Data ---\")\n",
    "    train_df_norm, scaler = normalize_data(train_df.copy(), feature_cols, scaler=None)\n",
    "    test_df_norm, _ = normalize_data(test_df.copy(), feature_cols, scaler=scaler)\n",
    "\n",
    "    return train_df_norm, test_df_norm, test_rul_df, scaler, feature_cols\n",
    "\n",
    "\n",
    "# Custom Dataset for NASA time series\n",
    "class NASADataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, feature_cols: List[str], window_size: int = 50,\n",
    "                 stride: int = 1, is_test: bool = False, test_rul_df: pd.DataFrame = None):\n",
    "        self.df = df\n",
    "        self.feature_cols = feature_cols\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.is_test = is_test\n",
    "        self.test_rul_df = test_rul_df\n",
    "        self.samples = []\n",
    "        self.targets = []\n",
    "\n",
    "        self._prepare_samples()\n",
    "\n",
    "    def _prepare_samples(self):\n",
    "        \"\"\"Create windowed samples from the dataframe\"\"\"\n",
    "        units = self.df['unit_number'].unique()\n",
    "\n",
    "        for unit in units:\n",
    "            unit_df = self.df[self.df['unit_number'] == unit].sort_values('time_in_cycles')\n",
    "\n",
    "            # For test data, we only need the last window for each unit\n",
    "            if self.is_test:\n",
    "                if len(unit_df) >= self.window_size:\n",
    "                    window_data = unit_df[self.feature_cols].iloc[-self.window_size:].values\n",
    "                    self.samples.append(window_data)\n",
    "                    # Get RUL from test_rul_df\n",
    "                    if self.test_rul_df is not None:\n",
    "                        self.targets.append(self.test_rul_df.iloc[unit - 1]['RUL'])\n",
    "                else:\n",
    "                    # Pad if necessary\n",
    "                    window_data = unit_df[self.feature_cols].values\n",
    "                    padded = np.zeros((self.window_size, len(self.feature_cols)))\n",
    "                    padded[-len(window_data):] = window_data\n",
    "                    self.samples.append(padded)\n",
    "                    if self.test_rul_df is not None:\n",
    "                        self.targets.append(self.test_rul_df.iloc[unit - 1]['RUL'])\n",
    "            else:\n",
    "                # For training data, create multiple windows\n",
    "                for i in range(0, len(unit_df) - self.window_size + 1, self.stride):\n",
    "                    window_data = unit_df[self.feature_cols].iloc[i:i + self.window_size].values\n",
    "                    rul = unit_df['RUL'].iloc[i + self.window_size - 1]\n",
    "                    self.samples.append(window_data)\n",
    "                    self.targets.append(rul)\n",
    "\n",
    "        self.samples = np.array(self.samples, dtype=np.float32)\n",
    "        self.targets = np.array(self.targets, dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Flatten the window for MLP\n",
    "        sample = self.samples[idx].flatten()\n",
    "        target = self.targets[idx]\n",
    "        return torch.FloatTensor(sample), torch.FloatTensor([target])\n",
    "\n",
    "\n",
    "# MLP Model Definition\n",
    "class NASAMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes=[256, 128, 64], dropout_rate=0.2):\n",
    "        super(NASAMLP, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "\n",
    "        # Output layer for regression\n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def get_data_loaders(data_dir='./data/NASA', batch_size=128, window_size=50, val_split=0.2, seed=42):\n",
    "    \"\"\"Load NASA C-MAPSS dataset with train/val/test splits\"\"\"\n",
    "    print(f\"Loading NASA C-MAPSS dataset from: {data_dir}\")\n",
    "\n",
    "    # Prepare data\n",
    "    train_df, test_df, test_rul_df, scaler, feature_cols = prepare_cmapss_data(\n",
    "        data_dir,\n",
    "        'train_FD001.txt',\n",
    "        'test_FD001.txt',\n",
    "        'RUL_FD001.txt'\n",
    "    )\n",
    "\n",
    "    # Create full training dataset\n",
    "    full_train_dataset = NASADataset(train_df, feature_cols, window_size=window_size)\n",
    "\n",
    "    # Split into train and validation\n",
    "    val_size = int(len(full_train_dataset) * val_split)\n",
    "    train_size = len(full_train_dataset) - val_size\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_train_dataset, [train_size, val_size], generator=generator\n",
    "    )\n",
    "\n",
    "    # Create test dataset\n",
    "    test_dataset = NASADataset(test_df, feature_cols, window_size=window_size,\n",
    "                               is_test=True, test_rul_df=test_rul_df)\n",
    "\n",
    "    # Create data loaders\n",
    "    num_workers = min(4, os.cpu_count() or 2)\n",
    "    pin_memory = True if DEVICE.type == 'cuda' else False\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=num_workers, pin_memory=pin_memory)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                            num_workers=num_workers, pin_memory=pin_memory)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                             num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    print(f\"DataLoaders created - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "    print(f\"Input size: {window_size * len(feature_cols)} (window_size * num_features)\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader, window_size * len(feature_cols)\n",
    "\n",
    "\n",
    "def get_mlp_model(input_size, hidden_sizes=[256, 128, 64], dropout_rate=0.2):\n",
    "    \"\"\"Get MLP model for NASA dataset\"\"\"\n",
    "    model = NASAMLP(input_size, hidden_sizes, dropout_rate)\n",
    "    print(f\"✅ Created MLP with architecture: {input_size} -> {' -> '.join(map(str, hidden_sizes))} -> 1\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_ignored_layers(model):\n",
    "    \"\"\"Get layers to ignore during pruning (typically final layer)\"\"\"\n",
    "    ignored_layers = []\n",
    "    # Get the last linear layer in the sequential model\n",
    "    for module in model.model:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            last_linear = module\n",
    "    ignored_layers.append(last_linear)\n",
    "    return ignored_layers\n",
    "\n",
    "\n",
    "def calculate_macs_params(model, example_input):\n",
    "    \"\"\"Calculate MACs and parameters using torch_pruning\"\"\"\n",
    "    model.eval()\n",
    "    target_device = example_input.device\n",
    "    model_on_device = model.to(target_device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        macs, params = tp.utils.count_ops_and_params(model_on_device, example_input)\n",
    "\n",
    "    return macs, params\n",
    "\n",
    "\n",
    "def save_model(model, save_path, example_input_cpu=None):\n",
    "    \"\"\"Save model state dict and optionally ONNX\"\"\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"✅ Model saved to {save_path}\")\n",
    "\n",
    "    if example_input_cpu is not None:\n",
    "        onnx_path = save_path.replace('.pth', '.onnx')\n",
    "        try:\n",
    "            model_cpu = model.to('cpu')\n",
    "            torch.onnx.export(\n",
    "                model_cpu, example_input_cpu, onnx_path,\n",
    "                export_params=True, opset_version=13,\n",
    "                input_names=['input'], output_names=['output'],\n",
    "                dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "            )\n",
    "            print(f\"✅ ONNX model saved to {onnx_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: ONNX export failed: {e}\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, example_input, criterion, device):\n",
    "    \"\"\"Evaluate model and return comprehensive metrics\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Calculate efficiency metrics\n",
    "    macs, params = calculate_macs_params(model, example_input.to(device))\n",
    "    model_size_mb = params * 4 / (1024 * 1024)  # Assuming float32\n",
    "\n",
    "    # Calculate MSE and MAE\n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            all_predictions.extend(output.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    mse = np.mean((all_predictions - all_targets) ** 2)\n",
    "    mae = np.mean(np.abs(all_predictions - all_targets))\n",
    "\n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'loss': total_loss / len(data_loader.dataset),\n",
    "        'macs': macs,\n",
    "        'params': params,\n",
    "        'size_mb': model_size_mb\n",
    "    }\n",
    "\n",
    "\n",
    "def prune_model(model, strategy_config, sparsity_ratio, example_input, ignored_layers=None):\n",
    "    \"\"\"Apply structured pruning to model\"\"\"\n",
    "    if sparsity_ratio == 0.0:\n",
    "        print(\"No pruning needed (sparsity = 0.0)\")\n",
    "        return model\n",
    "\n",
    "    model.eval()\n",
    "    pruned_model = copy.deepcopy(model)\n",
    "    pruned_model.to(example_input.device)\n",
    "\n",
    "    # Calculate initial MACs\n",
    "    initial_macs, _ = calculate_macs_params(pruned_model, example_input)\n",
    "    print(f\"Initial MACs: {initial_macs / 1e6:.2f}M\")\n",
    "\n",
    "    ignored_layers = ignored_layers or []\n",
    "\n",
    "    # Create pruner based on strategy\n",
    "    # Note: For MLP, we use Linear layers as root modules\n",
    "    pruner = strategy_config['pruner'](\n",
    "        pruned_model,\n",
    "        example_input,\n",
    "        importance=strategy_config['importance'],\n",
    "        iterative_steps=5,  # Use 5 iterative steps\n",
    "        ch_sparsity=sparsity_ratio,\n",
    "        root_module_types=[nn.Linear],  # Changed from Conv2d to Linear\n",
    "        ignored_layers=ignored_layers\n",
    "    )\n",
    "\n",
    "    print(f\"Applying {strategy_config['importance'].__class__.__name__} pruning at {sparsity_ratio:.1%} sparsity...\")\n",
    "\n",
    "    # Apply pruning\n",
    "    pruner.step()\n",
    "\n",
    "    # Calculate final MACs\n",
    "    final_macs, _ = calculate_macs_params(pruned_model, example_input)\n",
    "    reduction = (initial_macs - final_macs) / initial_macs * 100 if initial_macs > 0 else 0\n",
    "    print(f\"Final MACs: {final_macs / 1e6:.2f}M (Reduction: {reduction:.1f}%)\")\n",
    "\n",
    "    return pruned_model\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs,\n",
    "                val_loader=None, patience=7, log_prefix=\"\"):\n",
    "    \"\"\"Train model with early stopping\"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_mse': [],\n",
    "        'val_loss': [],\n",
    "        'val_mse': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_predictions = []\n",
    "        train_targets = []\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_predictions.extend(output.detach().cpu().numpy())\n",
    "            train_targets.extend(target.cpu().numpy())\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_mse = np.mean((np.array(train_predictions) - np.array(train_targets)) ** 2)\n",
    "\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_mse'].append(train_mse)\n",
    "\n",
    "        log_msg = f\"Epoch {epoch + 1}/{num_epochs} ({log_prefix}): Train Loss: {avg_train_loss:.4f}, Train MSE: {train_mse:.2f}\"\n",
    "\n",
    "        # Validation phase\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_predictions = []\n",
    "            val_targets = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for data, target in val_loader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                    val_predictions.extend(output.cpu().numpy())\n",
    "                    val_targets.extend(target.cpu().numpy())\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_mse = np.mean((np.array(val_predictions) - np.array(val_targets)) ** 2)\n",
    "\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            history['val_mse'].append(val_mse)\n",
    "\n",
    "            log_msg += f\", Val Loss: {avg_val_loss:.4f}, Val MSE: {val_mse:.2f}\"\n",
    "\n",
    "            # Early stopping check\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                epochs_no_improve = 0\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "                log_msg += \" (Best)\"\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"{log_msg}\")\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "                break\n",
    "        else:\n",
    "            history['val_loss'].append(None)\n",
    "            history['val_mse'].append(None)\n",
    "\n",
    "        print(log_msg)\n",
    "\n",
    "    # Load best model state if available\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"Loaded best model state\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def save_results_to_files(all_results, output_dir):\n",
    "    \"\"\"Save experimental results to JSON and CSV files\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save complete results as JSON\n",
    "    results_json_path = os.path.join(output_dir, 'complete_results.json')\n",
    "    with open(results_json_path, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2, default=str)\n",
    "    print(f\"✅ Complete results saved to {results_json_path}\")\n",
    "\n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for strategy, strategy_results in all_results.items():\n",
    "        for sparsity, metrics in strategy_results.items():\n",
    "            row = {\n",
    "                'strategy': strategy,\n",
    "                'sparsity_ratio': sparsity,\n",
    "                'mse': metrics['mse'],\n",
    "                'mae': metrics['mae'],\n",
    "                'loss': metrics['loss'],\n",
    "                'macs_millions': metrics['macs'] / 1e6,\n",
    "                'params_millions': metrics['params'] / 1e6,\n",
    "                'size_mb': metrics['size_mb']\n",
    "            }\n",
    "            summary_data.append(row)\n",
    "\n",
    "    # Save summary as CSV\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_csv_path = os.path.join(output_dir, 'summary_results.csv')\n",
    "    summary_df.to_csv(summary_csv_path, index=False)\n",
    "    print(f\"✅ Summary results saved to {summary_csv_path}\")\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "def create_results_plots(summary_df, output_dir):\n",
    "    \"\"\"Create visualization plots for regression metrics\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    strategies = summary_df['strategy'].unique()\n",
    "    sparsity_levels = sorted(summary_df['sparsity_ratio'].unique())\n",
    "\n",
    "    # Plot 1: MSE vs Sparsity\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for strategy in strategies:\n",
    "        strategy_data = summary_df[summary_df['strategy'] == strategy].sort_values('sparsity_ratio')\n",
    "        plt.plot(strategy_data['sparsity_ratio'] * 100, strategy_data['mse'],\n",
    "                 'o-', linewidth=2, markersize=8, label=strategy)\n",
    "\n",
    "    plt.xlabel('Sparsity (%)', fontsize=12)\n",
    "    plt.ylabel('MSE', fontsize=12)\n",
    "    plt.title('NASA MLP: MSE vs Sparsity', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.gca().invert_yaxis()  # Lower MSE is better\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(output_dir, 'mse_vs_sparsity.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✅ MSE plot saved to {plot_path}\")\n",
    "\n",
    "    # Plot 2: Efficiency frontier (MSE vs MACs)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for strategy in strategies:\n",
    "        strategy_data = summary_df[summary_df['strategy'] == strategy].sort_values('sparsity_ratio')\n",
    "        plt.scatter(strategy_data['macs_millions'], strategy_data['mse'],\n",
    "                    s=100, label=strategy, alpha=0.8)\n",
    "        plt.plot(strategy_data['macs_millions'], strategy_data['mse'],\n",
    "                 '--', alpha=0.6)\n",
    "\n",
    "    plt.xlabel('MACs (Millions)', fontsize=12)\n",
    "    plt.ylabel('MSE', fontsize=12)\n",
    "    plt.title('NASA MLP: Efficiency Frontier (MSE vs MACs)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.gca().invert_yaxis()  # Lower MSE is better\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(output_dir, 'efficiency_frontier.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✅ Efficiency frontier plot saved to {plot_path}\")\n",
    "\n",
    "\n",
    "def print_results_table(summary_df):\n",
    "    \"\"\"Print formatted results table\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXPERIMENTAL RESULTS SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Baseline results\n",
    "    baseline_results = summary_df[summary_df['sparsity_ratio'] == 0.0].iloc[0]\n",
    "    print(f\"\\nBaseline Performance:\")\n",
    "    print(f\"  MSE: {baseline_results['mse']:.2f}\")\n",
    "    print(f\"  MAE: {baseline_results['mae']:.2f}\")\n",
    "    print(f\"  MACs: {baseline_results['macs_millions']:.2f}M\")\n",
    "    print(f\"  Parameters: {baseline_results['params_millions']:.2f}M\")\n",
    "    print(f\"  Model Size: {baseline_results['size_mb']:.2f}MB\")\n",
    "\n",
    "    # Strategy comparison at 50% sparsity\n",
    "    print(f\"\\nStrategy Comparison at 50% Sparsity:\")\n",
    "    sparsity_50_data = summary_df[summary_df['sparsity_ratio'] == 0.5]\n",
    "    for _, row in sparsity_50_data.iterrows():\n",
    "        degradation = row['mse'] - baseline_results['mse']\n",
    "        relative_increase = (degradation / baseline_results['mse']) * 100\n",
    "        print(\n",
    "            f\"  {row['strategy']:>12}: MSE={row['mse']:>6.2f} ({degradation:>+5.2f}, {relative_increase:>+5.1f}% increase)\")\n",
    "\n",
    "    # Complete results table\n",
    "    print(f\"\\nComplete Results Table:\")\n",
    "    print(\"-\" * 90)\n",
    "    print(f\"{'Strategy':<12} {'Sparsity':<8} {'MSE':<8} {'MAE':<8} {'MACs(M)':<8} {'Params(M)':<9} {'Size(MB)':<8}\")\n",
    "    print(\"-\" * 90)\n",
    "\n",
    "    for _, row in summary_df.sort_values(['strategy', 'sparsity_ratio']).iterrows():\n",
    "        print(f\"{row['strategy']:<12} {row['sparsity_ratio'] * 100:>6.0f}% \"\n",
    "              f\"{row['mse']:>7.2f} {row['mae']:>7.2f} {row['macs_millions']:>7.2f} \"\n",
    "              f\"{row['params_millions']:>8.2f} {row['size_mb']:>7.2f}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main experimental workflow\"\"\"\n",
    "    print(\"Starting NASA MLP Pruning Experiments\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'strategies': {\n",
    "            'MagnitudeL2': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.MagnitudeImportance(p=2)\n",
    "            },\n",
    "            'Random': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.RandomImportance()\n",
    "            },\n",
    "            # Note: BNScale is not applicable to MLP as it doesn't have BatchNorm layers\n",
    "        },\n",
    "        'pruning_ratios': [0.0, 0.2, 0.5, 0.7],\n",
    "        'hidden_sizes': [256, 128, 64],\n",
    "        'dropout_rate': 0.2,\n",
    "        'window_size': 50,\n",
    "        'batch_size': 128,\n",
    "        'learning_rate': 0.0001,\n",
    "        'epochs': 50,  # More epochs for time series\n",
    "        'patience': 10,\n",
    "        'output_dir': './results_mlp_nasa',\n",
    "        'models_dir': './models_mlp_nasa',\n",
    "        'data_dir': './data/CMaps'\n",
    "    }\n",
    "\n",
    "    # Create output directories\n",
    "    os.makedirs(config['output_dir'], exist_ok=True)\n",
    "    os.makedirs(config['models_dir'], exist_ok=True)\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading NASA C-MAPSS dataset...\")\n",
    "    train_loader, val_loader, test_loader, input_size = get_data_loaders(\n",
    "        data_dir=config['data_dir'],\n",
    "        batch_size=config['batch_size'],\n",
    "        window_size=config['window_size']\n",
    "    )\n",
    "\n",
    "    # Prepare inputs and criterion\n",
    "    example_input_cpu = torch.randn(1, input_size)\n",
    "    example_input_device = example_input_cpu.to(DEVICE)\n",
    "    criterion = nn.MSELoss()  # Regression loss\n",
    "\n",
    "    # Get baseline model and train it\n",
    "    print(\"\\nCreating and training baseline model...\")\n",
    "    model = get_mlp_model(input_size, config['hidden_sizes'], config['dropout_rate'])\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Train baseline model\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    trained_model, training_history = train_model(\n",
    "        model, train_loader, criterion, optimizer, DEVICE,\n",
    "        config['epochs'], val_loader, config['patience'], \"Baseline Training\"\n",
    "    )\n",
    "\n",
    "    # Save baseline model\n",
    "    baseline_model_path = os.path.join(config['models_dir'], 'baseline_model.pth')\n",
    "    save_model(trained_model, baseline_model_path, example_input_cpu)\n",
    "\n",
    "    # Evaluate baseline\n",
    "    print(\"\\nEvaluating baseline model...\")\n",
    "    baseline_metrics = evaluate_model(trained_model, test_loader, example_input_device, criterion, DEVICE)\n",
    "    print(f\"Baseline Results: MSE={baseline_metrics['mse']:.2f}, \"\n",
    "          f\"MAE={baseline_metrics['mae']:.2f}, \"\n",
    "          f\"MACs={baseline_metrics['macs'] / 1e6:.2f}M, \"\n",
    "          f\"Params={baseline_metrics['params'] / 1e6:.2f}M\")\n",
    "\n",
    "    # Initialize results storage\n",
    "    all_results = {}\n",
    "    for strategy_name in config['strategies'].keys():\n",
    "        all_results[strategy_name] = {0.0: baseline_metrics}\n",
    "\n",
    "    # Get ignored layers\n",
    "    ignored_layers = get_ignored_layers(trained_model)\n",
    "\n",
    "    # Run pruning experiments\n",
    "    print(\"\\nStarting pruning experiments...\")\n",
    "    for strategy_name, strategy_config in config['strategies'].items():\n",
    "        print(f\"\\n--- Strategy: {strategy_name} ---\")\n",
    "\n",
    "        for sparsity_ratio in config['pruning_ratios']:\n",
    "            if sparsity_ratio == 0.0:\n",
    "                continue  # Skip baseline (already done)\n",
    "\n",
    "            print(f\"\\nProcessing {strategy_name} at {sparsity_ratio:.1%} sparsity...\")\n",
    "\n",
    "            # Load fresh copy of trained baseline\n",
    "            model_copy = get_mlp_model(input_size, config['hidden_sizes'], config['dropout_rate'])\n",
    "            model_copy.load_state_dict(torch.load(baseline_model_path, map_location=DEVICE))\n",
    "            model_copy.to(DEVICE)\n",
    "\n",
    "            # Apply pruning\n",
    "            pruned_model = prune_model(\n",
    "                model_copy, strategy_config, sparsity_ratio,\n",
    "                example_input_device, ignored_layers\n",
    "            )\n",
    "\n",
    "            # Fine-tune pruned model\n",
    "            print(\"Fine-tuning pruned model...\")\n",
    "            optimizer_ft = optim.Adam(pruned_model.parameters(), lr=config['learning_rate'])\n",
    "            fine_tuned_model, ft_history = train_model(\n",
    "                pruned_model, train_loader, criterion, optimizer_ft, DEVICE,\n",
    "                config['epochs'], val_loader, config['patience'],\n",
    "                f\"{strategy_name}-{sparsity_ratio:.1%}\"\n",
    "            )\n",
    "\n",
    "            # Evaluate fine-tuned model\n",
    "            final_metrics = evaluate_model(fine_tuned_model, test_loader, example_input_device, criterion, DEVICE)\n",
    "            all_results[strategy_name][sparsity_ratio] = final_metrics\n",
    "\n",
    "            print(f\"Results: MSE={final_metrics['mse']:.2f}, \"\n",
    "                  f\"MAE={final_metrics['mae']:.2f}, \"\n",
    "                  f\"MACs={final_metrics['macs'] / 1e6:.2f}M\")\n",
    "\n",
    "            # Save fine-tuned model\n",
    "            model_filename = f\"{strategy_name.lower()}_sparsity_{sparsity_ratio:.1f}.pth\"\n",
    "            model_path = os.path.join(config['models_dir'], model_filename)\n",
    "            save_model(fine_tuned_model, model_path, example_input_cpu)\n",
    "\n",
    "    # Save and analyze results\n",
    "    print(\"\\nSaving results...\")\n",
    "    summary_df = save_results_to_files(all_results, config['output_dir'])\n",
    "\n",
    "    # Create plots\n",
    "    print(\"Creating plots...\")\n",
    "    create_results_plots(summary_df, config['output_dir'])\n",
    "\n",
    "    # Print summary\n",
    "    print_results_table(summary_df)\n",
    "\n",
    "    print(f\"\\n🎉 All experiments completed!\")\n",
    "    print(f\"📁 Results saved to: {os.path.abspath(config['output_dir'])}\")\n",
    "    print(f\"📁 Models saved to: {os.path.abspath(config['models_dir'])}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "1269cb9144cb0eae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting NASA MLP Pruning Experiments\n",
      "============================================================\n",
      "Loading NASA C-MAPSS dataset...\n",
      "Loading NASA C-MAPSS dataset from: ./data/CMaps\n",
      "--- Preparing Training Data ---\n",
      "\n",
      "--- Preparing Test Data ---\n",
      "Columns with std < 0.02 (potential removal): ['op_setting_1', 'op_setting_2', 'op_setting_3', 'sensor_1', 'sensor_5', 'sensor_6', 'sensor_10', 'sensor_16', 'sensor_18', 'sensor_19']\n",
      "\n",
      "Using Features: ['sensor_2', 'sensor_3', 'sensor_4', 'sensor_7', 'sensor_8', 'sensor_9', 'sensor_11', 'sensor_12', 'sensor_13', 'sensor_14', 'sensor_15', 'sensor_17', 'sensor_20', 'sensor_21']\n",
      "\n",
      "--- Normalizing Data ---\n",
      "DataLoaders created - Train: 12585, Val: 3146, Test: 100\n",
      "Input size: 700 (window_size * num_features)\n",
      "\n",
      "Creating and training baseline model...\n",
      "✅ Created MLP with architecture: 700 -> 256 -> 128 -> 64 -> 1\n",
      "Epoch 1/50 (Baseline Training): Train Loss: 6510.5294, Train MSE: 6521.05, Val Loss: 4226.9632, Val MSE: 4222.02 (Best)\n",
      "Epoch 2/50 (Baseline Training): Train Loss: 2321.3150, Train MSE: 2328.04, Val Loss: 1816.8863, Val MSE: 1816.65 (Best)\n",
      "Epoch 3/50 (Baseline Training): Train Loss: 1774.2484, Train MSE: 1776.93, Val Loss: 1473.6371, Val MSE: 1473.07 (Best)\n",
      "Epoch 4/50 (Baseline Training): Train Loss: 1350.8382, Train MSE: 1352.02, Val Loss: 1031.4851, Val MSE: 1030.49 (Best)\n",
      "Epoch 5/50 (Baseline Training): Train Loss: 958.3455, Train MSE: 959.85, Val Loss: 750.3492, Val MSE: 749.17 (Best)\n",
      "Epoch 6/50 (Baseline Training): Train Loss: 784.9726, Train MSE: 785.27, Val Loss: 663.1185, Val MSE: 662.40 (Best)\n",
      "Epoch 7/50 (Baseline Training): Train Loss: 727.8442, Train MSE: 728.02, Val Loss: 616.9794, Val MSE: 615.95 (Best)\n",
      "Epoch 8/50 (Baseline Training): Train Loss: 685.6024, Train MSE: 685.06, Val Loss: 575.7140, Val MSE: 574.55 (Best)\n",
      "Epoch 9/50 (Baseline Training): Train Loss: 639.4574, Train MSE: 639.85, Val Loss: 537.4684, Val MSE: 536.99 (Best)\n",
      "Epoch 10/50 (Baseline Training): Train Loss: 602.4390, Train MSE: 600.84, Val Loss: 499.8256, Val MSE: 498.79 (Best)\n",
      "Epoch 11/50 (Baseline Training): Train Loss: 565.9630, Train MSE: 565.08, Val Loss: 473.7290, Val MSE: 472.50 (Best)\n",
      "Epoch 12/50 (Baseline Training): Train Loss: 541.7096, Train MSE: 542.46, Val Loss: 447.1709, Val MSE: 445.90 (Best)\n",
      "Epoch 13/50 (Baseline Training): Train Loss: 519.6771, Train MSE: 520.24, Val Loss: 426.3481, Val MSE: 424.93 (Best)\n",
      "Epoch 14/50 (Baseline Training): Train Loss: 498.1819, Train MSE: 498.28, Val Loss: 413.5629, Val MSE: 411.85 (Best)\n",
      "Epoch 15/50 (Baseline Training): Train Loss: 485.3104, Train MSE: 485.46, Val Loss: 380.8767, Val MSE: 379.46 (Best)\n",
      "Epoch 16/50 (Baseline Training): Train Loss: 466.6261, Train MSE: 466.69, Val Loss: 360.5935, Val MSE: 359.12 (Best)\n",
      "Epoch 17/50 (Baseline Training): Train Loss: 437.1250, Train MSE: 436.91, Val Loss: 341.3526, Val MSE: 340.23 (Best)\n",
      "Epoch 18/50 (Baseline Training): Train Loss: 426.3993, Train MSE: 426.70, Val Loss: 320.7912, Val MSE: 319.56 (Best)\n",
      "Epoch 19/50 (Baseline Training): Train Loss: 405.2006, Train MSE: 405.37, Val Loss: 302.2801, Val MSE: 301.13 (Best)\n",
      "Epoch 20/50 (Baseline Training): Train Loss: 384.4466, Train MSE: 384.50, Val Loss: 284.6303, Val MSE: 283.60 (Best)\n",
      "Epoch 21/50 (Baseline Training): Train Loss: 358.3692, Train MSE: 358.16, Val Loss: 268.3954, Val MSE: 267.31 (Best)\n",
      "Epoch 22/50 (Baseline Training): Train Loss: 349.5200, Train MSE: 349.22, Val Loss: 253.8098, Val MSE: 252.82 (Best)\n",
      "Epoch 23/50 (Baseline Training): Train Loss: 331.5687, Train MSE: 331.46, Val Loss: 238.0730, Val MSE: 237.24 (Best)\n",
      "Epoch 24/50 (Baseline Training): Train Loss: 322.5527, Train MSE: 322.64, Val Loss: 225.1116, Val MSE: 224.25 (Best)\n",
      "Epoch 25/50 (Baseline Training): Train Loss: 304.3805, Train MSE: 304.25, Val Loss: 215.3109, Val MSE: 214.53 (Best)\n",
      "Epoch 26/50 (Baseline Training): Train Loss: 292.6685, Train MSE: 292.50, Val Loss: 219.3017, Val MSE: 218.37\n",
      "Epoch 27/50 (Baseline Training): Train Loss: 281.5874, Train MSE: 280.73, Val Loss: 194.5104, Val MSE: 194.04 (Best)\n",
      "Epoch 28/50 (Baseline Training): Train Loss: 276.8989, Train MSE: 276.52, Val Loss: 186.3455, Val MSE: 185.85 (Best)\n",
      "Epoch 29/50 (Baseline Training): Train Loss: 269.5480, Train MSE: 269.85, Val Loss: 182.2752, Val MSE: 181.82 (Best)\n",
      "Epoch 30/50 (Baseline Training): Train Loss: 264.6468, Train MSE: 265.34, Val Loss: 182.7581, Val MSE: 182.30\n",
      "Epoch 31/50 (Baseline Training): Train Loss: 257.1158, Train MSE: 257.43, Val Loss: 182.4384, Val MSE: 181.94\n",
      "Epoch 32/50 (Baseline Training): Train Loss: 257.2772, Train MSE: 257.04, Val Loss: 171.1298, Val MSE: 170.83 (Best)\n",
      "Epoch 33/50 (Baseline Training): Train Loss: 256.1399, Train MSE: 256.07, Val Loss: 167.6683, Val MSE: 167.53 (Best)\n",
      "Epoch 34/50 (Baseline Training): Train Loss: 253.1166, Train MSE: 252.09, Val Loss: 165.2098, Val MSE: 164.99 (Best)\n",
      "Epoch 35/50 (Baseline Training): Train Loss: 249.3103, Train MSE: 249.51, Val Loss: 164.6274, Val MSE: 164.45 (Best)\n",
      "Epoch 36/50 (Baseline Training): Train Loss: 253.7747, Train MSE: 254.48, Val Loss: 169.5225, Val MSE: 169.27\n",
      "Epoch 37/50 (Baseline Training): Train Loss: 248.0308, Train MSE: 247.62, Val Loss: 162.4432, Val MSE: 162.36 (Best)\n",
      "Epoch 38/50 (Baseline Training): Train Loss: 242.9370, Train MSE: 242.45, Val Loss: 165.0099, Val MSE: 164.85\n",
      "Epoch 39/50 (Baseline Training): Train Loss: 243.1885, Train MSE: 242.92, Val Loss: 171.0036, Val MSE: 170.79\n",
      "Epoch 40/50 (Baseline Training): Train Loss: 244.1578, Train MSE: 244.41, Val Loss: 172.9222, Val MSE: 172.67\n",
      "Epoch 41/50 (Baseline Training): Train Loss: 242.1192, Train MSE: 242.61, Val Loss: 159.0918, Val MSE: 159.08 (Best)\n",
      "Epoch 42/50 (Baseline Training): Train Loss: 241.9631, Train MSE: 242.00, Val Loss: 157.3767, Val MSE: 157.44 (Best)\n",
      "Epoch 43/50 (Baseline Training): Train Loss: 243.3425, Train MSE: 243.65, Val Loss: 167.7323, Val MSE: 167.58\n",
      "Epoch 44/50 (Baseline Training): Train Loss: 239.3258, Train MSE: 239.24, Val Loss: 156.2050, Val MSE: 156.24 (Best)\n",
      "Epoch 45/50 (Baseline Training): Train Loss: 233.2037, Train MSE: 232.70, Val Loss: 157.8547, Val MSE: 157.86\n",
      "Epoch 46/50 (Baseline Training): Train Loss: 243.4904, Train MSE: 243.54, Val Loss: 164.4827, Val MSE: 164.40\n",
      "Epoch 47/50 (Baseline Training): Train Loss: 242.3960, Train MSE: 242.43, Val Loss: 159.9964, Val MSE: 160.00\n",
      "Epoch 48/50 (Baseline Training): Train Loss: 237.8022, Train MSE: 238.01, Val Loss: 156.0019, Val MSE: 156.02 (Best)\n",
      "Epoch 49/50 (Baseline Training): Train Loss: 236.1820, Train MSE: 235.82, Val Loss: 155.2154, Val MSE: 155.24 (Best)\n",
      "Epoch 50/50 (Baseline Training): Train Loss: 235.6161, Train MSE: 235.78, Val Loss: 153.4536, Val MSE: 153.59 (Best)\n",
      "Loaded best model state\n",
      "✅ Model saved to ./models_mlp_nasa/baseline_model.pth\n",
      "✅ ONNX model saved to ./models_mlp_nasa/baseline_model.onnx\n",
      "\n",
      "Evaluating baseline model...\n",
      "Baseline Results: MSE=526.92, MAE=14.07, MACs=0.22M, Params=0.22M\n",
      "\n",
      "Starting pruning experiments...\n",
      "\n",
      "--- Strategy: MagnitudeL2 ---\n",
      "\n",
      "Processing MagnitudeL2 at 20.0% sparsity...\n",
      "✅ Created MLP with architecture: 700 -> 256 -> 128 -> 64 -> 1\n",
      "Initial MACs: 0.22M\n",
      "Applying MagnitudeImportance pruning at 20.0% sparsity...\n",
      "Final MACs: 0.21M (Reduction: 5.1%)\n",
      "Fine-tuning pruned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch_pruning/pruner/algorithms/base_pruner.py:87: UserWarning: ch_sparsity is deprecated in v1.3.0. Please use pruning_ratio.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 (MagnitudeL2-20.0%): Train Loss: 239.4067, Train MSE: 239.09, Val Loss: 153.2818, Val MSE: 153.42 (Best)\n",
      "Epoch 2/50 (MagnitudeL2-20.0%): Train Loss: 238.0898, Train MSE: 238.00, Val Loss: 153.4911, Val MSE: 153.67\n",
      "Epoch 3/50 (MagnitudeL2-20.0%): Train Loss: 237.7995, Train MSE: 238.05, Val Loss: 152.3730, Val MSE: 152.54 (Best)\n",
      "Epoch 4/50 (MagnitudeL2-20.0%): Train Loss: 234.8743, Train MSE: 235.62, Val Loss: 159.2736, Val MSE: 159.25\n",
      "Epoch 5/50 (MagnitudeL2-20.0%): Train Loss: 229.7228, Train MSE: 229.62, Val Loss: 153.2949, Val MSE: 153.38\n",
      "Epoch 6/50 (MagnitudeL2-20.0%): Train Loss: 232.5620, Train MSE: 232.78, Val Loss: 150.6818, Val MSE: 150.82 (Best)\n",
      "Epoch 7/50 (MagnitudeL2-20.0%): Train Loss: 232.1248, Train MSE: 232.12, Val Loss: 150.5413, Val MSE: 150.66 (Best)\n",
      "Epoch 8/50 (MagnitudeL2-20.0%): Train Loss: 230.8470, Train MSE: 230.90, Val Loss: 152.8352, Val MSE: 152.91\n",
      "Epoch 9/50 (MagnitudeL2-20.0%): Train Loss: 235.4769, Train MSE: 235.05, Val Loss: 150.2495, Val MSE: 150.39 (Best)\n",
      "Epoch 10/50 (MagnitudeL2-20.0%): Train Loss: 227.6295, Train MSE: 228.33, Val Loss: 162.8003, Val MSE: 162.80\n",
      "Epoch 11/50 (MagnitudeL2-20.0%): Train Loss: 231.6830, Train MSE: 231.12, Val Loss: 154.0634, Val MSE: 154.12\n",
      "Epoch 12/50 (MagnitudeL2-20.0%): Train Loss: 226.9984, Train MSE: 227.26, Val Loss: 148.4451, Val MSE: 148.63 (Best)\n",
      "Epoch 13/50 (MagnitudeL2-20.0%): Train Loss: 226.8025, Train MSE: 225.66, Val Loss: 149.3042, Val MSE: 149.51\n",
      "Epoch 14/50 (MagnitudeL2-20.0%): Train Loss: 234.6761, Train MSE: 235.25, Val Loss: 148.9337, Val MSE: 149.15\n",
      "Epoch 15/50 (MagnitudeL2-20.0%): Train Loss: 223.0043, Train MSE: 223.51, Val Loss: 149.0572, Val MSE: 149.20\n",
      "Epoch 16/50 (MagnitudeL2-20.0%): Train Loss: 229.4396, Train MSE: 229.54, Val Loss: 147.5602, Val MSE: 147.74 (Best)\n",
      "Epoch 17/50 (MagnitudeL2-20.0%): Train Loss: 233.5020, Train MSE: 233.81, Val Loss: 147.7610, Val MSE: 147.93\n",
      "Epoch 18/50 (MagnitudeL2-20.0%): Train Loss: 227.8089, Train MSE: 227.85, Val Loss: 148.1521, Val MSE: 148.32\n",
      "Epoch 19/50 (MagnitudeL2-20.0%): Train Loss: 230.7742, Train MSE: 229.63, Val Loss: 167.9715, Val MSE: 167.94\n",
      "Epoch 20/50 (MagnitudeL2-20.0%): Train Loss: 228.9090, Train MSE: 228.10, Val Loss: 149.6454, Val MSE: 149.76\n",
      "Epoch 21/50 (MagnitudeL2-20.0%): Train Loss: 228.2539, Train MSE: 228.46, Val Loss: 148.5059, Val MSE: 148.65\n",
      "Epoch 22/50 (MagnitudeL2-20.0%): Train Loss: 226.0245, Train MSE: 226.06, Val Loss: 146.2066, Val MSE: 146.41 (Best)\n",
      "Epoch 23/50 (MagnitudeL2-20.0%): Train Loss: 223.6840, Train MSE: 224.05, Val Loss: 161.3783, Val MSE: 161.41\n",
      "Epoch 24/50 (MagnitudeL2-20.0%): Train Loss: 226.0669, Train MSE: 226.32, Val Loss: 146.4773, Val MSE: 146.71\n",
      "Epoch 25/50 (MagnitudeL2-20.0%): Train Loss: 230.7016, Train MSE: 230.48, Val Loss: 146.5835, Val MSE: 146.82\n",
      "Epoch 26/50 (MagnitudeL2-20.0%): Train Loss: 230.4407, Train MSE: 230.00, Val Loss: 149.1882, Val MSE: 149.48\n",
      "Epoch 27/50 (MagnitudeL2-20.0%): Train Loss: 222.8059, Train MSE: 223.03, Val Loss: 145.7828, Val MSE: 145.95 (Best)\n",
      "Epoch 28/50 (MagnitudeL2-20.0%): Train Loss: 220.0499, Train MSE: 220.28, Val Loss: 145.9243, Val MSE: 146.06\n",
      "Epoch 29/50 (MagnitudeL2-20.0%): Train Loss: 224.8178, Train MSE: 225.13, Val Loss: 145.7403, Val MSE: 145.91 (Best)\n",
      "Epoch 30/50 (MagnitudeL2-20.0%): Train Loss: 225.4976, Train MSE: 225.74, Val Loss: 148.8140, Val MSE: 148.95\n",
      "Epoch 31/50 (MagnitudeL2-20.0%): Train Loss: 220.6437, Train MSE: 220.77, Val Loss: 161.7581, Val MSE: 161.81\n",
      "Epoch 32/50 (MagnitudeL2-20.0%): Train Loss: 227.2787, Train MSE: 227.69, Val Loss: 147.5521, Val MSE: 147.72\n",
      "Epoch 33/50 (MagnitudeL2-20.0%): Train Loss: 225.1196, Train MSE: 225.37, Val Loss: 145.0534, Val MSE: 145.28 (Best)\n",
      "Epoch 34/50 (MagnitudeL2-20.0%): Train Loss: 224.1241, Train MSE: 223.89, Val Loss: 145.3281, Val MSE: 145.51\n",
      "Epoch 35/50 (MagnitudeL2-20.0%): Train Loss: 219.6387, Train MSE: 220.20, Val Loss: 148.5743, Val MSE: 148.70\n",
      "Epoch 36/50 (MagnitudeL2-20.0%): Train Loss: 222.6641, Train MSE: 222.57, Val Loss: 153.7528, Val MSE: 153.84\n",
      "Epoch 37/50 (MagnitudeL2-20.0%): Train Loss: 220.4090, Train MSE: 220.52, Val Loss: 143.6340, Val MSE: 143.83 (Best)\n",
      "Epoch 38/50 (MagnitudeL2-20.0%): Train Loss: 217.5991, Train MSE: 218.29, Val Loss: 152.4060, Val MSE: 152.50\n",
      "Epoch 39/50 (MagnitudeL2-20.0%): Train Loss: 221.9823, Train MSE: 221.62, Val Loss: 143.9264, Val MSE: 144.18\n",
      "Epoch 40/50 (MagnitudeL2-20.0%): Train Loss: 217.3302, Train MSE: 217.24, Val Loss: 152.3255, Val MSE: 152.45\n",
      "Epoch 41/50 (MagnitudeL2-20.0%): Train Loss: 222.9603, Train MSE: 223.37, Val Loss: 150.3632, Val MSE: 150.49\n",
      "Epoch 42/50 (MagnitudeL2-20.0%): Train Loss: 220.8438, Train MSE: 221.26, Val Loss: 143.8737, Val MSE: 144.12\n",
      "Epoch 43/50 (MagnitudeL2-20.0%): Train Loss: 219.8069, Train MSE: 220.41, Val Loss: 149.7518, Val MSE: 149.88\n",
      "Epoch 44/50 (MagnitudeL2-20.0%): Train Loss: 217.8467, Train MSE: 217.61, Val Loss: 144.0668, Val MSE: 144.27\n",
      "Epoch 45/50 (MagnitudeL2-20.0%): Train Loss: 221.1087, Train MSE: 220.58, Val Loss: 148.0937, Val MSE: 148.21\n",
      "Epoch 46/50 (MagnitudeL2-20.0%): Train Loss: 216.9081, Train MSE: 217.21, Val Loss: 142.5112, Val MSE: 142.73 (Best)\n",
      "Epoch 47/50 (MagnitudeL2-20.0%): Train Loss: 217.8269, Train MSE: 218.12, Val Loss: 146.9372, Val MSE: 147.09\n",
      "Epoch 48/50 (MagnitudeL2-20.0%): Train Loss: 214.8769, Train MSE: 214.30, Val Loss: 145.7694, Val MSE: 145.92\n",
      "Epoch 49/50 (MagnitudeL2-20.0%): Train Loss: 216.3102, Train MSE: 216.91, Val Loss: 142.2817, Val MSE: 142.52 (Best)\n",
      "Epoch 50/50 (MagnitudeL2-20.0%): Train Loss: 221.0250, Train MSE: 221.14, Val Loss: 142.8700, Val MSE: 143.08\n",
      "Loaded best model state\n",
      "Results: MSE=500.93, MAE=13.59, MACs=0.21M\n",
      "✅ Model saved to ./models_mlp_nasa/magnitudel2_sparsity_0.2.pth\n",
      "✅ ONNX model saved to ./models_mlp_nasa/magnitudel2_sparsity_0.2.onnx\n",
      "\n",
      "Processing MagnitudeL2 at 50.0% sparsity...\n",
      "✅ Created MLP with architecture: 700 -> 256 -> 128 -> 64 -> 1\n",
      "Initial MACs: 0.22M\n",
      "Applying MagnitudeImportance pruning at 50.0% sparsity...\n",
      "Final MACs: 0.19M (Reduction: 11.9%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/50 (MagnitudeL2-50.0%): Train Loss: 239.8197, Train MSE: 239.97, Val Loss: 157.7751, Val MSE: 157.78 (Best)\n",
      "Epoch 2/50 (MagnitudeL2-50.0%): Train Loss: 235.1599, Train MSE: 235.30, Val Loss: 152.6021, Val MSE: 152.71 (Best)\n",
      "Epoch 3/50 (MagnitudeL2-50.0%): Train Loss: 236.4841, Train MSE: 236.38, Val Loss: 173.5911, Val MSE: 173.50\n",
      "Epoch 4/50 (MagnitudeL2-50.0%): Train Loss: 234.1794, Train MSE: 234.35, Val Loss: 177.0425, Val MSE: 176.92\n",
      "Epoch 5/50 (MagnitudeL2-50.0%): Train Loss: 233.3175, Train MSE: 233.96, Val Loss: 151.1071, Val MSE: 151.23 (Best)\n",
      "Epoch 6/50 (MagnitudeL2-50.0%): Train Loss: 236.2872, Train MSE: 236.36, Val Loss: 179.7679, Val MSE: 179.65\n",
      "Epoch 7/50 (MagnitudeL2-50.0%): Train Loss: 232.5974, Train MSE: 232.49, Val Loss: 157.6248, Val MSE: 157.63\n",
      "Epoch 8/50 (MagnitudeL2-50.0%): Train Loss: 232.1343, Train MSE: 232.46, Val Loss: 152.7837, Val MSE: 153.02\n",
      "Epoch 9/50 (MagnitudeL2-50.0%): Train Loss: 231.5944, Train MSE: 231.76, Val Loss: 156.2520, Val MSE: 156.29\n",
      "Epoch 10/50 (MagnitudeL2-50.0%): Train Loss: 228.9328, Train MSE: 228.75, Val Loss: 151.1250, Val MSE: 151.23\n",
      "Epoch 11/50 (MagnitudeL2-50.0%): Train Loss: 228.7206, Train MSE: 229.22, Val Loss: 149.6057, Val MSE: 149.78 (Best)\n",
      "Epoch 12/50 (MagnitudeL2-50.0%): Train Loss: 231.2049, Train MSE: 230.57, Val Loss: 149.5237, Val MSE: 149.67 (Best)\n",
      "Epoch 13/50 (MagnitudeL2-50.0%): Train Loss: 230.6712, Train MSE: 230.78, Val Loss: 151.8935, Val MSE: 151.98\n",
      "Epoch 14/50 (MagnitudeL2-50.0%): Train Loss: 234.8839, Train MSE: 234.88, Val Loss: 149.7376, Val MSE: 149.94\n",
      "Epoch 15/50 (MagnitudeL2-50.0%): Train Loss: 232.7741, Train MSE: 232.51, Val Loss: 148.2951, Val MSE: 148.46 (Best)\n",
      "Epoch 16/50 (MagnitudeL2-50.0%): Train Loss: 227.0927, Train MSE: 227.47, Val Loss: 149.2052, Val MSE: 149.38\n",
      "Epoch 17/50 (MagnitudeL2-50.0%): Train Loss: 230.0772, Train MSE: 230.50, Val Loss: 148.4415, Val MSE: 148.60\n",
      "Epoch 18/50 (MagnitudeL2-50.0%): Train Loss: 226.6797, Train MSE: 226.52, Val Loss: 149.9357, Val MSE: 150.04\n",
      "Epoch 19/50 (MagnitudeL2-50.0%): Train Loss: 226.7201, Train MSE: 226.43, Val Loss: 159.0311, Val MSE: 159.09\n",
      "Epoch 20/50 (MagnitudeL2-50.0%): Train Loss: 225.3743, Train MSE: 224.69, Val Loss: 156.3707, Val MSE: 156.41\n",
      "Epoch 21/50 (MagnitudeL2-50.0%): Train Loss: 226.2759, Train MSE: 225.72, Val Loss: 154.5820, Val MSE: 154.61\n",
      "Epoch 22/50 (MagnitudeL2-50.0%): Train Loss: 227.4608, Train MSE: 227.32, Val Loss: 149.1161, Val MSE: 149.38\n",
      "Epoch 23/50 (MagnitudeL2-50.0%): Train Loss: 230.3946, Train MSE: 230.72, Val Loss: 153.2143, Val MSE: 153.28\n",
      "Epoch 24/50 (MagnitudeL2-50.0%): Train Loss: 227.7764, Train MSE: 227.83, Val Loss: 146.7281, Val MSE: 146.92 (Best)\n",
      "Epoch 25/50 (MagnitudeL2-50.0%): Train Loss: 222.0851, Train MSE: 221.78, Val Loss: 148.9174, Val MSE: 149.00\n",
      "Epoch 26/50 (MagnitudeL2-50.0%): Train Loss: 224.5800, Train MSE: 225.00, Val Loss: 146.1640, Val MSE: 146.35 (Best)\n",
      "Epoch 27/50 (MagnitudeL2-50.0%): Train Loss: 224.2835, Train MSE: 224.88, Val Loss: 148.6867, Val MSE: 148.79\n",
      "Epoch 28/50 (MagnitudeL2-50.0%): Train Loss: 225.3711, Train MSE: 225.64, Val Loss: 149.4877, Val MSE: 149.60\n",
      "Epoch 29/50 (MagnitudeL2-50.0%): Train Loss: 220.7096, Train MSE: 221.13, Val Loss: 145.3859, Val MSE: 145.54 (Best)\n",
      "Epoch 30/50 (MagnitudeL2-50.0%): Train Loss: 222.8618, Train MSE: 223.17, Val Loss: 145.9829, Val MSE: 146.14\n",
      "Epoch 31/50 (MagnitudeL2-50.0%): Train Loss: 227.7234, Train MSE: 226.72, Val Loss: 153.3274, Val MSE: 153.41\n",
      "Epoch 32/50 (MagnitudeL2-50.0%): Train Loss: 225.6415, Train MSE: 225.76, Val Loss: 147.3473, Val MSE: 147.46\n",
      "Epoch 33/50 (MagnitudeL2-50.0%): Train Loss: 223.5001, Train MSE: 223.63, Val Loss: 146.3888, Val MSE: 146.52\n",
      "Epoch 34/50 (MagnitudeL2-50.0%): Train Loss: 218.4435, Train MSE: 218.35, Val Loss: 145.3979, Val MSE: 145.64\n",
      "Epoch 35/50 (MagnitudeL2-50.0%): Train Loss: 221.4254, Train MSE: 221.97, Val Loss: 146.4655, Val MSE: 146.63\n",
      "Epoch 36/50 (MagnitudeL2-50.0%): Train Loss: 219.9542, Train MSE: 220.17, Val Loss: 150.4750, Val MSE: 150.55\n",
      "Epoch 37/50 (MagnitudeL2-50.0%): Train Loss: 225.3504, Train MSE: 225.33, Val Loss: 144.6663, Val MSE: 144.84 (Best)\n",
      "Epoch 38/50 (MagnitudeL2-50.0%): Train Loss: 221.7858, Train MSE: 221.76, Val Loss: 145.2020, Val MSE: 145.34\n",
      "Epoch 39/50 (MagnitudeL2-50.0%): Train Loss: 220.9599, Train MSE: 221.33, Val Loss: 145.0341, Val MSE: 145.20\n",
      "Epoch 40/50 (MagnitudeL2-50.0%): Train Loss: 219.8562, Train MSE: 218.93, Val Loss: 143.5139, Val MSE: 143.73 (Best)\n",
      "Epoch 41/50 (MagnitudeL2-50.0%): Train Loss: 221.8374, Train MSE: 222.43, Val Loss: 143.9931, Val MSE: 144.17\n",
      "Epoch 42/50 (MagnitudeL2-50.0%): Train Loss: 221.5545, Train MSE: 221.73, Val Loss: 157.0085, Val MSE: 157.08\n",
      "Epoch 43/50 (MagnitudeL2-50.0%): Train Loss: 221.4465, Train MSE: 221.76, Val Loss: 143.7899, Val MSE: 144.04\n",
      "Epoch 44/50 (MagnitudeL2-50.0%): Train Loss: 217.9387, Train MSE: 218.46, Val Loss: 146.6358, Val MSE: 146.80\n",
      "Epoch 45/50 (MagnitudeL2-50.0%): Train Loss: 220.1108, Train MSE: 220.08, Val Loss: 143.5068, Val MSE: 143.68 (Best)\n",
      "Epoch 46/50 (MagnitudeL2-50.0%): Train Loss: 219.7994, Train MSE: 220.19, Val Loss: 145.1851, Val MSE: 145.36\n",
      "Epoch 47/50 (MagnitudeL2-50.0%): Train Loss: 223.1625, Train MSE: 223.34, Val Loss: 154.9445, Val MSE: 155.04\n",
      "Epoch 48/50 (MagnitudeL2-50.0%): Train Loss: 218.5494, Train MSE: 218.74, Val Loss: 148.0500, Val MSE: 148.17\n",
      "Epoch 49/50 (MagnitudeL2-50.0%): Train Loss: 220.6028, Train MSE: 219.10, Val Loss: 142.7235, Val MSE: 142.90 (Best)\n",
      "Epoch 50/50 (MagnitudeL2-50.0%): Train Loss: 224.2132, Train MSE: 224.38, Val Loss: 142.6920, Val MSE: 142.89 (Best)\n",
      "Loaded best model state\n",
      "Results: MSE=507.94, MAE=13.65, MACs=0.19M\n",
      "✅ Model saved to ./models_mlp_nasa/magnitudel2_sparsity_0.5.pth\n",
      "✅ ONNX model saved to ./models_mlp_nasa/magnitudel2_sparsity_0.5.onnx\n",
      "\n",
      "Processing MagnitudeL2 at 70.0% sparsity...\n",
      "✅ Created MLP with architecture: 700 -> 256 -> 128 -> 64 -> 1\n",
      "Initial MACs: 0.22M\n",
      "Applying MagnitudeImportance pruning at 70.0% sparsity...\n",
      "Final MACs: 0.19M (Reduction: 16.3%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/50 (MagnitudeL2-70.0%): Train Loss: 239.3500, Train MSE: 239.43, Val Loss: 157.1814, Val MSE: 157.22 (Best)\n",
      "Epoch 2/50 (MagnitudeL2-70.0%): Train Loss: 235.1963, Train MSE: 235.42, Val Loss: 152.7033, Val MSE: 152.80 (Best)\n",
      "Epoch 3/50 (MagnitudeL2-70.0%): Train Loss: 234.5755, Train MSE: 234.71, Val Loss: 157.3817, Val MSE: 157.42\n",
      "Epoch 4/50 (MagnitudeL2-70.0%): Train Loss: 236.8574, Train MSE: 236.77, Val Loss: 155.4804, Val MSE: 155.50\n",
      "Epoch 5/50 (MagnitudeL2-70.0%): Train Loss: 235.2190, Train MSE: 235.04, Val Loss: 151.8765, Val MSE: 152.00 (Best)\n",
      "Epoch 6/50 (MagnitudeL2-70.0%): Train Loss: 237.9870, Train MSE: 238.17, Val Loss: 155.9997, Val MSE: 156.24\n",
      "Epoch 7/50 (MagnitudeL2-70.0%): Train Loss: 233.9269, Train MSE: 233.95, Val Loss: 151.9720, Val MSE: 152.02\n",
      "Epoch 8/50 (MagnitudeL2-70.0%): Train Loss: 229.3845, Train MSE: 229.34, Val Loss: 152.9156, Val MSE: 152.99\n",
      "Epoch 9/50 (MagnitudeL2-70.0%): Train Loss: 231.6336, Train MSE: 232.24, Val Loss: 152.1215, Val MSE: 152.18\n",
      "Epoch 10/50 (MagnitudeL2-70.0%): Train Loss: 231.5904, Train MSE: 231.98, Val Loss: 161.1908, Val MSE: 161.17\n",
      "Epoch 11/50 (MagnitudeL2-70.0%): Train Loss: 230.8513, Train MSE: 229.94, Val Loss: 150.0074, Val MSE: 150.16 (Best)\n",
      "Epoch 12/50 (MagnitudeL2-70.0%): Train Loss: 234.7390, Train MSE: 235.39, Val Loss: 151.1947, Val MSE: 151.29\n",
      "Epoch 13/50 (MagnitudeL2-70.0%): Train Loss: 230.5568, Train MSE: 230.90, Val Loss: 152.6788, Val MSE: 152.72\n",
      "Epoch 14/50 (MagnitudeL2-70.0%): Train Loss: 227.3906, Train MSE: 227.49, Val Loss: 162.4954, Val MSE: 162.46\n",
      "Epoch 15/50 (MagnitudeL2-70.0%): Train Loss: 230.7532, Train MSE: 230.84, Val Loss: 152.1905, Val MSE: 152.30\n",
      "Epoch 16/50 (MagnitudeL2-70.0%): Train Loss: 225.7563, Train MSE: 226.22, Val Loss: 148.7909, Val MSE: 148.96 (Best)\n",
      "Epoch 17/50 (MagnitudeL2-70.0%): Train Loss: 229.6303, Train MSE: 229.06, Val Loss: 151.0096, Val MSE: 151.29\n",
      "Epoch 18/50 (MagnitudeL2-70.0%): Train Loss: 227.3242, Train MSE: 227.17, Val Loss: 149.2532, Val MSE: 149.36\n",
      "Epoch 19/50 (MagnitudeL2-70.0%): Train Loss: 227.1228, Train MSE: 226.53, Val Loss: 157.0328, Val MSE: 157.06\n",
      "Epoch 20/50 (MagnitudeL2-70.0%): Train Loss: 227.3233, Train MSE: 226.49, Val Loss: 150.4094, Val MSE: 150.66\n",
      "Epoch 21/50 (MagnitudeL2-70.0%): Train Loss: 223.5828, Train MSE: 223.43, Val Loss: 155.8805, Val MSE: 155.91\n",
      "Epoch 22/50 (MagnitudeL2-70.0%): Train Loss: 225.2535, Train MSE: 225.53, Val Loss: 147.4014, Val MSE: 147.53 (Best)\n",
      "Epoch 23/50 (MagnitudeL2-70.0%): Train Loss: 230.3026, Train MSE: 230.45, Val Loss: 146.8674, Val MSE: 147.06 (Best)\n",
      "Epoch 24/50 (MagnitudeL2-70.0%): Train Loss: 229.4169, Train MSE: 229.42, Val Loss: 147.9842, Val MSE: 148.19\n",
      "Epoch 25/50 (MagnitudeL2-70.0%): Train Loss: 222.5435, Train MSE: 223.21, Val Loss: 153.2064, Val MSE: 153.32\n",
      "Epoch 26/50 (MagnitudeL2-70.0%): Train Loss: 224.7137, Train MSE: 224.96, Val Loss: 163.8473, Val MSE: 163.86\n",
      "Epoch 27/50 (MagnitudeL2-70.0%): Train Loss: 226.4111, Train MSE: 226.87, Val Loss: 149.2111, Val MSE: 149.51\n",
      "Epoch 28/50 (MagnitudeL2-70.0%): Train Loss: 222.7000, Train MSE: 222.39, Val Loss: 148.9716, Val MSE: 149.08\n",
      "Epoch 29/50 (MagnitudeL2-70.0%): Train Loss: 226.5090, Train MSE: 226.28, Val Loss: 145.5145, Val MSE: 145.74 (Best)\n",
      "Epoch 30/50 (MagnitudeL2-70.0%): Train Loss: 225.6971, Train MSE: 226.13, Val Loss: 148.1703, Val MSE: 148.28\n",
      "Epoch 31/50 (MagnitudeL2-70.0%): Train Loss: 222.2246, Train MSE: 222.06, Val Loss: 145.1029, Val MSE: 145.26 (Best)\n",
      "Epoch 32/50 (MagnitudeL2-70.0%): Train Loss: 222.9348, Train MSE: 223.12, Val Loss: 145.0056, Val MSE: 145.17 (Best)\n",
      "Epoch 33/50 (MagnitudeL2-70.0%): Train Loss: 224.9206, Train MSE: 225.05, Val Loss: 145.8376, Val MSE: 146.01\n",
      "Epoch 34/50 (MagnitudeL2-70.0%): Train Loss: 227.8165, Train MSE: 227.46, Val Loss: 146.3998, Val MSE: 146.57\n",
      "Epoch 35/50 (MagnitudeL2-70.0%): Train Loss: 220.6505, Train MSE: 220.75, Val Loss: 148.2392, Val MSE: 148.39\n",
      "Epoch 36/50 (MagnitudeL2-70.0%): Train Loss: 223.5513, Train MSE: 223.72, Val Loss: 153.9830, Val MSE: 154.30\n",
      "Epoch 37/50 (MagnitudeL2-70.0%): Train Loss: 227.4349, Train MSE: 227.81, Val Loss: 144.9725, Val MSE: 145.13 (Best)\n",
      "Epoch 38/50 (MagnitudeL2-70.0%): Train Loss: 222.6713, Train MSE: 222.27, Val Loss: 144.2724, Val MSE: 144.52 (Best)\n",
      "Epoch 39/50 (MagnitudeL2-70.0%): Train Loss: 222.7023, Train MSE: 222.36, Val Loss: 147.8521, Val MSE: 147.99\n",
      "Epoch 40/50 (MagnitudeL2-70.0%): Train Loss: 220.1555, Train MSE: 220.23, Val Loss: 155.9806, Val MSE: 156.06\n",
      "Epoch 41/50 (MagnitudeL2-70.0%): Train Loss: 223.5229, Train MSE: 223.42, Val Loss: 143.6433, Val MSE: 143.83 (Best)\n",
      "Epoch 42/50 (MagnitudeL2-70.0%): Train Loss: 218.5933, Train MSE: 218.02, Val Loss: 143.8610, Val MSE: 144.06\n",
      "Epoch 43/50 (MagnitudeL2-70.0%): Train Loss: 221.4030, Train MSE: 221.07, Val Loss: 148.4468, Val MSE: 148.57\n",
      "Epoch 44/50 (MagnitudeL2-70.0%): Train Loss: 220.1936, Train MSE: 220.70, Val Loss: 145.1039, Val MSE: 145.30\n",
      "Epoch 45/50 (MagnitudeL2-70.0%): Train Loss: 216.9926, Train MSE: 217.41, Val Loss: 142.7696, Val MSE: 142.98 (Best)\n",
      "Epoch 46/50 (MagnitudeL2-70.0%): Train Loss: 215.7189, Train MSE: 215.36, Val Loss: 145.1259, Val MSE: 145.28\n",
      "Epoch 47/50 (MagnitudeL2-70.0%): Train Loss: 220.5069, Train MSE: 220.91, Val Loss: 151.0015, Val MSE: 151.11\n",
      "Epoch 48/50 (MagnitudeL2-70.0%): Train Loss: 217.8928, Train MSE: 218.17, Val Loss: 143.4183, Val MSE: 143.65\n",
      "Epoch 49/50 (MagnitudeL2-70.0%): Train Loss: 219.4991, Train MSE: 219.38, Val Loss: 144.5946, Val MSE: 144.76\n",
      "Epoch 50/50 (MagnitudeL2-70.0%): Train Loss: 222.9157, Train MSE: 222.72, Val Loss: 146.3922, Val MSE: 146.52\n",
      "Loaded best model state\n",
      "Results: MSE=507.88, MAE=13.65, MACs=0.19M\n",
      "✅ Model saved to ./models_mlp_nasa/magnitudel2_sparsity_0.7.pth\n",
      "✅ ONNX model saved to ./models_mlp_nasa/magnitudel2_sparsity_0.7.onnx\n",
      "\n",
      "--- Strategy: Random ---\n",
      "\n",
      "Processing Random at 20.0% sparsity...\n",
      "✅ Created MLP with architecture: 700 -> 256 -> 128 -> 64 -> 1\n",
      "Initial MACs: 0.22M\n",
      "Applying RandomImportance pruning at 20.0% sparsity...\n",
      "Final MACs: 0.21M (Reduction: 5.1%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/50 (Random-20.0%): Train Loss: 244.6826, Train MSE: 244.70, Val Loss: 159.0725, Val MSE: 158.99 (Best)\n",
      "Epoch 2/50 (Random-20.0%): Train Loss: 241.8953, Train MSE: 242.27, Val Loss: 153.8794, Val MSE: 153.96 (Best)\n",
      "Epoch 3/50 (Random-20.0%): Train Loss: 232.8578, Train MSE: 233.01, Val Loss: 164.5913, Val MSE: 164.45\n",
      "Epoch 4/50 (Random-20.0%): Train Loss: 239.5343, Train MSE: 239.68, Val Loss: 152.2824, Val MSE: 152.37 (Best)\n",
      "Epoch 5/50 (Random-20.0%): Train Loss: 238.1797, Train MSE: 238.36, Val Loss: 156.6736, Val MSE: 156.69\n",
      "Epoch 6/50 (Random-20.0%): Train Loss: 236.7424, Train MSE: 235.98, Val Loss: 153.2176, Val MSE: 153.28\n",
      "Epoch 7/50 (Random-20.0%): Train Loss: 238.4534, Train MSE: 238.31, Val Loss: 153.9591, Val MSE: 154.00\n",
      "Epoch 8/50 (Random-20.0%): Train Loss: 241.5589, Train MSE: 241.63, Val Loss: 152.7519, Val MSE: 152.94\n",
      "Epoch 9/50 (Random-20.0%): Train Loss: 237.3429, Train MSE: 237.87, Val Loss: 157.4175, Val MSE: 157.47\n",
      "Epoch 10/50 (Random-20.0%): Train Loss: 230.1887, Train MSE: 230.72, Val Loss: 150.2878, Val MSE: 150.40 (Best)\n",
      "Epoch 11/50 (Random-20.0%): Train Loss: 237.1961, Train MSE: 237.71, Val Loss: 150.1033, Val MSE: 150.27 (Best)\n",
      "Epoch 12/50 (Random-20.0%): Train Loss: 233.4033, Train MSE: 233.46, Val Loss: 150.1096, Val MSE: 150.22\n",
      "Epoch 13/50 (Random-20.0%): Train Loss: 236.6900, Train MSE: 237.18, Val Loss: 154.6230, Val MSE: 154.67\n",
      "Epoch 14/50 (Random-20.0%): Train Loss: 231.2061, Train MSE: 231.75, Val Loss: 148.7403, Val MSE: 148.90 (Best)\n",
      "Epoch 15/50 (Random-20.0%): Train Loss: 234.0308, Train MSE: 234.30, Val Loss: 149.9125, Val MSE: 150.05\n",
      "Epoch 16/50 (Random-20.0%): Train Loss: 230.7031, Train MSE: 231.25, Val Loss: 148.4478, Val MSE: 148.60 (Best)\n",
      "Epoch 17/50 (Random-20.0%): Train Loss: 233.3425, Train MSE: 233.92, Val Loss: 149.5276, Val MSE: 149.63\n",
      "Epoch 18/50 (Random-20.0%): Train Loss: 229.4301, Train MSE: 229.70, Val Loss: 150.8419, Val MSE: 150.93\n",
      "Epoch 19/50 (Random-20.0%): Train Loss: 230.1416, Train MSE: 230.24, Val Loss: 157.6192, Val MSE: 157.66\n",
      "Epoch 20/50 (Random-20.0%): Train Loss: 228.9838, Train MSE: 228.91, Val Loss: 148.4971, Val MSE: 148.64\n",
      "Epoch 21/50 (Random-20.0%): Train Loss: 227.0470, Train MSE: 227.35, Val Loss: 149.9128, Val MSE: 150.02\n",
      "Epoch 22/50 (Random-20.0%): Train Loss: 232.6595, Train MSE: 233.03, Val Loss: 148.1755, Val MSE: 148.32 (Best)\n",
      "Epoch 23/50 (Random-20.0%): Train Loss: 231.0615, Train MSE: 231.38, Val Loss: 147.1572, Val MSE: 147.30 (Best)\n",
      "Epoch 24/50 (Random-20.0%): Train Loss: 232.1339, Train MSE: 231.81, Val Loss: 157.0226, Val MSE: 157.08\n",
      "Epoch 25/50 (Random-20.0%): Train Loss: 235.9006, Train MSE: 236.27, Val Loss: 164.3286, Val MSE: 164.32\n",
      "Epoch 26/50 (Random-20.0%): Train Loss: 227.7885, Train MSE: 228.07, Val Loss: 152.9930, Val MSE: 153.06\n",
      "Epoch 27/50 (Random-20.0%): Train Loss: 227.8666, Train MSE: 227.45, Val Loss: 146.0512, Val MSE: 146.27 (Best)\n",
      "Epoch 28/50 (Random-20.0%): Train Loss: 223.0712, Train MSE: 223.83, Val Loss: 146.8428, Val MSE: 147.09\n",
      "Epoch 29/50 (Random-20.0%): Train Loss: 228.7163, Train MSE: 228.54, Val Loss: 146.3159, Val MSE: 146.47\n",
      "Epoch 30/50 (Random-20.0%): Train Loss: 225.2024, Train MSE: 225.41, Val Loss: 148.8461, Val MSE: 148.96\n",
      "Epoch 31/50 (Random-20.0%): Train Loss: 227.3863, Train MSE: 226.96, Val Loss: 145.9625, Val MSE: 146.13 (Best)\n",
      "Epoch 32/50 (Random-20.0%): Train Loss: 225.5665, Train MSE: 225.99, Val Loss: 144.6861, Val MSE: 144.88 (Best)\n",
      "Epoch 33/50 (Random-20.0%): Train Loss: 226.7741, Train MSE: 227.02, Val Loss: 145.5461, Val MSE: 145.72\n",
      "Epoch 34/50 (Random-20.0%): Train Loss: 227.1122, Train MSE: 227.24, Val Loss: 150.6602, Val MSE: 150.79\n",
      "Epoch 35/50 (Random-20.0%): Train Loss: 220.2090, Train MSE: 220.28, Val Loss: 147.2581, Val MSE: 147.41\n",
      "Epoch 36/50 (Random-20.0%): Train Loss: 227.9423, Train MSE: 228.29, Val Loss: 145.0930, Val MSE: 145.35\n",
      "Epoch 37/50 (Random-20.0%): Train Loss: 223.3829, Train MSE: 223.21, Val Loss: 150.3309, Val MSE: 150.45\n",
      "Epoch 38/50 (Random-20.0%): Train Loss: 222.2004, Train MSE: 221.98, Val Loss: 148.1018, Val MSE: 148.24\n",
      "Epoch 39/50 (Random-20.0%): Train Loss: 223.1288, Train MSE: 222.73, Val Loss: 145.4911, Val MSE: 145.65\n",
      "Epoch 40/50 (Random-20.0%): Train Loss: 230.0197, Train MSE: 229.31, Val Loss: 149.6995, Val MSE: 149.81\n",
      "Epoch 41/50 (Random-20.0%): Train Loss: 223.9791, Train MSE: 223.37, Val Loss: 144.2565, Val MSE: 144.50 (Best)\n",
      "Epoch 42/50 (Random-20.0%): Train Loss: 224.8266, Train MSE: 224.71, Val Loss: 144.7067, Val MSE: 144.86\n",
      "Epoch 43/50 (Random-20.0%): Train Loss: 225.3428, Train MSE: 225.33, Val Loss: 147.4619, Val MSE: 147.58\n",
      "Epoch 44/50 (Random-20.0%): Train Loss: 223.7107, Train MSE: 223.95, Val Loss: 147.3678, Val MSE: 147.51\n",
      "Epoch 45/50 (Random-20.0%): Train Loss: 222.9845, Train MSE: 223.10, Val Loss: 150.6902, Val MSE: 150.79\n",
      "Epoch 46/50 (Random-20.0%): Train Loss: 221.3000, Train MSE: 221.24, Val Loss: 145.8390, Val MSE: 146.11\n",
      "Epoch 47/50 (Random-20.0%): Train Loss: 227.7761, Train MSE: 228.37, Val Loss: 145.2152, Val MSE: 145.48\n",
      "Epoch 48/50 (Random-20.0%): Train Loss: 221.5374, Train MSE: 221.97, Val Loss: 145.9013, Val MSE: 146.11\n",
      "Epoch 49/50 (Random-20.0%): Train Loss: 222.8140, Train MSE: 222.40, Val Loss: 143.9365, Val MSE: 144.11 (Best)\n",
      "Epoch 50/50 (Random-20.0%): Train Loss: 218.8465, Train MSE: 219.27, Val Loss: 159.8323, Val MSE: 159.89\n",
      "Loaded best model state\n",
      "Results: MSE=511.74, MAE=13.69, MACs=0.21M\n",
      "✅ Model saved to ./models_mlp_nasa/random_sparsity_0.2.pth\n",
      "✅ ONNX model saved to ./models_mlp_nasa/random_sparsity_0.2.onnx\n",
      "\n",
      "Processing Random at 50.0% sparsity...\n",
      "✅ Created MLP with architecture: 700 -> 256 -> 128 -> 64 -> 1\n",
      "Initial MACs: 0.22M\n",
      "Applying RandomImportance pruning at 50.0% sparsity...\n",
      "Final MACs: 0.19M (Reduction: 11.9%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/50 (Random-50.0%): Train Loss: 271.7862, Train MSE: 272.43, Val Loss: 158.3597, Val MSE: 158.29 (Best)\n",
      "Epoch 2/50 (Random-50.0%): Train Loss: 249.5036, Train MSE: 248.89, Val Loss: 160.2296, Val MSE: 160.11\n",
      "Epoch 3/50 (Random-50.0%): Train Loss: 249.3487, Train MSE: 249.85, Val Loss: 157.4456, Val MSE: 157.39 (Best)\n",
      "Epoch 4/50 (Random-50.0%): Train Loss: 241.2425, Train MSE: 241.70, Val Loss: 153.2672, Val MSE: 153.31 (Best)\n",
      "Epoch 5/50 (Random-50.0%): Train Loss: 244.5630, Train MSE: 244.17, Val Loss: 167.1982, Val MSE: 167.06\n",
      "Epoch 6/50 (Random-50.0%): Train Loss: 242.4295, Train MSE: 242.77, Val Loss: 161.6209, Val MSE: 161.55\n",
      "Epoch 7/50 (Random-50.0%): Train Loss: 240.5173, Train MSE: 241.22, Val Loss: 152.2027, Val MSE: 152.32 (Best)\n",
      "Epoch 8/50 (Random-50.0%): Train Loss: 237.7073, Train MSE: 237.98, Val Loss: 156.4682, Val MSE: 156.48\n",
      "Epoch 9/50 (Random-50.0%): Train Loss: 240.1112, Train MSE: 240.32, Val Loss: 156.6183, Val MSE: 156.63\n",
      "Epoch 10/50 (Random-50.0%): Train Loss: 239.3809, Train MSE: 239.45, Val Loss: 157.0023, Val MSE: 156.98\n",
      "Epoch 11/50 (Random-50.0%): Train Loss: 239.0237, Train MSE: 239.55, Val Loss: 153.6317, Val MSE: 153.69\n",
      "Epoch 12/50 (Random-50.0%): Train Loss: 242.6262, Train MSE: 242.70, Val Loss: 152.7275, Val MSE: 152.80\n",
      "Epoch 13/50 (Random-50.0%): Train Loss: 242.2727, Train MSE: 242.46, Val Loss: 150.3450, Val MSE: 150.43 (Best)\n",
      "Epoch 14/50 (Random-50.0%): Train Loss: 243.1449, Train MSE: 242.43, Val Loss: 149.8379, Val MSE: 150.03 (Best)\n",
      "Epoch 15/50 (Random-50.0%): Train Loss: 238.5979, Train MSE: 238.34, Val Loss: 158.9491, Val MSE: 158.96\n",
      "Epoch 16/50 (Random-50.0%): Train Loss: 238.5213, Train MSE: 238.57, Val Loss: 160.5747, Val MSE: 160.58\n",
      "Epoch 17/50 (Random-50.0%): Train Loss: 235.2237, Train MSE: 235.82, Val Loss: 148.7339, Val MSE: 148.87 (Best)\n",
      "Epoch 18/50 (Random-50.0%): Train Loss: 238.7234, Train MSE: 238.51, Val Loss: 148.9367, Val MSE: 149.05\n",
      "Epoch 19/50 (Random-50.0%): Train Loss: 232.8698, Train MSE: 233.17, Val Loss: 150.5525, Val MSE: 150.65\n",
      "Epoch 20/50 (Random-50.0%): Train Loss: 243.2194, Train MSE: 243.57, Val Loss: 153.6952, Val MSE: 153.75\n",
      "Epoch 21/50 (Random-50.0%): Train Loss: 232.4841, Train MSE: 232.62, Val Loss: 148.3481, Val MSE: 148.56 (Best)\n",
      "Epoch 22/50 (Random-50.0%): Train Loss: 238.7550, Train MSE: 238.65, Val Loss: 147.6599, Val MSE: 147.85 (Best)\n",
      "Epoch 23/50 (Random-50.0%): Train Loss: 233.9833, Train MSE: 234.19, Val Loss: 147.1357, Val MSE: 147.30 (Best)\n",
      "Epoch 24/50 (Random-50.0%): Train Loss: 234.9908, Train MSE: 234.69, Val Loss: 151.5745, Val MSE: 151.66\n",
      "Epoch 25/50 (Random-50.0%): Train Loss: 235.3496, Train MSE: 234.75, Val Loss: 154.9474, Val MSE: 154.99\n",
      "Epoch 26/50 (Random-50.0%): Train Loss: 234.5820, Train MSE: 234.21, Val Loss: 151.1796, Val MSE: 151.28\n",
      "Epoch 27/50 (Random-50.0%): Train Loss: 235.2871, Train MSE: 234.92, Val Loss: 146.9128, Val MSE: 147.08 (Best)\n",
      "Epoch 28/50 (Random-50.0%): Train Loss: 232.8307, Train MSE: 232.94, Val Loss: 148.4354, Val MSE: 148.56\n",
      "Epoch 29/50 (Random-50.0%): Train Loss: 229.9304, Train MSE: 229.63, Val Loss: 150.9019, Val MSE: 151.02\n",
      "Epoch 30/50 (Random-50.0%): Train Loss: 232.9442, Train MSE: 231.98, Val Loss: 149.9720, Val MSE: 150.12\n",
      "Epoch 31/50 (Random-50.0%): Train Loss: 226.5322, Train MSE: 226.39, Val Loss: 145.8823, Val MSE: 146.08 (Best)\n",
      "Epoch 32/50 (Random-50.0%): Train Loss: 229.7462, Train MSE: 228.88, Val Loss: 156.5965, Val MSE: 156.67\n",
      "Epoch 33/50 (Random-50.0%): Train Loss: 232.0426, Train MSE: 232.42, Val Loss: 151.7363, Val MSE: 151.82\n",
      "Epoch 34/50 (Random-50.0%): Train Loss: 232.3331, Train MSE: 232.62, Val Loss: 146.9399, Val MSE: 147.07\n",
      "Epoch 35/50 (Random-50.0%): Train Loss: 232.7326, Train MSE: 233.29, Val Loss: 158.9026, Val MSE: 158.94\n",
      "Epoch 36/50 (Random-50.0%): Train Loss: 233.4039, Train MSE: 233.37, Val Loss: 150.7248, Val MSE: 150.83\n",
      "Epoch 37/50 (Random-50.0%): Train Loss: 231.0296, Train MSE: 230.52, Val Loss: 147.4818, Val MSE: 147.60\n",
      "Epoch 38/50 (Random-50.0%): Train Loss: 233.5706, Train MSE: 233.27, Val Loss: 148.6681, Val MSE: 148.81\n",
      "Epoch 39/50 (Random-50.0%): Train Loss: 230.3205, Train MSE: 230.67, Val Loss: 148.6286, Val MSE: 148.75\n",
      "Epoch 40/50 (Random-50.0%): Train Loss: 228.2605, Train MSE: 228.56, Val Loss: 152.2106, Val MSE: 152.27\n",
      "Epoch 41/50 (Random-50.0%): Train Loss: 230.2258, Train MSE: 230.65, Val Loss: 170.9728, Val MSE: 170.98\n",
      "Early stopping triggered after 41 epochs\n",
      "Loaded best model state\n",
      "Results: MSE=515.48, MAE=13.70, MACs=0.19M\n",
      "✅ Model saved to ./models_mlp_nasa/random_sparsity_0.5.pth\n",
      "✅ ONNX model saved to ./models_mlp_nasa/random_sparsity_0.5.onnx\n",
      "\n",
      "Processing Random at 70.0% sparsity...\n",
      "✅ Created MLP with architecture: 700 -> 256 -> 128 -> 64 -> 1\n",
      "Initial MACs: 0.22M\n",
      "Applying RandomImportance pruning at 70.0% sparsity...\n",
      "Final MACs: 0.19M (Reduction: 16.3%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/50 (Random-70.0%): Train Loss: 336.3584, Train MSE: 335.91, Val Loss: 178.4681, Val MSE: 178.19 (Best)\n",
      "Epoch 2/50 (Random-70.0%): Train Loss: 266.0783, Train MSE: 266.05, Val Loss: 170.9896, Val MSE: 170.69 (Best)\n",
      "Epoch 3/50 (Random-70.0%): Train Loss: 256.5787, Train MSE: 257.24, Val Loss: 157.4843, Val MSE: 157.45 (Best)\n",
      "Epoch 4/50 (Random-70.0%): Train Loss: 256.4135, Train MSE: 256.02, Val Loss: 160.0085, Val MSE: 159.87\n",
      "Epoch 5/50 (Random-70.0%): Train Loss: 249.8324, Train MSE: 249.73, Val Loss: 175.2595, Val MSE: 175.00\n",
      "Epoch 6/50 (Random-70.0%): Train Loss: 250.8931, Train MSE: 250.96, Val Loss: 154.3092, Val MSE: 154.34 (Best)\n",
      "Epoch 7/50 (Random-70.0%): Train Loss: 247.2366, Train MSE: 247.05, Val Loss: 161.9006, Val MSE: 161.79\n",
      "Epoch 8/50 (Random-70.0%): Train Loss: 246.0451, Train MSE: 246.72, Val Loss: 155.3126, Val MSE: 155.25\n",
      "Epoch 9/50 (Random-70.0%): Train Loss: 248.2812, Train MSE: 247.76, Val Loss: 153.7640, Val MSE: 153.84 (Best)\n",
      "Epoch 10/50 (Random-70.0%): Train Loss: 239.2267, Train MSE: 240.31, Val Loss: 163.9305, Val MSE: 163.84\n",
      "Epoch 11/50 (Random-70.0%): Train Loss: 244.4143, Train MSE: 244.64, Val Loss: 151.6651, Val MSE: 151.76 (Best)\n",
      "Epoch 12/50 (Random-70.0%): Train Loss: 245.3385, Train MSE: 244.87, Val Loss: 153.8053, Val MSE: 153.85\n",
      "Epoch 13/50 (Random-70.0%): Train Loss: 243.2221, Train MSE: 243.24, Val Loss: 165.7528, Val MSE: 165.69\n",
      "Epoch 14/50 (Random-70.0%): Train Loss: 244.4789, Train MSE: 244.36, Val Loss: 158.4682, Val MSE: 158.48\n",
      "Epoch 15/50 (Random-70.0%): Train Loss: 248.1790, Train MSE: 248.17, Val Loss: 156.2253, Val MSE: 156.25\n",
      "Epoch 16/50 (Random-70.0%): Train Loss: 239.4373, Train MSE: 238.86, Val Loss: 152.1835, Val MSE: 152.27\n",
      "Epoch 17/50 (Random-70.0%): Train Loss: 242.3111, Train MSE: 242.08, Val Loss: 154.3695, Val MSE: 154.61\n",
      "Epoch 18/50 (Random-70.0%): Train Loss: 239.9597, Train MSE: 239.99, Val Loss: 159.2444, Val MSE: 159.23\n",
      "Epoch 19/50 (Random-70.0%): Train Loss: 245.6771, Train MSE: 245.36, Val Loss: 154.8379, Val MSE: 154.85\n",
      "Epoch 20/50 (Random-70.0%): Train Loss: 244.6940, Train MSE: 243.89, Val Loss: 153.5201, Val MSE: 153.56\n",
      "Epoch 21/50 (Random-70.0%): Train Loss: 240.5309, Train MSE: 240.90, Val Loss: 154.5092, Val MSE: 154.55\n",
      "Early stopping triggered after 21 epochs\n",
      "Loaded best model state\n",
      "Results: MSE=523.73, MAE=13.93, MACs=0.19M\n",
      "✅ Model saved to ./models_mlp_nasa/random_sparsity_0.7.pth\n",
      "✅ ONNX model saved to ./models_mlp_nasa/random_sparsity_0.7.onnx\n",
      "\n",
      "Saving results...\n",
      "✅ Complete results saved to ./results_mlp_nasa/complete_results.json\n",
      "✅ Summary results saved to ./results_mlp_nasa/summary_results.csv\n",
      "Creating plots...\n",
      "✅ MSE plot saved to ./results_mlp_nasa/mse_vs_sparsity.png\n",
      "✅ Efficiency frontier plot saved to ./results_mlp_nasa/efficiency_frontier.png\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENTAL RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Baseline Performance:\n",
      "  MSE: 526.92\n",
      "  MAE: 14.07\n",
      "  MACs: 0.22M\n",
      "  Parameters: 0.22M\n",
      "  Model Size: 0.84MB\n",
      "\n",
      "Strategy Comparison at 50% Sparsity:\n",
      "   MagnitudeL2: MSE=507.94 (-18.97,  -3.6% increase)\n",
      "        Random: MSE=515.48 (-11.44,  -2.2% increase)\n",
      "\n",
      "Complete Results Table:\n",
      "------------------------------------------------------------------------------------------\n",
      "Strategy     Sparsity MSE      MAE      MACs(M)  Params(M) Size(MB)\n",
      "------------------------------------------------------------------------------------------\n",
      "MagnitudeL2       0%  526.92   14.07    0.22     0.22    0.84\n",
      "MagnitudeL2      20%  500.93   13.59    0.21     0.21    0.80\n",
      "MagnitudeL2      50%  507.94   13.65    0.19     0.19    0.74\n",
      "MagnitudeL2      70%  507.88   13.65    0.19     0.18    0.70\n",
      "Random            0%  526.92   14.07    0.22     0.22    0.84\n",
      "Random           20%  511.74   13.69    0.21     0.21    0.80\n",
      "Random           50%  515.48   13.70    0.19     0.19    0.74\n",
      "Random           70%  523.73   13.93    0.19     0.18    0.70\n",
      "\n",
      "🎉 All experiments completed!\n",
      "📁 Results saved to: /home/muis/thesis/github-repo/master-thesis/mlp/results_mlp_nasa\n",
      "📁 Models saved to: /home/muis/thesis/github-repo/master-thesis/mlp/models_mlp_nasa\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
