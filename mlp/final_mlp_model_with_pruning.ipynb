{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T18:16:01.890351Z",
     "start_time": "2025-06-04T18:12:41.633865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_pruning as tp\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_BASE_NAME = \"mlp_nasa\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# NASA Dataset preprocessing functions from your provided code\n",
    "column_names = ['unit_number', 'time_in_cycles'] + [f'op_setting_{i}' for i in range(1, 4)] + [f'sensor_{i}' for i in\n",
    "                                                                                               range(1, 24)]\n",
    "\n",
    "\n",
    "def load_dataframe(file_path: str) -> pd.DataFrame | None:\n",
    "    \"\"\"Loads a single CMaps data file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=' ', header=None, names=column_names)\n",
    "        # Drop the last two columns if they are all NaNs (often artifacts of space delimiter)\n",
    "        df.dropna(axis=1, how='all', inplace=True)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def clean_data(df: pd.DataFrame) -> list:\n",
    "    \"\"\"Identifies columns to remove based on low std dev.\"\"\"\n",
    "    if df is None:\n",
    "        return []\n",
    "    # Columns with std dev < 0.02 (potential candidates for removal)\n",
    "    cols_to_check = [col for col in df.columns if 'sensor' in col or 'op_setting' in col]\n",
    "    low_std_cols = [col for col in cols_to_check if df[col].std() < 0.02]\n",
    "    print(f\"Columns with std < 0.02 (potential removal): {low_std_cols}\")\n",
    "    return low_std_cols\n",
    "\n",
    "\n",
    "def add_rul(df: pd.DataFrame) -> pd.DataFrame | None:\n",
    "    \"\"\"Calculates and adds the Remaining Useful Life (RUL) column.\"\"\"\n",
    "    if df is None:\n",
    "        return None\n",
    "    max_cycles = df.groupby('unit_number')['time_in_cycles'].max().reset_index()\n",
    "    max_cycles.columns = ['unit_number', 'max_cycle']\n",
    "    df = df.merge(max_cycles, on='unit_number', how='left')\n",
    "    df['RUL'] = df['max_cycle'] - df['time_in_cycles']\n",
    "    df.drop(columns=['max_cycle'], inplace=True)\n",
    "    # Clip RUL (optional, common practice to limit max RUL)\n",
    "    df['RUL'] = df['RUL'].clip(upper=125)\n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize_data(df: pd.DataFrame,\n",
    "                   columns_to_normalize: List[str], scaler: MinMaxScaler = None) -> Tuple[pd.DataFrame, MinMaxScaler] | \\\n",
    "                                                                                    Tuple[None, None]:\n",
    "    \"\"\"Normalizes specified columns using MinMaxScaler.\"\"\"\n",
    "    if df is None:\n",
    "        return None, None\n",
    "    data_to_scale = df[columns_to_normalize]\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler()\n",
    "        df[columns_to_normalize] = scaler.fit_transform(data_to_scale)\n",
    "    else:\n",
    "        # Ensure only columns present in the scaler are transformed\n",
    "        valid_cols = [col for col in columns_to_normalize if col in scaler.feature_names_in_]\n",
    "        if len(valid_cols) < len(columns_to_normalize):\n",
    "            print(\"Warning: Some columns not found in the provided scaler. Skipping them.\")\n",
    "        if valid_cols:\n",
    "            df[valid_cols] = scaler.transform(df[valid_cols])\n",
    "    return df, scaler\n",
    "\n",
    "\n",
    "def prepare_cmapss_data(data_dir: str, train_file: str, test_file: str, test_rul_file: str) -> Tuple[\n",
    "    pd.DataFrame, pd.DataFrame, pd.DataFrame, MinMaxScaler, List[str]]:\n",
    "    \"\"\"Loads, cleans, preprocesses train/test data and RUL.\"\"\"\n",
    "    print(\"--- Preparing Training Data ---\")\n",
    "    train_df = load_dataframe(os.path.join(data_dir, train_file))\n",
    "    train_df = add_rul(train_df)\n",
    "\n",
    "    print(\"\\n--- Preparing Test Data ---\")\n",
    "    test_df = load_dataframe(os.path.join(data_dir, test_file))\n",
    "    test_rul_df = pd.read_csv(os.path.join(data_dir, test_rul_file), header=None, names=['RUL'])\n",
    "\n",
    "    # Clean Data - identify columns based on TRAINING data variance\n",
    "    cols_to_remove = clean_data(train_df)\n",
    "    feature_cols = [col for col in train_df.columns if\n",
    "                    col not in ['unit_number', 'time_in_cycles', 'RUL'] + cols_to_remove]\n",
    "    print(f\"\\nUsing Features: {feature_cols}\")\n",
    "\n",
    "    # Drop removed columns from both train and test\n",
    "    train_df.drop(columns=cols_to_remove, inplace=True, errors='ignore')\n",
    "    test_df.drop(columns=cols_to_remove, inplace=True, errors='ignore')\n",
    "\n",
    "    # Normalize features based on TRAINING data\n",
    "    print(\"\\n--- Normalizing Data ---\")\n",
    "    train_df_norm, scaler = normalize_data(train_df.copy(), feature_cols, scaler=None)\n",
    "    test_df_norm, _ = normalize_data(test_df.copy(), feature_cols, scaler=scaler)\n",
    "\n",
    "    return train_df_norm, test_df_norm, test_rul_df, scaler, feature_cols\n",
    "\n",
    "\n",
    "# Custom Dataset for NASA time series\n",
    "class NASADataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, feature_cols: List[str], window_size: int = 50,\n",
    "                 stride: int = 1, is_test: bool = False, test_rul_df: pd.DataFrame = None):\n",
    "        self.df = df\n",
    "        self.feature_cols = feature_cols\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.is_test = is_test\n",
    "        self.test_rul_df = test_rul_df\n",
    "        self.samples = []\n",
    "        self.targets = []\n",
    "\n",
    "        self._prepare_samples()\n",
    "\n",
    "    def _prepare_samples(self):\n",
    "        \"\"\"Create windowed samples from the dataframe\"\"\"\n",
    "        units = self.df['unit_number'].unique()\n",
    "\n",
    "        for unit in units:\n",
    "            unit_df = self.df[self.df['unit_number'] == unit].sort_values('time_in_cycles')\n",
    "\n",
    "            # For test data, we only need the last window for each unit\n",
    "            if self.is_test:\n",
    "                if len(unit_df) >= self.window_size:\n",
    "                    window_data = unit_df[self.feature_cols].iloc[-self.window_size:].values\n",
    "                    self.samples.append(window_data)\n",
    "                    # Get RUL from test_rul_df\n",
    "                    if self.test_rul_df is not None:\n",
    "                        self.targets.append(self.test_rul_df.iloc[unit - 1]['RUL'])\n",
    "                else:\n",
    "                    # Pad if necessary\n",
    "                    window_data = unit_df[self.feature_cols].values\n",
    "                    padded = np.zeros((self.window_size, len(self.feature_cols)))\n",
    "                    padded[-len(window_data):] = window_data\n",
    "                    self.samples.append(padded)\n",
    "                    if self.test_rul_df is not None:\n",
    "                        self.targets.append(self.test_rul_df.iloc[unit - 1]['RUL'])\n",
    "            else:\n",
    "                # For training data, create multiple windows\n",
    "                for i in range(0, len(unit_df) - self.window_size + 1, self.stride):\n",
    "                    window_data = unit_df[self.feature_cols].iloc[i:i + self.window_size].values\n",
    "                    rul = unit_df['RUL'].iloc[i + self.window_size - 1]\n",
    "                    self.samples.append(window_data)\n",
    "                    self.targets.append(rul)\n",
    "\n",
    "        self.samples = np.array(self.samples, dtype=np.float32)\n",
    "        self.targets = np.array(self.targets, dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Flatten the window for MLP\n",
    "        sample = self.samples[idx].flatten()\n",
    "        target = self.targets[idx]\n",
    "        return torch.FloatTensor(sample), torch.FloatTensor([target])\n",
    "\n",
    "\n",
    "# MLP Model Definition\n",
    "class NASAMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes=[256, 128, 64], dropout_rate=0.2):\n",
    "        super(NASAMLP, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "\n",
    "        # Output layer for regression\n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def get_data_loaders(data_dir='./data/NASA', batch_size=128, window_size=50, val_split=0.2, seed=42):\n",
    "    \"\"\"Load NASA C-MAPSS dataset with train/val/test splits\"\"\"\n",
    "    print(f\"Loading NASA C-MAPSS dataset from: {data_dir}\")\n",
    "\n",
    "    # Prepare data\n",
    "    train_df, test_df, test_rul_df, scaler, feature_cols = prepare_cmapss_data(\n",
    "        data_dir,\n",
    "        'train_FD001.txt',\n",
    "        'test_FD001.txt',\n",
    "        'RUL_FD001.txt'\n",
    "    )\n",
    "\n",
    "    # Create full training dataset\n",
    "    full_train_dataset = NASADataset(train_df, feature_cols, window_size=window_size)\n",
    "\n",
    "    # Split into train and validation\n",
    "    val_size = int(len(full_train_dataset) * val_split)\n",
    "    train_size = len(full_train_dataset) - val_size\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_train_dataset, [train_size, val_size], generator=generator\n",
    "    )\n",
    "\n",
    "    # Create test dataset\n",
    "    test_dataset = NASADataset(test_df, feature_cols, window_size=window_size,\n",
    "                               is_test=True, test_rul_df=test_rul_df)\n",
    "\n",
    "    # Create data loaders\n",
    "    num_workers = min(4, os.cpu_count() or 2)\n",
    "    pin_memory = True if DEVICE.type == 'cuda' else False\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=num_workers, pin_memory=pin_memory)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                            num_workers=num_workers, pin_memory=pin_memory)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                             num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    print(f\"DataLoaders created - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "    print(f\"Input size: {window_size * len(feature_cols)} (window_size * num_features)\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader, window_size * len(feature_cols)\n",
    "\n",
    "\n",
    "def get_mlp_model(input_size, hidden_sizes=[256, 128, 64], dropout_rate=0.2):\n",
    "    \"\"\"Get MLP model for NASA dataset\"\"\"\n",
    "    model = NASAMLP(input_size, hidden_sizes, dropout_rate)\n",
    "    print(f\"✅ Created MLP with architecture: {input_size} -> {' -> '.join(map(str, hidden_sizes))} -> 1\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_ignored_layers(model):\n",
    "    \"\"\"Get layers to ignore during pruning (typically final layer)\"\"\"\n",
    "    ignored_layers = []\n",
    "    # Get the last linear layer in the sequential model\n",
    "    for module in model.model:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            last_linear = module\n",
    "    ignored_layers.append(last_linear)\n",
    "    return ignored_layers\n",
    "\n",
    "\n",
    "def calculate_macs_params(model, example_input):\n",
    "    \"\"\"Calculate MACs and parameters using torch_pruning\"\"\"\n",
    "    model.eval()\n",
    "    target_device = example_input.device\n",
    "    model_on_device = model.to(target_device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        macs, params = tp.utils.count_ops_and_params(model_on_device, example_input)\n",
    "\n",
    "    return macs, params\n",
    "\n",
    "\n",
    "def save_model(model, save_path, example_input_cpu=None):\n",
    "    \"\"\"Save model state dict and optionally ONNX\"\"\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"✅ Model saved to {save_path}\")\n",
    "\n",
    "    if example_input_cpu is not None:\n",
    "        onnx_path = save_path.replace('.pth', '.onnx')\n",
    "        try:\n",
    "            model_cpu = model.to('cpu')\n",
    "            torch.onnx.export(\n",
    "                model_cpu, example_input_cpu, onnx_path,\n",
    "                export_params=True, opset_version=13,\n",
    "                input_names=['input'], output_names=['output'],\n",
    "                dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "            )\n",
    "            print(f\"✅ ONNX model saved to {onnx_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: ONNX export failed: {e}\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, example_input, criterion, device):\n",
    "    \"\"\"Evaluate model and return comprehensive metrics\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Calculate efficiency metrics\n",
    "    macs, params = calculate_macs_params(model, example_input.to(device))\n",
    "    model_size_mb = params * 4 / (1024 * 1024)  # Assuming float32\n",
    "\n",
    "    # Calculate MSE and MAE\n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            all_predictions.extend(output.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    mse = np.mean((all_predictions - all_targets) ** 2)\n",
    "    mae = np.mean(np.abs(all_predictions - all_targets))\n",
    "\n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'loss': total_loss / len(data_loader.dataset),\n",
    "        'macs': macs,\n",
    "        'params': params,\n",
    "        'size_mb': model_size_mb\n",
    "    }\n",
    "\n",
    "\n",
    "def prune_model(model, strategy_config, sparsity_ratio, example_input, ignored_layers=None):\n",
    "    \"\"\"Apply structured pruning to model\"\"\"\n",
    "    if sparsity_ratio == 0.0:\n",
    "        print(\"No pruning needed (sparsity = 0.0)\")\n",
    "        return model\n",
    "\n",
    "    model.eval()\n",
    "    pruned_model = copy.deepcopy(model)\n",
    "    pruned_model.to(example_input.device)\n",
    "\n",
    "    # Calculate initial MACs\n",
    "    initial_macs, _ = calculate_macs_params(pruned_model, example_input)\n",
    "    print(f\"Initial MACs: {initial_macs / 1e6:.2f}M\")\n",
    "\n",
    "    ignored_layers = ignored_layers or []\n",
    "\n",
    "    # Create pruner based on strategy\n",
    "    # Note: For MLP, we use Linear layers as root modules\n",
    "    pruner = strategy_config['pruner'](\n",
    "        pruned_model,\n",
    "        example_input,\n",
    "        importance=strategy_config['importance'],\n",
    "        iterative_steps=5,  # Use 5 iterative steps\n",
    "        ch_sparsity=sparsity_ratio,\n",
    "        root_module_types=[nn.Linear],  # Changed from Conv2d to Linear\n",
    "        ignored_layers=ignored_layers\n",
    "    )\n",
    "\n",
    "    print(f\"Applying {strategy_config['importance'].__class__.__name__} pruning at {sparsity_ratio:.1%} sparsity...\")\n",
    "\n",
    "    # Apply pruning\n",
    "    pruner.step()\n",
    "\n",
    "    # Calculate final MACs\n",
    "    final_macs, _ = calculate_macs_params(pruned_model, example_input)\n",
    "    reduction = (initial_macs - final_macs) / initial_macs * 100 if initial_macs > 0 else 0\n",
    "    print(f\"Final MACs: {final_macs / 1e6:.2f}M (Reduction: {reduction:.1f}%)\")\n",
    "\n",
    "    return pruned_model\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs,\n",
    "                val_loader=None, patience=7, log_prefix=\"\"):\n",
    "    \"\"\"Train model with early stopping\"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_mse': [],\n",
    "        'val_loss': [],\n",
    "        'val_mse': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_predictions = []\n",
    "        train_targets = []\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_predictions.extend(output.detach().cpu().numpy())\n",
    "            train_targets.extend(target.cpu().numpy())\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_mse = np.mean((np.array(train_predictions) - np.array(train_targets)) ** 2)\n",
    "\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_mse'].append(train_mse)\n",
    "\n",
    "        log_msg = f\"Epoch {epoch + 1}/{num_epochs} ({log_prefix}): Train Loss: {avg_train_loss:.4f}, Train MSE: {train_mse:.2f}\"\n",
    "\n",
    "        # Validation phase\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_predictions = []\n",
    "            val_targets = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for data, target in val_loader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                    val_predictions.extend(output.cpu().numpy())\n",
    "                    val_targets.extend(target.cpu().numpy())\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_mse = np.mean((np.array(val_predictions) - np.array(val_targets)) ** 2)\n",
    "\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            history['val_mse'].append(val_mse)\n",
    "\n",
    "            log_msg += f\", Val Loss: {avg_val_loss:.4f}, Val MSE: {val_mse:.2f}\"\n",
    "\n",
    "            # Early stopping check\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                epochs_no_improve = 0\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "                log_msg += \" (Best)\"\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"{log_msg}\")\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "                break\n",
    "        else:\n",
    "            history['val_loss'].append(None)\n",
    "            history['val_mse'].append(None)\n",
    "\n",
    "        print(log_msg)\n",
    "\n",
    "    # Load best model state if available\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"Loaded best model state\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def save_results_to_files(all_results, output_dir):\n",
    "    \"\"\"Save experimental results to JSON and CSV files\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save complete results as JSON\n",
    "    results_json_path = os.path.join(output_dir, 'complete_results.json')\n",
    "    with open(results_json_path, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2, default=str)\n",
    "    print(f\"✅ Complete results saved to {results_json_path}\")\n",
    "\n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for strategy, strategy_results in all_results.items():\n",
    "        for sparsity, metrics in strategy_results.items():\n",
    "            row = {\n",
    "                'strategy': strategy,\n",
    "                'sparsity_ratio': sparsity,\n",
    "                'mse': metrics['mse'],\n",
    "                'mae': metrics['mae'],\n",
    "                'loss': metrics['loss'],\n",
    "                'macs_millions': metrics['macs'] / 1e6,\n",
    "                'params_millions': metrics['params'] / 1e6,\n",
    "                'size_mb': metrics['size_mb']\n",
    "            }\n",
    "            summary_data.append(row)\n",
    "\n",
    "    # Save summary as CSV\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_csv_path = os.path.join(output_dir, 'summary_results.csv')\n",
    "    summary_df.to_csv(summary_csv_path, index=False)\n",
    "    print(f\"✅ Summary results saved to {summary_csv_path}\")\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "def create_results_plots(summary_df, output_dir):\n",
    "    \"\"\"Create visualization plots for regression metrics\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    strategies = summary_df['strategy'].unique()\n",
    "    sparsity_levels = sorted(summary_df['sparsity_ratio'].unique())\n",
    "\n",
    "    # Plot 1: MSE vs Sparsity\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for strategy in strategies:\n",
    "        strategy_data = summary_df[summary_df['strategy'] == strategy].sort_values('sparsity_ratio')\n",
    "        plt.plot(strategy_data['sparsity_ratio'] * 100, strategy_data['mse'],\n",
    "                 'o-', linewidth=2, markersize=8, label=strategy)\n",
    "\n",
    "    plt.xlabel('Sparsity (%)', fontsize=12)\n",
    "    plt.ylabel('MSE', fontsize=12)\n",
    "    plt.title('NASA MLP: MSE vs Sparsity', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.gca().invert_yaxis()  # Lower MSE is better\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(output_dir, 'mse_vs_sparsity.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✅ MSE plot saved to {plot_path}\")\n",
    "\n",
    "    # Plot 2: Efficiency frontier (MSE vs MACs)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for strategy in strategies:\n",
    "        strategy_data = summary_df[summary_df['strategy'] == strategy].sort_values('sparsity_ratio')\n",
    "        plt.scatter(strategy_data['macs_millions'], strategy_data['mse'],\n",
    "                    s=100, label=strategy, alpha=0.8)\n",
    "        plt.plot(strategy_data['macs_millions'], strategy_data['mse'],\n",
    "                 '--', alpha=0.6)\n",
    "\n",
    "    plt.xlabel('MACs (Millions)', fontsize=12)\n",
    "    plt.ylabel('MSE', fontsize=12)\n",
    "    plt.title('NASA MLP: Efficiency Frontier (MSE vs MACs)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.gca().invert_yaxis()  # Lower MSE is better\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(output_dir, 'efficiency_frontier.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✅ Efficiency frontier plot saved to {plot_path}\")\n",
    "\n",
    "\n",
    "def print_results_table(summary_df):\n",
    "    \"\"\"Print formatted results table\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXPERIMENTAL RESULTS SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Baseline results\n",
    "    baseline_results = summary_df[summary_df['sparsity_ratio'] == 0.0].iloc[0]\n",
    "    print(f\"\\nBaseline Performance:\")\n",
    "    print(f\"  MSE: {baseline_results['mse']:.2f}\")\n",
    "    print(f\"  MAE: {baseline_results['mae']:.2f}\")\n",
    "    print(f\"  MACs: {baseline_results['macs_millions']:.2f}M\")\n",
    "    print(f\"  Parameters: {baseline_results['params_millions']:.2f}M\")\n",
    "    print(f\"  Model Size: {baseline_results['size_mb']:.2f}MB\")\n",
    "\n",
    "    # Strategy comparison at 50% sparsity\n",
    "    print(f\"\\nStrategy Comparison at 50% Sparsity:\")\n",
    "    sparsity_50_data = summary_df[summary_df['sparsity_ratio'] == 0.5]\n",
    "    for _, row in sparsity_50_data.iterrows():\n",
    "        degradation = row['mse'] - baseline_results['mse']\n",
    "        relative_increase = (degradation / baseline_results['mse']) * 100\n",
    "        print(\n",
    "            f\"  {row['strategy']:>12}: MSE={row['mse']:>6.2f} ({degradation:>+5.2f}, {relative_increase:>+5.1f}% increase)\")\n",
    "\n",
    "    # Complete results table\n",
    "    print(f\"\\nComplete Results Table:\")\n",
    "    print(\"-\" * 90)\n",
    "    print(f\"{'Strategy':<12} {'Sparsity':<8} {'MSE':<8} {'MAE':<8} {'MACs(M)':<8} {'Params(M)':<9} {'Size(MB)':<8}\")\n",
    "    print(\"-\" * 90)\n",
    "\n",
    "    for _, row in summary_df.sort_values(['strategy', 'sparsity_ratio']).iterrows():\n",
    "        print(f\"{row['strategy']:<12} {row['sparsity_ratio'] * 100:>6.0f}% \"\n",
    "              f\"{row['mse']:>7.2f} {row['mae']:>7.2f} {row['macs_millions']:>7.2f} \"\n",
    "              f\"{row['params_millions']:>8.2f} {row['size_mb']:>7.2f}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main experimental workflow\"\"\"\n",
    "    print(\"Starting NASA MLP Pruning Experiments\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'strategies': {\n",
    "            'MagnitudeL2': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.MagnitudeImportance(p=2)\n",
    "            },\n",
    "            'Random': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.RandomImportance()\n",
    "            },\n",
    "            # Note: BNScale is not applicable to MLP as it doesn't have BatchNorm layers\n",
    "        },\n",
    "        'pruning_ratios': [0.0, 0.2, 0.5, 0.7],\n",
    "        'hidden_sizes': [256, 128, 64],\n",
    "        'dropout_rate': 0.2,\n",
    "        'window_size': 50,\n",
    "        'batch_size': 128,\n",
    "        'learning_rate': 0.0001,\n",
    "        'epochs': 1000,  # More epochs for time series\n",
    "        'patience': 20,\n",
    "        'output_dir': './results_mlp_nasa',\n",
    "        'models_dir': './models_mlp_nasa',\n",
    "        'data_dir': './data/CMaps'\n",
    "    }\n",
    "\n",
    "    # Create output directories\n",
    "    os.makedirs(config['output_dir'], exist_ok=True)\n",
    "    os.makedirs(config['models_dir'], exist_ok=True)\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading NASA C-MAPSS dataset...\")\n",
    "    train_loader, val_loader, test_loader, input_size = get_data_loaders(\n",
    "        data_dir=config['data_dir'],\n",
    "        batch_size=config['batch_size'],\n",
    "        window_size=config['window_size']\n",
    "    )\n",
    "\n",
    "    # Prepare inputs and criterion\n",
    "    example_input_cpu = torch.randn(1, input_size)\n",
    "    example_input_device = example_input_cpu.to(DEVICE)\n",
    "    criterion = nn.MSELoss()  # Regression loss\n",
    "\n",
    "    # Get baseline model and train it\n",
    "    print(\"\\nCreating and training baseline model...\")\n",
    "    model = get_mlp_model(input_size, config['hidden_sizes'], config['dropout_rate'])\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Train baseline model\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    trained_model, training_history = train_model(\n",
    "        model, train_loader, criterion, optimizer, DEVICE,\n",
    "        config['epochs'], val_loader, config['patience'], \"Baseline Training\"\n",
    "    )\n",
    "\n",
    "    # Save baseline model\n",
    "    baseline_model_path = os.path.join(config['models_dir'], 'baseline_model.pth')\n",
    "    save_model(trained_model, baseline_model_path, example_input_cpu)\n",
    "\n",
    "    # Evaluate baseline\n",
    "    print(\"\\nEvaluating baseline model...\")\n",
    "    baseline_metrics = evaluate_model(trained_model, test_loader, example_input_device, criterion, DEVICE)\n",
    "    print(f\"Baseline Results: MSE={baseline_metrics['mse']:.2f}, \"\n",
    "          f\"MAE={baseline_metrics['mae']:.2f}, \"\n",
    "          f\"MACs={baseline_metrics['macs'] / 1e6:.2f}M, \"\n",
    "          f\"Params={baseline_metrics['params'] / 1e6:.2f}M\")\n",
    "\n",
    "    # Initialize results storage\n",
    "    all_results = {}\n",
    "    for strategy_name in config['strategies'].keys():\n",
    "        all_results[strategy_name] = {0.0: baseline_metrics}\n",
    "\n",
    "    # Get ignored layers\n",
    "    ignored_layers = get_ignored_layers(trained_model)\n",
    "\n",
    "    # Run pruning experiments\n",
    "    print(\"\\nStarting pruning experiments...\")\n",
    "    for strategy_name, strategy_config in config['strategies'].items():\n",
    "        print(f\"\\n--- Strategy: {strategy_name} ---\")\n",
    "\n",
    "        for sparsity_ratio in config['pruning_ratios']:\n",
    "            if sparsity_ratio == 0.0:\n",
    "                continue  # Skip baseline (already done)\n",
    "\n",
    "            print(f\"\\nProcessing {strategy_name} at {sparsity_ratio:.1%} sparsity...\")\n",
    "\n",
    "            # Load fresh copy of trained baseline\n",
    "            model_copy = get_mlp_model(input_size, config['hidden_sizes'], config['dropout_rate'])\n",
    "            model_copy.load_state_dict(torch.load(baseline_model_path, map_location=DEVICE))\n",
    "            model_copy.to(DEVICE)\n",
    "\n",
    "            # Apply pruning\n",
    "            pruned_model = prune_model(\n",
    "                model_copy, strategy_config, sparsity_ratio,\n",
    "                example_input_device, ignored_layers\n",
    "            )\n",
    "\n",
    "            # Fine-tune pruned model\n",
    "            print(\"Fine-tuning pruned model...\")\n",
    "            optimizer_ft = optim.Adam(pruned_model.parameters(), lr=config['learning_rate'])\n",
    "            fine_tuned_model, ft_history = train_model(\n",
    "                pruned_model, train_loader, criterion, optimizer_ft, DEVICE,\n",
    "                config['epochs'], val_loader, config['patience'],\n",
    "                f\"{strategy_name}-{sparsity_ratio:.1%}\"\n",
    "            )\n",
    "\n",
    "            # Evaluate fine-tuned model\n",
    "            final_metrics = evaluate_model(fine_tuned_model, test_loader, example_input_device, criterion, DEVICE)\n",
    "            all_results[strategy_name][sparsity_ratio] = final_metrics\n",
    "\n",
    "            print(f\"Results: MSE={final_metrics['mse']:.2f}, \"\n",
    "                  f\"MAE={final_metrics['mae']:.2f}, \"\n",
    "                  f\"MACs={final_metrics['macs'] / 1e6:.2f}M\")\n",
    "\n",
    "            # Save fine-tuned model\n",
    "            model_filename = f\"{strategy_name.lower()}_sparsity_{sparsity_ratio:.1f}.pth\"\n",
    "            model_path = os.path.join(config['models_dir'], model_filename)\n",
    "            save_model(fine_tuned_model, model_path, example_input_cpu)\n",
    "\n",
    "    # Save and analyze results\n",
    "    print(\"\\nSaving results...\")\n",
    "    summary_df = save_results_to_files(all_results, config['output_dir'])\n",
    "\n",
    "    # Create plots\n",
    "    print(\"Creating plots...\")\n",
    "    create_results_plots(summary_df, config['output_dir'])\n",
    "\n",
    "    # Print summary\n",
    "    print_results_table(summary_df)\n",
    "\n",
    "    print(f\"\\n🎉 All experiments completed!\")\n",
    "    print(f\"📁 Results saved to: {os.path.abspath(config['output_dir'])}\")\n",
    "    print(f\"📁 Models saved to: {os.path.abspath(config['models_dir'])}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "1269cb9144cb0eae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting NASA MLP Pruning Experiments\n",
      "============================================================\n",
      "Loading NASA C-MAPSS dataset...\n",
      "Loading NASA C-MAPSS dataset from: ./data/CMaps\n",
      "--- Preparing Training Data ---\n",
      "\n",
      "--- Preparing Test Data ---\n",
      "Columns with std < 0.02 (potential removal): ['op_setting_1', 'op_setting_2', 'op_setting_3', 'sensor_1', 'sensor_5', 'sensor_6', 'sensor_10', 'sensor_16', 'sensor_18', 'sensor_19']\n",
      "\n",
      "Using Features: ['sensor_2', 'sensor_3', 'sensor_4', 'sensor_7', 'sensor_8', 'sensor_9', 'sensor_11', 'sensor_12', 'sensor_13', 'sensor_14', 'sensor_15', 'sensor_17', 'sensor_20', 'sensor_21']\n",
      "\n",
      "--- Normalizing Data ---\n",
      "DataLoaders created - Train: 12585, Val: 3146, Test: 100\n",
      "Input size: 700 (window_size * num_features)\n",
      "\n",
      "Creating and training baseline model...\n",
      "✅ Created MLP with architecture: 700 -> 256 -> 128 -> 64 -> 1\n",
      "Epoch 1/1000 (Baseline Training): Train Loss: 6727.8263, Train MSE: 6739.54, Val Loss: 4981.0175, Val MSE: 4975.25 (Best)\n",
      "Epoch 2/1000 (Baseline Training): Train Loss: 2604.4373, Train MSE: 2609.69, Val Loss: 1876.5783, Val MSE: 1876.12 (Best)\n",
      "Epoch 3/1000 (Baseline Training): Train Loss: 1849.8720, Train MSE: 1848.92, Val Loss: 1580.4582, Val MSE: 1579.83 (Best)\n",
      "Epoch 4/1000 (Baseline Training): Train Loss: 1515.4327, Train MSE: 1517.93, Val Loss: 1210.9518, Val MSE: 1210.15 (Best)\n",
      "Epoch 5/1000 (Baseline Training): Train Loss: 1138.0658, Train MSE: 1139.66, Val Loss: 874.1538, Val MSE: 873.00 (Best)\n",
      "Epoch 6/1000 (Baseline Training): Train Loss: 876.1842, Train MSE: 877.14, Val Loss: 716.4656, Val MSE: 715.31 (Best)\n",
      "Epoch 7/1000 (Baseline Training): Train Loss: 778.6072, Train MSE: 778.55, Val Loss: 666.2327, Val MSE: 664.89 (Best)\n",
      "Epoch 8/1000 (Baseline Training): Train Loss: 729.4886, Train MSE: 731.00, Val Loss: 611.9583, Val MSE: 610.88 (Best)\n",
      "Epoch 9/1000 (Baseline Training): Train Loss: 679.6892, Train MSE: 680.40, Val Loss: 570.4471, Val MSE: 569.36 (Best)\n",
      "Epoch 10/1000 (Baseline Training): Train Loss: 638.7610, Train MSE: 639.56, Val Loss: 530.9977, Val MSE: 529.92 (Best)\n",
      "Epoch 11/1000 (Baseline Training): Train Loss: 605.0767, Train MSE: 605.79, Val Loss: 497.0560, Val MSE: 496.02 (Best)\n",
      "Epoch 12/1000 (Baseline Training): Train Loss: 569.4325, Train MSE: 570.12, Val Loss: 473.6552, Val MSE: 472.26 (Best)\n",
      "Epoch 13/1000 (Baseline Training): Train Loss: 547.5592, Train MSE: 547.13, Val Loss: 444.0517, Val MSE: 442.66 (Best)\n",
      "Epoch 14/1000 (Baseline Training): Train Loss: 518.2138, Train MSE: 519.14, Val Loss: 422.1732, Val MSE: 420.62 (Best)\n",
      "Epoch 15/1000 (Baseline Training): Train Loss: 498.0604, Train MSE: 498.56, Val Loss: 404.9753, Val MSE: 403.25 (Best)\n",
      "Epoch 16/1000 (Baseline Training): Train Loss: 487.5405, Train MSE: 487.14, Val Loss: 381.0160, Val MSE: 379.35 (Best)\n",
      "Epoch 17/1000 (Baseline Training): Train Loss: 451.5284, Train MSE: 452.02, Val Loss: 355.5586, Val MSE: 354.08 (Best)\n",
      "Epoch 18/1000 (Baseline Training): Train Loss: 435.3879, Train MSE: 435.90, Val Loss: 333.9643, Val MSE: 332.69 (Best)\n",
      "Epoch 19/1000 (Baseline Training): Train Loss: 416.4370, Train MSE: 417.01, Val Loss: 317.0863, Val MSE: 315.70 (Best)\n",
      "Epoch 20/1000 (Baseline Training): Train Loss: 397.2672, Train MSE: 398.05, Val Loss: 308.9000, Val MSE: 307.39 (Best)\n",
      "Epoch 21/1000 (Baseline Training): Train Loss: 381.0682, Train MSE: 381.45, Val Loss: 280.2803, Val MSE: 279.06 (Best)\n",
      "Epoch 22/1000 (Baseline Training): Train Loss: 369.2505, Train MSE: 369.51, Val Loss: 264.6774, Val MSE: 263.55 (Best)\n",
      "Epoch 23/1000 (Baseline Training): Train Loss: 356.3953, Train MSE: 356.65, Val Loss: 253.6027, Val MSE: 252.52 (Best)\n",
      "Epoch 24/1000 (Baseline Training): Train Loss: 336.2291, Train MSE: 336.11, Val Loss: 234.9977, Val MSE: 234.15 (Best)\n",
      "Epoch 25/1000 (Baseline Training): Train Loss: 326.2999, Train MSE: 326.34, Val Loss: 223.8235, Val MSE: 223.04 (Best)\n",
      "Epoch 26/1000 (Baseline Training): Train Loss: 311.6673, Train MSE: 312.56, Val Loss: 212.7104, Val MSE: 212.05 (Best)\n",
      "Epoch 27/1000 (Baseline Training): Train Loss: 304.4463, Train MSE: 304.09, Val Loss: 205.5279, Val MSE: 204.83 (Best)\n",
      "Epoch 28/1000 (Baseline Training): Train Loss: 302.6019, Train MSE: 302.09, Val Loss: 197.8196, Val MSE: 197.21 (Best)\n",
      "Epoch 29/1000 (Baseline Training): Train Loss: 288.9740, Train MSE: 288.89, Val Loss: 189.2082, Val MSE: 188.77 (Best)\n",
      "Epoch 30/1000 (Baseline Training): Train Loss: 283.5779, Train MSE: 283.43, Val Loss: 189.5460, Val MSE: 189.04\n",
      "Epoch 31/1000 (Baseline Training): Train Loss: 275.4174, Train MSE: 275.32, Val Loss: 182.3490, Val MSE: 181.91 (Best)\n",
      "Epoch 32/1000 (Baseline Training): Train Loss: 275.1944, Train MSE: 275.54, Val Loss: 185.2365, Val MSE: 184.77\n",
      "Epoch 33/1000 (Baseline Training): Train Loss: 270.4382, Train MSE: 270.88, Val Loss: 176.4903, Val MSE: 176.16 (Best)\n",
      "Epoch 34/1000 (Baseline Training): Train Loss: 266.1548, Train MSE: 266.17, Val Loss: 171.8669, Val MSE: 171.74 (Best)\n",
      "Epoch 35/1000 (Baseline Training): Train Loss: 267.5968, Train MSE: 267.43, Val Loss: 169.9636, Val MSE: 169.89 (Best)\n",
      "Epoch 36/1000 (Baseline Training): Train Loss: 267.8141, Train MSE: 267.32, Val Loss: 168.9831, Val MSE: 168.78 (Best)\n",
      "Epoch 37/1000 (Baseline Training): Train Loss: 264.0063, Train MSE: 263.99, Val Loss: 169.5859, Val MSE: 169.38\n",
      "Epoch 38/1000 (Baseline Training): Train Loss: 259.7790, Train MSE: 259.21, Val Loss: 175.0689, Val MSE: 174.80\n",
      "Epoch 39/1000 (Baseline Training): Train Loss: 256.9516, Train MSE: 257.15, Val Loss: 166.6134, Val MSE: 166.48 (Best)\n",
      "Epoch 40/1000 (Baseline Training): Train Loss: 256.7237, Train MSE: 256.70, Val Loss: 163.0813, Val MSE: 163.13 (Best)\n",
      "Epoch 41/1000 (Baseline Training): Train Loss: 260.1075, Train MSE: 260.22, Val Loss: 170.5882, Val MSE: 170.42\n",
      "Epoch 42/1000 (Baseline Training): Train Loss: 251.3515, Train MSE: 252.05, Val Loss: 166.3639, Val MSE: 166.27\n",
      "Epoch 43/1000 (Baseline Training): Train Loss: 250.2760, Train MSE: 249.97, Val Loss: 165.4014, Val MSE: 165.23\n",
      "Epoch 44/1000 (Baseline Training): Train Loss: 250.8171, Train MSE: 251.06, Val Loss: 159.0942, Val MSE: 159.13 (Best)\n",
      "Epoch 45/1000 (Baseline Training): Train Loss: 253.9464, Train MSE: 253.94, Val Loss: 159.1332, Val MSE: 159.09\n",
      "Epoch 46/1000 (Baseline Training): Train Loss: 249.2512, Train MSE: 249.07, Val Loss: 161.0498, Val MSE: 161.00\n",
      "Epoch 47/1000 (Baseline Training): Train Loss: 250.4712, Train MSE: 250.34, Val Loss: 158.3385, Val MSE: 158.33 (Best)\n",
      "Epoch 48/1000 (Baseline Training): Train Loss: 253.8491, Train MSE: 252.72, Val Loss: 159.9862, Val MSE: 159.97\n",
      "Epoch 49/1000 (Baseline Training): Train Loss: 250.1953, Train MSE: 250.34, Val Loss: 167.0173, Val MSE: 166.91\n",
      "Epoch 50/1000 (Baseline Training): Train Loss: 250.5250, Train MSE: 250.39, Val Loss: 157.0634, Val MSE: 157.09 (Best)\n",
      "Epoch 51/1000 (Baseline Training): Train Loss: 250.6284, Train MSE: 250.60, Val Loss: 155.8897, Val MSE: 155.95 (Best)\n",
      "Epoch 52/1000 (Baseline Training): Train Loss: 241.7393, Train MSE: 241.88, Val Loss: 156.8652, Val MSE: 156.89\n",
      "Epoch 53/1000 (Baseline Training): Train Loss: 241.2889, Train MSE: 241.55, Val Loss: 160.4983, Val MSE: 160.49\n",
      "Epoch 54/1000 (Baseline Training): Train Loss: 243.2974, Train MSE: 244.08, Val Loss: 166.5178, Val MSE: 166.48\n",
      "Epoch 55/1000 (Baseline Training): Train Loss: 244.9768, Train MSE: 244.90, Val Loss: 154.7205, Val MSE: 154.79 (Best)\n",
      "Epoch 56/1000 (Baseline Training): Train Loss: 239.1691, Train MSE: 239.04, Val Loss: 164.4266, Val MSE: 164.35\n",
      "Epoch 57/1000 (Baseline Training): Train Loss: 242.9416, Train MSE: 242.62, Val Loss: 153.5647, Val MSE: 153.69 (Best)\n",
      "Epoch 58/1000 (Baseline Training): Train Loss: 244.5752, Train MSE: 244.89, Val Loss: 159.1278, Val MSE: 159.39\n",
      "Epoch 59/1000 (Baseline Training): Train Loss: 239.8957, Train MSE: 239.74, Val Loss: 177.8933, Val MSE: 177.78\n",
      "Epoch 60/1000 (Baseline Training): Train Loss: 242.4292, Train MSE: 242.32, Val Loss: 152.4377, Val MSE: 152.57 (Best)\n",
      "Epoch 61/1000 (Baseline Training): Train Loss: 241.7053, Train MSE: 242.09, Val Loss: 158.2304, Val MSE: 158.31\n",
      "Epoch 62/1000 (Baseline Training): Train Loss: 244.1710, Train MSE: 243.14, Val Loss: 161.3628, Val MSE: 161.39\n",
      "Epoch 63/1000 (Baseline Training): Train Loss: 240.9238, Train MSE: 240.88, Val Loss: 160.3812, Val MSE: 160.40\n",
      "Epoch 64/1000 (Baseline Training): Train Loss: 241.1491, Train MSE: 240.85, Val Loss: 155.9402, Val MSE: 155.99\n",
      "Epoch 65/1000 (Baseline Training): Train Loss: 240.6685, Train MSE: 240.97, Val Loss: 150.9377, Val MSE: 151.11 (Best)\n",
      "Epoch 66/1000 (Baseline Training): Train Loss: 241.0013, Train MSE: 241.17, Val Loss: 151.0566, Val MSE: 151.26\n",
      "Epoch 67/1000 (Baseline Training): Train Loss: 237.1212, Train MSE: 236.69, Val Loss: 149.7062, Val MSE: 149.88 (Best)\n",
      "Epoch 68/1000 (Baseline Training): Train Loss: 246.7313, Train MSE: 246.88, Val Loss: 150.0252, Val MSE: 150.20\n",
      "Epoch 69/1000 (Baseline Training): Train Loss: 237.8999, Train MSE: 238.49, Val Loss: 150.1515, Val MSE: 150.34\n",
      "Epoch 70/1000 (Baseline Training): Train Loss: 231.3662, Train MSE: 231.33, Val Loss: 162.0304, Val MSE: 162.05\n",
      "Epoch 71/1000 (Baseline Training): Train Loss: 239.9038, Train MSE: 239.67, Val Loss: 149.5949, Val MSE: 149.80 (Best)\n",
      "Epoch 72/1000 (Baseline Training): Train Loss: 238.9078, Train MSE: 238.81, Val Loss: 149.2465, Val MSE: 149.44 (Best)\n",
      "Epoch 73/1000 (Baseline Training): Train Loss: 240.7730, Train MSE: 240.71, Val Loss: 161.3838, Val MSE: 161.41\n",
      "Epoch 74/1000 (Baseline Training): Train Loss: 230.4213, Train MSE: 230.45, Val Loss: 152.0632, Val MSE: 152.19\n",
      "Epoch 75/1000 (Baseline Training): Train Loss: 230.4124, Train MSE: 230.68, Val Loss: 155.9974, Val MSE: 156.08\n",
      "Epoch 76/1000 (Baseline Training): Train Loss: 235.4313, Train MSE: 235.34, Val Loss: 148.3254, Val MSE: 148.52 (Best)\n",
      "Epoch 77/1000 (Baseline Training): Train Loss: 229.4123, Train MSE: 229.31, Val Loss: 148.3651, Val MSE: 148.55\n",
      "Epoch 78/1000 (Baseline Training): Train Loss: 234.5772, Train MSE: 233.51, Val Loss: 173.1345, Val MSE: 173.14\n",
      "Epoch 79/1000 (Baseline Training): Train Loss: 233.9497, Train MSE: 233.32, Val Loss: 148.7583, Val MSE: 148.96\n",
      "Epoch 80/1000 (Baseline Training): Train Loss: 240.3604, Train MSE: 238.97, Val Loss: 150.9876, Val MSE: 151.12\n",
      "Epoch 81/1000 (Baseline Training): Train Loss: 234.1590, Train MSE: 234.37, Val Loss: 148.0643, Val MSE: 148.28 (Best)\n",
      "Epoch 82/1000 (Baseline Training): Train Loss: 231.8837, Train MSE: 231.71, Val Loss: 149.0609, Val MSE: 149.23\n",
      "Epoch 83/1000 (Baseline Training): Train Loss: 234.0505, Train MSE: 234.62, Val Loss: 166.3199, Val MSE: 166.36\n",
      "Epoch 84/1000 (Baseline Training): Train Loss: 233.6780, Train MSE: 234.27, Val Loss: 147.7289, Val MSE: 147.89 (Best)\n",
      "Epoch 85/1000 (Baseline Training): Train Loss: 232.7135, Train MSE: 232.78, Val Loss: 157.8765, Val MSE: 157.97\n",
      "Epoch 86/1000 (Baseline Training): Train Loss: 237.7916, Train MSE: 237.64, Val Loss: 148.1156, Val MSE: 148.29\n",
      "Epoch 87/1000 (Baseline Training): Train Loss: 231.6113, Train MSE: 231.90, Val Loss: 146.9061, Val MSE: 147.14 (Best)\n",
      "Epoch 88/1000 (Baseline Training): Train Loss: 235.3726, Train MSE: 235.14, Val Loss: 150.4062, Val MSE: 150.55\n",
      "Epoch 89/1000 (Baseline Training): Train Loss: 235.7697, Train MSE: 235.97, Val Loss: 149.6339, Val MSE: 149.80\n",
      "Epoch 90/1000 (Baseline Training): Train Loss: 230.1173, Train MSE: 230.64, Val Loss: 169.9052, Val MSE: 169.94\n",
      "Epoch 91/1000 (Baseline Training): Train Loss: 228.0367, Train MSE: 228.60, Val Loss: 146.0090, Val MSE: 146.17 (Best)\n",
      "Epoch 92/1000 (Baseline Training): Train Loss: 232.6670, Train MSE: 232.77, Val Loss: 171.5589, Val MSE: 171.60\n",
      "Epoch 93/1000 (Baseline Training): Train Loss: 233.4301, Train MSE: 232.78, Val Loss: 148.0502, Val MSE: 148.23\n",
      "Epoch 94/1000 (Baseline Training): Train Loss: 234.8575, Train MSE: 234.46, Val Loss: 144.8325, Val MSE: 145.04 (Best)\n",
      "Epoch 95/1000 (Baseline Training): Train Loss: 229.3257, Train MSE: 229.49, Val Loss: 146.4590, Val MSE: 146.65\n",
      "Epoch 96/1000 (Baseline Training): Train Loss: 231.3263, Train MSE: 231.18, Val Loss: 153.8477, Val MSE: 153.99\n",
      "Epoch 97/1000 (Baseline Training): Train Loss: 232.1098, Train MSE: 232.00, Val Loss: 146.1314, Val MSE: 146.30\n",
      "Epoch 98/1000 (Baseline Training): Train Loss: 226.3964, Train MSE: 225.59, Val Loss: 146.7631, Val MSE: 146.92\n",
      "Epoch 99/1000 (Baseline Training): Train Loss: 229.7838, Train MSE: 229.63, Val Loss: 153.0647, Val MSE: 153.20\n",
      "Epoch 100/1000 (Baseline Training): Train Loss: 231.1611, Train MSE: 231.87, Val Loss: 148.3092, Val MSE: 148.47\n",
      "Epoch 101/1000 (Baseline Training): Train Loss: 231.2796, Train MSE: 231.02, Val Loss: 143.8999, Val MSE: 144.14 (Best)\n",
      "Epoch 102/1000 (Baseline Training): Train Loss: 233.6987, Train MSE: 233.30, Val Loss: 143.5982, Val MSE: 143.83 (Best)\n",
      "Epoch 103/1000 (Baseline Training): Train Loss: 229.6317, Train MSE: 229.59, Val Loss: 148.0610, Val MSE: 148.25\n",
      "Epoch 104/1000 (Baseline Training): Train Loss: 226.3577, Train MSE: 226.06, Val Loss: 145.4067, Val MSE: 145.65\n",
      "Epoch 105/1000 (Baseline Training): Train Loss: 229.0098, Train MSE: 229.49, Val Loss: 147.2484, Val MSE: 147.42\n",
      "Epoch 106/1000 (Baseline Training): Train Loss: 229.7060, Train MSE: 229.64, Val Loss: 147.6132, Val MSE: 147.80\n",
      "Epoch 107/1000 (Baseline Training): Train Loss: 226.6934, Train MSE: 226.99, Val Loss: 143.5336, Val MSE: 143.79 (Best)\n",
      "Epoch 108/1000 (Baseline Training): Train Loss: 231.8959, Train MSE: 232.16, Val Loss: 148.7517, Val MSE: 148.93\n",
      "Epoch 109/1000 (Baseline Training): Train Loss: 232.8097, Train MSE: 233.17, Val Loss: 146.6280, Val MSE: 146.79\n",
      "Epoch 110/1000 (Baseline Training): Train Loss: 225.0157, Train MSE: 225.45, Val Loss: 143.4980, Val MSE: 143.72 (Best)\n",
      "Epoch 111/1000 (Baseline Training): Train Loss: 224.6175, Train MSE: 224.67, Val Loss: 143.7180, Val MSE: 143.94\n",
      "Epoch 112/1000 (Baseline Training): Train Loss: 227.3939, Train MSE: 227.64, Val Loss: 145.7263, Val MSE: 145.94\n",
      "Epoch 113/1000 (Baseline Training): Train Loss: 233.5830, Train MSE: 233.82, Val Loss: 143.2714, Val MSE: 143.49 (Best)\n",
      "Epoch 114/1000 (Baseline Training): Train Loss: 225.2354, Train MSE: 225.92, Val Loss: 146.7644, Val MSE: 146.96\n",
      "Epoch 115/1000 (Baseline Training): Train Loss: 224.2210, Train MSE: 224.30, Val Loss: 142.0182, Val MSE: 142.27 (Best)\n",
      "Epoch 116/1000 (Baseline Training): Train Loss: 224.4708, Train MSE: 225.20, Val Loss: 142.1376, Val MSE: 142.40\n",
      "Epoch 117/1000 (Baseline Training): Train Loss: 228.0828, Train MSE: 227.92, Val Loss: 145.1296, Val MSE: 145.35\n",
      "Epoch 118/1000 (Baseline Training): Train Loss: 222.9564, Train MSE: 222.73, Val Loss: 142.5127, Val MSE: 142.77\n",
      "Epoch 119/1000 (Baseline Training): Train Loss: 223.7396, Train MSE: 224.07, Val Loss: 143.3583, Val MSE: 143.65\n",
      "Epoch 120/1000 (Baseline Training): Train Loss: 223.1138, Train MSE: 223.11, Val Loss: 142.2225, Val MSE: 142.49\n",
      "Epoch 121/1000 (Baseline Training): Train Loss: 225.7744, Train MSE: 225.87, Val Loss: 144.0583, Val MSE: 144.31\n",
      "Epoch 122/1000 (Baseline Training): Train Loss: 223.2869, Train MSE: 223.58, Val Loss: 142.7395, Val MSE: 142.97\n",
      "Epoch 123/1000 (Baseline Training): Train Loss: 225.9988, Train MSE: 225.84, Val Loss: 153.4922, Val MSE: 153.67\n",
      "Epoch 124/1000 (Baseline Training): Train Loss: 227.2603, Train MSE: 227.00, Val Loss: 144.5359, Val MSE: 144.76\n",
      "Epoch 125/1000 (Baseline Training): Train Loss: 225.7536, Train MSE: 226.00, Val Loss: 146.6025, Val MSE: 146.81\n",
      "Epoch 126/1000 (Baseline Training): Train Loss: 227.9030, Train MSE: 228.04, Val Loss: 141.8279, Val MSE: 142.06 (Best)\n",
      "Epoch 127/1000 (Baseline Training): Train Loss: 225.7836, Train MSE: 225.76, Val Loss: 142.6738, Val MSE: 142.96\n",
      "Epoch 128/1000 (Baseline Training): Train Loss: 227.1760, Train MSE: 226.76, Val Loss: 142.3215, Val MSE: 142.57\n",
      "Epoch 129/1000 (Baseline Training): Train Loss: 222.1882, Train MSE: 221.94, Val Loss: 151.4715, Val MSE: 151.67\n",
      "Epoch 130/1000 (Baseline Training): Train Loss: 224.2799, Train MSE: 223.65, Val Loss: 156.1751, Val MSE: 156.34\n",
      "Epoch 131/1000 (Baseline Training): Train Loss: 223.4721, Train MSE: 223.93, Val Loss: 141.8881, Val MSE: 142.14\n",
      "Epoch 132/1000 (Baseline Training): Train Loss: 227.3871, Train MSE: 226.71, Val Loss: 144.3707, Val MSE: 144.59\n",
      "Epoch 133/1000 (Baseline Training): Train Loss: 219.7804, Train MSE: 220.55, Val Loss: 159.9458, Val MSE: 160.09\n",
      "Epoch 134/1000 (Baseline Training): Train Loss: 227.1921, Train MSE: 226.99, Val Loss: 151.1732, Val MSE: 151.38\n",
      "Epoch 135/1000 (Baseline Training): Train Loss: 226.2787, Train MSE: 225.37, Val Loss: 142.8764, Val MSE: 143.12\n",
      "Epoch 136/1000 (Baseline Training): Train Loss: 225.9305, Train MSE: 226.09, Val Loss: 142.1424, Val MSE: 142.38\n",
      "Epoch 137/1000 (Baseline Training): Train Loss: 224.3998, Train MSE: 224.05, Val Loss: 148.8872, Val MSE: 149.06\n",
      "Epoch 138/1000 (Baseline Training): Train Loss: 224.7720, Train MSE: 224.96, Val Loss: 141.5415, Val MSE: 141.77 (Best)\n",
      "Epoch 139/1000 (Baseline Training): Train Loss: 223.4924, Train MSE: 223.87, Val Loss: 144.1954, Val MSE: 144.41\n",
      "Epoch 140/1000 (Baseline Training): Train Loss: 230.2253, Train MSE: 230.86, Val Loss: 152.6595, Val MSE: 152.84\n",
      "Epoch 141/1000 (Baseline Training): Train Loss: 223.1115, Train MSE: 222.78, Val Loss: 145.9798, Val MSE: 146.19\n",
      "Epoch 142/1000 (Baseline Training): Train Loss: 222.6605, Train MSE: 223.03, Val Loss: 144.0293, Val MSE: 144.28\n",
      "Epoch 143/1000 (Baseline Training): Train Loss: 225.2850, Train MSE: 224.75, Val Loss: 140.9828, Val MSE: 141.25 (Best)\n",
      "Epoch 144/1000 (Baseline Training): Train Loss: 220.5854, Train MSE: 220.22, Val Loss: 140.2161, Val MSE: 140.48 (Best)\n",
      "Epoch 145/1000 (Baseline Training): Train Loss: 220.8167, Train MSE: 220.89, Val Loss: 140.0943, Val MSE: 140.36 (Best)\n",
      "Epoch 146/1000 (Baseline Training): Train Loss: 221.7805, Train MSE: 221.32, Val Loss: 139.9954, Val MSE: 140.27 (Best)\n",
      "Epoch 147/1000 (Baseline Training): Train Loss: 221.8842, Train MSE: 222.16, Val Loss: 150.3998, Val MSE: 150.58\n",
      "Epoch 148/1000 (Baseline Training): Train Loss: 222.8614, Train MSE: 222.80, Val Loss: 140.6246, Val MSE: 140.89\n",
      "Epoch 149/1000 (Baseline Training): Train Loss: 220.7544, Train MSE: 220.08, Val Loss: 139.8488, Val MSE: 140.13 (Best)\n",
      "Epoch 150/1000 (Baseline Training): Train Loss: 225.5723, Train MSE: 225.49, Val Loss: 144.9599, Val MSE: 145.19\n",
      "Epoch 151/1000 (Baseline Training): Train Loss: 225.3186, Train MSE: 224.69, Val Loss: 139.8359, Val MSE: 140.11 (Best)\n",
      "Epoch 152/1000 (Baseline Training): Train Loss: 224.8999, Train MSE: 224.98, Val Loss: 140.2751, Val MSE: 140.58\n",
      "Epoch 153/1000 (Baseline Training): Train Loss: 218.1286, Train MSE: 218.23, Val Loss: 155.7960, Val MSE: 156.00\n",
      "Epoch 154/1000 (Baseline Training): Train Loss: 224.8763, Train MSE: 224.67, Val Loss: 139.5522, Val MSE: 139.80 (Best)\n",
      "Epoch 155/1000 (Baseline Training): Train Loss: 220.3996, Train MSE: 220.73, Val Loss: 139.9496, Val MSE: 140.21\n",
      "Epoch 156/1000 (Baseline Training): Train Loss: 217.9601, Train MSE: 217.47, Val Loss: 139.5849, Val MSE: 139.88\n",
      "Epoch 157/1000 (Baseline Training): Train Loss: 223.2858, Train MSE: 223.43, Val Loss: 142.4092, Val MSE: 142.65\n",
      "Epoch 158/1000 (Baseline Training): Train Loss: 223.5931, Train MSE: 223.79, Val Loss: 139.4471, Val MSE: 139.73 (Best)\n",
      "Epoch 159/1000 (Baseline Training): Train Loss: 221.4607, Train MSE: 221.26, Val Loss: 140.5082, Val MSE: 140.85\n",
      "Epoch 160/1000 (Baseline Training): Train Loss: 223.4789, Train MSE: 223.82, Val Loss: 140.9113, Val MSE: 141.15\n",
      "Epoch 161/1000 (Baseline Training): Train Loss: 219.9804, Train MSE: 219.99, Val Loss: 140.2068, Val MSE: 140.46\n",
      "Epoch 162/1000 (Baseline Training): Train Loss: 226.7950, Train MSE: 226.23, Val Loss: 143.0554, Val MSE: 143.32\n",
      "Epoch 163/1000 (Baseline Training): Train Loss: 220.1882, Train MSE: 219.98, Val Loss: 141.2326, Val MSE: 141.50\n",
      "Epoch 164/1000 (Baseline Training): Train Loss: 219.9507, Train MSE: 219.18, Val Loss: 142.7080, Val MSE: 142.92\n",
      "Epoch 165/1000 (Baseline Training): Train Loss: 220.8565, Train MSE: 220.80, Val Loss: 139.9774, Val MSE: 140.28\n",
      "Epoch 166/1000 (Baseline Training): Train Loss: 219.0513, Train MSE: 219.09, Val Loss: 143.1378, Val MSE: 143.38\n",
      "Epoch 167/1000 (Baseline Training): Train Loss: 217.9278, Train MSE: 218.56, Val Loss: 143.9901, Val MSE: 144.25\n",
      "Epoch 168/1000 (Baseline Training): Train Loss: 220.5208, Train MSE: 220.56, Val Loss: 143.8726, Val MSE: 144.10\n",
      "Epoch 169/1000 (Baseline Training): Train Loss: 220.8907, Train MSE: 220.26, Val Loss: 139.4559, Val MSE: 139.71\n",
      "Epoch 170/1000 (Baseline Training): Train Loss: 217.1632, Train MSE: 217.03, Val Loss: 139.2210, Val MSE: 139.53 (Best)\n",
      "Epoch 171/1000 (Baseline Training): Train Loss: 219.6339, Train MSE: 220.00, Val Loss: 139.8337, Val MSE: 140.15\n",
      "Epoch 172/1000 (Baseline Training): Train Loss: 218.5869, Train MSE: 219.13, Val Loss: 138.9333, Val MSE: 139.24 (Best)\n",
      "Epoch 173/1000 (Baseline Training): Train Loss: 214.0480, Train MSE: 214.25, Val Loss: 147.1036, Val MSE: 147.35\n",
      "Epoch 174/1000 (Baseline Training): Train Loss: 224.3162, Train MSE: 223.29, Val Loss: 149.1292, Val MSE: 149.32\n",
      "Epoch 175/1000 (Baseline Training): Train Loss: 224.6088, Train MSE: 224.59, Val Loss: 138.6031, Val MSE: 138.89 (Best)\n",
      "Epoch 176/1000 (Baseline Training): Train Loss: 212.8203, Train MSE: 213.21, Val Loss: 141.3716, Val MSE: 141.67\n",
      "Epoch 177/1000 (Baseline Training): Train Loss: 219.0783, Train MSE: 219.21, Val Loss: 142.9093, Val MSE: 143.14\n",
      "Epoch 178/1000 (Baseline Training): Train Loss: 222.9828, Train MSE: 222.35, Val Loss: 140.0489, Val MSE: 140.32\n",
      "Epoch 179/1000 (Baseline Training): Train Loss: 218.9983, Train MSE: 218.97, Val Loss: 139.0990, Val MSE: 139.36\n",
      "Epoch 180/1000 (Baseline Training): Train Loss: 220.6482, Train MSE: 221.07, Val Loss: 141.2313, Val MSE: 141.47\n",
      "Epoch 181/1000 (Baseline Training): Train Loss: 216.2353, Train MSE: 216.19, Val Loss: 147.6862, Val MSE: 147.95\n",
      "Epoch 182/1000 (Baseline Training): Train Loss: 216.2272, Train MSE: 215.79, Val Loss: 141.6099, Val MSE: 141.87\n",
      "Epoch 183/1000 (Baseline Training): Train Loss: 218.6932, Train MSE: 219.03, Val Loss: 138.6205, Val MSE: 138.90\n",
      "Epoch 184/1000 (Baseline Training): Train Loss: 214.8480, Train MSE: 214.93, Val Loss: 138.9853, Val MSE: 139.25\n",
      "Epoch 185/1000 (Baseline Training): Train Loss: 214.4963, Train MSE: 214.80, Val Loss: 141.9423, Val MSE: 142.19\n",
      "Epoch 186/1000 (Baseline Training): Train Loss: 218.1436, Train MSE: 218.00, Val Loss: 141.8035, Val MSE: 142.09\n",
      "Epoch 187/1000 (Baseline Training): Train Loss: 218.3507, Train MSE: 218.53, Val Loss: 143.7726, Val MSE: 144.03\n",
      "Epoch 188/1000 (Baseline Training): Train Loss: 216.1013, Train MSE: 215.96, Val Loss: 147.5721, Val MSE: 147.83\n",
      "Epoch 189/1000 (Baseline Training): Train Loss: 219.0471, Train MSE: 218.97, Val Loss: 138.1522, Val MSE: 138.47 (Best)\n",
      "Epoch 190/1000 (Baseline Training): Train Loss: 217.7715, Train MSE: 217.54, Val Loss: 143.9504, Val MSE: 144.20\n",
      "Epoch 191/1000 (Baseline Training): Train Loss: 220.4474, Train MSE: 220.04, Val Loss: 138.8334, Val MSE: 139.14\n",
      "Epoch 192/1000 (Baseline Training): Train Loss: 215.1912, Train MSE: 215.59, Val Loss: 142.4226, Val MSE: 142.68\n",
      "Epoch 193/1000 (Baseline Training): Train Loss: 218.0008, Train MSE: 217.32, Val Loss: 145.6975, Val MSE: 145.93\n",
      "Epoch 194/1000 (Baseline Training): Train Loss: 218.8492, Train MSE: 219.00, Val Loss: 140.3690, Val MSE: 140.62\n",
      "Epoch 195/1000 (Baseline Training): Train Loss: 215.6643, Train MSE: 215.94, Val Loss: 142.1719, Val MSE: 142.44\n",
      "Epoch 196/1000 (Baseline Training): Train Loss: 216.2353, Train MSE: 216.29, Val Loss: 138.0823, Val MSE: 138.39 (Best)\n",
      "Epoch 197/1000 (Baseline Training): Train Loss: 213.8678, Train MSE: 213.60, Val Loss: 148.9341, Val MSE: 149.16\n",
      "Epoch 198/1000 (Baseline Training): Train Loss: 214.9383, Train MSE: 215.03, Val Loss: 139.1454, Val MSE: 139.42\n",
      "Epoch 199/1000 (Baseline Training): Train Loss: 216.2334, Train MSE: 215.69, Val Loss: 146.4622, Val MSE: 146.72\n",
      "Epoch 200/1000 (Baseline Training): Train Loss: 210.7055, Train MSE: 210.47, Val Loss: 140.0512, Val MSE: 140.33\n",
      "Epoch 201/1000 (Baseline Training): Train Loss: 213.0770, Train MSE: 213.74, Val Loss: 138.8932, Val MSE: 139.18\n",
      "Epoch 202/1000 (Baseline Training): Train Loss: 221.6563, Train MSE: 221.66, Val Loss: 139.5215, Val MSE: 139.83\n",
      "Epoch 203/1000 (Baseline Training): Train Loss: 218.5273, Train MSE: 217.96, Val Loss: 139.1634, Val MSE: 139.46\n",
      "Epoch 204/1000 (Baseline Training): Train Loss: 216.2958, Train MSE: 215.62, Val Loss: 143.5746, Val MSE: 143.84\n",
      "Epoch 205/1000 (Baseline Training): Train Loss: 211.9576, Train MSE: 211.75, Val Loss: 141.9499, Val MSE: 142.20\n",
      "Epoch 206/1000 (Baseline Training): Train Loss: 216.1211, Train MSE: 215.87, Val Loss: 138.9189, Val MSE: 139.22\n",
      "Epoch 207/1000 (Baseline Training): Train Loss: 219.9181, Train MSE: 219.25, Val Loss: 142.0113, Val MSE: 142.33\n",
      "Epoch 208/1000 (Baseline Training): Train Loss: 216.8339, Train MSE: 215.96, Val Loss: 158.0326, Val MSE: 158.29\n",
      "Epoch 209/1000 (Baseline Training): Train Loss: 217.7231, Train MSE: 217.91, Val Loss: 140.1169, Val MSE: 140.41\n",
      "Epoch 210/1000 (Baseline Training): Train Loss: 217.7794, Train MSE: 217.88, Val Loss: 138.2757, Val MSE: 138.56\n",
      "Epoch 211/1000 (Baseline Training): Train Loss: 213.5199, Train MSE: 212.73, Val Loss: 138.4703, Val MSE: 138.76\n",
      "Epoch 212/1000 (Baseline Training): Train Loss: 211.9057, Train MSE: 211.52, Val Loss: 138.8404, Val MSE: 139.14\n",
      "Epoch 213/1000 (Baseline Training): Train Loss: 213.6035, Train MSE: 213.56, Val Loss: 137.2465, Val MSE: 137.53 (Best)\n",
      "Epoch 214/1000 (Baseline Training): Train Loss: 219.2537, Train MSE: 219.05, Val Loss: 140.6265, Val MSE: 140.88\n",
      "Epoch 215/1000 (Baseline Training): Train Loss: 209.7739, Train MSE: 209.50, Val Loss: 137.4926, Val MSE: 137.77\n",
      "Epoch 216/1000 (Baseline Training): Train Loss: 216.9669, Train MSE: 217.17, Val Loss: 151.9151, Val MSE: 152.19\n",
      "Epoch 217/1000 (Baseline Training): Train Loss: 213.0651, Train MSE: 213.72, Val Loss: 147.0668, Val MSE: 147.31\n",
      "Epoch 218/1000 (Baseline Training): Train Loss: 217.6127, Train MSE: 217.28, Val Loss: 137.2425, Val MSE: 137.54 (Best)\n",
      "Epoch 219/1000 (Baseline Training): Train Loss: 214.6267, Train MSE: 214.55, Val Loss: 138.8993, Val MSE: 139.23\n",
      "Epoch 220/1000 (Baseline Training): Train Loss: 211.8503, Train MSE: 211.92, Val Loss: 136.6883, Val MSE: 136.99 (Best)\n",
      "Epoch 221/1000 (Baseline Training): Train Loss: 213.2291, Train MSE: 213.56, Val Loss: 137.3135, Val MSE: 137.63\n",
      "Epoch 222/1000 (Baseline Training): Train Loss: 207.7488, Train MSE: 208.44, Val Loss: 137.2588, Val MSE: 137.59\n",
      "Epoch 223/1000 (Baseline Training): Train Loss: 213.4139, Train MSE: 213.61, Val Loss: 139.1382, Val MSE: 139.40\n",
      "Epoch 224/1000 (Baseline Training): Train Loss: 212.8128, Train MSE: 212.87, Val Loss: 137.8395, Val MSE: 138.09\n",
      "Epoch 225/1000 (Baseline Training): Train Loss: 212.2323, Train MSE: 212.59, Val Loss: 142.3247, Val MSE: 142.62\n",
      "Epoch 226/1000 (Baseline Training): Train Loss: 215.9105, Train MSE: 216.35, Val Loss: 137.3163, Val MSE: 137.63\n",
      "Epoch 227/1000 (Baseline Training): Train Loss: 211.4322, Train MSE: 212.10, Val Loss: 138.4230, Val MSE: 138.72\n",
      "Epoch 228/1000 (Baseline Training): Train Loss: 215.1159, Train MSE: 215.20, Val Loss: 139.1276, Val MSE: 139.43\n",
      "Epoch 229/1000 (Baseline Training): Train Loss: 215.7178, Train MSE: 215.30, Val Loss: 138.2385, Val MSE: 138.52\n",
      "Epoch 230/1000 (Baseline Training): Train Loss: 209.8622, Train MSE: 210.13, Val Loss: 141.4044, Val MSE: 141.70\n",
      "Epoch 231/1000 (Baseline Training): Train Loss: 213.4593, Train MSE: 212.49, Val Loss: 136.7520, Val MSE: 137.08\n",
      "Epoch 232/1000 (Baseline Training): Train Loss: 216.2157, Train MSE: 216.58, Val Loss: 137.2448, Val MSE: 137.54\n",
      "Epoch 233/1000 (Baseline Training): Train Loss: 211.1920, Train MSE: 211.53, Val Loss: 138.8321, Val MSE: 139.14\n",
      "Epoch 234/1000 (Baseline Training): Train Loss: 213.0705, Train MSE: 213.10, Val Loss: 137.2758, Val MSE: 137.58\n",
      "Epoch 235/1000 (Baseline Training): Train Loss: 213.7058, Train MSE: 213.44, Val Loss: 139.0435, Val MSE: 139.35\n",
      "Epoch 236/1000 (Baseline Training): Train Loss: 211.8988, Train MSE: 211.86, Val Loss: 138.1117, Val MSE: 138.41\n",
      "Epoch 237/1000 (Baseline Training): Train Loss: 214.8885, Train MSE: 214.21, Val Loss: 136.4855, Val MSE: 136.78 (Best)\n",
      "Epoch 238/1000 (Baseline Training): Train Loss: 211.5835, Train MSE: 211.08, Val Loss: 148.5613, Val MSE: 148.85\n",
      "Epoch 239/1000 (Baseline Training): Train Loss: 206.6677, Train MSE: 206.61, Val Loss: 141.0653, Val MSE: 141.34\n",
      "Epoch 240/1000 (Baseline Training): Train Loss: 215.5507, Train MSE: 216.00, Val Loss: 140.5698, Val MSE: 140.86\n",
      "Epoch 241/1000 (Baseline Training): Train Loss: 210.9772, Train MSE: 210.98, Val Loss: 138.4798, Val MSE: 138.82\n",
      "Epoch 242/1000 (Baseline Training): Train Loss: 215.2879, Train MSE: 214.55, Val Loss: 138.7411, Val MSE: 139.01\n",
      "Epoch 243/1000 (Baseline Training): Train Loss: 209.3513, Train MSE: 209.48, Val Loss: 137.0799, Val MSE: 137.39\n",
      "Epoch 244/1000 (Baseline Training): Train Loss: 211.7030, Train MSE: 212.23, Val Loss: 137.4170, Val MSE: 137.72\n",
      "Epoch 245/1000 (Baseline Training): Train Loss: 208.8274, Train MSE: 209.43, Val Loss: 137.6906, Val MSE: 137.98\n",
      "Epoch 246/1000 (Baseline Training): Train Loss: 210.0635, Train MSE: 209.70, Val Loss: 144.2783, Val MSE: 144.56\n",
      "Epoch 247/1000 (Baseline Training): Train Loss: 211.7421, Train MSE: 211.71, Val Loss: 141.1441, Val MSE: 141.47\n",
      "Epoch 248/1000 (Baseline Training): Train Loss: 211.5050, Train MSE: 211.44, Val Loss: 138.2992, Val MSE: 138.64\n",
      "Epoch 249/1000 (Baseline Training): Train Loss: 215.5878, Train MSE: 215.99, Val Loss: 143.8202, Val MSE: 144.10\n",
      "Epoch 250/1000 (Baseline Training): Train Loss: 213.3704, Train MSE: 212.82, Val Loss: 137.0116, Val MSE: 137.30\n",
      "Epoch 251/1000 (Baseline Training): Train Loss: 207.5130, Train MSE: 207.42, Val Loss: 136.6891, Val MSE: 137.02\n",
      "Epoch 252/1000 (Baseline Training): Train Loss: 210.6727, Train MSE: 210.88, Val Loss: 136.8916, Val MSE: 137.19\n",
      "Epoch 253/1000 (Baseline Training): Train Loss: 211.1719, Train MSE: 211.61, Val Loss: 136.8608, Val MSE: 137.17\n",
      "Epoch 254/1000 (Baseline Training): Train Loss: 211.8966, Train MSE: 211.77, Val Loss: 138.5282, Val MSE: 138.81\n",
      "Epoch 255/1000 (Baseline Training): Train Loss: 212.7568, Train MSE: 213.09, Val Loss: 144.0984, Val MSE: 144.38\n",
      "Epoch 256/1000 (Baseline Training): Train Loss: 210.8461, Train MSE: 211.47, Val Loss: 143.1903, Val MSE: 143.48\n",
      "Epoch 257/1000 (Baseline Training): Train Loss: 206.6273, Train MSE: 206.26, Val Loss: 141.0244, Val MSE: 141.29\n",
      "Early stopping triggered after 257 epochs\n",
      "Loaded best model state\n",
      "✅ Model saved to ./models_mlp_nasa/baseline_model.pth\n",
      "✅ ONNX model saved to ./models_mlp_nasa/baseline_model.onnx\n",
      "\n",
      "Evaluating baseline model...\n",
      "Baseline Results: MSE=438.44, MAE=12.95, MACs=0.22M, Params=0.22M\n",
      "\n",
      "Starting pruning experiments...\n",
      "\n",
      "--- Strategy: MagnitudeL2 ---\n",
      "\n",
      "Processing MagnitudeL2 at 20.0% sparsity...\n",
      "✅ Created MLP with architecture: 700 -> 256 -> 128 -> 64 -> 1\n",
      "Initial MACs: 0.22M\n",
      "Applying MagnitudeImportance pruning at 20.0% sparsity...\n",
      "Final MACs: 0.21M (Reduction: 5.1%)\n",
      "Fine-tuning pruned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch_pruning/pruner/algorithms/base_pruner.py:87: UserWarning: ch_sparsity is deprecated in v1.3.0. Please use pruning_ratio.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 (MagnitudeL2-20.0%): Train Loss: 212.6503, Train MSE: 213.00, Val Loss: 139.7663, Val MSE: 140.08 (Best)\n",
      "Epoch 2/1000 (MagnitudeL2-20.0%): Train Loss: 212.8761, Train MSE: 212.93, Val Loss: 136.6698, Val MSE: 136.99 (Best)\n",
      "Epoch 3/1000 (MagnitudeL2-20.0%): Train Loss: 215.6130, Train MSE: 215.56, Val Loss: 138.8912, Val MSE: 139.18\n",
      "Epoch 4/1000 (MagnitudeL2-20.0%): Train Loss: 216.2879, Train MSE: 215.63, Val Loss: 137.4016, Val MSE: 137.68\n",
      "Epoch 5/1000 (MagnitudeL2-20.0%): Train Loss: 214.3031, Train MSE: 213.53, Val Loss: 138.6409, Val MSE: 138.94\n",
      "Epoch 6/1000 (MagnitudeL2-20.0%): Train Loss: 212.1220, Train MSE: 212.07, Val Loss: 145.0006, Val MSE: 145.25\n",
      "Epoch 7/1000 (MagnitudeL2-20.0%): Train Loss: 211.7846, Train MSE: 212.55, Val Loss: 136.6600, Val MSE: 136.95 (Best)\n",
      "Epoch 8/1000 (MagnitudeL2-20.0%): Train Loss: 210.8035, Train MSE: 211.10, Val Loss: 141.0712, Val MSE: 141.33\n",
      "Epoch 9/1000 (MagnitudeL2-20.0%): Train Loss: 211.7502, Train MSE: 211.14, Val Loss: 141.1410, Val MSE: 141.45\n",
      "Epoch 10/1000 (MagnitudeL2-20.0%): Train Loss: 211.9797, Train MSE: 212.19, Val Loss: 137.1008, Val MSE: 137.41\n",
      "Epoch 11/1000 (MagnitudeL2-20.0%): Train Loss: 213.2549, Train MSE: 213.52, Val Loss: 140.1618, Val MSE: 140.46\n",
      "Epoch 12/1000 (MagnitudeL2-20.0%): Train Loss: 214.7915, Train MSE: 214.26, Val Loss: 137.8267, Val MSE: 138.11\n",
      "Epoch 13/1000 (MagnitudeL2-20.0%): Train Loss: 211.2946, Train MSE: 211.54, Val Loss: 140.2530, Val MSE: 140.52\n",
      "Epoch 14/1000 (MagnitudeL2-20.0%): Train Loss: 216.0206, Train MSE: 215.74, Val Loss: 136.6853, Val MSE: 136.98\n",
      "Epoch 15/1000 (MagnitudeL2-20.0%): Train Loss: 212.2542, Train MSE: 212.55, Val Loss: 140.9625, Val MSE: 141.29\n",
      "Epoch 16/1000 (MagnitudeL2-20.0%): Train Loss: 208.1657, Train MSE: 208.66, Val Loss: 137.1227, Val MSE: 137.45\n",
      "Epoch 17/1000 (MagnitudeL2-20.0%): Train Loss: 212.1646, Train MSE: 212.65, Val Loss: 140.4855, Val MSE: 140.75\n",
      "Epoch 18/1000 (MagnitudeL2-20.0%): Train Loss: 211.7113, Train MSE: 211.22, Val Loss: 138.7076, Val MSE: 138.99\n",
      "Epoch 19/1000 (MagnitudeL2-20.0%): Train Loss: 213.1033, Train MSE: 213.38, Val Loss: 145.9056, Val MSE: 146.21\n",
      "Epoch 20/1000 (MagnitudeL2-20.0%): Train Loss: 211.9460, Train MSE: 212.31, Val Loss: 136.6730, Val MSE: 136.95\n",
      "Epoch 21/1000 (MagnitudeL2-20.0%): Train Loss: 213.3840, Train MSE: 212.96, Val Loss: 141.8205, Val MSE: 142.12\n",
      "Epoch 22/1000 (MagnitudeL2-20.0%): Train Loss: 212.1874, Train MSE: 212.65, Val Loss: 136.8584, Val MSE: 137.09\n",
      "Epoch 23/1000 (MagnitudeL2-20.0%): Train Loss: 208.3752, Train MSE: 208.20, Val Loss: 143.2566, Val MSE: 143.50\n",
      "Epoch 24/1000 (MagnitudeL2-20.0%): Train Loss: 213.4498, Train MSE: 212.96, Val Loss: 137.4861, Val MSE: 137.79\n",
      "Epoch 25/1000 (MagnitudeL2-20.0%): Train Loss: 214.1212, Train MSE: 213.58, Val Loss: 143.4318, Val MSE: 143.70\n",
      "Epoch 26/1000 (MagnitudeL2-20.0%): Train Loss: 212.7922, Train MSE: 213.38, Val Loss: 136.9880, Val MSE: 137.28\n",
      "Epoch 27/1000 (MagnitudeL2-20.0%): Train Loss: 209.3855, Train MSE: 209.24, Val Loss: 136.2656, Val MSE: 136.56 (Best)\n",
      "Epoch 28/1000 (MagnitudeL2-20.0%): Train Loss: 215.4951, Train MSE: 215.86, Val Loss: 137.7635, Val MSE: 138.10\n",
      "Epoch 29/1000 (MagnitudeL2-20.0%): Train Loss: 213.4032, Train MSE: 213.59, Val Loss: 143.1081, Val MSE: 143.41\n",
      "Epoch 30/1000 (MagnitudeL2-20.0%): Train Loss: 215.8674, Train MSE: 215.09, Val Loss: 137.5567, Val MSE: 137.88\n",
      "Epoch 31/1000 (MagnitudeL2-20.0%): Train Loss: 210.3427, Train MSE: 209.99, Val Loss: 137.4991, Val MSE: 137.85\n",
      "Epoch 32/1000 (MagnitudeL2-20.0%): Train Loss: 209.9076, Train MSE: 210.26, Val Loss: 141.6542, Val MSE: 141.95\n",
      "Epoch 33/1000 (MagnitudeL2-20.0%): Train Loss: 211.1574, Train MSE: 211.02, Val Loss: 138.2956, Val MSE: 138.57\n",
      "Epoch 34/1000 (MagnitudeL2-20.0%): Train Loss: 211.6915, Train MSE: 211.74, Val Loss: 136.9659, Val MSE: 137.24\n",
      "Epoch 35/1000 (MagnitudeL2-20.0%): Train Loss: 214.2616, Train MSE: 213.71, Val Loss: 136.4720, Val MSE: 136.81\n",
      "Epoch 36/1000 (MagnitudeL2-20.0%): Train Loss: 209.4406, Train MSE: 209.59, Val Loss: 136.3648, Val MSE: 136.65\n",
      "Epoch 37/1000 (MagnitudeL2-20.0%): Train Loss: 208.9512, Train MSE: 209.18, Val Loss: 137.2868, Val MSE: 137.56\n",
      "Epoch 38/1000 (MagnitudeL2-20.0%): Train Loss: 209.2003, Train MSE: 209.40, Val Loss: 137.7807, Val MSE: 138.10\n",
      "Epoch 39/1000 (MagnitudeL2-20.0%): Train Loss: 211.9467, Train MSE: 212.27, Val Loss: 136.6289, Val MSE: 136.92\n",
      "Epoch 40/1000 (MagnitudeL2-20.0%): Train Loss: 207.4949, Train MSE: 207.77, Val Loss: 136.7380, Val MSE: 137.05\n",
      "Epoch 41/1000 (MagnitudeL2-20.0%): Train Loss: 211.1828, Train MSE: 210.42, Val Loss: 140.8341, Val MSE: 141.13\n",
      "Epoch 42/1000 (MagnitudeL2-20.0%): Train Loss: 210.1676, Train MSE: 210.79, Val Loss: 138.2268, Val MSE: 138.56\n",
      "Epoch 43/1000 (MagnitudeL2-20.0%): Train Loss: 214.7684, Train MSE: 214.73, Val Loss: 143.9584, Val MSE: 144.27\n",
      "Epoch 44/1000 (MagnitudeL2-20.0%): Train Loss: 212.5688, Train MSE: 211.88, Val Loss: 136.4659, Val MSE: 136.75\n",
      "Epoch 45/1000 (MagnitudeL2-20.0%): Train Loss: 210.8205, Train MSE: 211.29, Val Loss: 146.6868, Val MSE: 147.01\n",
      "Epoch 46/1000 (MagnitudeL2-20.0%): Train Loss: 208.5778, Train MSE: 208.76, Val Loss: 143.9085, Val MSE: 144.20\n",
      "Epoch 47/1000 (MagnitudeL2-20.0%): Train Loss: 212.3404, Train MSE: 212.70, Val Loss: 141.1209, Val MSE: 141.40\n",
      "Early stopping triggered after 47 epochs\n",
      "Loaded best model state\n",
      "Results: MSE=426.84, MAE=12.79, MACs=0.21M\n",
      "✅ Model saved to ./models_mlp_nasa/magnitudel2_sparsity_0.2.pth\n",
      "✅ ONNX model saved to ./models_mlp_nasa/magnitudel2_sparsity_0.2.onnx\n",
      "\n",
      "Processing MagnitudeL2 at 50.0% sparsity...\n",
      "✅ Created MLP with architecture: 700 -> 256 -> 128 -> 64 -> 1\n",
      "Initial MACs: 0.22M\n",
      "Applying MagnitudeImportance pruning at 50.0% sparsity...\n",
      "Final MACs: 0.19M (Reduction: 11.9%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/1000 (MagnitudeL2-50.0%): Train Loss: 221.0399, Train MSE: 220.99, Val Loss: 138.3825, Val MSE: 138.64 (Best)\n",
      "Epoch 2/1000 (MagnitudeL2-50.0%): Train Loss: 214.5492, Train MSE: 214.95, Val Loss: 140.4710, Val MSE: 140.71\n",
      "Epoch 3/1000 (MagnitudeL2-50.0%): Train Loss: 218.9804, Train MSE: 219.04, Val Loss: 139.3937, Val MSE: 139.70\n",
      "Epoch 4/1000 (MagnitudeL2-50.0%): Train Loss: 216.8238, Train MSE: 216.38, Val Loss: 141.4067, Val MSE: 141.67\n",
      "Epoch 5/1000 (MagnitudeL2-50.0%): Train Loss: 213.3662, Train MSE: 214.17, Val Loss: 137.6093, Val MSE: 137.89 (Best)\n",
      "Epoch 6/1000 (MagnitudeL2-50.0%): Train Loss: 212.9497, Train MSE: 212.42, Val Loss: 136.6385, Val MSE: 136.89 (Best)\n",
      "Epoch 7/1000 (MagnitudeL2-50.0%): Train Loss: 218.0256, Train MSE: 217.77, Val Loss: 138.2775, Val MSE: 138.56\n",
      "Epoch 8/1000 (MagnitudeL2-50.0%): Train Loss: 212.0972, Train MSE: 212.49, Val Loss: 137.0853, Val MSE: 137.38\n",
      "Epoch 9/1000 (MagnitudeL2-50.0%): Train Loss: 217.4633, Train MSE: 217.12, Val Loss: 136.4388, Val MSE: 136.72 (Best)\n",
      "Epoch 10/1000 (MagnitudeL2-50.0%): Train Loss: 212.9003, Train MSE: 212.57, Val Loss: 139.5821, Val MSE: 139.85\n",
      "Epoch 11/1000 (MagnitudeL2-50.0%): Train Loss: 216.4436, Train MSE: 216.71, Val Loss: 137.3344, Val MSE: 137.65\n",
      "Epoch 12/1000 (MagnitudeL2-50.0%): Train Loss: 210.9083, Train MSE: 210.78, Val Loss: 139.0628, Val MSE: 139.40\n",
      "Epoch 13/1000 (MagnitudeL2-50.0%): Train Loss: 220.0761, Train MSE: 219.98, Val Loss: 139.6362, Val MSE: 139.91\n",
      "Epoch 14/1000 (MagnitudeL2-50.0%): Train Loss: 213.1936, Train MSE: 213.40, Val Loss: 139.9616, Val MSE: 140.21\n",
      "Epoch 15/1000 (MagnitudeL2-50.0%): Train Loss: 212.8548, Train MSE: 212.24, Val Loss: 137.3547, Val MSE: 137.66\n",
      "Epoch 16/1000 (MagnitudeL2-50.0%): Train Loss: 216.6952, Train MSE: 216.82, Val Loss: 137.1679, Val MSE: 137.43\n",
      "Epoch 17/1000 (MagnitudeL2-50.0%): Train Loss: 210.0100, Train MSE: 210.19, Val Loss: 136.5033, Val MSE: 136.81\n",
      "Epoch 18/1000 (MagnitudeL2-50.0%): Train Loss: 214.3338, Train MSE: 214.97, Val Loss: 136.3387, Val MSE: 136.66 (Best)\n",
      "Epoch 19/1000 (MagnitudeL2-50.0%): Train Loss: 214.2419, Train MSE: 214.61, Val Loss: 154.0479, Val MSE: 154.32\n",
      "Epoch 20/1000 (MagnitudeL2-50.0%): Train Loss: 216.2105, Train MSE: 215.95, Val Loss: 138.0144, Val MSE: 138.32\n",
      "Epoch 21/1000 (MagnitudeL2-50.0%): Train Loss: 213.4287, Train MSE: 213.56, Val Loss: 136.9669, Val MSE: 137.24\n",
      "Epoch 22/1000 (MagnitudeL2-50.0%): Train Loss: 210.8676, Train MSE: 210.68, Val Loss: 137.3138, Val MSE: 137.66\n",
      "Epoch 23/1000 (MagnitudeL2-50.0%): Train Loss: 210.7506, Train MSE: 211.01, Val Loss: 136.0634, Val MSE: 136.38 (Best)\n",
      "Epoch 24/1000 (MagnitudeL2-50.0%): Train Loss: 215.6845, Train MSE: 215.56, Val Loss: 135.8682, Val MSE: 136.18 (Best)\n",
      "Epoch 25/1000 (MagnitudeL2-50.0%): Train Loss: 213.8749, Train MSE: 212.91, Val Loss: 138.7537, Val MSE: 139.07\n",
      "Epoch 26/1000 (MagnitudeL2-50.0%): Train Loss: 213.2343, Train MSE: 213.36, Val Loss: 136.1656, Val MSE: 136.51\n",
      "Epoch 27/1000 (MagnitudeL2-50.0%): Train Loss: 214.4425, Train MSE: 213.91, Val Loss: 145.4428, Val MSE: 145.69\n",
      "Epoch 28/1000 (MagnitudeL2-50.0%): Train Loss: 215.9164, Train MSE: 215.36, Val Loss: 148.0194, Val MSE: 148.29\n",
      "Epoch 29/1000 (MagnitudeL2-50.0%): Train Loss: 213.0224, Train MSE: 213.45, Val Loss: 144.1557, Val MSE: 144.44\n",
      "Epoch 30/1000 (MagnitudeL2-50.0%): Train Loss: 211.1829, Train MSE: 211.19, Val Loss: 141.1923, Val MSE: 141.50\n",
      "Epoch 31/1000 (MagnitudeL2-50.0%): Train Loss: 216.0342, Train MSE: 216.35, Val Loss: 139.0506, Val MSE: 139.36\n",
      "Epoch 32/1000 (MagnitudeL2-50.0%): Train Loss: 216.4217, Train MSE: 216.64, Val Loss: 136.6596, Val MSE: 136.98\n",
      "Epoch 33/1000 (MagnitudeL2-50.0%): Train Loss: 212.0983, Train MSE: 211.74, Val Loss: 135.8979, Val MSE: 136.17\n",
      "Epoch 34/1000 (MagnitudeL2-50.0%): Train Loss: 206.8908, Train MSE: 206.66, Val Loss: 137.0884, Val MSE: 137.40\n",
      "Epoch 35/1000 (MagnitudeL2-50.0%): Train Loss: 211.4225, Train MSE: 211.64, Val Loss: 137.0654, Val MSE: 137.36\n",
      "Epoch 36/1000 (MagnitudeL2-50.0%): Train Loss: 215.5320, Train MSE: 215.19, Val Loss: 136.7042, Val MSE: 137.04\n",
      "Epoch 37/1000 (MagnitudeL2-50.0%): Train Loss: 210.4156, Train MSE: 210.13, Val Loss: 136.4337, Val MSE: 136.76\n",
      "Epoch 38/1000 (MagnitudeL2-50.0%): Train Loss: 209.0346, Train MSE: 208.81, Val Loss: 142.1319, Val MSE: 142.44\n",
      "Epoch 39/1000 (MagnitudeL2-50.0%): Train Loss: 212.3114, Train MSE: 211.74, Val Loss: 136.3554, Val MSE: 136.68\n",
      "Epoch 40/1000 (MagnitudeL2-50.0%): Train Loss: 209.0736, Train MSE: 209.45, Val Loss: 138.6905, Val MSE: 139.03\n",
      "Epoch 41/1000 (MagnitudeL2-50.0%): Train Loss: 215.2164, Train MSE: 215.23, Val Loss: 137.0907, Val MSE: 137.40\n",
      "Epoch 42/1000 (MagnitudeL2-50.0%): Train Loss: 209.8592, Train MSE: 210.09, Val Loss: 136.5093, Val MSE: 136.80\n",
      "Epoch 43/1000 (MagnitudeL2-50.0%): Train Loss: 209.1716, Train MSE: 208.75, Val Loss: 144.5792, Val MSE: 144.88\n",
      "Epoch 44/1000 (MagnitudeL2-50.0%): Train Loss: 206.6248, Train MSE: 206.41, Val Loss: 137.7324, Val MSE: 138.03\n",
      "Early stopping triggered after 44 epochs\n",
      "Loaded best model state\n",
      "Results: MSE=433.25, MAE=12.91, MACs=0.19M\n",
      "✅ Model saved to ./models_mlp_nasa/magnitudel2_sparsity_0.5.pth\n",
      "✅ ONNX model saved to ./models_mlp_nasa/magnitudel2_sparsity_0.5.onnx\n",
      "\n",
      "Processing MagnitudeL2 at 70.0% sparsity...\n",
      "✅ Created MLP with architecture: 700 -> 256 -> 128 -> 64 -> 1\n",
      "Initial MACs: 0.22M\n",
      "Applying MagnitudeImportance pruning at 70.0% sparsity...\n",
      "Final MACs: 0.19M (Reduction: 16.3%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/1000 (MagnitudeL2-70.0%): Train Loss: 218.8945, Train MSE: 219.02, Val Loss: 138.5207, Val MSE: 138.79 (Best)\n",
      "Epoch 2/1000 (MagnitudeL2-70.0%): Train Loss: 216.2136, Train MSE: 216.38, Val Loss: 138.6412, Val MSE: 138.88\n",
      "Epoch 3/1000 (MagnitudeL2-70.0%): Train Loss: 216.5818, Train MSE: 216.88, Val Loss: 137.1060, Val MSE: 137.39 (Best)\n",
      "Epoch 4/1000 (MagnitudeL2-70.0%): Train Loss: 213.0223, Train MSE: 213.43, Val Loss: 140.9221, Val MSE: 141.19\n",
      "Epoch 5/1000 (MagnitudeL2-70.0%): Train Loss: 214.5841, Train MSE: 214.30, Val Loss: 136.9718, Val MSE: 137.24 (Best)\n",
      "Epoch 6/1000 (MagnitudeL2-70.0%): Train Loss: 211.7308, Train MSE: 211.87, Val Loss: 142.4591, Val MSE: 142.73\n",
      "Epoch 7/1000 (MagnitudeL2-70.0%): Train Loss: 213.8110, Train MSE: 213.46, Val Loss: 136.1646, Val MSE: 136.46 (Best)\n",
      "Epoch 8/1000 (MagnitudeL2-70.0%): Train Loss: 217.4856, Train MSE: 217.78, Val Loss: 137.3254, Val MSE: 137.62\n",
      "Epoch 9/1000 (MagnitudeL2-70.0%): Train Loss: 218.5976, Train MSE: 218.58, Val Loss: 138.2963, Val MSE: 138.63\n",
      "Epoch 10/1000 (MagnitudeL2-70.0%): Train Loss: 213.5221, Train MSE: 213.31, Val Loss: 137.4544, Val MSE: 137.73\n",
      "Epoch 11/1000 (MagnitudeL2-70.0%): Train Loss: 213.7694, Train MSE: 214.08, Val Loss: 137.7604, Val MSE: 138.06\n",
      "Epoch 12/1000 (MagnitudeL2-70.0%): Train Loss: 216.5369, Train MSE: 216.94, Val Loss: 141.0856, Val MSE: 141.37\n",
      "Epoch 13/1000 (MagnitudeL2-70.0%): Train Loss: 213.4385, Train MSE: 213.41, Val Loss: 137.4235, Val MSE: 137.77\n",
      "Epoch 14/1000 (MagnitudeL2-70.0%): Train Loss: 210.2300, Train MSE: 210.76, Val Loss: 142.6876, Val MSE: 142.99\n",
      "Epoch 15/1000 (MagnitudeL2-70.0%): Train Loss: 214.3575, Train MSE: 214.53, Val Loss: 138.5110, Val MSE: 138.77\n",
      "Epoch 16/1000 (MagnitudeL2-70.0%): Train Loss: 217.1806, Train MSE: 217.55, Val Loss: 140.0181, Val MSE: 140.30\n",
      "Epoch 17/1000 (MagnitudeL2-70.0%): Train Loss: 213.3551, Train MSE: 213.38, Val Loss: 139.0410, Val MSE: 139.31\n",
      "Epoch 18/1000 (MagnitudeL2-70.0%): Train Loss: 211.3443, Train MSE: 211.22, Val Loss: 139.9802, Val MSE: 140.25\n",
      "Epoch 19/1000 (MagnitudeL2-70.0%): Train Loss: 211.4273, Train MSE: 211.75, Val Loss: 138.2004, Val MSE: 138.53\n",
      "Epoch 20/1000 (MagnitudeL2-70.0%): Train Loss: 213.4624, Train MSE: 214.17, Val Loss: 138.3070, Val MSE: 138.59\n",
      "Epoch 21/1000 (MagnitudeL2-70.0%): Train Loss: 214.8918, Train MSE: 215.21, Val Loss: 137.3568, Val MSE: 137.67\n",
      "Epoch 22/1000 (MagnitudeL2-70.0%): Train Loss: 212.1480, Train MSE: 212.53, Val Loss: 138.9460, Val MSE: 139.20\n",
      "Epoch 23/1000 (MagnitudeL2-70.0%): Train Loss: 208.9429, Train MSE: 209.11, Val Loss: 137.7724, Val MSE: 138.06\n",
      "Epoch 24/1000 (MagnitudeL2-70.0%): Train Loss: 212.7481, Train MSE: 212.06, Val Loss: 137.5275, Val MSE: 137.82\n",
      "Epoch 25/1000 (MagnitudeL2-70.0%): Train Loss: 212.4265, Train MSE: 212.56, Val Loss: 136.6925, Val MSE: 136.97\n",
      "Epoch 26/1000 (MagnitudeL2-70.0%): Train Loss: 212.6854, Train MSE: 212.30, Val Loss: 140.5497, Val MSE: 140.88\n",
      "Epoch 27/1000 (MagnitudeL2-70.0%): Train Loss: 213.0874, Train MSE: 212.98, Val Loss: 136.7232, Val MSE: 137.04\n",
      "Early stopping triggered after 27 epochs\n",
      "Loaded best model state\n",
      "Results: MSE=444.26, MAE=12.98, MACs=0.19M\n",
      "✅ Model saved to ./models_mlp_nasa/magnitudel2_sparsity_0.7.pth\n",
      "✅ ONNX model saved to ./models_mlp_nasa/magnitudel2_sparsity_0.7.onnx\n",
      "\n",
      "--- Strategy: Random ---\n",
      "\n",
      "Processing Random at 20.0% sparsity...\n",
      "✅ Created MLP with architecture: 700 -> 256 -> 128 -> 64 -> 1\n",
      "Initial MACs: 0.22M\n",
      "Applying RandomImportance pruning at 20.0% sparsity...\n",
      "Final MACs: 0.21M (Reduction: 5.1%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/1000 (Random-20.0%): Train Loss: 227.8280, Train MSE: 228.17, Val Loss: 138.1795, Val MSE: 138.42 (Best)\n",
      "Epoch 2/1000 (Random-20.0%): Train Loss: 218.2666, Train MSE: 218.18, Val Loss: 137.7620, Val MSE: 138.01 (Best)\n",
      "Epoch 3/1000 (Random-20.0%): Train Loss: 221.2764, Train MSE: 221.56, Val Loss: 144.2267, Val MSE: 144.47\n",
      "Epoch 4/1000 (Random-20.0%): Train Loss: 221.1760, Train MSE: 221.62, Val Loss: 140.9318, Val MSE: 141.18\n",
      "Epoch 5/1000 (Random-20.0%): Train Loss: 218.3356, Train MSE: 217.95, Val Loss: 137.2328, Val MSE: 137.56 (Best)\n",
      "Epoch 6/1000 (Random-20.0%): Train Loss: 216.6302, Train MSE: 216.84, Val Loss: 144.0931, Val MSE: 144.32\n",
      "Epoch 7/1000 (Random-20.0%): Train Loss: 221.1287, Train MSE: 221.81, Val Loss: 146.6438, Val MSE: 146.88\n",
      "Epoch 8/1000 (Random-20.0%): Train Loss: 218.8017, Train MSE: 218.50, Val Loss: 138.8863, Val MSE: 139.18\n",
      "Epoch 9/1000 (Random-20.0%): Train Loss: 219.1504, Train MSE: 219.64, Val Loss: 138.9955, Val MSE: 139.28\n",
      "Epoch 10/1000 (Random-20.0%): Train Loss: 221.5828, Train MSE: 221.29, Val Loss: 138.2879, Val MSE: 138.56\n",
      "Epoch 11/1000 (Random-20.0%): Train Loss: 216.4398, Train MSE: 216.62, Val Loss: 136.9593, Val MSE: 137.22 (Best)\n",
      "Epoch 12/1000 (Random-20.0%): Train Loss: 217.8594, Train MSE: 218.19, Val Loss: 139.7167, Val MSE: 139.96\n",
      "Epoch 13/1000 (Random-20.0%): Train Loss: 217.3189, Train MSE: 216.59, Val Loss: 137.2679, Val MSE: 137.57\n",
      "Epoch 14/1000 (Random-20.0%): Train Loss: 217.0672, Train MSE: 217.09, Val Loss: 152.9657, Val MSE: 153.18\n",
      "Epoch 15/1000 (Random-20.0%): Train Loss: 220.6583, Train MSE: 220.48, Val Loss: 146.5165, Val MSE: 146.77\n",
      "Epoch 16/1000 (Random-20.0%): Train Loss: 218.6658, Train MSE: 219.15, Val Loss: 136.9489, Val MSE: 137.27 (Best)\n",
      "Epoch 17/1000 (Random-20.0%): Train Loss: 222.9895, Train MSE: 223.39, Val Loss: 139.0297, Val MSE: 139.32\n",
      "Epoch 18/1000 (Random-20.0%): Train Loss: 218.9652, Train MSE: 218.53, Val Loss: 148.3679, Val MSE: 148.62\n",
      "Epoch 19/1000 (Random-20.0%): Train Loss: 216.8728, Train MSE: 217.26, Val Loss: 137.4109, Val MSE: 137.74\n",
      "Epoch 20/1000 (Random-20.0%): Train Loss: 218.1198, Train MSE: 217.85, Val Loss: 148.1924, Val MSE: 148.49\n",
      "Epoch 21/1000 (Random-20.0%): Train Loss: 219.2036, Train MSE: 218.92, Val Loss: 137.4350, Val MSE: 137.76\n",
      "Epoch 22/1000 (Random-20.0%): Train Loss: 218.3337, Train MSE: 218.17, Val Loss: 136.3618, Val MSE: 136.63 (Best)\n",
      "Epoch 23/1000 (Random-20.0%): Train Loss: 219.8187, Train MSE: 220.42, Val Loss: 139.1254, Val MSE: 139.42\n",
      "Epoch 24/1000 (Random-20.0%): Train Loss: 215.2611, Train MSE: 215.44, Val Loss: 139.4675, Val MSE: 139.76\n",
      "Epoch 25/1000 (Random-20.0%): Train Loss: 217.5705, Train MSE: 217.16, Val Loss: 138.8156, Val MSE: 139.12\n",
      "Epoch 26/1000 (Random-20.0%): Train Loss: 215.8049, Train MSE: 215.34, Val Loss: 137.0078, Val MSE: 137.27\n",
      "Epoch 27/1000 (Random-20.0%): Train Loss: 221.3712, Train MSE: 221.16, Val Loss: 137.1325, Val MSE: 137.45\n",
      "Epoch 28/1000 (Random-20.0%): Train Loss: 221.4072, Train MSE: 220.55, Val Loss: 137.8692, Val MSE: 138.13\n",
      "Epoch 29/1000 (Random-20.0%): Train Loss: 212.5144, Train MSE: 213.29, Val Loss: 136.7646, Val MSE: 137.07\n",
      "Epoch 30/1000 (Random-20.0%): Train Loss: 215.1383, Train MSE: 214.35, Val Loss: 142.2730, Val MSE: 142.59\n",
      "Epoch 31/1000 (Random-20.0%): Train Loss: 216.5451, Train MSE: 216.36, Val Loss: 136.6457, Val MSE: 136.95\n",
      "Epoch 32/1000 (Random-20.0%): Train Loss: 218.4646, Train MSE: 218.66, Val Loss: 145.8569, Val MSE: 146.11\n",
      "Epoch 33/1000 (Random-20.0%): Train Loss: 212.4455, Train MSE: 212.44, Val Loss: 137.0082, Val MSE: 137.30\n",
      "Epoch 34/1000 (Random-20.0%): Train Loss: 220.0255, Train MSE: 220.13, Val Loss: 147.0055, Val MSE: 147.22\n",
      "Epoch 35/1000 (Random-20.0%): Train Loss: 214.4811, Train MSE: 214.75, Val Loss: 143.5864, Val MSE: 143.90\n",
      "Epoch 36/1000 (Random-20.0%): Train Loss: 215.7209, Train MSE: 215.01, Val Loss: 136.6149, Val MSE: 136.92\n",
      "Epoch 37/1000 (Random-20.0%): Train Loss: 217.0762, Train MSE: 216.96, Val Loss: 136.6907, Val MSE: 136.99\n",
      "Epoch 38/1000 (Random-20.0%): Train Loss: 218.7198, Train MSE: 218.48, Val Loss: 136.9030, Val MSE: 137.20\n",
      "Epoch 39/1000 (Random-20.0%): Train Loss: 216.3062, Train MSE: 216.04, Val Loss: 137.5806, Val MSE: 137.89\n",
      "Epoch 40/1000 (Random-20.0%): Train Loss: 211.4717, Train MSE: 211.50, Val Loss: 142.8034, Val MSE: 143.09\n",
      "Epoch 41/1000 (Random-20.0%): Train Loss: 220.6993, Train MSE: 220.88, Val Loss: 139.0708, Val MSE: 139.38\n",
      "Epoch 42/1000 (Random-20.0%): Train Loss: 214.7350, Train MSE: 214.29, Val Loss: 137.0439, Val MSE: 137.35\n",
      "Early stopping triggered after 42 epochs\n",
      "Loaded best model state\n",
      "Results: MSE=438.64, MAE=12.97, MACs=0.21M\n",
      "✅ Model saved to ./models_mlp_nasa/random_sparsity_0.2.pth\n",
      "✅ ONNX model saved to ./models_mlp_nasa/random_sparsity_0.2.onnx\n",
      "\n",
      "Processing Random at 50.0% sparsity...\n",
      "✅ Created MLP with architecture: 700 -> 256 -> 128 -> 64 -> 1\n",
      "Initial MACs: 0.22M\n",
      "Applying RandomImportance pruning at 50.0% sparsity...\n",
      "Final MACs: 0.19M (Reduction: 11.9%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/1000 (Random-50.0%): Train Loss: 249.6108, Train MSE: 249.66, Val Loss: 138.8192, Val MSE: 139.06 (Best)\n",
      "Epoch 2/1000 (Random-50.0%): Train Loss: 223.9269, Train MSE: 224.21, Val Loss: 139.7427, Val MSE: 139.94\n",
      "Epoch 3/1000 (Random-50.0%): Train Loss: 221.6689, Train MSE: 221.80, Val Loss: 137.3381, Val MSE: 137.57 (Best)\n",
      "Epoch 4/1000 (Random-50.0%): Train Loss: 220.3850, Train MSE: 220.49, Val Loss: 139.6335, Val MSE: 139.84\n",
      "Epoch 5/1000 (Random-50.0%): Train Loss: 222.9161, Train MSE: 222.31, Val Loss: 136.8311, Val MSE: 137.10 (Best)\n",
      "Epoch 6/1000 (Random-50.0%): Train Loss: 221.3362, Train MSE: 221.49, Val Loss: 138.2718, Val MSE: 138.52\n",
      "Epoch 7/1000 (Random-50.0%): Train Loss: 226.9308, Train MSE: 226.78, Val Loss: 142.8233, Val MSE: 143.04\n",
      "Epoch 8/1000 (Random-50.0%): Train Loss: 223.0645, Train MSE: 222.69, Val Loss: 136.5112, Val MSE: 136.76 (Best)\n",
      "Epoch 9/1000 (Random-50.0%): Train Loss: 223.8786, Train MSE: 223.77, Val Loss: 137.1067, Val MSE: 137.35\n",
      "Epoch 10/1000 (Random-50.0%): Train Loss: 224.1487, Train MSE: 224.35, Val Loss: 136.8302, Val MSE: 137.11\n",
      "Epoch 11/1000 (Random-50.0%): Train Loss: 225.7873, Train MSE: 226.31, Val Loss: 145.4378, Val MSE: 145.66\n",
      "Epoch 12/1000 (Random-50.0%): Train Loss: 228.9412, Train MSE: 228.77, Val Loss: 139.5185, Val MSE: 139.75\n",
      "Epoch 13/1000 (Random-50.0%): Train Loss: 223.5773, Train MSE: 223.65, Val Loss: 136.6775, Val MSE: 136.94\n",
      "Epoch 14/1000 (Random-50.0%): Train Loss: 218.8826, Train MSE: 218.71, Val Loss: 137.5232, Val MSE: 137.84\n",
      "Epoch 15/1000 (Random-50.0%): Train Loss: 222.5103, Train MSE: 222.64, Val Loss: 140.8506, Val MSE: 141.12\n",
      "Epoch 16/1000 (Random-50.0%): Train Loss: 219.0780, Train MSE: 219.74, Val Loss: 143.4862, Val MSE: 143.76\n",
      "Epoch 17/1000 (Random-50.0%): Train Loss: 219.1450, Train MSE: 219.49, Val Loss: 143.9283, Val MSE: 144.23\n",
      "Epoch 18/1000 (Random-50.0%): Train Loss: 221.3675, Train MSE: 220.95, Val Loss: 137.6196, Val MSE: 137.89\n",
      "Epoch 19/1000 (Random-50.0%): Train Loss: 220.5343, Train MSE: 221.18, Val Loss: 142.7108, Val MSE: 142.98\n",
      "Epoch 20/1000 (Random-50.0%): Train Loss: 228.9527, Train MSE: 228.29, Val Loss: 139.7134, Val MSE: 140.00\n",
      "Epoch 21/1000 (Random-50.0%): Train Loss: 221.8012, Train MSE: 221.72, Val Loss: 146.7211, Val MSE: 146.99\n",
      "Epoch 22/1000 (Random-50.0%): Train Loss: 220.9205, Train MSE: 221.03, Val Loss: 137.4804, Val MSE: 137.73\n",
      "Epoch 23/1000 (Random-50.0%): Train Loss: 225.0278, Train MSE: 224.88, Val Loss: 136.9036, Val MSE: 137.21\n",
      "Epoch 24/1000 (Random-50.0%): Train Loss: 227.0366, Train MSE: 227.27, Val Loss: 144.0112, Val MSE: 144.24\n",
      "Epoch 25/1000 (Random-50.0%): Train Loss: 223.4499, Train MSE: 223.35, Val Loss: 139.0694, Val MSE: 139.37\n",
      "Epoch 26/1000 (Random-50.0%): Train Loss: 225.5417, Train MSE: 224.75, Val Loss: 137.0551, Val MSE: 137.36\n",
      "Epoch 27/1000 (Random-50.0%): Train Loss: 224.3436, Train MSE: 224.03, Val Loss: 137.1102, Val MSE: 137.40\n",
      "Epoch 28/1000 (Random-50.0%): Train Loss: 225.1036, Train MSE: 225.23, Val Loss: 137.1335, Val MSE: 137.44\n",
      "Early stopping triggered after 28 epochs\n",
      "Loaded best model state\n",
      "Results: MSE=437.19, MAE=12.91, MACs=0.19M\n",
      "✅ Model saved to ./models_mlp_nasa/random_sparsity_0.5.pth\n",
      "✅ ONNX model saved to ./models_mlp_nasa/random_sparsity_0.5.onnx\n",
      "\n",
      "Processing Random at 70.0% sparsity...\n",
      "✅ Created MLP with architecture: 700 -> 256 -> 128 -> 64 -> 1\n",
      "Initial MACs: 0.22M\n",
      "Applying RandomImportance pruning at 70.0% sparsity...\n",
      "Final MACs: 0.19M (Reduction: 16.3%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/1000 (Random-70.0%): Train Loss: 283.6851, Train MSE: 283.92, Val Loss: 144.0749, Val MSE: 144.23 (Best)\n",
      "Epoch 2/1000 (Random-70.0%): Train Loss: 233.5212, Train MSE: 233.21, Val Loss: 140.8276, Val MSE: 141.01 (Best)\n",
      "Epoch 3/1000 (Random-70.0%): Train Loss: 228.7833, Train MSE: 229.08, Val Loss: 137.9432, Val MSE: 138.18 (Best)\n",
      "Epoch 4/1000 (Random-70.0%): Train Loss: 229.1015, Train MSE: 229.09, Val Loss: 139.0815, Val MSE: 139.31\n",
      "Epoch 5/1000 (Random-70.0%): Train Loss: 223.6231, Train MSE: 223.38, Val Loss: 141.0365, Val MSE: 141.25\n",
      "Epoch 6/1000 (Random-70.0%): Train Loss: 226.1035, Train MSE: 225.91, Val Loss: 139.3726, Val MSE: 139.65\n",
      "Epoch 7/1000 (Random-70.0%): Train Loss: 223.9134, Train MSE: 224.60, Val Loss: 146.8469, Val MSE: 147.03\n",
      "Epoch 8/1000 (Random-70.0%): Train Loss: 228.3989, Train MSE: 228.38, Val Loss: 137.4869, Val MSE: 137.71 (Best)\n",
      "Epoch 9/1000 (Random-70.0%): Train Loss: 224.1356, Train MSE: 224.36, Val Loss: 152.5612, Val MSE: 152.70\n",
      "Epoch 10/1000 (Random-70.0%): Train Loss: 226.1719, Train MSE: 226.75, Val Loss: 138.9064, Val MSE: 139.13\n",
      "Epoch 11/1000 (Random-70.0%): Train Loss: 224.2804, Train MSE: 224.50, Val Loss: 138.9481, Val MSE: 139.15\n",
      "Epoch 12/1000 (Random-70.0%): Train Loss: 223.9747, Train MSE: 224.29, Val Loss: 137.5103, Val MSE: 137.76\n",
      "Epoch 13/1000 (Random-70.0%): Train Loss: 225.0685, Train MSE: 225.53, Val Loss: 139.2348, Val MSE: 139.50\n",
      "Epoch 14/1000 (Random-70.0%): Train Loss: 223.0450, Train MSE: 223.40, Val Loss: 137.9534, Val MSE: 138.22\n",
      "Epoch 15/1000 (Random-70.0%): Train Loss: 218.1628, Train MSE: 218.69, Val Loss: 139.1279, Val MSE: 139.40\n",
      "Epoch 16/1000 (Random-70.0%): Train Loss: 222.6740, Train MSE: 221.55, Val Loss: 140.4382, Val MSE: 140.70\n",
      "Epoch 17/1000 (Random-70.0%): Train Loss: 226.3402, Train MSE: 226.80, Val Loss: 137.1337, Val MSE: 137.41 (Best)\n",
      "Epoch 18/1000 (Random-70.0%): Train Loss: 226.2711, Train MSE: 225.87, Val Loss: 137.6590, Val MSE: 137.92\n",
      "Epoch 19/1000 (Random-70.0%): Train Loss: 227.0598, Train MSE: 227.01, Val Loss: 141.6108, Val MSE: 141.86\n",
      "Epoch 20/1000 (Random-70.0%): Train Loss: 223.2693, Train MSE: 223.28, Val Loss: 137.4400, Val MSE: 137.69\n",
      "Epoch 21/1000 (Random-70.0%): Train Loss: 228.0569, Train MSE: 228.16, Val Loss: 137.2018, Val MSE: 137.48\n",
      "Epoch 22/1000 (Random-70.0%): Train Loss: 226.2254, Train MSE: 225.93, Val Loss: 138.5213, Val MSE: 138.77\n",
      "Epoch 23/1000 (Random-70.0%): Train Loss: 220.3832, Train MSE: 220.47, Val Loss: 144.0472, Val MSE: 144.25\n",
      "Epoch 24/1000 (Random-70.0%): Train Loss: 222.0910, Train MSE: 222.44, Val Loss: 139.0748, Val MSE: 139.38\n",
      "Epoch 25/1000 (Random-70.0%): Train Loss: 220.3359, Train MSE: 220.70, Val Loss: 137.4555, Val MSE: 137.75\n",
      "Epoch 26/1000 (Random-70.0%): Train Loss: 223.5211, Train MSE: 223.13, Val Loss: 139.4326, Val MSE: 139.70\n",
      "Epoch 27/1000 (Random-70.0%): Train Loss: 224.0818, Train MSE: 224.76, Val Loss: 144.0375, Val MSE: 144.29\n",
      "Epoch 28/1000 (Random-70.0%): Train Loss: 225.8689, Train MSE: 226.07, Val Loss: 140.8840, Val MSE: 141.16\n",
      "Epoch 29/1000 (Random-70.0%): Train Loss: 222.4140, Train MSE: 223.14, Val Loss: 140.9264, Val MSE: 141.16\n",
      "Epoch 30/1000 (Random-70.0%): Train Loss: 223.2095, Train MSE: 223.70, Val Loss: 139.0512, Val MSE: 139.38\n",
      "Epoch 31/1000 (Random-70.0%): Train Loss: 222.3956, Train MSE: 222.60, Val Loss: 141.1094, Val MSE: 141.34\n",
      "Epoch 32/1000 (Random-70.0%): Train Loss: 221.3703, Train MSE: 221.08, Val Loss: 137.5696, Val MSE: 137.85\n",
      "Epoch 33/1000 (Random-70.0%): Train Loss: 225.3683, Train MSE: 225.75, Val Loss: 143.7679, Val MSE: 144.01\n",
      "Epoch 34/1000 (Random-70.0%): Train Loss: 222.9435, Train MSE: 222.45, Val Loss: 137.5297, Val MSE: 137.86\n",
      "Epoch 35/1000 (Random-70.0%): Train Loss: 220.3797, Train MSE: 220.02, Val Loss: 137.5096, Val MSE: 137.80\n",
      "Epoch 36/1000 (Random-70.0%): Train Loss: 228.3480, Train MSE: 228.27, Val Loss: 147.0890, Val MSE: 147.38\n",
      "Epoch 37/1000 (Random-70.0%): Train Loss: 224.0900, Train MSE: 223.90, Val Loss: 137.7032, Val MSE: 138.02\n",
      "Early stopping triggered after 37 epochs\n",
      "Loaded best model state\n",
      "Results: MSE=438.26, MAE=12.95, MACs=0.19M\n",
      "✅ Model saved to ./models_mlp_nasa/random_sparsity_0.7.pth\n",
      "✅ ONNX model saved to ./models_mlp_nasa/random_sparsity_0.7.onnx\n",
      "\n",
      "Saving results...\n",
      "✅ Complete results saved to ./results_mlp_nasa/complete_results.json\n",
      "✅ Summary results saved to ./results_mlp_nasa/summary_results.csv\n",
      "Creating plots...\n",
      "✅ MSE plot saved to ./results_mlp_nasa/mse_vs_sparsity.png\n",
      "✅ Efficiency frontier plot saved to ./results_mlp_nasa/efficiency_frontier.png\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENTAL RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Baseline Performance:\n",
      "  MSE: 438.44\n",
      "  MAE: 12.95\n",
      "  MACs: 0.22M\n",
      "  Parameters: 0.22M\n",
      "  Model Size: 0.84MB\n",
      "\n",
      "Strategy Comparison at 50% Sparsity:\n",
      "   MagnitudeL2: MSE=433.25 (-5.20,  -1.2% increase)\n",
      "        Random: MSE=437.19 (-1.25,  -0.3% increase)\n",
      "\n",
      "Complete Results Table:\n",
      "------------------------------------------------------------------------------------------\n",
      "Strategy     Sparsity MSE      MAE      MACs(M)  Params(M) Size(MB)\n",
      "------------------------------------------------------------------------------------------\n",
      "MagnitudeL2       0%  438.44   12.95    0.22     0.22    0.84\n",
      "MagnitudeL2      20%  426.84   12.79    0.21     0.21    0.80\n",
      "MagnitudeL2      50%  433.25   12.91    0.19     0.19    0.74\n",
      "MagnitudeL2      70%  444.26   12.98    0.19     0.18    0.70\n",
      "Random            0%  438.44   12.95    0.22     0.22    0.84\n",
      "Random           20%  438.64   12.97    0.21     0.21    0.80\n",
      "Random           50%  437.19   12.91    0.19     0.19    0.74\n",
      "Random           70%  438.26   12.95    0.19     0.18    0.70\n",
      "\n",
      "🎉 All experiments completed!\n",
      "📁 Results saved to: /home/muis/thesis/github-repo/master-thesis/mlp/results_mlp_nasa\n",
      "📁 Models saved to: /home/muis/thesis/github-repo/master-thesis/mlp/models_mlp_nasa\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# v2",
   "id": "d1fd72b2a34de9fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T19:11:59.475311Z",
     "start_time": "2025-06-04T19:06:10.330041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_pruning as tp\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from typing import List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_BASE_NAME = \"mlp_nasa\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# SIMPLIFIED preprocessing based on proven NASA C-MAPSS research\n",
    "column_names = ['unit_number', 'time_in_cycles'] + [f'op_setting_{i}' for i in range(1, 4)] + [f'sensor_{i}' for i in range(1, 24)]\n",
    "\n",
    "def load_dataframe(file_path: str) -> pd.DataFrame | None:\n",
    "    \"\"\"Load NASA data file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=' ', header=None, names=column_names)\n",
    "        df.dropna(axis=1, how='all', inplace=True)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def proven_feature_selection(df: pd.DataFrame) -> list:\n",
    "    \"\"\"Use PROVEN feature selection from NASA literature - SIMPLE approach.\"\"\"\n",
    "    if df is None:\n",
    "        return []\n",
    "\n",
    "    # Based on multiple research papers - these are the PROVEN effective sensors for FD001\n",
    "    # Remove sensors with minimal variation (proven approach)\n",
    "    sensors_to_remove = [\n",
    "        'sensor_1',   # Lever position - constant for FD001\n",
    "        'sensor_5',   # Static pressure - minimal variation\n",
    "        'sensor_6',   # Physical fan speed - redundant with sensor_8\n",
    "        'sensor_10',  # Static pressure - minimal variation\n",
    "        'sensor_16',  # Static pressure - minimal variation\n",
    "        'sensor_18',  # Bleed enthalpy - minimal variation\n",
    "        'sensor_19'   # Demanded fan speed - minimal variation\n",
    "    ]\n",
    "\n",
    "    # Remove operational settings - constant for FD001\n",
    "    ops_to_remove = ['op_setting_1', 'op_setting_2', 'op_setting_3']\n",
    "\n",
    "    cols_to_remove = sensors_to_remove + ops_to_remove\n",
    "\n",
    "    print(f\"Removing {len(cols_to_remove)} non-informative features\")\n",
    "    print(f\"Keeping sensors: [2,3,4,7,8,9,11,12,13,14,15,17,20,21]\")\n",
    "    return cols_to_remove\n",
    "\n",
    "def add_rul_simple(df: pd.DataFrame) -> pd.DataFrame | None:\n",
    "    \"\"\"Simple but PROVEN RUL calculation.\"\"\"\n",
    "    if df is None:\n",
    "        return None\n",
    "\n",
    "    max_cycles = df.groupby('unit_number')['time_in_cycles'].max().reset_index()\n",
    "    max_cycles.columns = ['unit_number', 'max_cycle']\n",
    "    df = df.merge(max_cycles, on='unit_number', how='left')\n",
    "    df['RUL'] = df['max_cycle'] - df['time_in_cycles']\n",
    "    df.drop(columns=['max_cycle'], inplace=True)\n",
    "\n",
    "    # PROVEN: Piece-wise linear RUL - engines don't degrade early in life\n",
    "    # This is the STANDARD approach in NASA research\n",
    "    df['RUL'] = df['RUL'].apply(lambda x: min(x, 125))\n",
    "\n",
    "    return df\n",
    "\n",
    "def normalize_data_simple(df: pd.DataFrame, columns_to_normalize: List[str],\n",
    "                         scaler: MinMaxScaler = None) -> Tuple[pd.DataFrame, MinMaxScaler]:\n",
    "    \"\"\"Simple normalization that works.\"\"\"\n",
    "    if df is None:\n",
    "        return None, None\n",
    "\n",
    "    data_to_scale = df[columns_to_normalize]\n",
    "\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        df[columns_to_normalize] = scaler.fit_transform(data_to_scale)\n",
    "    else:\n",
    "        df[columns_to_normalize] = scaler.transform(data_to_scale)\n",
    "\n",
    "    return df, scaler\n",
    "\n",
    "def prepare_cmapss_data_simple(data_dir: str, train_file: str, test_file: str, test_rul_file: str) -> Tuple[\n",
    "    pd.DataFrame, pd.DataFrame, pd.DataFrame, MinMaxScaler, List[str]]:\n",
    "    \"\"\"SIMPLE but PROVEN data preparation.\"\"\"\n",
    "    print(\"--- Simple Proven Data Preparation ---\")\n",
    "\n",
    "    # Load data\n",
    "    train_df = load_dataframe(os.path.join(data_dir, train_file))\n",
    "    test_df = load_dataframe(os.path.join(data_dir, test_file))\n",
    "    test_rul_df = pd.read_csv(os.path.join(data_dir, test_rul_file), header=None, names=['RUL'])\n",
    "\n",
    "    # Add RUL\n",
    "    train_df = add_rul_simple(train_df)\n",
    "\n",
    "    # Simple feature selection - PROVEN approach\n",
    "    cols_to_remove = proven_feature_selection(train_df)\n",
    "    feature_cols = [col for col in train_df.columns if\n",
    "                   col not in ['unit_number', 'time_in_cycles', 'RUL'] + cols_to_remove]\n",
    "\n",
    "    # Remove unwanted columns\n",
    "    train_df.drop(columns=cols_to_remove, inplace=True, errors='ignore')\n",
    "    test_df.drop(columns=cols_to_remove, inplace=True, errors='ignore')\n",
    "\n",
    "    print(f\"Using {len(feature_cols)} proven effective sensors\")\n",
    "\n",
    "    # Simple normalization\n",
    "    train_df_norm, scaler = normalize_data_simple(train_df.copy(), feature_cols)\n",
    "    test_df_norm, _ = normalize_data_simple(test_df.copy(), feature_cols, scaler=scaler)\n",
    "\n",
    "    return train_df_norm, test_df_norm, test_rul_df, scaler, feature_cols\n",
    "\n",
    "# SIMPLE Dataset - no over-engineering\n",
    "class NASADatasetSimple(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, feature_cols: List[str], window_size: int = 30,\n",
    "                 stride: int = 1, is_test: bool = False, test_rul_df: pd.DataFrame = None):\n",
    "        self.df = df\n",
    "        self.feature_cols = feature_cols\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.is_test = is_test\n",
    "        self.test_rul_df = test_rul_df\n",
    "        self.samples = []\n",
    "        self.targets = []\n",
    "\n",
    "        self._prepare_samples()\n",
    "\n",
    "    def _prepare_samples(self):\n",
    "        \"\"\"SIMPLE windowing strategy - PROVEN approach.\"\"\"\n",
    "        units = self.df['unit_number'].unique()\n",
    "\n",
    "        for unit in units:\n",
    "            unit_df = self.df[self.df['unit_number'] == unit].sort_values('time_in_cycles')\n",
    "\n",
    "            if self.is_test:\n",
    "                # Test: only last window\n",
    "                if len(unit_df) >= self.window_size:\n",
    "                    window_data = unit_df[self.feature_cols].iloc[-self.window_size:].values\n",
    "                    self.samples.append(window_data)\n",
    "                    if self.test_rul_df is not None:\n",
    "                        rul = min(self.test_rul_df.iloc[unit - 1]['RUL'], 125)\n",
    "                        self.targets.append(rul)\n",
    "                else:\n",
    "                    # Pad if needed\n",
    "                    window_data = unit_df[self.feature_cols].values\n",
    "                    padded = np.zeros((self.window_size, len(self.feature_cols)))\n",
    "                    if len(window_data) > 0:\n",
    "                        padded[-len(window_data):] = window_data\n",
    "                    self.samples.append(padded)\n",
    "                    if self.test_rul_df is not None:\n",
    "                        rul = min(self.test_rul_df.iloc[unit - 1]['RUL'], 125)\n",
    "                        self.targets.append(rul)\n",
    "            else:\n",
    "                # Training: simple sliding window\n",
    "                for i in range(0, len(unit_df) - self.window_size + 1, self.stride):\n",
    "                    window_data = unit_df[self.feature_cols].iloc[i:i + self.window_size].values\n",
    "                    rul = unit_df['RUL'].iloc[i + self.window_size - 1]\n",
    "                    self.samples.append(window_data)\n",
    "                    self.targets.append(rul)\n",
    "\n",
    "        self.samples = np.array(self.samples, dtype=np.float32)\n",
    "        self.targets = np.array(self.targets, dtype=np.float32)\n",
    "\n",
    "        print(f\"Created {len(self.samples)} samples with simple windowing\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx].flatten()\n",
    "        target = self.targets[idx]\n",
    "        return torch.FloatTensor(sample), torch.FloatTensor([target])\n",
    "\n",
    "# SIMPLE MLP - proven architecture\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes=[80, 40], dropout_rate=0.2):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "\n",
    "        # Output layer - NO activation for regression\n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def get_data_loaders_simple(data_dir='./data/NASA', batch_size=32, window_size=30, val_split=0.2, seed=42):\n",
    "    \"\"\"Simple data loading - proven parameters.\"\"\"\n",
    "    print(f\"Loading NASA C-MAPSS dataset from: {data_dir}\")\n",
    "\n",
    "    train_df, test_df, test_rul_df, scaler, feature_cols = prepare_cmapss_data_simple(\n",
    "        data_dir, 'train_FD001.txt', 'test_FD001.txt', 'RUL_FD001.txt'\n",
    "    )\n",
    "\n",
    "    # Create datasets with simple approach\n",
    "    full_train_dataset = NASADatasetSimple(train_df, feature_cols, window_size=window_size, stride=1)\n",
    "\n",
    "    # Train/val split\n",
    "    val_size = int(len(full_train_dataset) * val_split)\n",
    "    train_size = len(full_train_dataset) - val_size\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_train_dataset, [train_size, val_size], generator=generator\n",
    "    )\n",
    "\n",
    "    test_dataset = NASADatasetSimple(test_df, feature_cols, window_size=window_size,\n",
    "                                    is_test=True, test_rul_df=test_rul_df)\n",
    "\n",
    "    # Simple data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    input_size = window_size * len(feature_cols)\n",
    "    print(f\"Input size: {input_size} (window: {window_size} × features: {len(feature_cols)})\")\n",
    "    print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader, input_size\n",
    "\n",
    "def get_simple_mlp_model(input_size, hidden_sizes=[80, 40], dropout_rate=0.2):\n",
    "    \"\"\"Simple but effective MLP.\"\"\"\n",
    "    model = SimpleMLP(input_size, hidden_sizes, dropout_rate)\n",
    "    print(f\"✅ Created Simple MLP: {input_size} -> {' -> '.join(map(str, hidden_sizes))} -> 1\")\n",
    "    return model\n",
    "\n",
    "def get_ignored_layers(model):\n",
    "    \"\"\"Get final layer to ignore during pruning.\"\"\"\n",
    "    ignored_layers = []\n",
    "    for module in reversed(list(model.model)):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            ignored_layers.append(module)\n",
    "            break\n",
    "    return ignored_layers\n",
    "\n",
    "def calculate_macs_params(model, example_input):\n",
    "    \"\"\"Calculate efficiency metrics.\"\"\"\n",
    "    model.eval()\n",
    "    target_device = example_input.device\n",
    "    model_on_device = model.to(target_device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        macs, params = tp.utils.count_ops_and_params(model_on_device, example_input)\n",
    "\n",
    "    return macs, params\n",
    "\n",
    "def save_model(model, save_path, example_input_cpu=None):\n",
    "    \"\"\"Save model.\"\"\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"✅ Model saved to {save_path}\")\n",
    "\n",
    "def evaluate_model(model, data_loader, example_input, criterion, device):\n",
    "    \"\"\"Simple evaluation.\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Calculate efficiency metrics\n",
    "    macs, params = calculate_macs_params(model, example_input.to(device))\n",
    "    model_size_mb = params * 4 / (1024 * 1024)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            all_predictions.extend(output.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    all_predictions = np.array(all_predictions).flatten()\n",
    "    all_targets = np.array(all_targets).flatten()\n",
    "\n",
    "    mse = np.mean((all_predictions - all_targets) ** 2)\n",
    "    mae = np.mean(np.abs(all_predictions - all_targets))\n",
    "\n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'loss': total_loss / len(data_loader.dataset),\n",
    "        'macs': macs,\n",
    "        'params': params,\n",
    "        'size_mb': model_size_mb\n",
    "    }\n",
    "\n",
    "def prune_model(model, strategy_config, sparsity_ratio, example_input, ignored_layers=None):\n",
    "    \"\"\"Simple pruning.\"\"\"\n",
    "    if sparsity_ratio == 0.0:\n",
    "        return model\n",
    "\n",
    "    model.eval()\n",
    "    pruned_model = copy.deepcopy(model)\n",
    "    pruned_model.to(example_input.device)\n",
    "\n",
    "    initial_macs, _ = calculate_macs_params(pruned_model, example_input)\n",
    "    print(f\"Initial MACs: {initial_macs / 1e6:.2f}M\")\n",
    "\n",
    "    ignored_layers = ignored_layers or []\n",
    "\n",
    "    # Simple pruner\n",
    "    pruner = strategy_config['pruner'](\n",
    "        pruned_model,\n",
    "        example_input,\n",
    "        importance=strategy_config['importance'],\n",
    "        iterative_steps=3,  # Fewer steps\n",
    "        ch_sparsity=sparsity_ratio,\n",
    "        root_module_types=[nn.Linear],\n",
    "        ignored_layers=ignored_layers\n",
    "    )\n",
    "\n",
    "    print(f\"Applying {strategy_config['importance'].__class__.__name__} pruning at {sparsity_ratio:.1%}\")\n",
    "    pruner.step()\n",
    "\n",
    "    final_macs, _ = calculate_macs_params(pruned_model, example_input)\n",
    "    reduction = (initial_macs - final_macs) / initial_macs * 100 if initial_macs > 0 else 0\n",
    "    print(f\"Final MACs: {final_macs / 1e6:.2f}M ({reduction:.1f}% reduction)\")\n",
    "\n",
    "    return pruned_model\n",
    "\n",
    "def train_model_simple(model, train_loader, criterion, optimizer, device, num_epochs,\n",
    "                      val_loader=None, patience=10, log_prefix=\"\"):\n",
    "    \"\"\"SIMPLE but effective training.\"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for data, target in val_loader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "            # Early stopping\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                epochs_no_improve = 0\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epoch % 50 == 0:\n",
    "                print(f\"Epoch {epoch+1}: Train: {avg_train_loss:.4f}, Val: {avg_val_loss:.4f}\")\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"Loaded best model state\")\n",
    "\n",
    "    return model, {}\n",
    "\n",
    "def save_results_to_files(all_results, output_dir):\n",
    "    \"\"\"Save results.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # JSON\n",
    "    with open(os.path.join(output_dir, 'complete_results.json'), 'w') as f:\n",
    "        json.dump(all_results, f, indent=2, default=str)\n",
    "\n",
    "    # CSV\n",
    "    summary_data = []\n",
    "    for strategy, strategy_results in all_results.items():\n",
    "        for sparsity, metrics in strategy_results.items():\n",
    "            row = {\n",
    "                'strategy': strategy,\n",
    "                'sparsity_ratio': sparsity,\n",
    "                'mse': metrics['mse'],\n",
    "                'mae': metrics['mae'],\n",
    "                'loss': metrics['loss'],\n",
    "                'macs_millions': metrics['macs'] / 1e6,\n",
    "                'params_millions': metrics['params'] / 1e6,\n",
    "                'size_mb': metrics['size_mb']\n",
    "            }\n",
    "            summary_data.append(row)\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_csv(os.path.join(output_dir, 'summary_results.csv'), index=False)\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "def create_results_plots(summary_df, output_dir):\n",
    "    \"\"\"Create plots.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # MSE vs Sparsity\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for strategy in summary_df['strategy'].unique():\n",
    "        data = summary_df[summary_df['strategy'] == strategy].sort_values('sparsity_ratio')\n",
    "        plt.plot(data['sparsity_ratio'] * 100, data['mse'], 'o-', label=strategy, linewidth=2, markersize=8)\n",
    "\n",
    "    plt.xlabel('Sparsity (%)')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.title('NASA MLP: MSE vs Sparsity (Simple Approach)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'mse_vs_sparsity.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Efficiency frontier\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for strategy in summary_df['strategy'].unique():\n",
    "        data = summary_df[summary_df['strategy'] == strategy].sort_values('sparsity_ratio')\n",
    "        plt.scatter(data['macs_millions'], data['mse'], label=strategy, s=100, alpha=0.8)\n",
    "        plt.plot(data['macs_millions'], data['mse'], '--', alpha=0.6)\n",
    "\n",
    "    plt.xlabel('MACs (Millions)')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.title('NASA MLP: Efficiency Frontier (Simple Approach)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'efficiency_frontier.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def print_results_table(summary_df):\n",
    "    \"\"\"Print results table.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SIMPLE APPROACH RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Baseline\n",
    "    baseline = summary_df[summary_df['sparsity_ratio'] == 0.0].iloc[0]\n",
    "    print(f\"\\nBaseline Performance:\")\n",
    "    print(f\"  MSE: {baseline['mse']:.2f}\")\n",
    "    print(f\"  MAE: {baseline['mae']:.2f}\")\n",
    "    print(f\"  MACs: {baseline['macs_millions']:.2f}M\")\n",
    "    print(f\"  Parameters: {baseline['params_millions']:.2f}M\")\n",
    "\n",
    "    # Complete table\n",
    "    print(f\"\\nComplete Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Strategy':<12} {'Sparsity':<8} {'MSE':<8} {'MAE':<8} {'MACs(M)':<8} {'Params(M)':<9} {'Size(MB)':<8}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for _, row in summary_df.sort_values(['strategy', 'sparsity_ratio']).iterrows():\n",
    "        print(f\"{row['strategy']:<12} {row['sparsity_ratio']*100:>6.0f}% \"\n",
    "              f\"{row['mse']:>7.2f} {row['mae']:>7.2f} {row['macs_millions']:>7.2f} \"\n",
    "              f\"{row['params_millions']:>8.2f} {row['size_mb']:>7.2f}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"SIMPLE approach - back to basics with research-proven optimizations.\"\"\"\n",
    "    print(\"Starting SIMPLE NASA MLP Approach - Back to Proven Basics\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # SIMPLE configuration - proven parameters from literature\n",
    "    config = {\n",
    "        'strategies': {\n",
    "            'MagnitudeL2': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.MagnitudeImportance(p=2)\n",
    "            },\n",
    "            'Random': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.RandomImportance()\n",
    "            },\n",
    "        },\n",
    "        'pruning_ratios': [0.0, 0.2, 0.5, 0.7],\n",
    "        'hidden_sizes': [80, 40],      # Small proven architecture from literature\n",
    "        'dropout_rate': 0.1,           # Lower dropout - less regularization needed\n",
    "        'window_size': 30,             # Proven optimal window size\n",
    "        'batch_size': 16,              # Smaller batches for better convergence\n",
    "        'learning_rate': 0.005,        # Optimized learning rate\n",
    "        'epochs': 1000,                # More epochs for convergence\n",
    "        'patience': 30,                # More patience for convergence\n",
    "        'output_dir': './results_mlp_nasa_simple',\n",
    "        'models_dir': './models_mlp_nasa_simple',\n",
    "        'data_dir': './data/CMaps'\n",
    "    }\n",
    "\n",
    "    os.makedirs(config['output_dir'], exist_ok=True)\n",
    "    os.makedirs(config['models_dir'], exist_ok=True)\n",
    "\n",
    "    # Simple data loading\n",
    "    train_loader, val_loader, test_loader, input_size = get_data_loaders_simple(\n",
    "        data_dir=config['data_dir'],\n",
    "        batch_size=config['batch_size'],\n",
    "        window_size=config['window_size']\n",
    "    )\n",
    "\n",
    "    example_input_cpu = torch.randn(1, input_size)\n",
    "    example_input_device = example_input_cpu.to(DEVICE)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Simple baseline training with optimized parameters\n",
    "    print(\"\\nTraining baseline with simple but optimized approach...\")\n",
    "    model = get_simple_mlp_model(input_size, config['hidden_sizes'], config['dropout_rate'])\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Optimized simple optimizer - research shows this works best for NASA dataset\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=1e-6)\n",
    "\n",
    "    trained_model, _ = train_model_simple(\n",
    "        model, train_loader, criterion, optimizer, DEVICE,\n",
    "        config['epochs'], val_loader, config['patience'], \"Optimized Simple Baseline\"\n",
    "    )\n",
    "\n",
    "    # Save and evaluate baseline\n",
    "    baseline_path = os.path.join(config['models_dir'], 'baseline_model.pth')\n",
    "    save_model(trained_model, baseline_path, example_input_cpu)\n",
    "\n",
    "    baseline_metrics = evaluate_model(trained_model, test_loader, example_input_device, criterion, DEVICE)\n",
    "    print(f\"\\nSimple Baseline Results:\")\n",
    "    print(f\"  MSE: {baseline_metrics['mse']:.2f}\")\n",
    "    print(f\"  MAE: {baseline_metrics['mae']:.2f}\")\n",
    "    print(f\"  MACs: {baseline_metrics['macs']/1e6:.2f}M\")\n",
    "    print(f\"  Params: {baseline_metrics['params']/1e6:.2f}M\")\n",
    "\n",
    "    # Initialize results\n",
    "    all_results = {}\n",
    "    for strategy_name in config['strategies'].keys():\n",
    "        all_results[strategy_name] = {0.0: baseline_metrics}\n",
    "\n",
    "    ignored_layers = get_ignored_layers(trained_model)\n",
    "\n",
    "    # Simple pruning experiments\n",
    "    print(\"\\nStarting simple pruning experiments...\")\n",
    "    for strategy_name, strategy_config in config['strategies'].items():\n",
    "        print(f\"\\n--- Strategy: {strategy_name} ---\")\n",
    "\n",
    "        for sparsity_ratio in config['pruning_ratios']:\n",
    "            if sparsity_ratio == 0.0:\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nTesting {strategy_name} at {sparsity_ratio:.1%} sparsity...\")\n",
    "\n",
    "            # Load fresh copy\n",
    "            model_copy = get_simple_mlp_model(input_size, config['hidden_sizes'], config['dropout_rate'])\n",
    "            model_copy.load_state_dict(torch.load(baseline_path, map_location=DEVICE))\n",
    "            model_copy.to(DEVICE)\n",
    "\n",
    "            # Simple pruning\n",
    "            pruned_model = prune_model(model_copy, strategy_config, sparsity_ratio,\n",
    "                                     example_input_device, ignored_layers)\n",
    "\n",
    "            # Optimized fine-tuning\n",
    "            print(\"Optimized fine-tuning...\")\n",
    "            optimizer_ft = optim.Adam(pruned_model.parameters(), lr=config['learning_rate']/2, weight_decay=1e-6)\n",
    "\n",
    "            fine_tuned_model, _ = train_model_simple(\n",
    "                pruned_model, train_loader, criterion, optimizer_ft, DEVICE,\n",
    "                config['epochs']//2, val_loader, config['patience'],\n",
    "                f\"{strategy_name}-{sparsity_ratio:.1%}\"\n",
    "            )\n",
    "\n",
    "            # Evaluate\n",
    "            final_metrics = evaluate_model(fine_tuned_model, test_loader, example_input_device, criterion, DEVICE)\n",
    "            all_results[strategy_name][sparsity_ratio] = final_metrics\n",
    "\n",
    "            print(f\"Results: MSE={final_metrics['mse']:.2f}, MAE={final_metrics['mae']:.2f}\")\n",
    "\n",
    "            # Save\n",
    "            model_filename = f\"{strategy_name.lower()}_sparsity_{sparsity_ratio:.1f}.pth\"\n",
    "            save_model(fine_tuned_model, os.path.join(config['models_dir'], model_filename), example_input_cpu)\n",
    "\n",
    "    # Results\n",
    "    summary_df = save_results_to_files(all_results, config['output_dir'])\n",
    "    create_results_plots(summary_df, config['output_dir'])\n",
    "    print_results_table(summary_df)\n",
    "\n",
    "    # Analysis\n",
    "    best_mse = summary_df['mse'].min()\n",
    "    print(f\"\\nBest MSE achieved: {best_mse:.2f}\")\n",
    "\n",
    "    print(f\"\\n📁 Results: {os.path.abspath(config['output_dir'])}\")\n",
    "    print(f\"📁 Models: {os.path.abspath(config['models_dir'])}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "55107d75e92ba07b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting SIMPLE NASA MLP Approach - Back to Proven Basics\n",
      "============================================================\n",
      "Loading NASA C-MAPSS dataset from: ./data/CMaps\n",
      "--- Simple Proven Data Preparation ---\n",
      "Removing 10 non-informative features\n",
      "Keeping sensors: [2,3,4,7,8,9,11,12,13,14,15,17,20,21]\n",
      "Using 14 proven effective sensors\n",
      "Created 17731 samples with simple windowing\n",
      "Created 100 samples with simple windowing\n",
      "Input size: 420 (window: 30 × features: 14)\n",
      "Train: 14185, Val: 3546, Test: 100\n",
      "\n",
      "Training baseline with simple but optimized approach...\n",
      "✅ Created Simple MLP: 420 -> 80 -> 40 -> 1\n",
      "Epoch 1: Train: 727.5921, Val: 437.7029\n",
      "Epoch 51: Train: 242.6553, Val: 324.4560\n",
      "Early stopping at epoch 68\n",
      "Loaded best model state\n",
      "✅ Model saved to ./models_mlp_nasa_simple/baseline_model.pth\n",
      "\n",
      "Simple Baseline Results:\n",
      "  MSE: 182.18\n",
      "  MAE: 9.93\n",
      "  MACs: 0.04M\n",
      "  Params: 0.04M\n",
      "\n",
      "Starting simple pruning experiments...\n",
      "\n",
      "--- Strategy: MagnitudeL2 ---\n",
      "\n",
      "Testing MagnitudeL2 at 20.0% sparsity...\n",
      "✅ Created Simple MLP: 420 -> 80 -> 40 -> 1\n",
      "Initial MACs: 0.04M\n",
      "Applying MagnitudeImportance pruning at 20.0%\n",
      "Final MACs: 0.03M (8.1% reduction)\n",
      "Optimized fine-tuning...\n",
      "Epoch 1: Train: 238.5514, Val: 233.1246\n",
      "Epoch 51: Train: 214.6669, Val: 184.1862\n",
      "Epoch 101: Train: 199.3586, Val: 197.1609\n",
      "Early stopping at epoch 101\n",
      "Loaded best model state\n",
      "Results: MSE=191.93, MAE=10.24\n",
      "✅ Model saved to ./models_mlp_nasa_simple/magnitudel2_sparsity_0.2.pth\n",
      "\n",
      "Testing MagnitudeL2 at 50.0% sparsity...\n",
      "✅ Created Simple MLP: 420 -> 80 -> 40 -> 1\n",
      "Initial MACs: 0.04M\n",
      "Applying MagnitudeImportance pruning at 50.0%\n",
      "Final MACs: 0.03M (18.7% reduction)\n",
      "Optimized fine-tuning...\n",
      "Epoch 1: Train: 236.2193, Val: 275.7967\n",
      "Epoch 51: Train: 213.5810, Val: 180.4837\n",
      "Early stopping at epoch 80\n",
      "Loaded best model state\n",
      "Results: MSE=181.13, MAE=10.20\n",
      "✅ Model saved to ./models_mlp_nasa_simple/magnitudel2_sparsity_0.5.pth\n",
      "\n",
      "Testing MagnitudeL2 at 70.0% sparsity...\n",
      "✅ Created Simple MLP: 420 -> 80 -> 40 -> 1\n",
      "Initial MACs: 0.04M\n",
      "Applying MagnitudeImportance pruning at 70.0%\n",
      "Final MACs: 0.03M (25.4% reduction)\n",
      "Optimized fine-tuning...\n",
      "Epoch 1: Train: 239.2088, Val: 191.9055\n",
      "Epoch 51: Train: 216.0078, Val: 200.4490\n",
      "Early stopping at epoch 65\n",
      "Loaded best model state\n",
      "Results: MSE=186.82, MAE=9.99\n",
      "✅ Model saved to ./models_mlp_nasa_simple/magnitudel2_sparsity_0.7.pth\n",
      "\n",
      "--- Strategy: Random ---\n",
      "\n",
      "Testing Random at 20.0% sparsity...\n",
      "✅ Created Simple MLP: 420 -> 80 -> 40 -> 1\n",
      "Initial MACs: 0.04M\n",
      "Applying RandomImportance pruning at 20.0%\n",
      "Final MACs: 0.03M (8.1% reduction)\n",
      "Optimized fine-tuning...\n",
      "Epoch 1: Train: 243.2811, Val: 191.7144\n",
      "Early stopping at epoch 49\n",
      "Loaded best model state\n",
      "Results: MSE=173.10, MAE=9.61\n",
      "✅ Model saved to ./models_mlp_nasa_simple/random_sparsity_0.2.pth\n",
      "\n",
      "Testing Random at 50.0% sparsity...\n",
      "✅ Created Simple MLP: 420 -> 80 -> 40 -> 1\n",
      "Initial MACs: 0.04M\n",
      "Applying RandomImportance pruning at 50.0%\n",
      "Final MACs: 0.03M (18.7% reduction)\n",
      "Optimized fine-tuning...\n",
      "Epoch 1: Train: 257.2367, Val: 200.4415\n",
      "Early stopping at epoch 41\n",
      "Loaded best model state\n",
      "Results: MSE=177.08, MAE=9.87\n",
      "✅ Model saved to ./models_mlp_nasa_simple/random_sparsity_0.5.pth\n",
      "\n",
      "Testing Random at 70.0% sparsity...\n",
      "✅ Created Simple MLP: 420 -> 80 -> 40 -> 1\n",
      "Initial MACs: 0.04M\n",
      "Applying RandomImportance pruning at 70.0%\n",
      "Final MACs: 0.03M (25.4% reduction)\n",
      "Optimized fine-tuning...\n",
      "Epoch 1: Train: 265.7770, Val: 204.2962\n",
      "Early stopping at epoch 45\n",
      "Loaded best model state\n",
      "Results: MSE=183.24, MAE=10.13\n",
      "✅ Model saved to ./models_mlp_nasa_simple/random_sparsity_0.7.pth\n",
      "\n",
      "================================================================================\n",
      "SIMPLE APPROACH RESULTS\n",
      "================================================================================\n",
      "\n",
      "Baseline Performance:\n",
      "  MSE: 182.18\n",
      "  MAE: 9.93\n",
      "  MACs: 0.04M\n",
      "  Parameters: 0.04M\n",
      "\n",
      "Complete Results:\n",
      "--------------------------------------------------------------------------------\n",
      "Strategy     Sparsity MSE      MAE      MACs(M)  Params(M) Size(MB)\n",
      "--------------------------------------------------------------------------------\n",
      "MagnitudeL2       0%  182.18    9.93    0.04     0.04    0.14\n",
      "MagnitudeL2      20%  191.93   10.24    0.03     0.03    0.13\n",
      "MagnitudeL2      50%  181.13   10.20    0.03     0.03    0.11\n",
      "MagnitudeL2      70%  186.82    9.99    0.03     0.03    0.11\n",
      "Random            0%  182.18    9.93    0.04     0.04    0.14\n",
      "Random           20%  173.10    9.61    0.03     0.03    0.13\n",
      "Random           50%  177.08    9.87    0.03     0.03    0.11\n",
      "Random           70%  183.24   10.13    0.03     0.03    0.11\n",
      "\n",
      "Best MSE achieved: 173.10\n",
      "\n",
      "📁 Results: /home/muis/thesis/github-repo/master-thesis/mlp/results_mlp_nasa_simple\n",
      "📁 Models: /home/muis/thesis/github-repo/master-thesis/mlp/models_mlp_nasa_simple\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
