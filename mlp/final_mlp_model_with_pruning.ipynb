{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# v2",
   "id": "d1fd72b2a34de9fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T11:58:23.470724Z",
     "start_time": "2025-06-11T11:48:03.038405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_pruning as tp\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from typing import List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_BASE_NAME = \"mlp_nasa\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# SIMPLIFIED preprocessing based on proven NASA C-MAPSS research\n",
    "column_names = ['unit_number', 'time_in_cycles'] + [f'op_setting_{i}' for i in range(1, 4)] + [f'sensor_{i}' for i in range(1, 24)]\n",
    "\n",
    "def load_dataframe(file_path: str) -> pd.DataFrame | None:\n",
    "    \"\"\"Load NASA data file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=' ', header=None, names=column_names)\n",
    "        df.dropna(axis=1, how='all', inplace=True)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def proven_feature_selection(df: pd.DataFrame) -> list:\n",
    "    \"\"\"Use PROVEN feature selection from NASA literature - SIMPLE approach.\"\"\"\n",
    "    if df is None:\n",
    "        return []\n",
    "\n",
    "    # Based on multiple research papers - these are the PROVEN effective sensors for FD001\n",
    "    # Remove sensors with minimal variation (proven approach)\n",
    "    sensors_to_remove = [\n",
    "        'sensor_1',   # Lever position - constant for FD001\n",
    "        'sensor_5',   # Static pressure - minimal variation\n",
    "        'sensor_6',   # Physical fan speed - redundant with sensor_8\n",
    "        'sensor_10',  # Static pressure - minimal variation\n",
    "        'sensor_16',  # Static pressure - minimal variation\n",
    "        'sensor_18',  # Bleed enthalpy - minimal variation\n",
    "        'sensor_19'   # Demanded fan speed - minimal variation\n",
    "    ]\n",
    "\n",
    "    # Remove operational settings - constant for FD001\n",
    "    ops_to_remove = ['op_setting_1', 'op_setting_2', 'op_setting_3']\n",
    "\n",
    "    cols_to_remove = sensors_to_remove + ops_to_remove\n",
    "\n",
    "    print(f\"Removing {len(cols_to_remove)} non-informative features\")\n",
    "    print(f\"Keeping sensors: [2,3,4,7,8,9,11,12,13,14,15,17,20,21]\")\n",
    "    return cols_to_remove\n",
    "\n",
    "def add_rul_simple(df: pd.DataFrame) -> pd.DataFrame | None:\n",
    "    \"\"\"Simple but PROVEN RUL calculation.\"\"\"\n",
    "    if df is None:\n",
    "        return None\n",
    "\n",
    "    max_cycles = df.groupby('unit_number')['time_in_cycles'].max().reset_index()\n",
    "    max_cycles.columns = ['unit_number', 'max_cycle']\n",
    "    df = df.merge(max_cycles, on='unit_number', how='left')\n",
    "    df['RUL'] = df['max_cycle'] - df['time_in_cycles']\n",
    "    df.drop(columns=['max_cycle'], inplace=True)\n",
    "\n",
    "    # PROVEN: Piece-wise linear RUL - engines don't degrade early in life\n",
    "    # This is the STANDARD approach in NASA research\n",
    "    df['RUL'] = df['RUL'].apply(lambda x: min(x, 125))\n",
    "\n",
    "    return df\n",
    "\n",
    "def normalize_data_simple(df: pd.DataFrame, columns_to_normalize: List[str],\n",
    "                         scaler: MinMaxScaler = None) -> Tuple[pd.DataFrame, MinMaxScaler]:\n",
    "    \"\"\"Simple normalization that works.\"\"\"\n",
    "    if df is None:\n",
    "        return None, None\n",
    "\n",
    "    data_to_scale = df[columns_to_normalize]\n",
    "\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        df[columns_to_normalize] = scaler.fit_transform(data_to_scale)\n",
    "    else:\n",
    "        df[columns_to_normalize] = scaler.transform(data_to_scale)\n",
    "\n",
    "    return df, scaler\n",
    "\n",
    "def prepare_cmapss_data_simple(data_dir: str, train_file: str, test_file: str, test_rul_file: str) -> Tuple[\n",
    "    pd.DataFrame, pd.DataFrame, pd.DataFrame, MinMaxScaler, List[str]]:\n",
    "    \"\"\"SIMPLE but PROVEN data preparation.\"\"\"\n",
    "    print(\"--- Simple Proven Data Preparation ---\")\n",
    "\n",
    "    # Load data\n",
    "    train_df = load_dataframe(os.path.join(data_dir, train_file))\n",
    "    test_df = load_dataframe(os.path.join(data_dir, test_file))\n",
    "    test_rul_df = pd.read_csv(os.path.join(data_dir, test_rul_file), header=None, names=['RUL'])\n",
    "\n",
    "    # Add RUL\n",
    "    train_df = add_rul_simple(train_df)\n",
    "\n",
    "    # Simple feature selection - PROVEN approach\n",
    "    cols_to_remove = proven_feature_selection(train_df)\n",
    "    feature_cols = [col for col in train_df.columns if\n",
    "                   col not in ['unit_number', 'time_in_cycles', 'RUL'] + cols_to_remove]\n",
    "\n",
    "    # Remove unwanted columns\n",
    "    train_df.drop(columns=cols_to_remove, inplace=True, errors='ignore')\n",
    "    test_df.drop(columns=cols_to_remove, inplace=True, errors='ignore')\n",
    "\n",
    "    print(f\"Using {len(feature_cols)} proven effective sensors\")\n",
    "\n",
    "    # Simple normalization\n",
    "    train_df_norm, scaler = normalize_data_simple(train_df.copy(), feature_cols)\n",
    "    test_df_norm, _ = normalize_data_simple(test_df.copy(), feature_cols, scaler=scaler)\n",
    "\n",
    "    return train_df_norm, test_df_norm, test_rul_df, scaler, feature_cols\n",
    "\n",
    "# SIMPLE Dataset - no over-engineering\n",
    "class NASADatasetSimple(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, feature_cols: List[str], window_size: int = 30,\n",
    "                 stride: int = 1, is_test: bool = False, test_rul_df: pd.DataFrame = None):\n",
    "        self.df = df\n",
    "        self.feature_cols = feature_cols\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.is_test = is_test\n",
    "        self.test_rul_df = test_rul_df\n",
    "        self.samples = []\n",
    "        self.targets = []\n",
    "\n",
    "        self._prepare_samples()\n",
    "\n",
    "    def _prepare_samples(self):\n",
    "        \"\"\"SIMPLE windowing strategy - PROVEN approach.\"\"\"\n",
    "        units = self.df['unit_number'].unique()\n",
    "\n",
    "        for unit in units:\n",
    "            unit_df = self.df[self.df['unit_number'] == unit].sort_values('time_in_cycles')\n",
    "\n",
    "            if self.is_test:\n",
    "                # Test: only last window\n",
    "                if len(unit_df) >= self.window_size:\n",
    "                    window_data = unit_df[self.feature_cols].iloc[-self.window_size:].values\n",
    "                    self.samples.append(window_data)\n",
    "                    if self.test_rul_df is not None:\n",
    "                        rul = min(self.test_rul_df.iloc[unit - 1]['RUL'], 125)\n",
    "                        self.targets.append(rul)\n",
    "                else:\n",
    "                    # Pad if needed\n",
    "                    window_data = unit_df[self.feature_cols].values\n",
    "                    padded = np.zeros((self.window_size, len(self.feature_cols)))\n",
    "                    if len(window_data) > 0:\n",
    "                        padded[-len(window_data):] = window_data\n",
    "                    self.samples.append(padded)\n",
    "                    if self.test_rul_df is not None:\n",
    "                        rul = min(self.test_rul_df.iloc[unit - 1]['RUL'], 125)\n",
    "                        self.targets.append(rul)\n",
    "            else:\n",
    "                # Training: simple sliding window\n",
    "                for i in range(0, len(unit_df) - self.window_size + 1, self.stride):\n",
    "                    window_data = unit_df[self.feature_cols].iloc[i:i + self.window_size].values\n",
    "                    rul = unit_df['RUL'].iloc[i + self.window_size - 1]\n",
    "                    self.samples.append(window_data)\n",
    "                    self.targets.append(rul)\n",
    "\n",
    "        self.samples = np.array(self.samples, dtype=np.float32)\n",
    "        self.targets = np.array(self.targets, dtype=np.float32)\n",
    "\n",
    "        print(f\"Created {len(self.samples)} samples with simple windowing\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx].flatten()\n",
    "        target = self.targets[idx]\n",
    "        return torch.FloatTensor(sample), torch.FloatTensor([target])\n",
    "\n",
    "# SIMPLE MLP - proven architecture\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes=[80, 40], dropout_rate=0.2):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "\n",
    "        # Output layer - NO activation for regression\n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def get_data_loaders_simple(data_dir='./data/NASA', batch_size=32, window_size=30, val_split=0.2, seed=42):\n",
    "    \"\"\"Simple data loading - proven parameters.\"\"\"\n",
    "    print(f\"Loading NASA C-MAPSS dataset from: {data_dir}\")\n",
    "\n",
    "    train_df, test_df, test_rul_df, scaler, feature_cols = prepare_cmapss_data_simple(\n",
    "        data_dir, 'train_FD001.txt', 'test_FD001.txt', 'RUL_FD001.txt'\n",
    "    )\n",
    "\n",
    "    # Create datasets with simple approach\n",
    "    full_train_dataset = NASADatasetSimple(train_df, feature_cols, window_size=window_size, stride=1)\n",
    "\n",
    "    # Train/val split\n",
    "    val_size = int(len(full_train_dataset) * val_split)\n",
    "    train_size = len(full_train_dataset) - val_size\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_train_dataset, [train_size, val_size], generator=generator\n",
    "    )\n",
    "\n",
    "    test_dataset = NASADatasetSimple(test_df, feature_cols, window_size=window_size,\n",
    "                                    is_test=True, test_rul_df=test_rul_df)\n",
    "\n",
    "    # Simple data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    input_size = window_size * len(feature_cols)\n",
    "    print(f\"Input size: {input_size} (window: {window_size} × features: {len(feature_cols)})\")\n",
    "    print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader, input_size\n",
    "\n",
    "def get_simple_mlp_model(input_size, hidden_sizes=[80, 40], dropout_rate=0.2):\n",
    "    \"\"\"Simple but effective MLP.\"\"\"\n",
    "    model = SimpleMLP(input_size, hidden_sizes, dropout_rate)\n",
    "    print(f\"✅ Created Simple MLP: {input_size} -> {' -> '.join(map(str, hidden_sizes))} -> 1\")\n",
    "    return model\n",
    "\n",
    "def get_ignored_layers(model):\n",
    "    \"\"\"Get final layer to ignore during pruning.\"\"\"\n",
    "    ignored_layers = []\n",
    "    for module in reversed(list(model.model)):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            ignored_layers.append(module)\n",
    "            break\n",
    "    return ignored_layers\n",
    "\n",
    "def calculate_macs_params(model, example_input):\n",
    "    \"\"\"Calculate efficiency metrics.\"\"\"\n",
    "    model.eval()\n",
    "    target_device = example_input.device\n",
    "    model_on_device = model.to(target_device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        macs, params = tp.utils.count_ops_and_params(model_on_device, example_input)\n",
    "\n",
    "    return macs, params\n",
    "\n",
    "def save_model(model, save_path, example_input_cpu=None):\n",
    "    \"\"\"Save model.\"\"\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"✅ Model saved to {save_path}\")\n",
    "\n",
    "def evaluate_model(model, data_loader, example_input, criterion, device):\n",
    "    \"\"\"Simple evaluation.\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Calculate efficiency metrics\n",
    "    macs, params = calculate_macs_params(model, example_input.to(device))\n",
    "    model_size_mb = params * 4 / (1024 * 1024)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            all_predictions.extend(output.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    all_predictions = np.array(all_predictions).flatten()\n",
    "    all_targets = np.array(all_targets).flatten()\n",
    "\n",
    "    mse = np.mean((all_predictions - all_targets) ** 2)\n",
    "    mae = np.mean(np.abs(all_predictions - all_targets))\n",
    "\n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'loss': total_loss / len(data_loader.dataset),\n",
    "        'macs': macs,\n",
    "        'params': params,\n",
    "        'size_mb': model_size_mb\n",
    "    }\n",
    "\n",
    "def prune_model(model, strategy_config, sparsity_ratio, example_input, ignored_layers=None):\n",
    "    \"\"\"Simple pruning.\"\"\"\n",
    "    if sparsity_ratio == 0.0:\n",
    "        return model\n",
    "\n",
    "    model.eval()\n",
    "    pruned_model = copy.deepcopy(model)\n",
    "    pruned_model.to(example_input.device)\n",
    "\n",
    "    initial_macs, _ = calculate_macs_params(pruned_model, example_input)\n",
    "    print(f\"Initial MACs: {initial_macs / 1e6:.2f}M\")\n",
    "\n",
    "    ignored_layers = ignored_layers or []\n",
    "\n",
    "    # Simple pruner\n",
    "    pruner = strategy_config['pruner'](\n",
    "        pruned_model,\n",
    "        example_input,\n",
    "        importance=strategy_config['importance'],\n",
    "        iterative_steps=3,  # Fewer steps\n",
    "        ch_sparsity=sparsity_ratio,\n",
    "        root_module_types=[nn.Linear],\n",
    "        ignored_layers=ignored_layers\n",
    "    )\n",
    "\n",
    "    print(f\"Applying {strategy_config['importance'].__class__.__name__} pruning at {sparsity_ratio:.1%}\")\n",
    "    pruner.step()\n",
    "\n",
    "    final_macs, _ = calculate_macs_params(pruned_model, example_input)\n",
    "    reduction = (initial_macs - final_macs) / initial_macs * 100 if initial_macs > 0 else 0\n",
    "    print(f\"Final MACs: {final_macs / 1e6:.2f}M ({reduction:.1f}% reduction)\")\n",
    "\n",
    "    return pruned_model\n",
    "\n",
    "def train_model_simple(model, train_loader, criterion, optimizer, device, num_epochs,\n",
    "                      val_loader=None, patience=10, log_prefix=\"\"):\n",
    "    \"\"\"SIMPLE but effective training.\"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for data, target in val_loader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "            # Early stopping\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                epochs_no_improve = 0\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epoch % 50 == 0:\n",
    "                print(f\"Epoch {epoch+1}: Train: {avg_train_loss:.4f}, Val: {avg_val_loss:.4f}\")\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"Loaded best model state\")\n",
    "\n",
    "    return model, {}\n",
    "\n",
    "def save_results_to_files(all_results, output_dir):\n",
    "    \"\"\"Save results.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # JSON\n",
    "    with open(os.path.join(output_dir, 'complete_results.json'), 'w') as f:\n",
    "        json.dump(all_results, f, indent=2, default=str)\n",
    "\n",
    "    # CSV\n",
    "    summary_data = []\n",
    "    for strategy, strategy_results in all_results.items():\n",
    "        for sparsity, metrics in strategy_results.items():\n",
    "            row = {\n",
    "                'strategy': strategy,\n",
    "                'sparsity_ratio': sparsity,\n",
    "                'mse': metrics['mse'],\n",
    "                'mae': metrics['mae'],\n",
    "                'loss': metrics['loss'],\n",
    "                'macs_millions': metrics['macs'] / 1e6,\n",
    "                'params_millions': metrics['params'] / 1e6,\n",
    "                'size_mb': metrics['size_mb']\n",
    "            }\n",
    "            summary_data.append(row)\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_csv(os.path.join(output_dir, 'summary_results.csv'), index=False)\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "def create_results_plots(summary_df, output_dir):\n",
    "    \"\"\"Create plots.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # MSE vs Sparsity\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for strategy in summary_df['strategy'].unique():\n",
    "        data = summary_df[summary_df['strategy'] == strategy].sort_values('sparsity_ratio')\n",
    "        plt.plot(data['sparsity_ratio'] * 100, data['mse'], 'o-', label=strategy, linewidth=2, markersize=8)\n",
    "\n",
    "    plt.xlabel('Sparsity (%)')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.title('NASA MLP: MSE vs Sparsity (Simple Approach)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'mse_vs_sparsity.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Efficiency frontier\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for strategy in summary_df['strategy'].unique():\n",
    "        data = summary_df[summary_df['strategy'] == strategy].sort_values('sparsity_ratio')\n",
    "        plt.scatter(data['macs_millions'], data['mse'], label=strategy, s=100, alpha=0.8)\n",
    "        plt.plot(data['macs_millions'], data['mse'], '--', alpha=0.6)\n",
    "\n",
    "    plt.xlabel('MACs (Millions)')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.title('NASA MLP: Efficiency Frontier (Simple Approach)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'efficiency_frontier.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def print_results_table(summary_df):\n",
    "    \"\"\"Print results table.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SIMPLE APPROACH RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Baseline\n",
    "    baseline = summary_df[summary_df['sparsity_ratio'] == 0.0].iloc[0]\n",
    "    print(f\"\\nBaseline Performance:\")\n",
    "    print(f\"  MSE: {baseline['mse']:.2f}\")\n",
    "    print(f\"  MAE: {baseline['mae']:.2f}\")\n",
    "    print(f\"  MACs: {baseline['macs_millions']:.2f}M\")\n",
    "    print(f\"  Parameters: {baseline['params_millions']:.2f}M\")\n",
    "\n",
    "    # Complete table\n",
    "    print(f\"\\nComplete Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Strategy':<12} {'Sparsity':<8} {'MSE':<8} {'MAE':<8} {'MACs(M)':<8} {'Params(M)':<9} {'Size(MB)':<8}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for _, row in summary_df.sort_values(['strategy', 'sparsity_ratio']).iterrows():\n",
    "        print(f\"{row['strategy']:<12} {row['sparsity_ratio']*100:>6.0f}% \"\n",
    "              f\"{row['mse']:>7.2f} {row['mae']:>7.2f} {row['macs_millions']:>7.2f} \"\n",
    "              f\"{row['params_millions']:>8.2f} {row['size_mb']:>7.2f}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"SIMPLE approach - back to basics with research-proven optimizations.\"\"\"\n",
    "    print(\"Starting SIMPLE NASA MLP Approach - Back to Proven Basics\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # SIMPLE configuration - proven parameters from literature\n",
    "    config = {\n",
    "        'strategies': {\n",
    "            'MagnitudeL2': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.MagnitudeImportance(p=2)\n",
    "            },\n",
    "            'Random': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.RandomImportance()\n",
    "            },\n",
    "        },\n",
    "        'pruning_ratios': [0.0, 0.2, 0.5, 0.7],\n",
    "        'hidden_sizes': [512, 512, 512,128, 64, 32],      # Small proven architecture from literature\n",
    "        'dropout_rate': 0.1,           # Lower dropout - less regularization needed\n",
    "        'window_size': 30,             # Proven optimal window size\n",
    "        'batch_size': 16,              # Smaller batches for better convergence\n",
    "        'learning_rate': 0.005,        # Optimized learning rate\n",
    "        'epochs': 1000,                # More epochs for convergence\n",
    "        'patience': 30,                # More patience for convergence\n",
    "        'output_dir': './results_mlp_nasa_simple',\n",
    "        'models_dir': './models_mlp_nasa_simple',\n",
    "        'data_dir': './data/CMaps'\n",
    "    }\n",
    "\n",
    "    os.makedirs(config['output_dir'], exist_ok=True)\n",
    "    os.makedirs(config['models_dir'], exist_ok=True)\n",
    "\n",
    "    # Simple data loading\n",
    "    train_loader, val_loader, test_loader, input_size = get_data_loaders_simple(\n",
    "        data_dir=config['data_dir'],\n",
    "        batch_size=config['batch_size'],\n",
    "        window_size=config['window_size']\n",
    "    )\n",
    "\n",
    "    example_input_cpu = torch.randn(1, input_size)\n",
    "    example_input_device = example_input_cpu.to(DEVICE)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Simple baseline training with optimized parameters\n",
    "    print(\"\\nTraining baseline with simple but optimized approach...\")\n",
    "    model = get_simple_mlp_model(input_size, config['hidden_sizes'], config['dropout_rate'])\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Optimized simple optimizer - research shows this works best for NASA dataset\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=1e-6)\n",
    "\n",
    "    trained_model, _ = train_model_simple(\n",
    "        model, train_loader, criterion, optimizer, DEVICE,\n",
    "        config['epochs'], val_loader, config['patience'], \"Optimized Simple Baseline\"\n",
    "    )\n",
    "\n",
    "    # Save and evaluate baseline\n",
    "    baseline_path = os.path.join(config['models_dir'], 'baseline_model.pth')\n",
    "    save_model(trained_model, baseline_path, example_input_cpu)\n",
    "\n",
    "    baseline_metrics = evaluate_model(trained_model, test_loader, example_input_device, criterion, DEVICE)\n",
    "    print(f\"\\nSimple Baseline Results:\")\n",
    "    print(f\"  MSE: {baseline_metrics['mse']:.2f}\")\n",
    "    print(f\"  MAE: {baseline_metrics['mae']:.2f}\")\n",
    "    print(f\"  MACs: {baseline_metrics['macs']/1e6:.2f}M\")\n",
    "    print(f\"  Params: {baseline_metrics['params']/1e6:.2f}M\")\n",
    "\n",
    "    # Initialize results\n",
    "    all_results = {}\n",
    "    for strategy_name in config['strategies'].keys():\n",
    "        all_results[strategy_name] = {0.0: baseline_metrics}\n",
    "\n",
    "    ignored_layers = get_ignored_layers(trained_model)\n",
    "\n",
    "    # Simple pruning experiments\n",
    "    print(\"\\nStarting simple pruning experiments...\")\n",
    "    for strategy_name, strategy_config in config['strategies'].items():\n",
    "        print(f\"\\n--- Strategy: {strategy_name} ---\")\n",
    "\n",
    "        for sparsity_ratio in config['pruning_ratios']:\n",
    "            if sparsity_ratio == 0.0:\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nTesting {strategy_name} at {sparsity_ratio:.1%} sparsity...\")\n",
    "\n",
    "            # Load fresh copy\n",
    "            model_copy = get_simple_mlp_model(input_size, config['hidden_sizes'], config['dropout_rate'])\n",
    "            model_copy.load_state_dict(torch.load(baseline_path, map_location=DEVICE))\n",
    "            model_copy.to(DEVICE)\n",
    "\n",
    "            # Simple pruning\n",
    "            pruned_model = prune_model(model_copy, strategy_config, sparsity_ratio,\n",
    "                                     example_input_device, ignored_layers)\n",
    "\n",
    "            # Optimized fine-tuning\n",
    "            print(\"Optimized fine-tuning...\")\n",
    "            optimizer_ft = optim.Adam(pruned_model.parameters(), lr=config['learning_rate']/2, weight_decay=1e-6)\n",
    "\n",
    "            fine_tuned_model, _ = train_model_simple(\n",
    "                pruned_model, train_loader, criterion, optimizer_ft, DEVICE,\n",
    "                config['epochs']//2, val_loader, config['patience'],\n",
    "                f\"{strategy_name}-{sparsity_ratio:.1%}\"\n",
    "            )\n",
    "\n",
    "            # Evaluate\n",
    "            final_metrics = evaluate_model(fine_tuned_model, test_loader, example_input_device, criterion, DEVICE)\n",
    "            all_results[strategy_name][sparsity_ratio] = final_metrics\n",
    "\n",
    "            print(f\"Results: MSE={final_metrics['mse']:.2f}, MAE={final_metrics['mae']:.2f}\")\n",
    "\n",
    "            # Save\n",
    "            model_filename = f\"{strategy_name.lower()}_sparsity_{sparsity_ratio:.1f}.pth\"\n",
    "            save_model(fine_tuned_model, os.path.join(config['models_dir'], model_filename), example_input_cpu)\n",
    "\n",
    "    # Results\n",
    "    summary_df = save_results_to_files(all_results, config['output_dir'])\n",
    "    create_results_plots(summary_df, config['output_dir'])\n",
    "    print_results_table(summary_df)\n",
    "\n",
    "    # Analysis\n",
    "    best_mse = summary_df['mse'].min()\n",
    "    print(f\"\\nBest MSE achieved: {best_mse:.2f}\")\n",
    "\n",
    "    print(f\"\\n📁 Results: {os.path.abspath(config['output_dir'])}\")\n",
    "    print(f\"📁 Models: {os.path.abspath(config['models_dir'])}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "55107d75e92ba07b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting SIMPLE NASA MLP Approach - Back to Proven Basics\n",
      "============================================================\n",
      "Loading NASA C-MAPSS dataset from: ./data/CMaps\n",
      "--- Simple Proven Data Preparation ---\n",
      "Removing 10 non-informative features\n",
      "Keeping sensors: [2,3,4,7,8,9,11,12,13,14,15,17,20,21]\n",
      "Using 14 proven effective sensors\n",
      "Created 17731 samples with simple windowing\n",
      "Created 100 samples with simple windowing\n",
      "Input size: 420 (window: 30 × features: 14)\n",
      "Train: 14185, Val: 3546, Test: 100\n",
      "\n",
      "Training baseline with simple but optimized approach...\n",
      "✅ Created Simple MLP: 420 -> 512 -> 512 -> 512 -> 128 -> 64 -> 32 -> 1\n",
      "Epoch 1: Train: 949.4472, Val: 1923.2146\n",
      "Early stopping at epoch 46\n",
      "Loaded best model state\n",
      "✅ Model saved to ./models_mlp_nasa_simple/baseline_model.pth\n",
      "\n",
      "Simple Baseline Results:\n",
      "  MSE: 211.03\n",
      "  MAE: 10.55\n",
      "  MACs: 0.82M\n",
      "  Params: 0.82M\n",
      "\n",
      "Starting simple pruning experiments...\n",
      "\n",
      "--- Strategy: MagnitudeL2 ---\n",
      "\n",
      "Testing MagnitudeL2 at 20.0% sparsity...\n",
      "✅ Created Simple MLP: 420 -> 512 -> 512 -> 512 -> 128 -> 64 -> 32 -> 1\n",
      "Initial MACs: 0.82M\n",
      "Applying MagnitudeImportance pruning at 20.0%\n",
      "Final MACs: 0.72M (11.5% reduction)\n",
      "Optimized fine-tuning...\n",
      "Epoch 1: Train: 312.8301, Val: 277.2047\n",
      "Epoch 51: Train: 243.6344, Val: 224.8211\n",
      "Early stopping at epoch 91\n",
      "Loaded best model state\n",
      "Results: MSE=176.46, MAE=9.96\n",
      "✅ Model saved to ./models_mlp_nasa_simple/magnitudel2_sparsity_0.2.pth\n",
      "\n",
      "Testing MagnitudeL2 at 50.0% sparsity...\n",
      "✅ Created Simple MLP: 420 -> 512 -> 512 -> 512 -> 128 -> 64 -> 32 -> 1\n",
      "Initial MACs: 0.82M\n",
      "Applying MagnitudeImportance pruning at 50.0%\n",
      "Final MACs: 0.60M (27.1% reduction)\n",
      "Optimized fine-tuning...\n",
      "Epoch 1: Train: 328.2351, Val: 212.0781\n",
      "Early stopping at epoch 46\n",
      "Loaded best model state\n",
      "Results: MSE=183.79, MAE=10.31\n",
      "✅ Model saved to ./models_mlp_nasa_simple/magnitudel2_sparsity_0.5.pth\n",
      "\n",
      "Testing MagnitudeL2 at 70.0% sparsity...\n",
      "✅ Created Simple MLP: 420 -> 512 -> 512 -> 512 -> 128 -> 64 -> 32 -> 1\n",
      "Initial MACs: 0.82M\n",
      "Applying MagnitudeImportance pruning at 70.0%\n",
      "Final MACs: 0.52M (36.6% reduction)\n",
      "Optimized fine-tuning...\n",
      "Epoch 1: Train: 315.8286, Val: 196.5170\n",
      "Early stopping at epoch 45\n",
      "Loaded best model state\n",
      "Results: MSE=180.10, MAE=9.91\n",
      "✅ Model saved to ./models_mlp_nasa_simple/magnitudel2_sparsity_0.7.pth\n",
      "\n",
      "--- Strategy: Random ---\n",
      "\n",
      "Testing Random at 20.0% sparsity...\n",
      "✅ Created Simple MLP: 420 -> 512 -> 512 -> 512 -> 128 -> 64 -> 32 -> 1\n",
      "Initial MACs: 0.82M\n",
      "Applying RandomImportance pruning at 20.0%\n",
      "Final MACs: 0.72M (11.5% reduction)\n",
      "Optimized fine-tuning...\n",
      "Epoch 1: Train: 336.3298, Val: 242.8402\n",
      "Epoch 51: Train: 233.2667, Val: 216.3352\n",
      "Early stopping at epoch 87\n",
      "Loaded best model state\n",
      "Results: MSE=178.73, MAE=10.05\n",
      "✅ Model saved to ./models_mlp_nasa_simple/random_sparsity_0.2.pth\n",
      "\n",
      "Testing Random at 50.0% sparsity...\n",
      "✅ Created Simple MLP: 420 -> 512 -> 512 -> 512 -> 128 -> 64 -> 32 -> 1\n",
      "Initial MACs: 0.82M\n",
      "Applying RandomImportance pruning at 50.0%\n",
      "Final MACs: 0.60M (27.1% reduction)\n",
      "Optimized fine-tuning...\n",
      "Epoch 1: Train: 365.8073, Val: 205.0672\n",
      "Epoch 51: Train: 263.7484, Val: 382.2530\n",
      "Early stopping at epoch 86\n",
      "Loaded best model state\n",
      "Results: MSE=179.35, MAE=9.74\n",
      "✅ Model saved to ./models_mlp_nasa_simple/random_sparsity_0.5.pth\n",
      "\n",
      "Testing Random at 70.0% sparsity...\n",
      "✅ Created Simple MLP: 420 -> 512 -> 512 -> 512 -> 128 -> 64 -> 32 -> 1\n",
      "Initial MACs: 0.82M\n",
      "Applying RandomImportance pruning at 70.0%\n",
      "Final MACs: 0.52M (36.6% reduction)\n",
      "Optimized fine-tuning...\n",
      "Epoch 1: Train: 394.5270, Val: 284.8260\n",
      "Early stopping at epoch 41\n",
      "Loaded best model state\n",
      "Results: MSE=190.74, MAE=10.70\n",
      "✅ Model saved to ./models_mlp_nasa_simple/random_sparsity_0.7.pth\n",
      "\n",
      "================================================================================\n",
      "SIMPLE APPROACH RESULTS\n",
      "================================================================================\n",
      "\n",
      "Baseline Performance:\n",
      "  MSE: 211.03\n",
      "  MAE: 10.55\n",
      "  MACs: 0.82M\n",
      "  Parameters: 0.82M\n",
      "\n",
      "Complete Results:\n",
      "--------------------------------------------------------------------------------\n",
      "Strategy     Sparsity MSE      MAE      MACs(M)  Params(M) Size(MB)\n",
      "--------------------------------------------------------------------------------\n",
      "MagnitudeL2       0%  211.03   10.55    0.82     0.82    3.12\n",
      "MagnitudeL2      20%  176.46    9.96    0.72     0.72    2.76\n",
      "MagnitudeL2      50%  183.79   10.31    0.60     0.60    2.27\n",
      "MagnitudeL2      70%  180.10    9.91    0.52     0.52    1.97\n",
      "Random            0%  211.03   10.55    0.82     0.82    3.12\n",
      "Random           20%  178.73   10.05    0.72     0.72    2.76\n",
      "Random           50%  179.35    9.74    0.60     0.60    2.27\n",
      "Random           70%  190.74   10.70    0.52     0.52    1.97\n",
      "\n",
      "Best MSE achieved: 176.46\n",
      "\n",
      "📁 Results: /home/muis/thesis/github-repo/master-thesis/mlp/results_mlp_nasa_simple\n",
      "📁 Models: /home/muis/thesis/github-repo/master-thesis/mlp/models_mlp_nasa_simple\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "825d0cf4e74f5e41"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
