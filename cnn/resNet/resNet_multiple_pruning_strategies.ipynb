{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-08T10:52:33.935134Z",
     "start_time": "2025-05-08T10:52:31.397975Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F # Added for ResNet (F.relu)\n",
    "import torch_pruning as tp\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "from torch_pruning.pruner.algorithms.scheduler import linear_scheduler\n",
    "# from torchsummary import summary # Not used directly, can be kept or removed\n",
    "\n",
    "# For data loading (if not using a custom one)\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os # Already implicitly used by some functions, made explicit\n",
    "from cnn.resNet.resnet_example import get_data_loaders"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ResNet Blocks and Model Definition",
   "id": "ff63c1511dde0fd2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T10:52:36.184799Z",
     "start_time": "2025-05-08T10:52:36.173808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        # Adapted for CIFAR-10: kernel_size 3, stride 1\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        # The final FC layer name is 'linear'\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride_val in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride_val))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        # For CIFAR-10, output of layer4 will be 4x4 if input is 32x32\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18(num_classes=10):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
    "\n",
    "def ResNet34(num_classes=10):\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes)\n",
    "\n",
    "def ResNet50(num_classes=10):\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes)\n"
   ],
   "id": "7604bf46058c7c78",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T10:52:43.097787Z",
     "start_time": "2025-05-08T10:52:43.056529Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
   "id": "9b43e70684af2e8",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T10:52:44.426380Z",
     "start_time": "2025-05-08T10:52:44.422510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_model_as_onnx(model, example_input, output_path):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    # Export to ONNX\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        example_input,\n",
    "        output_path,\n",
    "        export_params=True,\n",
    "        opset_version=13,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "    )\n",
    "    print(f\"✅ Model saved as ONNX to {output_path}\")"
   ],
   "id": "5c71b687c1f36104",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T10:52:46.144740Z",
     "start_time": "2025-05-08T10:52:46.141555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_macs(model, example_input):\n",
    "    macs, params = tp.utils.count_ops_and_params(model, example_input)\n",
    "    return macs, params"
   ],
   "id": "45ac3df7fad6d230",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### compare results of different pruning strategies",
   "id": "13b3277bfa392f82"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T10:52:47.722545Z",
     "start_time": "2025-05-08T10:52:47.719021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compare_results(results):\n",
    "    print(\"\\n=== Pruning Strategy Comparison ===\")\n",
    "    print(f\"{'Strategy':<12} | {'MACs':<12} | {'Size (MB)':<10} | {'Accuracy (%)':<12}\")\n",
    "    print(\"-\" * 55)\n",
    "    for strategy, metrics in results.items():\n",
    "        print(f\"{strategy:<12} | {metrics['macs']:.2e} | {metrics['size_mb']:>9.2f} | {metrics['accuracy']:>12.2f}\")"
   ],
   "id": "95f0f63179a4d2d2",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### compare and plot results",
   "id": "be0aec494fcb0c5b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T10:52:49.426366Z",
     "start_time": "2025-05-08T10:52:49.419328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compare_results_and_plot(results, output_dir='output'):\n",
    "    \"\"\"\n",
    "    Print a comparison table and generate bar charts for MACs, model size, and accuracy\n",
    "    for each pruning strategy, including the initial model.\n",
    "    \"\"\"\n",
    "    # Print comparison table\n",
    "    print(\"\\n=== Pruning Strategy Comparison ===\")\n",
    "    print(f\"{'Strategy':<12} | {'MACs':<12} | {'Size (MB)':<10} | {'Accuracy (%)':<12}\")\n",
    "    print(\"-\" * 55)\n",
    "    for strategy, metrics in results.items():\n",
    "        print(f\"{strategy:<12} | {metrics['macs']:.2e} | {metrics['size_mb']:>9.2f} | {metrics['accuracy']:>12.2f}\")\n",
    "\n",
    "    # Generate bar charts\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    strategies = ['initial'] + [s for s in results if s != 'initial']\n",
    "    metrics_to_plot = ['macs', 'size_mb', 'accuracy'] # Renamed to avoid conflict\n",
    "    titles = {\n",
    "        'macs': 'MACs Comparison',\n",
    "        'size_mb': 'Model Size (MB) Comparison',\n",
    "        'accuracy': 'Accuracy (%) Comparison'\n",
    "    }\n",
    "    y_labels = {\n",
    "        'macs': 'MACs (Millions)',\n",
    "        'size_mb': 'Size (MB)',\n",
    "        'accuracy': 'Accuracy (%)'\n",
    "    }\n",
    "\n",
    "    colors = plt.cm.tab10(range(len(strategies)))\n",
    "    for metric_name in metrics_to_plot:\n",
    "        values = [results[strategy][metric_name] / 1e6 if metric_name == 'macs' else results[strategy][metric_name]\n",
    "                  for strategy in strategies]  # Convert MACs to millions\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bars = plt.bar(strategies, values, color=colors)\n",
    "        plt.xlabel('Strategy')\n",
    "        plt.ylabel(y_labels[metric_name])\n",
    "        plt.title(titles[metric_name])\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "        # Add value labels\n",
    "        for bar in bars:\n",
    "            yval = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., yval, f'{yval:.2f}', ha='center', va='bottom')\n",
    "\n",
    "        # Add initial model reference line\n",
    "        if 'initial' in results: # Ensure initial results exist\n",
    "            initial_value = results['initial'][metric_name] / 1e6 if metric_name == 'macs' else results['initial'][metric_name]\n",
    "            plt.axhline(y=initial_value, color='r', linestyle='--', label='Initial')\n",
    "            plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f'{metric_name}_comparison.png'))\n",
    "        plt.close()"
   ],
   "id": "a66fc2df36581084",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Model",
   "id": "839fd8aec0102217"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T10:52:54.435203Z",
     "start_time": "2025-05-08T10:52:54.431514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_model(model_class, num_classes, path, device): # model_class and num_classes added for flexibility\n",
    "    model = model_class(num_classes=num_classes).to(device)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    return model"
   ],
   "id": "9e6901cb385a9827",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Utility function to save the model",
   "id": "f04222e9289a359b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T10:52:56.872745Z",
     "start_time": "2025-05-08T10:52:56.869079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_model(model, path, example_input=None):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), path)\n",
    "    if example_input is not None:\n",
    "        onnx_path = path.replace('.pth', '.onnx')\n",
    "        save_model_as_onnx(model, example_input, onnx_path)"
   ],
   "id": "9a218511c4dbb077",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluate the model",
   "id": "cdabff4c285236b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T10:52:59.238426Z",
     "start_time": "2025-05-08T10:52:59.231630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, test_loader, example_input, device):\n",
    "    model.eval()\n",
    "    # Calculate metrics\n",
    "    macs, _ = calculate_macs(model, example_input.to(device)) # ensure example_input on correct device\n",
    "    params = sum(p.numel() for p in model.parameters() if p.requires_grad) # Count only trainable params\n",
    "    size_mb = params * 4 / 1e6 # Approximation using 4 bytes per param (float32)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = [d.to(device) for d in data]\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy_val = 0\n",
    "    if total > 0:\n",
    "        accuracy_val = 100 * correct / total\n",
    "\n",
    "\n",
    "    return {\n",
    "        'macs': macs,\n",
    "        'size_mb': size_mb,\n",
    "        'accuracy': accuracy_val\n",
    "    }"
   ],
   "id": "bc63e176a8657f07",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prune the model\n",
   "id": "559bdffab16bafab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T10:53:02.458148Z",
     "start_time": "2025-05-08T10:53:02.451625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prune_model(model, example_input, target_macs, strategy, iterative_steps=5):\n",
    "    if isinstance(strategy['importance'], tp.importance.TaylorImportance):\n",
    "        pruning_ratio = 0.1\n",
    "    else:\n",
    "        pruning_ratio = 0.5\n",
    "\n",
    "    # IMPORTANT: Update ignored_layers for ResNet's classifier\n",
    "    ignored_layers_list = [model.linear] if hasattr(model, 'linear') else []\n",
    "    if hasattr(model, 'fc') and model.fc not in ignored_layers_list: # for general models\n",
    "        ignored_layers_list.append(model.fc)\n",
    "\n",
    "\n",
    "    pruner = strategy['pruner'](\n",
    "        model,\n",
    "        example_input,\n",
    "        importance=strategy['importance'],\n",
    "        iterative_steps=iterative_steps,\n",
    "        ch_sparsity=pruning_ratio,\n",
    "        root_module_types=[nn.Conv2d],\n",
    "        ignored_layers=ignored_layers_list, # Use updated list\n",
    "    )\n",
    "\n",
    "    current_macs, base_nparams = calculate_macs(model, example_input)\n",
    "    initial_macs = current_macs # Store initial MACs for logging\n",
    "\n",
    "    for i in range(iterative_steps):\n",
    "            if isinstance(strategy['importance'], tp.importance.TaylorImportance):\n",
    "                # Ensure model is in training mode for grad calculation if needed\n",
    "                is_training = model.training\n",
    "                model.train()\n",
    "                loss = model(example_input).sum() # a dummy loss for TaylorImportance\n",
    "                loss.backward()\n",
    "                if not is_training: model.eval() # Revert to original mode\n",
    "\n",
    "            pruning_occurred_this_step = False\n",
    "            for g in pruner.step(interactive=True):\n",
    "                g.prune()\n",
    "                pruning_occurred_this_step = True\n",
    "\n",
    "            if not pruning_occurred_this_step:\n",
    "                print(f\"  Iter %d/%d, No prunable groups found. Stopping.\" % (i + 1, iterative_steps))\n",
    "                break\n",
    "\n",
    "\n",
    "            macs, nparams = tp.utils.count_ops_and_params(model, example_input)\n",
    "            print(\n",
    "                \"  Iter %d/%d, Params: %.2f M => %.2f M\"\n",
    "                % (i + 1, iterative_steps, base_nparams / 1e6, nparams / 1e6)\n",
    "            )\n",
    "            print(\n",
    "                \"  Iter %d/%d, MACs: %.2f G => %.2f G (Target: %.2f G)\"\n",
    "                % (i + 1, iterative_steps, initial_macs / 1e9, macs / 1e9, target_macs / 1e9)\n",
    "            )\n",
    "            initial_macs = macs # Update for next iteration's \"before\" MACs\n",
    "            base_nparams = nparams\n",
    "\n",
    "            # Optional: Check if target_macs is met, though the original loop structure doesn't use it to break\n",
    "            if macs <= target_macs:\n",
    "                print(f\"  Target MACs ({target_macs/1e9:.2f}G) reached or passed.\")\n",
    "                # break # uncomment if you want to stop early\n",
    "\n",
    "    return model"
   ],
   "id": "b7bd2fe38bb39c2c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train the model",
   "id": "1bbe7172b741a9b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T10:57:07.707562Z",
     "start_time": "2025-05-08T10:57:07.702047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            inputs, labels = [d.to(device) for d in data]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 100 == 0: # Print progress every 100 batches\n",
    "                 print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}: \"\n",
    "                       f\"Loss={loss.item():.4f}\")\n",
    "\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Avg Loss={epoch_loss:.4f}, Accuracy={epoch_acc:.2f}%\")\n",
    "    return model"
   ],
   "id": "d69fc3f1c05ff087",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Main workflow",
   "id": "507a033ad415de65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T10:56:16.094807Z",
     "start_time": "2025-05-08T10:56:16.084072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    CURRENT_MODEL_NAME = \"resnet18\" # Change to \"resnet34\", \"resnet50\" as needed\n",
    "    MODEL_CLASS = ResNet18 # Change to ResNet34, ResNet50\n",
    "    NUM_CLASSES = 10\n",
    "\n",
    "\n",
    "    config = {\n",
    "        'strategies': {\n",
    "            'magnitude': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.MagnitudeImportance(p=2)},\n",
    "            'bn_scale': {'pruner': tp.pruner.BNScalePruner, 'importance': tp.importance.BNScaleImportance()},\n",
    "            'group_norm': {'pruner': tp.pruner.GroupNormPruner, 'importance': tp.importance.GroupMagnitudeImportance(p=1)},\n",
    "            'random': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.RandomImportance()},\n",
    "            'Taylor': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.TaylorImportance()},\n",
    "            'Hessian': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.GroupHessianImportance()},\n",
    "            'lamp': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.LAMPImportance(p=2)},\n",
    "            'geometry': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.FPGMImportance()}\n",
    "        },\n",
    "        'target_macs_sparsity': 0.5, # Target 50% reduction in MACs from initial\n",
    "        'train_epochs': 10,       # Epochs for initial training\n",
    "        'fine_tune_epochs': 20, # Epochs for fine-tuning after pruning\n",
    "        'data_dir': './data', # CIFAR-10 data directory\n",
    "        'output_dir': f'./output_resnet/{CURRENT_MODEL_NAME}/strategies',\n",
    "        'iterative_steps': 5,     # Iterative steps for pruning methods\n",
    "        'learning_rate_initial': 0.001,\n",
    "        'learning_rate_finetune': 0.0005, # Potentially smaller LR for fine-tuning\n",
    "    }\n",
    "    os.makedirs(config['output_dir'], exist_ok=True)\n",
    "\n",
    "\n",
    "    # Initialize model and data\n",
    "    model = MODEL_CLASS(num_classes=NUM_CLASSES).to(device)\n",
    "    train_loader, test_loader = get_data_loaders(config['data_dir']) # Using the defined CIFAR-10 loader\n",
    "    example_input = torch.randn(1, 3, 32, 32).to(device) # CIFAR-10 example input\n",
    "\n",
    "    initial_model_path = os.path.join(config['output_dir'], f\"{CURRENT_MODEL_NAME}_initial.pth\")\n",
    "\n",
    "    if not os.path.exists(initial_model_path):\n",
    "        print(f\"--- Initial Training for {CURRENT_MODEL_NAME} ---\")\n",
    "        model = train_model(\n",
    "            model=model, train_loader=train_loader,\n",
    "            criterion=nn.CrossEntropyLoss().to(device),\n",
    "            optimizer=optim.Adam(model.parameters(), lr=config['learning_rate_initial']),\n",
    "            device=device, num_epochs=config['train_epochs']\n",
    "        )\n",
    "        save_model(model, initial_model_path, example_input)\n",
    "        print(f\"Initial model saved to {initial_model_path}\")\n",
    "    else:\n",
    "        print(f\"Loading existing initial model from {initial_model_path}\")\n",
    "        # model = MODEL_CLASS(num_classes=NUM_CLASSES).to(device) # Create instance first\n",
    "        # model.load_state_dict(torch.load(initial_model_path))\n",
    "        model = load_model(MODEL_CLASS, NUM_CLASSES, initial_model_path, device)\n",
    "\n",
    "\n",
    "    results = {}\n",
    "    print(f\"\\n--- Evaluating Initial {CURRENT_MODEL_NAME} ---\")\n",
    "    initial_metrics = evaluate_model(model, test_loader, example_input, device)\n",
    "    results['initial'] = initial_metrics\n",
    "    print(f\"Initial Metrics: {initial_metrics}\")\n",
    "\n",
    "    initial_macs, _ = calculate_macs(model, example_input)\n",
    "    # target_macs_absolute = initial_macs * config['target_macs_sparsity']\n",
    "    target_macs_for_prune_model = initial_macs * config['target_macs_sparsity'] # For `prune_model`\n",
    "    target_macs_for_gr = initial_macs * 0.3 # Example: Tighter target for GR threshold method (e.g., 70% reduction)\n",
    "    target_size_mb_for_gr = results['initial']['size_mb'] * 0.3 # Example: Target 30% of original size\n",
    "    target_params_for_gem = sum(p.numel() for p in model.parameters()) * 0.3 # Example: Target 30% of original params\n",
    "\n",
    "\n",
    "    # Select which pruning method to use from the main function options:\n",
    "    # 1. prune_model (original simple iterative pruning)\n",
    "    # 2. gr_prune_model_with_threshold\n",
    "    # 3. gem_prune_model_by_threshold\n",
    "    CHOSEN_PRUNING_FUNCTION = \"gr\" # \"simple\", \"gr\", or \"gem\"\n",
    "\n",
    "\n",
    "    for strategy_name, strategy_details in config['strategies'].items():\n",
    "        print(f\"\\n--- Pruning {CURRENT_MODEL_NAME} with Strategy: {strategy_name} ---\")\n",
    "        # model_copy = MODEL_CLASS(num_classes=NUM_CLASSES).to(device)\n",
    "        # model_copy.load_state_dict(torch.load(initial_model_path)) # Load fresh copy\n",
    "        model_copy = load_model(MODEL_CLASS, NUM_CLASSES, initial_model_path, device)\n",
    "\n",
    "\n",
    "        if CHOSEN_PRUNING_FUNCTION == \"simple\":\n",
    "            pruned_model = prune_model(\n",
    "                model=model_copy, example_input=example_input,\n",
    "                target_macs=target_macs_for_prune_model, # Pass the absolute target MACs\n",
    "                strategy=strategy_details,\n",
    "                iterative_steps=config['iterative_steps'],\n",
    "            )\n",
    "        elif CHOSEN_PRUNING_FUNCTION == \"gr\":\n",
    "            pruned_model = gr_prune_model_with_threshold(\n",
    "                model=model_copy, example_input=example_input,\n",
    "                target_macs=target_macs_for_gr, target_size_mb=target_size_mb_for_gr,\n",
    "                strategy=strategy_details, iterative_steps=config['iterative_steps']\n",
    "            )\n",
    "        elif CHOSEN_PRUNING_FUNCTION == \"gem\":\n",
    "             pruned_model = gem_prune_model_by_threshold(\n",
    "                model=model_copy, example_input=example_input,\n",
    "                target_macs=target_macs_for_gr, # Use appropriate target\n",
    "                target_params=target_params_for_gem,\n",
    "                strategy=strategy_details,\n",
    "                max_iterations=20, # Adjust as needed for GEM\n",
    "                step_ch_sparsity=0.2 # Tune this for GEM\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown CHOSEN_PRUNING_FUNCTION: {CHOSEN_PRUNING_FUNCTION}\")\n",
    "\n",
    "\n",
    "        pruned_path = os.path.join(config['output_dir'], f\"{CURRENT_MODEL_NAME}_{strategy_name}_pruned.pth\")\n",
    "        save_model(pruned_model, pruned_path, example_input)\n",
    "        print(f\"Pruned model saved to {pruned_path}\")\n",
    "\n",
    "        print(f\"\\n--- Fine-tuning {CURRENT_MODEL_NAME} after {strategy_name} pruning ---\")\n",
    "        fine_tuned_model = train_model(\n",
    "            model=pruned_model, train_loader=train_loader,\n",
    "            criterion=nn.CrossEntropyLoss().to(device),\n",
    "            optimizer=optim.Adam(pruned_model.parameters(), lr=config['learning_rate_finetune']),\n",
    "            device=device, num_epochs=config['fine_tune_epochs']\n",
    "        )\n",
    "\n",
    "        print(f\"\\n--- Evaluating fine-tuned {CURRENT_MODEL_NAME} ({strategy_name}) ---\")\n",
    "        results[strategy_name] = evaluate_model(\n",
    "            model=fine_tuned_model, test_loader=test_loader,\n",
    "            example_input=example_input, device=device\n",
    "        )\n",
    "        print(f\"Metrics for {strategy_name}: {results[strategy_name]}\")\n",
    "\n",
    "\n",
    "        final_path = os.path.join(config['output_dir'], f\"{CURRENT_MODEL_NAME}_{strategy_name}_final.pth\")\n",
    "        save_model(fine_tuned_model, final_path, example_input)\n",
    "        print(f\"Final fine-tuned model saved to {final_path}\")\n",
    "\n",
    "\n",
    "    compare_results_and_plot(results, output_dir=config['output_dir'])\n",
    "    print(\"ResNet pruning workflow completed successfully!\")"
   ],
   "id": "15165bf722d638e1",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### GR Pruning Model",
   "id": "1398be86380afb3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T10:56:02.873702Z",
     "start_time": "2025-05-08T10:56:02.863987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gr_prune_model_with_threshold(model, example_input, target_macs, target_size_mb, strategy, iterative_steps=5):\n",
    "    current_macs, nparams = tp.utils.count_ops_and_params(model, example_input)\n",
    "    current_size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 ** 2)\n",
    "    print(f\"Initial MACs: {current_macs / 1e9:.3f} G, Size: {current_size_mb:.2f} MB\")\n",
    "\n",
    "    pruning_ratio = 0.5\n",
    "    ignored_layers_list = [model.linear] if hasattr(model, 'linear') else []\n",
    "    if hasattr(model, 'fc') and model.fc not in ignored_layers_list:\n",
    "        ignored_layers_list.append(model.fc)\n",
    "\n",
    "\n",
    "    if isinstance(strategy['importance'], tp.importance.TaylorImportance):\n",
    "        pruner = strategy['pruner'](\n",
    "            model, example_input, importance=strategy['importance'],\n",
    "            iterative_steps=1, ch_sparsity=pruning_ratio / iterative_steps if iterative_steps > 0 else pruning_ratio,\n",
    "            root_module_types=[nn.Conv2d], ignored_layers=ignored_layers_list\n",
    "        )\n",
    "        for i in range(iterative_steps):\n",
    "            is_training = model.training\n",
    "            model.train()\n",
    "            model.zero_grad()\n",
    "            loss = model(example_input).sum()\n",
    "            loss.backward()\n",
    "            if not is_training: model.eval()\n",
    "\n",
    "            pruned_something = False\n",
    "            for group in pruner.step(interactive=True):\n",
    "                group.prune()\n",
    "                pruned_something = True\n",
    "\n",
    "            if not pruned_something and i > 0 : # Don't break on first iter if nothing found\n",
    "                print(f\"Step {i+1}/{iterative_steps}: No more prunable elements found.\")\n",
    "                break\n",
    "\n",
    "\n",
    "            current_macs, _ = tp.utils.count_ops_and_params(model, example_input)\n",
    "            current_size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 ** 2)\n",
    "            print(f\"Step {i+1}/{iterative_steps}: MACs {current_macs / 1e9:.3f} G, Size {current_size_mb:.2f} MB\")\n",
    "            if current_macs <= target_macs and current_size_mb <= target_size_mb:\n",
    "                print(f\"Targets reached at step {i+1}\")\n",
    "                break\n",
    "            model.zero_grad() # important for Taylor if used in loop without re-instantiating pruner\n",
    "    else:\n",
    "        pruner = strategy['pruner'](\n",
    "            model, example_input, importance=strategy['importance'],\n",
    "            iterative_steps=iterative_steps, ch_sparsity=pruning_ratio,\n",
    "            root_module_types=[nn.Conv2d], ignored_layers=ignored_layers_list\n",
    "        )\n",
    "        # pruner.step() # This typically means run all iterative steps internally.\n",
    "        # If you want fine-grained control like Taylor above, loop pruner.step()\n",
    "        for i in range(iterative_steps):\n",
    "            pruned_something = False\n",
    "            for group in pruner.step(interactive=True): # Use interactive for explicit pruning\n",
    "                group.prune()\n",
    "                pruned_something = True\n",
    "\n",
    "            if not pruned_something and i > 0:\n",
    "                 print(f\"Step {i+1}/{iterative_steps}: No more prunable elements found for non-Taylor strategy.\")\n",
    "                 break\n",
    "\n",
    "            current_macs_step, _ = tp.utils.count_ops_and_params(model, example_input)\n",
    "            current_size_mb_step = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 ** 2)\n",
    "            print(f\"Step {i+1}/{iterative_steps} (Non-Taylor): MACs {current_macs_step / 1e9:.3f} G, Size {current_size_mb_step:.2f} MB\")\n",
    "            if current_macs_step <= target_macs and current_size_mb_step <= target_size_mb:\n",
    "                print(f\"Targets reached at step {i+1} for non-Taylor strategy.\")\n",
    "                break\n",
    "\n",
    "\n",
    "    final_macs, _ = tp.utils.count_ops_and_params(model, example_input)\n",
    "    final_size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 ** 2)\n",
    "    print(f\"After pruning: MACs {final_macs / 1e9:.3f} G, Size {final_size_mb:.2f} MB\")\n",
    "\n",
    "    if final_macs <= target_macs and final_size_mb <= target_size_mb:\n",
    "        print(\"Pruning targets achieved.\")\n",
    "    else:\n",
    "        print(\"Warning: Pruning targets not fully achieved.\")\n",
    "    return model"
   ],
   "id": "6b7eb134043b7290",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### gem pruning model",
   "id": "40607f616072024d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def gem_prune_model_by_threshold(model, example_input, target_macs, target_params, strategy, max_iterations=100, step_ch_sparsity=0.1):\n",
    "    device = example_input.device\n",
    "    model.to(device)\n",
    "\n",
    "    print(f\"--- Starting Pruning (Strategy: {strategy['importance'].__class__.__name__}) ---\")\n",
    "    print(f\"Target MACs: {target_macs:,.0f}, Target Params: {target_params:,.0f}\")\n",
    "\n",
    "    ignored_layers_list = [model.linear] if hasattr(model, 'linear') else []\n",
    "    if hasattr(model, 'fc') and model.fc not in ignored_layers_list:\n",
    "        ignored_layers_list.append(model.fc)\n",
    "\n",
    "    pruner = strategy['pruner'](\n",
    "        model, example_input, importance=strategy['importance'],\n",
    "        ch_sparsity=step_ch_sparsity, root_module_types=[nn.Conv2d],\n",
    "        ignored_layers=ignored_layers_list,\n",
    "        # round_to=8, # Optional\n",
    "    )\n",
    "\n",
    "    current_macs, current_params = calculate_macs(model, example_input)\n",
    "    initial_macs, initial_params = current_macs, current_params\n",
    "    print(f\"Initial | MACs: {current_macs:,.0f}, Params: {current_params:,.0f}\")\n",
    "\n",
    "    iteration = 0\n",
    "    # model.eval() # Moved inside loop for Taylor/Hessian\n",
    "\n",
    "    while (current_macs > target_macs or current_params > target_params) and iteration < max_iterations:\n",
    "        iteration += 1\n",
    "        macs_before_step = current_macs\n",
    "        params_before_step = current_params\n",
    "\n",
    "        original_mode_is_train = model.training # Store original mode\n",
    "\n",
    "        if isinstance(strategy['importance'], (tp.importance.TaylorImportance, tp.importance.GroupHessianImportance)):\n",
    "             model.train()\n",
    "             for param in model.parameters():\n",
    "                 param.requires_grad_(True)\n",
    "             loss = model(example_input).mean()\n",
    "             try:\n",
    "                 model.zero_grad() # Zero gradients before backward\n",
    "                 loss.backward()\n",
    "             except Exception as e:\n",
    "                 print(f\"Error during backward pass for importance calc (Iter {iteration}): {e}\")\n",
    "                 if not original_mode_is_train: model.eval() # Restore mode\n",
    "                 break\n",
    "        else: # Ensure eval mode for other strategies\n",
    "            model.eval()\n",
    "\n",
    "\n",
    "        try:\n",
    "            pruning_groups = list(pruner.step(interactive=True))\n",
    "        except Exception as e:\n",
    "             print(f\"Error during pruner.step() (Iter {iteration}): {e}\")\n",
    "             if not original_mode_is_train and isinstance(strategy['importance'], (tp.importance.TaylorImportance, tp.importance.GroupHessianImportance)):\n",
    "                 model.eval() # Restore eval mode\n",
    "             elif original_mode_is_train and not isinstance(strategy['importance'], (tp.importance.TaylorImportance, tp.importance.GroupHessianImportance)):\n",
    "                 model.train() # Restore train mode if it was initially training\n",
    "             break\n",
    "\n",
    "        if not pruning_groups:\n",
    "            print(f\"Iteration {iteration}: Pruner found no more candidates. Stopping.\")\n",
    "            break\n",
    "\n",
    "        for group in pruning_groups:\n",
    "            group.prune()\n",
    "\n",
    "        if isinstance(strategy['importance'], (tp.importance.TaylorImportance, tp.importance.GroupHessianImportance)):\n",
    "            model.zero_grad() # Clean up gradients\n",
    "            if not original_mode_is_train: # If model was originally in eval, set it back\n",
    "                 model.eval()\n",
    "        elif original_mode_is_train: # If it was originally training and not Taylor/Hessian\n",
    "            model.train()\n",
    "\n",
    "\n",
    "        current_macs, current_params = calculate_macs(model, example_input)\n",
    "        print(\n",
    "            f\"Iter {iteration: >3}/{max_iterations} | \"\n",
    "            f\"MACs: {macs_before_step:,.0f} -> {current_macs:,.0f} \"\n",
    "            f\"({(macs_before_step-current_macs)/macs_before_step*100 if macs_before_step > 0 else 0:+.1f}%) | \"\n",
    "            f\"Params: {params_before_step:,.0f} -> {current_params:,.0f} \"\n",
    "            f\"({(params_before_step-current_params)/params_before_step*100 if params_before_step > 0 else 0:+.1f}%)\"\n",
    "        )\n",
    "\n",
    "        if current_macs >= macs_before_step and current_params >= params_before_step and iteration > 1:\n",
    "            print(f\"Iteration {iteration}: No reduction. Stopping.\")\n",
    "            break\n",
    "\n",
    "        if not original_mode_is_train : model.eval() # Ensure eval mode if it was initially\n",
    "\n",
    "    print(f\"--- Finished Pruning (Strategy: {strategy['importance'].__class__.__name__}) ---\")\n",
    "    if iteration >= max_iterations:\n",
    "        print(f\"Warning: Reached maximum pruning iterations ({max_iterations}).\")\n",
    "\n",
    "    final_macs, final_params = calculate_macs(model, example_input)\n",
    "    print(f\"Initial | MACs: {initial_macs:,.0f}, Params: {initial_params:,.0f}\")\n",
    "    print(f\"Final   | MACs: {final_macs:,.0f}, Params: {final_params:,.0f}\")\n",
    "    print(f\"Target  | MACs: {target_macs:,.0f}, Params: {target_params:,.0f}\")\n",
    "\n",
    "    macs_reduction = (initial_macs - final_macs) / initial_macs * 100 if initial_macs > 0 else 0\n",
    "    params_reduction = (initial_params - final_params) / initial_params * 100 if initial_params > 0 else 0\n",
    "    print(f\"Reduction | MACs: {macs_reduction:.2f}%, Params: {params_reduction:.2f}%\")\n",
    "\n",
    "    if final_macs > target_macs or final_params > target_params:\n",
    "         print(\"Warning: Pruning finished, but target threshold(s) were not fully met.\")\n",
    "\n",
    "    model.eval() # Ensure final model state is eval\n",
    "    return model\n"
   ],
   "id": "69e1ec0dac84318e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T11:18:56.689866Z",
     "start_time": "2025-05-08T10:57:20.179737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "abe4e338df2d4e9f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset directory: /home/muis/thesis/github-repo/master-thesis/cnn/resNet/data\n",
      "--- Initial Training for resnet18 ---\n",
      "Epoch 1/10, Batch 0/782: Loss=2.4236\n",
      "Epoch 1/10, Batch 100/782: Loss=1.4629\n",
      "Epoch 1/10, Batch 200/782: Loss=1.2940\n",
      "Epoch 1/10, Batch 300/782: Loss=1.3454\n",
      "Epoch 1/10, Batch 400/782: Loss=1.1538\n",
      "Epoch 1/10, Batch 500/782: Loss=1.0824\n",
      "Epoch 1/10, Batch 600/782: Loss=1.4907\n",
      "Epoch 1/10, Batch 700/782: Loss=1.0082\n",
      "Epoch 1/10: Avg Loss=1.3176, Accuracy=52.05%\n",
      "Epoch 2/10, Batch 0/782: Loss=1.1416\n",
      "Epoch 2/10, Batch 100/782: Loss=1.0295\n",
      "Epoch 2/10, Batch 200/782: Loss=0.9159\n",
      "Epoch 2/10, Batch 300/782: Loss=0.8815\n",
      "Epoch 2/10, Batch 400/782: Loss=0.7155\n",
      "Epoch 2/10, Batch 500/782: Loss=0.6321\n",
      "Epoch 2/10, Batch 600/782: Loss=0.8202\n",
      "Epoch 2/10, Batch 700/782: Loss=0.6910\n",
      "Epoch 2/10: Avg Loss=0.8107, Accuracy=71.51%\n",
      "Epoch 3/10, Batch 0/782: Loss=0.5486\n",
      "Epoch 3/10, Batch 100/782: Loss=0.5575\n",
      "Epoch 3/10, Batch 200/782: Loss=0.6189\n",
      "Epoch 3/10, Batch 300/782: Loss=0.4937\n",
      "Epoch 3/10, Batch 400/782: Loss=0.4430\n",
      "Epoch 3/10, Batch 500/782: Loss=0.6794\n",
      "Epoch 3/10, Batch 600/782: Loss=0.3972\n",
      "Epoch 3/10, Batch 700/782: Loss=0.4831\n",
      "Epoch 3/10: Avg Loss=0.5988, Accuracy=79.35%\n",
      "Epoch 4/10, Batch 0/782: Loss=0.4387\n",
      "Epoch 4/10, Batch 100/782: Loss=0.4759\n",
      "Epoch 4/10, Batch 200/782: Loss=0.2776\n",
      "Epoch 4/10, Batch 300/782: Loss=0.6086\n",
      "Epoch 4/10, Batch 400/782: Loss=0.4468\n",
      "Epoch 4/10, Batch 500/782: Loss=0.3148\n",
      "Epoch 4/10, Batch 600/782: Loss=0.3876\n",
      "Epoch 4/10, Batch 700/782: Loss=0.4263\n",
      "Epoch 4/10: Avg Loss=0.4651, Accuracy=83.82%\n",
      "Epoch 5/10, Batch 0/782: Loss=0.3840\n",
      "Epoch 5/10, Batch 100/782: Loss=0.4190\n",
      "Epoch 5/10, Batch 200/782: Loss=0.4941\n",
      "Epoch 5/10, Batch 300/782: Loss=0.3031\n",
      "Epoch 5/10, Batch 400/782: Loss=0.4439\n",
      "Epoch 5/10, Batch 500/782: Loss=0.4251\n",
      "Epoch 5/10, Batch 600/782: Loss=0.4154\n",
      "Epoch 5/10, Batch 700/782: Loss=0.3170\n",
      "Epoch 5/10: Avg Loss=0.3726, Accuracy=87.13%\n",
      "Epoch 6/10, Batch 0/782: Loss=0.3990\n",
      "Epoch 6/10, Batch 100/782: Loss=0.2167\n",
      "Epoch 6/10, Batch 200/782: Loss=0.3973\n",
      "Epoch 6/10, Batch 300/782: Loss=0.2123\n",
      "Epoch 6/10, Batch 400/782: Loss=0.3718\n",
      "Epoch 6/10, Batch 500/782: Loss=0.3007\n",
      "Epoch 6/10, Batch 600/782: Loss=0.2221\n",
      "Epoch 6/10, Batch 700/782: Loss=0.1589\n",
      "Epoch 6/10: Avg Loss=0.2770, Accuracy=90.33%\n",
      "Epoch 7/10, Batch 0/782: Loss=0.1822\n",
      "Epoch 7/10, Batch 100/782: Loss=0.1360\n",
      "Epoch 7/10, Batch 200/782: Loss=0.1727\n",
      "Epoch 7/10, Batch 300/782: Loss=0.2464\n",
      "Epoch 7/10, Batch 400/782: Loss=0.2158\n",
      "Epoch 7/10, Batch 500/782: Loss=0.1849\n",
      "Epoch 7/10, Batch 600/782: Loss=0.2705\n",
      "Epoch 7/10, Batch 700/782: Loss=0.2664\n",
      "Epoch 7/10: Avg Loss=0.2023, Accuracy=92.78%\n",
      "Epoch 8/10, Batch 0/782: Loss=0.1337\n",
      "Epoch 8/10, Batch 100/782: Loss=0.1324\n",
      "Epoch 8/10, Batch 200/782: Loss=0.1994\n",
      "Epoch 8/10, Batch 300/782: Loss=0.1576\n",
      "Epoch 8/10, Batch 400/782: Loss=0.1416\n",
      "Epoch 8/10, Batch 500/782: Loss=0.1620\n",
      "Epoch 8/10, Batch 600/782: Loss=0.2732\n",
      "Epoch 8/10, Batch 700/782: Loss=0.2057\n",
      "Epoch 8/10: Avg Loss=0.1481, Accuracy=94.78%\n",
      "Epoch 9/10, Batch 0/782: Loss=0.0647\n",
      "Epoch 9/10, Batch 100/782: Loss=0.0447\n",
      "Epoch 9/10, Batch 200/782: Loss=0.0723\n",
      "Epoch 9/10, Batch 300/782: Loss=0.0564\n",
      "Epoch 9/10, Batch 400/782: Loss=0.2084\n",
      "Epoch 9/10, Batch 500/782: Loss=0.1273\n",
      "Epoch 9/10, Batch 600/782: Loss=0.0965\n",
      "Epoch 9/10, Batch 700/782: Loss=0.0838\n",
      "Epoch 9/10: Avg Loss=0.1117, Accuracy=96.06%\n",
      "Epoch 10/10, Batch 0/782: Loss=0.0696\n",
      "Epoch 10/10, Batch 100/782: Loss=0.0541\n",
      "Epoch 10/10, Batch 200/782: Loss=0.0558\n",
      "Epoch 10/10, Batch 300/782: Loss=0.0823\n",
      "Epoch 10/10, Batch 400/782: Loss=0.1280\n",
      "Epoch 10/10, Batch 500/782: Loss=0.0774\n",
      "Epoch 10/10, Batch 600/782: Loss=0.0710\n",
      "Epoch 10/10, Batch 700/782: Loss=0.2213\n",
      "Epoch 10/10: Avg Loss=0.0894, Accuracy=96.84%\n",
      "✅ Model saved as ONNX to ./output_resnet/resnet18/strategies/resnet18_initial.onnx\n",
      "Initial model saved to ./output_resnet/resnet18/strategies/resnet18_initial.pth\n",
      "\n",
      "--- Evaluating Initial resnet18 ---\n",
      "Initial Metrics: {'macs': 556651530.0, 'size_mb': 44.695848, 'accuracy': 84.55}\n",
      "\n",
      "--- Pruning resnet18 with Strategy: magnitude ---\n",
      "Initial MACs: 0.557 G, Size: 42.63 MB\n",
      "Step 1/5 (Non-Taylor): MACs 0.447 G, Size 34.40 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch_pruning/pruner/algorithms/base_pruner.py:87: UserWarning: ch_sparsity is deprecated in v1.3.0. Please use pruning_ratio.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2/5 (Non-Taylor): MACs 0.354 G, Size 27.17 MB\n",
      "Step 3/5 (Non-Taylor): MACs 0.269 G, Size 20.83 MB\n",
      "Step 4/5 (Non-Taylor): MACs 0.198 G, Size 15.29 MB\n",
      "Step 5/5 (Non-Taylor): MACs 0.140 G, Size 10.67 MB\n",
      "Targets reached at step 5 for non-Taylor strategy.\n",
      "After pruning: MACs 0.140 G, Size 10.67 MB\n",
      "Pruning targets achieved.\n",
      "✅ Model saved as ONNX to ./output_resnet/resnet18/strategies/resnet18_magnitude_pruned.onnx\n",
      "Pruned model saved to ./output_resnet/resnet18/strategies/resnet18_magnitude_pruned.pth\n",
      "\n",
      "--- Fine-tuning resnet18 after magnitude pruning ---\n",
      "Epoch 1/20, Batch 0/782: Loss=2.0543\n",
      "Epoch 1/20, Batch 100/782: Loss=0.4324\n",
      "Epoch 1/20, Batch 200/782: Loss=0.4775\n",
      "Epoch 1/20, Batch 300/782: Loss=0.1718\n",
      "Epoch 1/20, Batch 400/782: Loss=0.1591\n",
      "Epoch 1/20, Batch 500/782: Loss=0.1989\n",
      "Epoch 1/20, Batch 600/782: Loss=0.0997\n",
      "Epoch 1/20, Batch 700/782: Loss=0.1946\n",
      "Epoch 1/20: Avg Loss=0.2654, Accuracy=91.14%\n",
      "Epoch 2/20, Batch 0/782: Loss=0.0930\n",
      "Epoch 2/20, Batch 100/782: Loss=0.0716\n",
      "Epoch 2/20, Batch 200/782: Loss=0.0610\n",
      "Epoch 2/20, Batch 300/782: Loss=0.0783\n",
      "Epoch 2/20, Batch 400/782: Loss=0.0907\n",
      "Epoch 2/20, Batch 500/782: Loss=0.0500\n",
      "Epoch 2/20, Batch 600/782: Loss=0.1376\n",
      "Epoch 2/20, Batch 700/782: Loss=0.0368\n",
      "Epoch 2/20: Avg Loss=0.0688, Accuracy=97.90%\n",
      "Epoch 3/20, Batch 0/782: Loss=0.0378\n",
      "Epoch 3/20, Batch 100/782: Loss=0.0155\n",
      "Epoch 3/20, Batch 200/782: Loss=0.0460\n",
      "Epoch 3/20, Batch 300/782: Loss=0.0128\n",
      "Epoch 3/20, Batch 400/782: Loss=0.0708\n",
      "Epoch 3/20, Batch 500/782: Loss=0.0058\n",
      "Epoch 3/20, Batch 600/782: Loss=0.0643\n",
      "Epoch 3/20, Batch 700/782: Loss=0.0509\n",
      "Epoch 3/20: Avg Loss=0.0375, Accuracy=98.88%\n",
      "Epoch 4/20, Batch 0/782: Loss=0.0313\n",
      "Epoch 4/20, Batch 100/782: Loss=0.0330\n",
      "Epoch 4/20, Batch 200/782: Loss=0.0187\n",
      "Epoch 4/20, Batch 300/782: Loss=0.0105\n",
      "Epoch 4/20, Batch 400/782: Loss=0.0202\n",
      "Epoch 4/20, Batch 500/782: Loss=0.0235\n",
      "Epoch 4/20, Batch 600/782: Loss=0.0584\n",
      "Epoch 4/20, Batch 700/782: Loss=0.0612\n",
      "Epoch 4/20: Avg Loss=0.0331, Accuracy=98.91%\n",
      "Epoch 5/20, Batch 0/782: Loss=0.0115\n",
      "Epoch 5/20, Batch 100/782: Loss=0.0057\n",
      "Epoch 5/20, Batch 200/782: Loss=0.0266\n",
      "Epoch 5/20, Batch 300/782: Loss=0.0034\n",
      "Epoch 5/20, Batch 400/782: Loss=0.0124\n",
      "Epoch 5/20, Batch 500/782: Loss=0.0283\n",
      "Epoch 5/20, Batch 600/782: Loss=0.0106\n",
      "Epoch 5/20, Batch 700/782: Loss=0.0065\n",
      "Epoch 5/20: Avg Loss=0.0285, Accuracy=99.07%\n",
      "Epoch 6/20, Batch 0/782: Loss=0.0229\n",
      "Epoch 6/20, Batch 100/782: Loss=0.0092\n",
      "Epoch 6/20, Batch 200/782: Loss=0.0258\n",
      "Epoch 6/20, Batch 300/782: Loss=0.0207\n",
      "Epoch 6/20, Batch 400/782: Loss=0.0139\n",
      "Epoch 6/20, Batch 500/782: Loss=0.0152\n",
      "Epoch 6/20, Batch 600/782: Loss=0.0068\n",
      "Epoch 6/20, Batch 700/782: Loss=0.0268\n",
      "Epoch 6/20: Avg Loss=0.0247, Accuracy=99.16%\n",
      "Epoch 7/20, Batch 0/782: Loss=0.0753\n",
      "Epoch 7/20, Batch 100/782: Loss=0.0090\n",
      "Epoch 7/20, Batch 200/782: Loss=0.0064\n",
      "Epoch 7/20, Batch 300/782: Loss=0.0100\n",
      "Epoch 7/20, Batch 400/782: Loss=0.0069\n",
      "Epoch 7/20, Batch 500/782: Loss=0.0355\n",
      "Epoch 7/20, Batch 600/782: Loss=0.0052\n",
      "Epoch 7/20, Batch 700/782: Loss=0.0422\n",
      "Epoch 7/20: Avg Loss=0.0263, Accuracy=99.04%\n",
      "Epoch 8/20, Batch 0/782: Loss=0.0058\n",
      "Epoch 8/20, Batch 100/782: Loss=0.0343\n",
      "Epoch 8/20, Batch 200/782: Loss=0.0047\n",
      "Epoch 8/20, Batch 300/782: Loss=0.0009\n",
      "Epoch 8/20, Batch 400/782: Loss=0.0893\n",
      "Epoch 8/20, Batch 500/782: Loss=0.0107\n",
      "Epoch 8/20, Batch 600/782: Loss=0.0024\n",
      "Epoch 8/20, Batch 700/782: Loss=0.0117\n",
      "Epoch 8/20: Avg Loss=0.0236, Accuracy=99.21%\n",
      "Epoch 9/20, Batch 0/782: Loss=0.0160\n",
      "Epoch 9/20, Batch 100/782: Loss=0.0022\n",
      "Epoch 9/20, Batch 200/782: Loss=0.0075\n",
      "Epoch 9/20, Batch 300/782: Loss=0.0342\n",
      "Epoch 9/20, Batch 400/782: Loss=0.0028\n",
      "Epoch 9/20, Batch 500/782: Loss=0.0104\n",
      "Epoch 9/20, Batch 600/782: Loss=0.0069\n",
      "Epoch 9/20, Batch 700/782: Loss=0.0124\n",
      "Epoch 9/20: Avg Loss=0.0192, Accuracy=99.38%\n",
      "Epoch 10/20, Batch 0/782: Loss=0.0579\n",
      "Epoch 10/20, Batch 100/782: Loss=0.0083\n",
      "Epoch 10/20, Batch 200/782: Loss=0.0436\n",
      "Epoch 10/20, Batch 300/782: Loss=0.0504\n",
      "Epoch 10/20, Batch 400/782: Loss=0.0006\n",
      "Epoch 10/20, Batch 500/782: Loss=0.0018\n",
      "Epoch 10/20, Batch 600/782: Loss=0.0107\n",
      "Epoch 10/20, Batch 700/782: Loss=0.0229\n",
      "Epoch 10/20: Avg Loss=0.0258, Accuracy=99.11%\n",
      "Epoch 11/20, Batch 0/782: Loss=0.0184\n",
      "Epoch 11/20, Batch 100/782: Loss=0.0205\n",
      "Epoch 11/20, Batch 200/782: Loss=0.0031\n",
      "Epoch 11/20, Batch 300/782: Loss=0.0038\n",
      "Epoch 11/20, Batch 400/782: Loss=0.0170\n",
      "Epoch 11/20, Batch 500/782: Loss=0.0190\n",
      "Epoch 11/20, Batch 600/782: Loss=0.0117\n",
      "Epoch 11/20, Batch 700/782: Loss=0.0323\n",
      "Epoch 11/20: Avg Loss=0.0186, Accuracy=99.35%\n",
      "Epoch 12/20, Batch 0/782: Loss=0.0268\n",
      "Epoch 12/20, Batch 100/782: Loss=0.0007\n",
      "Epoch 12/20, Batch 200/782: Loss=0.0501\n",
      "Epoch 12/20, Batch 300/782: Loss=0.0007\n",
      "Epoch 12/20, Batch 400/782: Loss=0.0173\n",
      "Epoch 12/20, Batch 500/782: Loss=0.0006\n",
      "Epoch 12/20, Batch 600/782: Loss=0.0124\n",
      "Epoch 12/20, Batch 700/782: Loss=0.0395\n",
      "Epoch 12/20: Avg Loss=0.0168, Accuracy=99.42%\n",
      "Epoch 13/20, Batch 0/782: Loss=0.0012\n",
      "Epoch 13/20, Batch 100/782: Loss=0.0065\n",
      "Epoch 13/20, Batch 200/782: Loss=0.0210\n",
      "Epoch 13/20, Batch 300/782: Loss=0.0029\n",
      "Epoch 13/20, Batch 400/782: Loss=0.0121\n",
      "Epoch 13/20, Batch 500/782: Loss=0.0111\n",
      "Epoch 13/20, Batch 600/782: Loss=0.0302\n",
      "Epoch 13/20, Batch 700/782: Loss=0.0221\n",
      "Epoch 13/20: Avg Loss=0.0191, Accuracy=99.35%\n",
      "Epoch 14/20, Batch 0/782: Loss=0.0334\n",
      "Epoch 14/20, Batch 100/782: Loss=0.0112\n",
      "Epoch 14/20, Batch 200/782: Loss=0.0007\n",
      "Epoch 14/20, Batch 300/782: Loss=0.0015\n",
      "Epoch 14/20, Batch 400/782: Loss=0.0019\n",
      "Epoch 14/20, Batch 500/782: Loss=0.0023\n",
      "Epoch 14/20, Batch 600/782: Loss=0.0067\n",
      "Epoch 14/20, Batch 700/782: Loss=0.0005\n",
      "Epoch 14/20: Avg Loss=0.0170, Accuracy=99.40%\n",
      "Epoch 15/20, Batch 0/782: Loss=0.0012\n",
      "Epoch 15/20, Batch 100/782: Loss=0.0185\n",
      "Epoch 15/20, Batch 200/782: Loss=0.0130\n",
      "Epoch 15/20, Batch 300/782: Loss=0.0012\n",
      "Epoch 15/20, Batch 400/782: Loss=0.0139\n",
      "Epoch 15/20, Batch 500/782: Loss=0.1284\n",
      "Epoch 15/20, Batch 600/782: Loss=0.0284\n",
      "Epoch 15/20, Batch 700/782: Loss=0.0125\n",
      "Epoch 15/20: Avg Loss=0.0203, Accuracy=99.30%\n",
      "Epoch 16/20, Batch 0/782: Loss=0.0051\n",
      "Epoch 16/20, Batch 100/782: Loss=0.0009\n",
      "Epoch 16/20, Batch 200/782: Loss=0.0011\n",
      "Epoch 16/20, Batch 300/782: Loss=0.0502\n",
      "Epoch 16/20, Batch 400/782: Loss=0.0020\n",
      "Epoch 16/20, Batch 500/782: Loss=0.0062\n",
      "Epoch 16/20, Batch 600/782: Loss=0.0400\n",
      "Epoch 16/20, Batch 700/782: Loss=0.0015\n",
      "Epoch 16/20: Avg Loss=0.0112, Accuracy=99.65%\n",
      "Epoch 17/20, Batch 0/782: Loss=0.0006\n",
      "Epoch 17/20, Batch 100/782: Loss=0.0013\n",
      "Epoch 17/20, Batch 200/782: Loss=0.0205\n",
      "Epoch 17/20, Batch 300/782: Loss=0.0068\n",
      "Epoch 17/20, Batch 400/782: Loss=0.0005\n",
      "Epoch 17/20, Batch 500/782: Loss=0.0189\n",
      "Epoch 17/20, Batch 600/782: Loss=0.0698\n",
      "Epoch 17/20, Batch 700/782: Loss=0.0090\n",
      "Epoch 17/20: Avg Loss=0.0159, Accuracy=99.47%\n",
      "Epoch 18/20, Batch 0/782: Loss=0.0481\n",
      "Epoch 18/20, Batch 100/782: Loss=0.0064\n",
      "Epoch 18/20, Batch 200/782: Loss=0.0077\n",
      "Epoch 18/20, Batch 300/782: Loss=0.0024\n",
      "Epoch 18/20, Batch 400/782: Loss=0.0393\n",
      "Epoch 18/20, Batch 500/782: Loss=0.0544\n",
      "Epoch 18/20, Batch 600/782: Loss=0.0078\n",
      "Epoch 18/20, Batch 700/782: Loss=0.0149\n",
      "Epoch 18/20: Avg Loss=0.0191, Accuracy=99.33%\n",
      "Epoch 19/20, Batch 0/782: Loss=0.0018\n",
      "Epoch 19/20, Batch 100/782: Loss=0.0087\n",
      "Epoch 19/20, Batch 200/782: Loss=0.0018\n",
      "Epoch 19/20, Batch 300/782: Loss=0.0971\n",
      "Epoch 19/20, Batch 400/782: Loss=0.0016\n",
      "Epoch 19/20, Batch 500/782: Loss=0.0008\n",
      "Epoch 19/20, Batch 600/782: Loss=0.0052\n",
      "Epoch 19/20, Batch 700/782: Loss=0.0318\n",
      "Epoch 19/20: Avg Loss=0.0161, Accuracy=99.46%\n",
      "Epoch 20/20, Batch 0/782: Loss=0.0027\n",
      "Epoch 20/20, Batch 100/782: Loss=0.0489\n",
      "Epoch 20/20, Batch 200/782: Loss=0.0012\n",
      "Epoch 20/20, Batch 300/782: Loss=0.0007\n",
      "Epoch 20/20, Batch 400/782: Loss=0.0004\n",
      "Epoch 20/20, Batch 500/782: Loss=0.0057\n",
      "Epoch 20/20, Batch 600/782: Loss=0.0033\n",
      "Epoch 20/20, Batch 700/782: Loss=0.0019\n",
      "Epoch 20/20: Avg Loss=0.0100, Accuracy=99.69%\n",
      "\n",
      "--- Evaluating fine-tuned resnet18 (magnitude) ---\n",
      "Metrics for magnitude: {'macs': 139913738.0, 'size_mb': 11.19044, 'accuracy': 83.86}\n",
      "✅ Model saved as ONNX to ./output_resnet/resnet18/strategies/resnet18_magnitude_final.onnx\n",
      "Final fine-tuned model saved to ./output_resnet/resnet18/strategies/resnet18_magnitude_final.pth\n",
      "\n",
      "--- Pruning resnet18 with Strategy: bn_scale ---\n",
      "Initial MACs: 0.557 G, Size: 42.63 MB\n",
      "Step 1/5 (Non-Taylor): MACs 0.447 G, Size 34.40 MB\n",
      "Step 2/5 (Non-Taylor): MACs 0.354 G, Size 27.17 MB\n",
      "Step 3/5 (Non-Taylor): MACs 0.269 G, Size 20.83 MB\n",
      "Step 4/5 (Non-Taylor): MACs 0.198 G, Size 15.29 MB\n",
      "Step 5/5 (Non-Taylor): MACs 0.140 G, Size 10.67 MB\n",
      "Targets reached at step 5 for non-Taylor strategy.\n",
      "After pruning: MACs 0.140 G, Size 10.67 MB\n",
      "Pruning targets achieved.\n",
      "✅ Model saved as ONNX to ./output_resnet/resnet18/strategies/resnet18_bn_scale_pruned.onnx\n",
      "Pruned model saved to ./output_resnet/resnet18/strategies/resnet18_bn_scale_pruned.pth\n",
      "\n",
      "--- Fine-tuning resnet18 after bn_scale pruning ---\n",
      "Epoch 1/20, Batch 0/782: Loss=1.8613\n",
      "Epoch 1/20, Batch 100/782: Loss=0.3466\n",
      "Epoch 1/20, Batch 200/782: Loss=0.1989\n",
      "Epoch 1/20, Batch 300/782: Loss=0.2557\n",
      "Epoch 1/20, Batch 400/782: Loss=0.2327\n",
      "Epoch 1/20, Batch 500/782: Loss=0.2426\n",
      "Epoch 1/20, Batch 600/782: Loss=0.2400\n",
      "Epoch 1/20, Batch 700/782: Loss=0.0830\n",
      "Epoch 1/20: Avg Loss=0.2382, Accuracy=91.94%\n",
      "Epoch 2/20, Batch 0/782: Loss=0.0732\n",
      "Epoch 2/20, Batch 100/782: Loss=0.0765\n",
      "Epoch 2/20, Batch 200/782: Loss=0.0359\n",
      "Epoch 2/20, Batch 300/782: Loss=0.1407\n",
      "Epoch 2/20, Batch 400/782: Loss=0.0623\n",
      "Epoch 2/20, Batch 500/782: Loss=0.1539\n",
      "Epoch 2/20, Batch 600/782: Loss=0.0406\n",
      "Epoch 2/20, Batch 700/782: Loss=0.0530\n",
      "Epoch 2/20: Avg Loss=0.0636, Accuracy=98.10%\n",
      "Epoch 3/20, Batch 0/782: Loss=0.0221\n",
      "Epoch 3/20, Batch 100/782: Loss=0.0145\n",
      "Epoch 3/20, Batch 200/782: Loss=0.0285\n",
      "Epoch 3/20, Batch 300/782: Loss=0.0197\n",
      "Epoch 3/20, Batch 400/782: Loss=0.0080\n",
      "Epoch 3/20, Batch 500/782: Loss=0.0360\n",
      "Epoch 3/20, Batch 600/782: Loss=0.0437\n",
      "Epoch 3/20, Batch 700/782: Loss=0.0226\n",
      "Epoch 3/20: Avg Loss=0.0338, Accuracy=98.97%\n",
      "Epoch 4/20, Batch 0/782: Loss=0.0194\n",
      "Epoch 4/20, Batch 100/782: Loss=0.0086\n",
      "Epoch 4/20, Batch 200/782: Loss=0.0112\n",
      "Epoch 4/20, Batch 300/782: Loss=0.0092\n",
      "Epoch 4/20, Batch 400/782: Loss=0.0219\n",
      "Epoch 4/20, Batch 500/782: Loss=0.0987\n",
      "Epoch 4/20, Batch 600/782: Loss=0.0959\n",
      "Epoch 4/20, Batch 700/782: Loss=0.0226\n",
      "Epoch 4/20: Avg Loss=0.0357, Accuracy=98.83%\n",
      "Epoch 5/20, Batch 0/782: Loss=0.0662\n",
      "Epoch 5/20, Batch 100/782: Loss=0.0318\n",
      "Epoch 5/20, Batch 200/782: Loss=0.0081\n",
      "Epoch 5/20, Batch 300/782: Loss=0.0067\n",
      "Epoch 5/20, Batch 400/782: Loss=0.0163\n",
      "Epoch 5/20, Batch 500/782: Loss=0.0104\n",
      "Epoch 5/20, Batch 600/782: Loss=0.0086\n",
      "Epoch 5/20, Batch 700/782: Loss=0.0772\n",
      "Epoch 5/20: Avg Loss=0.0250, Accuracy=99.20%\n",
      "Epoch 6/20, Batch 0/782: Loss=0.0236\n",
      "Epoch 6/20, Batch 100/782: Loss=0.0307\n",
      "Epoch 6/20, Batch 200/782: Loss=0.0028\n",
      "Epoch 6/20, Batch 300/782: Loss=0.0813\n",
      "Epoch 6/20, Batch 400/782: Loss=0.0074\n",
      "Epoch 6/20, Batch 500/782: Loss=0.0060\n",
      "Epoch 6/20, Batch 600/782: Loss=0.0068\n",
      "Epoch 6/20, Batch 700/782: Loss=0.0512\n",
      "Epoch 6/20: Avg Loss=0.0297, Accuracy=99.00%\n",
      "Epoch 7/20, Batch 0/782: Loss=0.0067\n",
      "Epoch 7/20, Batch 100/782: Loss=0.0023\n",
      "Epoch 7/20, Batch 200/782: Loss=0.0049\n",
      "Epoch 7/20, Batch 300/782: Loss=0.0509\n",
      "Epoch 7/20, Batch 400/782: Loss=0.0246\n",
      "Epoch 7/20, Batch 500/782: Loss=0.1814\n",
      "Epoch 7/20, Batch 600/782: Loss=0.0506\n",
      "Epoch 7/20, Batch 700/782: Loss=0.0196\n",
      "Epoch 7/20: Avg Loss=0.0251, Accuracy=99.16%\n",
      "Epoch 8/20, Batch 0/782: Loss=0.0101\n",
      "Epoch 8/20, Batch 100/782: Loss=0.0474\n",
      "Epoch 8/20, Batch 200/782: Loss=0.0264\n",
      "Epoch 8/20, Batch 300/782: Loss=0.0265\n",
      "Epoch 8/20, Batch 400/782: Loss=0.0115\n",
      "Epoch 8/20, Batch 500/782: Loss=0.0187\n",
      "Epoch 8/20, Batch 600/782: Loss=0.0018\n",
      "Epoch 8/20, Batch 700/782: Loss=0.0739\n",
      "Epoch 8/20: Avg Loss=0.0251, Accuracy=99.14%\n",
      "Epoch 9/20, Batch 0/782: Loss=0.0007\n",
      "Epoch 9/20, Batch 100/782: Loss=0.0598\n",
      "Epoch 9/20, Batch 200/782: Loss=0.0090\n",
      "Epoch 9/20, Batch 300/782: Loss=0.0048\n",
      "Epoch 9/20, Batch 400/782: Loss=0.0059\n",
      "Epoch 9/20, Batch 500/782: Loss=0.0230\n",
      "Epoch 9/20, Batch 600/782: Loss=0.0127\n",
      "Epoch 9/20, Batch 700/782: Loss=0.0671\n",
      "Epoch 9/20: Avg Loss=0.0191, Accuracy=99.36%\n",
      "Epoch 10/20, Batch 0/782: Loss=0.0042\n",
      "Epoch 10/20, Batch 100/782: Loss=0.0112\n",
      "Epoch 10/20, Batch 200/782: Loss=0.0046\n",
      "Epoch 10/20, Batch 300/782: Loss=0.1136\n",
      "Epoch 10/20, Batch 400/782: Loss=0.0426\n",
      "Epoch 10/20, Batch 500/782: Loss=0.0105\n",
      "Epoch 10/20, Batch 600/782: Loss=0.0144\n",
      "Epoch 10/20, Batch 700/782: Loss=0.0757\n",
      "Epoch 10/20: Avg Loss=0.0254, Accuracy=99.10%\n",
      "Epoch 11/20, Batch 0/782: Loss=0.0059\n",
      "Epoch 11/20, Batch 100/782: Loss=0.0184\n",
      "Epoch 11/20, Batch 200/782: Loss=0.0213\n",
      "Epoch 11/20, Batch 300/782: Loss=0.0017\n",
      "Epoch 11/20, Batch 400/782: Loss=0.0061\n",
      "Epoch 11/20, Batch 500/782: Loss=0.0156\n",
      "Epoch 11/20, Batch 600/782: Loss=0.1506\n",
      "Epoch 11/20, Batch 700/782: Loss=0.0269\n",
      "Epoch 11/20: Avg Loss=0.0186, Accuracy=99.34%\n",
      "Epoch 12/20, Batch 0/782: Loss=0.0109\n",
      "Epoch 12/20, Batch 100/782: Loss=0.0057\n",
      "Epoch 12/20, Batch 200/782: Loss=0.0047\n",
      "Epoch 12/20, Batch 300/782: Loss=0.0042\n",
      "Epoch 12/20, Batch 400/782: Loss=0.0040\n",
      "Epoch 12/20, Batch 500/782: Loss=0.0090\n",
      "Epoch 12/20, Batch 600/782: Loss=0.0082\n",
      "Epoch 12/20, Batch 700/782: Loss=0.0225\n",
      "Epoch 12/20: Avg Loss=0.0166, Accuracy=99.44%\n",
      "Epoch 13/20, Batch 0/782: Loss=0.0211\n",
      "Epoch 13/20, Batch 100/782: Loss=0.0343\n",
      "Epoch 13/20, Batch 200/782: Loss=0.0322\n",
      "Epoch 13/20, Batch 300/782: Loss=0.0168\n",
      "Epoch 13/20, Batch 400/782: Loss=0.0670\n",
      "Epoch 13/20, Batch 500/782: Loss=0.0254\n",
      "Epoch 13/20, Batch 600/782: Loss=0.0860\n",
      "Epoch 13/20, Batch 700/782: Loss=0.0646\n",
      "Epoch 13/20: Avg Loss=0.0187, Accuracy=99.40%\n",
      "Epoch 14/20, Batch 0/782: Loss=0.0094\n",
      "Epoch 14/20, Batch 100/782: Loss=0.0128\n",
      "Epoch 14/20, Batch 200/782: Loss=0.0331\n",
      "Epoch 14/20, Batch 300/782: Loss=0.0046\n",
      "Epoch 14/20, Batch 400/782: Loss=0.0011\n",
      "Epoch 14/20, Batch 500/782: Loss=0.0008\n",
      "Epoch 14/20, Batch 600/782: Loss=0.0668\n",
      "Epoch 14/20, Batch 700/782: Loss=0.0005\n",
      "Epoch 14/20: Avg Loss=0.0202, Accuracy=99.29%\n",
      "Epoch 15/20, Batch 0/782: Loss=0.0162\n",
      "Epoch 15/20, Batch 100/782: Loss=0.0014\n",
      "Epoch 15/20, Batch 200/782: Loss=0.0074\n",
      "Epoch 15/20, Batch 300/782: Loss=0.0012\n",
      "Epoch 15/20, Batch 400/782: Loss=0.0075\n",
      "Epoch 15/20, Batch 500/782: Loss=0.0501\n",
      "Epoch 15/20, Batch 600/782: Loss=0.0096\n",
      "Epoch 15/20, Batch 700/782: Loss=0.0061\n",
      "Epoch 15/20: Avg Loss=0.0126, Accuracy=99.57%\n",
      "Epoch 16/20, Batch 0/782: Loss=0.1859\n",
      "Epoch 16/20, Batch 100/782: Loss=0.0502\n",
      "Epoch 16/20, Batch 200/782: Loss=0.0111\n",
      "Epoch 16/20, Batch 300/782: Loss=0.0044\n",
      "Epoch 16/20, Batch 400/782: Loss=0.0021\n",
      "Epoch 16/20, Batch 500/782: Loss=0.0029\n",
      "Epoch 16/20, Batch 600/782: Loss=0.0301\n",
      "Epoch 16/20, Batch 700/782: Loss=0.0047\n",
      "Epoch 16/20: Avg Loss=0.0175, Accuracy=99.40%\n",
      "Epoch 17/20, Batch 0/782: Loss=0.0004\n",
      "Epoch 17/20, Batch 100/782: Loss=0.0031\n",
      "Epoch 17/20, Batch 200/782: Loss=0.0004\n",
      "Epoch 17/20, Batch 300/782: Loss=0.0473\n",
      "Epoch 17/20, Batch 400/782: Loss=0.0112\n",
      "Epoch 17/20, Batch 500/782: Loss=0.0088\n",
      "Epoch 17/20, Batch 600/782: Loss=0.0008\n",
      "Epoch 17/20, Batch 700/782: Loss=0.0128\n",
      "Epoch 17/20: Avg Loss=0.0153, Accuracy=99.44%\n",
      "Epoch 18/20, Batch 0/782: Loss=0.0018\n",
      "Epoch 18/20, Batch 100/782: Loss=0.0365\n",
      "Epoch 18/20, Batch 200/782: Loss=0.0775\n",
      "Epoch 18/20, Batch 300/782: Loss=0.0374\n",
      "Epoch 18/20, Batch 400/782: Loss=0.0031\n",
      "Epoch 18/20, Batch 500/782: Loss=0.0443\n",
      "Epoch 18/20, Batch 600/782: Loss=0.0011\n",
      "Epoch 18/20, Batch 700/782: Loss=0.0190\n",
      "Epoch 18/20: Avg Loss=0.0173, Accuracy=99.42%\n",
      "Epoch 19/20, Batch 0/782: Loss=0.0020\n",
      "Epoch 19/20, Batch 100/782: Loss=0.0079\n",
      "Epoch 19/20, Batch 200/782: Loss=0.0094\n",
      "Epoch 19/20, Batch 300/782: Loss=0.0013\n",
      "Epoch 19/20, Batch 400/782: Loss=0.0049\n",
      "Epoch 19/20, Batch 500/782: Loss=0.0003\n",
      "Epoch 19/20, Batch 600/782: Loss=0.0343\n",
      "Epoch 19/20, Batch 700/782: Loss=0.0015\n",
      "Epoch 19/20: Avg Loss=0.0127, Accuracy=99.54%\n",
      "Epoch 20/20, Batch 0/782: Loss=0.0508\n",
      "Epoch 20/20, Batch 100/782: Loss=0.0008\n",
      "Epoch 20/20, Batch 200/782: Loss=0.0003\n",
      "Epoch 20/20, Batch 300/782: Loss=0.0004\n",
      "Epoch 20/20, Batch 400/782: Loss=0.0099\n",
      "Epoch 20/20, Batch 500/782: Loss=0.0009\n",
      "Epoch 20/20, Batch 600/782: Loss=0.0015\n",
      "Epoch 20/20, Batch 700/782: Loss=0.0229\n",
      "Epoch 20/20: Avg Loss=0.0141, Accuracy=99.50%\n",
      "\n",
      "--- Evaluating fine-tuned resnet18 (bn_scale) ---\n",
      "Metrics for bn_scale: {'macs': 139913738.0, 'size_mb': 11.19044, 'accuracy': 84.27}\n",
      "✅ Model saved as ONNX to ./output_resnet/resnet18/strategies/resnet18_bn_scale_final.onnx\n",
      "Final fine-tuned model saved to ./output_resnet/resnet18/strategies/resnet18_bn_scale_final.pth\n",
      "\n",
      "--- Pruning resnet18 with Strategy: group_norm ---\n",
      "Initial MACs: 0.557 G, Size: 42.63 MB\n",
      "Step 1/5 (Non-Taylor): MACs 0.447 G, Size 34.40 MB\n",
      "Step 2/5 (Non-Taylor): MACs 0.354 G, Size 27.17 MB\n",
      "Step 3/5 (Non-Taylor): MACs 0.269 G, Size 20.83 MB\n",
      "Step 4/5 (Non-Taylor): MACs 0.198 G, Size 15.29 MB\n",
      "Step 5/5 (Non-Taylor): MACs 0.140 G, Size 10.67 MB\n",
      "Targets reached at step 5 for non-Taylor strategy.\n",
      "After pruning: MACs 0.140 G, Size 10.67 MB\n",
      "Pruning targets achieved.\n",
      "✅ Model saved as ONNX to ./output_resnet/resnet18/strategies/resnet18_group_norm_pruned.onnx\n",
      "Pruned model saved to ./output_resnet/resnet18/strategies/resnet18_group_norm_pruned.pth\n",
      "\n",
      "--- Fine-tuning resnet18 after group_norm pruning ---\n",
      "Epoch 1/20, Batch 0/782: Loss=2.8002\n",
      "Epoch 1/20, Batch 100/782: Loss=0.2530\n",
      "Epoch 1/20, Batch 200/782: Loss=0.3501\n",
      "Epoch 1/20, Batch 300/782: Loss=0.2570\n",
      "Epoch 1/20, Batch 400/782: Loss=0.1421\n",
      "Epoch 1/20, Batch 500/782: Loss=0.1131\n",
      "Epoch 1/20, Batch 600/782: Loss=0.1104\n",
      "Epoch 1/20, Batch 700/782: Loss=0.2832\n",
      "Epoch 1/20: Avg Loss=0.2733, Accuracy=90.99%\n",
      "Epoch 2/20, Batch 0/782: Loss=0.0275\n",
      "Epoch 2/20, Batch 100/782: Loss=0.0583\n",
      "Epoch 2/20, Batch 200/782: Loss=0.0393\n",
      "Epoch 2/20, Batch 300/782: Loss=0.0939\n",
      "Epoch 2/20, Batch 400/782: Loss=0.0492\n",
      "Epoch 2/20, Batch 500/782: Loss=0.0566\n",
      "Epoch 2/20, Batch 600/782: Loss=0.1366\n",
      "Epoch 2/20, Batch 700/782: Loss=0.0211\n",
      "Epoch 2/20: Avg Loss=0.0688, Accuracy=97.97%\n",
      "Epoch 3/20, Batch 0/782: Loss=0.0572\n",
      "Epoch 3/20, Batch 100/782: Loss=0.0342\n",
      "Epoch 3/20, Batch 200/782: Loss=0.0201\n",
      "Epoch 3/20, Batch 300/782: Loss=0.0125\n",
      "Epoch 3/20, Batch 400/782: Loss=0.0414\n",
      "Epoch 3/20, Batch 500/782: Loss=0.0105\n",
      "Epoch 3/20, Batch 600/782: Loss=0.0838\n",
      "Epoch 3/20, Batch 700/782: Loss=0.0376\n",
      "Epoch 3/20: Avg Loss=0.0357, Accuracy=98.94%\n",
      "Epoch 4/20, Batch 0/782: Loss=0.0405\n",
      "Epoch 4/20, Batch 100/782: Loss=0.0058\n",
      "Epoch 4/20, Batch 200/782: Loss=0.0918\n",
      "Epoch 4/20, Batch 300/782: Loss=0.0033\n",
      "Epoch 4/20, Batch 400/782: Loss=0.0136\n",
      "Epoch 4/20, Batch 500/782: Loss=0.0266\n",
      "Epoch 4/20, Batch 600/782: Loss=0.0263\n",
      "Epoch 4/20, Batch 700/782: Loss=0.0376\n",
      "Epoch 4/20: Avg Loss=0.0299, Accuracy=99.06%\n",
      "Epoch 5/20, Batch 0/782: Loss=0.0295\n",
      "Epoch 5/20, Batch 100/782: Loss=0.0040\n",
      "Epoch 5/20, Batch 200/782: Loss=0.0135\n",
      "Epoch 5/20, Batch 300/782: Loss=0.0074\n",
      "Epoch 5/20, Batch 400/782: Loss=0.0145\n",
      "Epoch 5/20, Batch 500/782: Loss=0.0866\n",
      "Epoch 5/20, Batch 600/782: Loss=0.0461\n",
      "Epoch 5/20, Batch 700/782: Loss=0.0277\n",
      "Epoch 5/20: Avg Loss=0.0304, Accuracy=98.95%\n",
      "Epoch 6/20, Batch 0/782: Loss=0.0129\n",
      "Epoch 6/20, Batch 100/782: Loss=0.0479\n",
      "Epoch 6/20, Batch 200/782: Loss=0.0030\n",
      "Epoch 6/20, Batch 300/782: Loss=0.0018\n",
      "Epoch 6/20, Batch 400/782: Loss=0.1036\n",
      "Epoch 6/20, Batch 500/782: Loss=0.0081\n",
      "Epoch 6/20, Batch 600/782: Loss=0.0073\n",
      "Epoch 6/20, Batch 700/782: Loss=0.0074\n",
      "Epoch 6/20: Avg Loss=0.0292, Accuracy=99.02%\n",
      "Epoch 7/20, Batch 0/782: Loss=0.0298\n",
      "Epoch 7/20, Batch 100/782: Loss=0.0085\n",
      "Epoch 7/20, Batch 200/782: Loss=0.0376\n",
      "Epoch 7/20, Batch 300/782: Loss=0.0203\n",
      "Epoch 7/20, Batch 400/782: Loss=0.0127\n",
      "Epoch 7/20, Batch 500/782: Loss=0.0193\n",
      "Epoch 7/20, Batch 600/782: Loss=0.0064\n",
      "Epoch 7/20, Batch 700/782: Loss=0.0191\n",
      "Epoch 7/20: Avg Loss=0.0211, Accuracy=99.29%\n",
      "Epoch 8/20, Batch 0/782: Loss=0.0039\n",
      "Epoch 8/20, Batch 100/782: Loss=0.0050\n",
      "Epoch 8/20, Batch 200/782: Loss=0.0037\n",
      "Epoch 8/20, Batch 300/782: Loss=0.0192\n",
      "Epoch 8/20, Batch 400/782: Loss=0.0140\n",
      "Epoch 8/20, Batch 500/782: Loss=0.0135\n",
      "Epoch 8/20, Batch 600/782: Loss=0.1061\n",
      "Epoch 8/20, Batch 700/782: Loss=0.0424\n",
      "Epoch 8/20: Avg Loss=0.0233, Accuracy=99.22%\n",
      "Epoch 9/20, Batch 0/782: Loss=0.0064\n",
      "Epoch 9/20, Batch 100/782: Loss=0.0064\n",
      "Epoch 9/20, Batch 200/782: Loss=0.0524\n",
      "Epoch 9/20, Batch 300/782: Loss=0.0186\n",
      "Epoch 9/20, Batch 400/782: Loss=0.0009\n",
      "Epoch 9/20, Batch 500/782: Loss=0.1005\n",
      "Epoch 9/20, Batch 600/782: Loss=0.0015\n",
      "Epoch 9/20, Batch 700/782: Loss=0.0060\n",
      "Epoch 9/20: Avg Loss=0.0235, Accuracy=99.22%\n",
      "Epoch 10/20, Batch 0/782: Loss=0.0147\n",
      "Epoch 10/20, Batch 100/782: Loss=0.0012\n",
      "Epoch 10/20, Batch 200/782: Loss=0.0023\n",
      "Epoch 10/20, Batch 300/782: Loss=0.0252\n",
      "Epoch 10/20, Batch 400/782: Loss=0.0122\n",
      "Epoch 10/20, Batch 500/782: Loss=0.0278\n",
      "Epoch 10/20, Batch 600/782: Loss=0.0031\n",
      "Epoch 10/20, Batch 700/782: Loss=0.0045\n",
      "Epoch 10/20: Avg Loss=0.0191, Accuracy=99.36%\n",
      "Epoch 11/20, Batch 0/782: Loss=0.0016\n",
      "Epoch 11/20, Batch 100/782: Loss=0.0129\n",
      "Epoch 11/20, Batch 200/782: Loss=0.0046\n",
      "Epoch 11/20, Batch 300/782: Loss=0.0128\n",
      "Epoch 11/20, Batch 400/782: Loss=0.0597\n",
      "Epoch 11/20, Batch 500/782: Loss=0.0149\n",
      "Epoch 11/20, Batch 600/782: Loss=0.0156\n",
      "Epoch 11/20, Batch 700/782: Loss=0.0035\n",
      "Epoch 11/20: Avg Loss=0.0246, Accuracy=99.16%\n",
      "Epoch 12/20, Batch 0/782: Loss=0.0036\n",
      "Epoch 12/20, Batch 100/782: Loss=0.0006\n",
      "Epoch 12/20, Batch 200/782: Loss=0.0141\n",
      "Epoch 12/20, Batch 300/782: Loss=0.0041\n",
      "Epoch 12/20, Batch 400/782: Loss=0.0179\n",
      "Epoch 12/20, Batch 500/782: Loss=0.0013\n",
      "Epoch 12/20, Batch 600/782: Loss=0.0015\n",
      "Epoch 12/20, Batch 700/782: Loss=0.0005\n",
      "Epoch 12/20: Avg Loss=0.0116, Accuracy=99.63%\n",
      "Epoch 13/20, Batch 0/782: Loss=0.0005\n",
      "Epoch 13/20, Batch 100/782: Loss=0.0814\n",
      "Epoch 13/20, Batch 200/782: Loss=0.0241\n",
      "Epoch 13/20, Batch 300/782: Loss=0.0794\n",
      "Epoch 13/20, Batch 400/782: Loss=0.0191\n",
      "Epoch 13/20, Batch 500/782: Loss=0.0143\n",
      "Epoch 13/20, Batch 600/782: Loss=0.0025\n",
      "Epoch 13/20, Batch 700/782: Loss=0.0229\n",
      "Epoch 13/20: Avg Loss=0.0228, Accuracy=99.25%\n",
      "Epoch 14/20, Batch 0/782: Loss=0.0051\n",
      "Epoch 14/20, Batch 100/782: Loss=0.0738\n",
      "Epoch 14/20, Batch 200/782: Loss=0.0025\n",
      "Epoch 14/20, Batch 300/782: Loss=0.0394\n",
      "Epoch 14/20, Batch 400/782: Loss=0.0111\n",
      "Epoch 14/20, Batch 500/782: Loss=0.0462\n",
      "Epoch 14/20, Batch 600/782: Loss=0.0187\n",
      "Epoch 14/20, Batch 700/782: Loss=0.0016\n",
      "Epoch 14/20: Avg Loss=0.0232, Accuracy=99.24%\n",
      "Epoch 15/20, Batch 0/782: Loss=0.0132\n",
      "Epoch 15/20, Batch 100/782: Loss=0.0008\n",
      "Epoch 15/20, Batch 200/782: Loss=0.0045\n",
      "Epoch 15/20, Batch 300/782: Loss=0.0003\n",
      "Epoch 15/20, Batch 400/782: Loss=0.0197\n",
      "Epoch 15/20, Batch 500/782: Loss=0.0004\n",
      "Epoch 15/20, Batch 600/782: Loss=0.0629\n",
      "Epoch 15/20, Batch 700/782: Loss=0.0019\n",
      "Epoch 15/20: Avg Loss=0.0117, Accuracy=99.63%\n",
      "Epoch 16/20, Batch 0/782: Loss=0.0201\n",
      "Epoch 16/20, Batch 100/782: Loss=0.0048\n",
      "Epoch 16/20, Batch 200/782: Loss=0.0005\n",
      "Epoch 16/20, Batch 300/782: Loss=0.0004\n",
      "Epoch 16/20, Batch 400/782: Loss=0.0041\n",
      "Epoch 16/20, Batch 500/782: Loss=0.1115\n",
      "Epoch 16/20, Batch 600/782: Loss=0.0027\n",
      "Epoch 16/20, Batch 700/782: Loss=0.0039\n",
      "Epoch 16/20: Avg Loss=0.0156, Accuracy=99.49%\n",
      "Epoch 17/20, Batch 0/782: Loss=0.0262\n",
      "Epoch 17/20, Batch 100/782: Loss=0.0133\n",
      "Epoch 17/20, Batch 200/782: Loss=0.0009\n",
      "Epoch 17/20, Batch 300/782: Loss=0.0149\n",
      "Epoch 17/20, Batch 400/782: Loss=0.0048\n",
      "Epoch 17/20, Batch 500/782: Loss=0.0171\n",
      "Epoch 17/20, Batch 600/782: Loss=0.0063\n",
      "Epoch 17/20, Batch 700/782: Loss=0.0133\n",
      "Epoch 17/20: Avg Loss=0.0161, Accuracy=99.45%\n",
      "Epoch 18/20, Batch 0/782: Loss=0.0054\n",
      "Epoch 18/20, Batch 100/782: Loss=0.0801\n",
      "Epoch 18/20, Batch 200/782: Loss=0.0004\n",
      "Epoch 18/20, Batch 300/782: Loss=0.0013\n",
      "Epoch 18/20, Batch 400/782: Loss=0.0286\n",
      "Epoch 18/20, Batch 500/782: Loss=0.0127\n",
      "Epoch 18/20, Batch 600/782: Loss=0.0242\n",
      "Epoch 18/20, Batch 700/782: Loss=0.0023\n",
      "Epoch 18/20: Avg Loss=0.0158, Accuracy=99.48%\n",
      "Epoch 19/20, Batch 0/782: Loss=0.0142\n",
      "Epoch 19/20, Batch 100/782: Loss=0.0002\n",
      "Epoch 19/20, Batch 200/782: Loss=0.0003\n",
      "Epoch 19/20, Batch 300/782: Loss=0.0284\n",
      "Epoch 19/20, Batch 400/782: Loss=0.0071\n",
      "Epoch 19/20, Batch 500/782: Loss=0.0049\n",
      "Epoch 19/20, Batch 600/782: Loss=0.0202\n",
      "Epoch 19/20, Batch 700/782: Loss=0.0430\n",
      "Epoch 19/20: Avg Loss=0.0172, Accuracy=99.42%\n",
      "Epoch 20/20, Batch 0/782: Loss=0.0101\n",
      "Epoch 20/20, Batch 100/782: Loss=0.0295\n",
      "Epoch 20/20, Batch 200/782: Loss=0.0138\n",
      "Epoch 20/20, Batch 300/782: Loss=0.0008\n",
      "Epoch 20/20, Batch 400/782: Loss=0.0071\n",
      "Epoch 20/20, Batch 500/782: Loss=0.0092\n",
      "Epoch 20/20, Batch 600/782: Loss=0.0040\n",
      "Epoch 20/20, Batch 700/782: Loss=0.0158\n",
      "Epoch 20/20: Avg Loss=0.0172, Accuracy=99.40%\n",
      "\n",
      "--- Evaluating fine-tuned resnet18 (group_norm) ---\n",
      "Metrics for group_norm: {'macs': 139913738.0, 'size_mb': 11.19044, 'accuracy': 83.83}\n",
      "✅ Model saved as ONNX to ./output_resnet/resnet18/strategies/resnet18_group_norm_final.onnx\n",
      "Final fine-tuned model saved to ./output_resnet/resnet18/strategies/resnet18_group_norm_final.pth\n",
      "\n",
      "--- Pruning resnet18 with Strategy: random ---\n",
      "Initial MACs: 0.557 G, Size: 42.63 MB\n",
      "Step 1/5 (Non-Taylor): MACs 0.447 G, Size 34.40 MB\n",
      "Step 2/5 (Non-Taylor): MACs 0.354 G, Size 27.17 MB\n",
      "Step 3/5 (Non-Taylor): MACs 0.269 G, Size 20.83 MB\n",
      "Step 4/5 (Non-Taylor): MACs 0.198 G, Size 15.29 MB\n",
      "Step 5/5 (Non-Taylor): MACs 0.140 G, Size 10.67 MB\n",
      "Targets reached at step 5 for non-Taylor strategy.\n",
      "After pruning: MACs 0.140 G, Size 10.67 MB\n",
      "Pruning targets achieved.\n",
      "✅ Model saved as ONNX to ./output_resnet/resnet18/strategies/resnet18_random_pruned.onnx\n",
      "Pruned model saved to ./output_resnet/resnet18/strategies/resnet18_random_pruned.pth\n",
      "\n",
      "--- Fine-tuning resnet18 after random pruning ---\n",
      "Epoch 1/20, Batch 0/782: Loss=1.1909\n",
      "Epoch 1/20, Batch 100/782: Loss=0.2957\n",
      "Epoch 1/20, Batch 200/782: Loss=0.3707\n",
      "Epoch 1/20, Batch 300/782: Loss=0.2274\n",
      "Epoch 1/20, Batch 400/782: Loss=0.1988\n",
      "Epoch 1/20, Batch 500/782: Loss=0.1199\n",
      "Epoch 1/20, Batch 600/782: Loss=0.1169\n",
      "Epoch 1/20, Batch 700/782: Loss=0.2685\n",
      "Epoch 1/20: Avg Loss=0.2402, Accuracy=91.72%\n",
      "Epoch 2/20, Batch 0/782: Loss=0.0622\n",
      "Epoch 2/20, Batch 100/782: Loss=0.0367\n",
      "Epoch 2/20, Batch 200/782: Loss=0.1245\n",
      "Epoch 2/20, Batch 300/782: Loss=0.0681\n",
      "Epoch 2/20, Batch 400/782: Loss=0.0249\n",
      "Epoch 2/20, Batch 500/782: Loss=0.1422\n",
      "Epoch 2/20, Batch 600/782: Loss=0.0621\n",
      "Epoch 2/20, Batch 700/782: Loss=0.0252\n",
      "Epoch 2/20: Avg Loss=0.0695, Accuracy=97.78%\n",
      "Epoch 3/20, Batch 0/782: Loss=0.0527\n",
      "Epoch 3/20, Batch 100/782: Loss=0.0525\n",
      "Epoch 3/20, Batch 200/782: Loss=0.0283\n",
      "Epoch 3/20, Batch 300/782: Loss=0.0274\n",
      "Epoch 3/20, Batch 400/782: Loss=0.0415\n",
      "Epoch 3/20, Batch 500/782: Loss=0.0477\n",
      "Epoch 3/20, Batch 600/782: Loss=0.0180\n",
      "Epoch 3/20, Batch 700/782: Loss=0.0730\n",
      "Epoch 3/20: Avg Loss=0.0407, Accuracy=98.72%\n",
      "Epoch 4/20, Batch 0/782: Loss=0.0717\n",
      "Epoch 4/20, Batch 100/782: Loss=0.0647\n",
      "Epoch 4/20, Batch 200/782: Loss=0.0134\n",
      "Epoch 4/20, Batch 300/782: Loss=0.0477\n",
      "Epoch 4/20, Batch 400/782: Loss=0.0197\n",
      "Epoch 4/20, Batch 500/782: Loss=0.0345\n",
      "Epoch 4/20, Batch 600/782: Loss=0.0182\n",
      "Epoch 4/20, Batch 700/782: Loss=0.0668\n",
      "Epoch 4/20: Avg Loss=0.0388, Accuracy=98.70%\n",
      "Epoch 5/20, Batch 0/782: Loss=0.0773\n",
      "Epoch 5/20, Batch 100/782: Loss=0.0251\n",
      "Epoch 5/20, Batch 200/782: Loss=0.0785\n",
      "Epoch 5/20, Batch 300/782: Loss=0.0234\n",
      "Epoch 5/20, Batch 400/782: Loss=0.0032\n",
      "Epoch 5/20, Batch 500/782: Loss=0.2390\n",
      "Epoch 5/20, Batch 600/782: Loss=0.0681\n",
      "Epoch 5/20, Batch 700/782: Loss=0.0208\n",
      "Epoch 5/20: Avg Loss=0.0332, Accuracy=98.89%\n",
      "Epoch 6/20, Batch 0/782: Loss=0.0145\n",
      "Epoch 6/20, Batch 100/782: Loss=0.0050\n",
      "Epoch 6/20, Batch 200/782: Loss=0.1205\n",
      "Epoch 6/20, Batch 300/782: Loss=0.0043\n",
      "Epoch 6/20, Batch 400/782: Loss=0.0106\n",
      "Epoch 6/20, Batch 500/782: Loss=0.1225\n",
      "Epoch 6/20, Batch 600/782: Loss=0.1362\n",
      "Epoch 6/20, Batch 700/782: Loss=0.0582\n",
      "Epoch 6/20: Avg Loss=0.0255, Accuracy=99.15%\n",
      "Epoch 7/20, Batch 0/782: Loss=0.0117\n",
      "Epoch 7/20, Batch 100/782: Loss=0.0072\n",
      "Epoch 7/20, Batch 200/782: Loss=0.0387\n",
      "Epoch 7/20, Batch 300/782: Loss=0.0118\n",
      "Epoch 7/20, Batch 400/782: Loss=0.0471\n",
      "Epoch 7/20, Batch 500/782: Loss=0.0168\n",
      "Epoch 7/20, Batch 600/782: Loss=0.1335\n",
      "Epoch 7/20, Batch 700/782: Loss=0.0371\n",
      "Epoch 7/20: Avg Loss=0.0282, Accuracy=99.01%\n",
      "Epoch 8/20, Batch 0/782: Loss=0.0175\n",
      "Epoch 8/20, Batch 100/782: Loss=0.0193\n",
      "Epoch 8/20, Batch 200/782: Loss=0.0093\n",
      "Epoch 8/20, Batch 300/782: Loss=0.0026\n",
      "Epoch 8/20, Batch 400/782: Loss=0.0686\n",
      "Epoch 8/20, Batch 500/782: Loss=0.0389\n",
      "Epoch 8/20, Batch 600/782: Loss=0.0455\n",
      "Epoch 8/20, Batch 700/782: Loss=0.0241\n",
      "Epoch 8/20: Avg Loss=0.0277, Accuracy=99.01%\n",
      "Epoch 9/20, Batch 0/782: Loss=0.0125\n",
      "Epoch 9/20, Batch 100/782: Loss=0.0151\n",
      "Epoch 9/20, Batch 200/782: Loss=0.0023\n",
      "Epoch 9/20, Batch 300/782: Loss=0.0024\n",
      "Epoch 9/20, Batch 400/782: Loss=0.0207\n",
      "Epoch 9/20, Batch 500/782: Loss=0.0039\n",
      "Epoch 9/20, Batch 600/782: Loss=0.0078\n",
      "Epoch 9/20, Batch 700/782: Loss=0.0417\n",
      "Epoch 9/20: Avg Loss=0.0194, Accuracy=99.35%\n",
      "Epoch 10/20, Batch 0/782: Loss=0.0115\n",
      "Epoch 10/20, Batch 100/782: Loss=0.0031\n",
      "Epoch 10/20, Batch 200/782: Loss=0.0075\n",
      "Epoch 10/20, Batch 300/782: Loss=0.0734\n",
      "Epoch 10/20, Batch 400/782: Loss=0.0031\n",
      "Epoch 10/20, Batch 500/782: Loss=0.0298\n",
      "Epoch 10/20, Batch 600/782: Loss=0.0266\n",
      "Epoch 10/20, Batch 700/782: Loss=0.0587\n",
      "Epoch 10/20: Avg Loss=0.0245, Accuracy=99.20%\n",
      "Epoch 11/20, Batch 0/782: Loss=0.0060\n",
      "Epoch 11/20, Batch 100/782: Loss=0.0068\n",
      "Epoch 11/20, Batch 200/782: Loss=0.0023\n",
      "Epoch 11/20, Batch 300/782: Loss=0.0239\n",
      "Epoch 11/20, Batch 400/782: Loss=0.0112\n",
      "Epoch 11/20, Batch 500/782: Loss=0.0449\n",
      "Epoch 11/20, Batch 600/782: Loss=0.0057\n",
      "Epoch 11/20, Batch 700/782: Loss=0.0252\n",
      "Epoch 11/20: Avg Loss=0.0247, Accuracy=99.16%\n",
      "Epoch 12/20, Batch 0/782: Loss=0.0185\n",
      "Epoch 12/20, Batch 100/782: Loss=0.0019\n",
      "Epoch 12/20, Batch 200/782: Loss=0.0110\n",
      "Epoch 12/20, Batch 300/782: Loss=0.0160\n",
      "Epoch 12/20, Batch 400/782: Loss=0.0035\n",
      "Epoch 12/20, Batch 500/782: Loss=0.0012\n",
      "Epoch 12/20, Batch 600/782: Loss=0.0021\n",
      "Epoch 12/20, Batch 700/782: Loss=0.0029\n",
      "Epoch 12/20: Avg Loss=0.0180, Accuracy=99.39%\n",
      "Epoch 13/20, Batch 0/782: Loss=0.0176\n",
      "Epoch 13/20, Batch 100/782: Loss=0.0008\n",
      "Epoch 13/20, Batch 200/782: Loss=0.0024\n",
      "Epoch 13/20, Batch 300/782: Loss=0.0050\n",
      "Epoch 13/20, Batch 400/782: Loss=0.0009\n",
      "Epoch 13/20, Batch 500/782: Loss=0.0628\n",
      "Epoch 13/20, Batch 600/782: Loss=0.0303\n",
      "Epoch 13/20, Batch 700/782: Loss=0.0391\n",
      "Epoch 13/20: Avg Loss=0.0233, Accuracy=99.18%\n",
      "Epoch 14/20, Batch 0/782: Loss=0.0038\n",
      "Epoch 14/20, Batch 100/782: Loss=0.0051\n",
      "Epoch 14/20, Batch 200/782: Loss=0.0011\n",
      "Epoch 14/20, Batch 300/782: Loss=0.0007\n",
      "Epoch 14/20, Batch 400/782: Loss=0.0059\n",
      "Epoch 14/20, Batch 500/782: Loss=0.0504\n",
      "Epoch 14/20, Batch 600/782: Loss=0.0108\n",
      "Epoch 14/20, Batch 700/782: Loss=0.0096\n",
      "Epoch 14/20: Avg Loss=0.0186, Accuracy=99.34%\n",
      "Epoch 15/20, Batch 0/782: Loss=0.0146\n",
      "Epoch 15/20, Batch 100/782: Loss=0.0178\n",
      "Epoch 15/20, Batch 200/782: Loss=0.0211\n",
      "Epoch 15/20, Batch 300/782: Loss=0.0096\n",
      "Epoch 15/20, Batch 400/782: Loss=0.0722\n",
      "Epoch 15/20, Batch 500/782: Loss=0.0006\n",
      "Epoch 15/20, Batch 600/782: Loss=0.0150\n",
      "Epoch 15/20, Batch 700/782: Loss=0.0392\n",
      "Epoch 15/20: Avg Loss=0.0174, Accuracy=99.38%\n",
      "Epoch 16/20, Batch 0/782: Loss=0.0007\n",
      "Epoch 16/20, Batch 100/782: Loss=0.0034\n",
      "Epoch 16/20, Batch 200/782: Loss=0.0119\n",
      "Epoch 16/20, Batch 300/782: Loss=0.0165\n",
      "Epoch 16/20, Batch 400/782: Loss=0.0009\n",
      "Epoch 16/20, Batch 500/782: Loss=0.0800\n",
      "Epoch 16/20, Batch 600/782: Loss=0.0331\n",
      "Epoch 16/20, Batch 700/782: Loss=0.0066\n",
      "Epoch 16/20: Avg Loss=0.0181, Accuracy=99.39%\n",
      "Epoch 17/20, Batch 0/782: Loss=0.0184\n",
      "Epoch 17/20, Batch 100/782: Loss=0.0025\n",
      "Epoch 17/20, Batch 200/782: Loss=0.0007\n",
      "Epoch 17/20, Batch 300/782: Loss=0.0403\n",
      "Epoch 17/20, Batch 400/782: Loss=0.0079\n",
      "Epoch 17/20, Batch 500/782: Loss=0.0138\n",
      "Epoch 17/20, Batch 600/782: Loss=0.0258\n",
      "Epoch 17/20, Batch 700/782: Loss=0.0137\n",
      "Epoch 17/20: Avg Loss=0.0159, Accuracy=99.46%\n",
      "Epoch 18/20, Batch 0/782: Loss=0.0186\n",
      "Epoch 18/20, Batch 100/782: Loss=0.0856\n",
      "Epoch 18/20, Batch 200/782: Loss=0.0004\n",
      "Epoch 18/20, Batch 300/782: Loss=0.0056\n",
      "Epoch 18/20, Batch 400/782: Loss=0.0543\n",
      "Epoch 18/20, Batch 500/782: Loss=0.0084\n",
      "Epoch 18/20, Batch 600/782: Loss=0.0114\n",
      "Epoch 18/20, Batch 700/782: Loss=0.0010\n",
      "Epoch 18/20: Avg Loss=0.0210, Accuracy=99.25%\n",
      "Epoch 19/20, Batch 0/782: Loss=0.0015\n",
      "Epoch 19/20, Batch 100/782: Loss=0.0016\n",
      "Epoch 19/20, Batch 200/782: Loss=0.0106\n",
      "Epoch 19/20, Batch 300/782: Loss=0.0041\n",
      "Epoch 19/20, Batch 400/782: Loss=0.0097\n",
      "Epoch 19/20, Batch 500/782: Loss=0.0144\n",
      "Epoch 19/20, Batch 600/782: Loss=0.0168\n",
      "Epoch 19/20, Batch 700/782: Loss=0.0503\n",
      "Epoch 19/20: Avg Loss=0.0171, Accuracy=99.41%\n",
      "Epoch 20/20, Batch 0/782: Loss=0.0004\n",
      "Epoch 20/20, Batch 100/782: Loss=0.0188\n",
      "Epoch 20/20, Batch 200/782: Loss=0.0189\n",
      "Epoch 20/20, Batch 300/782: Loss=0.0077\n",
      "Epoch 20/20, Batch 400/782: Loss=0.0027\n",
      "Epoch 20/20, Batch 500/782: Loss=0.0098\n",
      "Epoch 20/20, Batch 600/782: Loss=0.0010\n",
      "Epoch 20/20, Batch 700/782: Loss=0.0169\n",
      "Epoch 20/20: Avg Loss=0.0192, Accuracy=99.37%\n",
      "\n",
      "--- Evaluating fine-tuned resnet18 (random) ---\n",
      "Metrics for random: {'macs': 139913738.0, 'size_mb': 11.19044, 'accuracy': 84.42}\n",
      "✅ Model saved as ONNX to ./output_resnet/resnet18/strategies/resnet18_random_final.onnx\n",
      "Final fine-tuned model saved to ./output_resnet/resnet18/strategies/resnet18_random_final.pth\n",
      "\n",
      "--- Pruning resnet18 with Strategy: Taylor ---\n",
      "Initial MACs: 0.557 G, Size: 42.63 MB\n",
      "Step 1/5: MACs 0.447 G, Size 34.40 MB\n",
      "Step 2/5: No more prunable elements found.\n",
      "After pruning: MACs 0.447 G, Size 34.40 MB\n",
      "Warning: Pruning targets not fully achieved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch_pruning/pruner/algorithms/base_pruner.py:407: UserWarning: Pruning exceed the maximum iterative steps, no pruning will be performed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved as ONNX to ./output_resnet/resnet18/strategies/resnet18_Taylor_pruned.onnx\n",
      "Pruned model saved to ./output_resnet/resnet18/strategies/resnet18_Taylor_pruned.pth\n",
      "\n",
      "--- Fine-tuning resnet18 after Taylor pruning ---\n",
      "Epoch 1/20, Batch 0/782: Loss=0.0542\n",
      "Epoch 1/20, Batch 100/782: Loss=0.0340\n",
      "Epoch 1/20, Batch 200/782: Loss=0.0478\n",
      "Epoch 1/20, Batch 300/782: Loss=0.0250\n",
      "Epoch 1/20, Batch 400/782: Loss=0.0634\n",
      "Epoch 1/20, Batch 500/782: Loss=0.0072\n",
      "Epoch 1/20, Batch 600/782: Loss=0.0255\n",
      "Epoch 1/20, Batch 700/782: Loss=0.0442\n",
      "Epoch 1/20: Avg Loss=0.0426, Accuracy=98.55%\n",
      "Epoch 2/20, Batch 0/782: Loss=0.0318\n",
      "Epoch 2/20, Batch 100/782: Loss=0.0122\n",
      "Epoch 2/20, Batch 200/782: Loss=0.0130\n",
      "Epoch 2/20, Batch 300/782: Loss=0.0107\n",
      "Epoch 2/20, Batch 400/782: Loss=0.0149\n",
      "Epoch 2/20, Batch 500/782: Loss=0.0125\n",
      "Epoch 2/20, Batch 600/782: Loss=0.0124\n",
      "Epoch 2/20, Batch 700/782: Loss=0.0012\n",
      "Epoch 2/20: Avg Loss=0.0194, Accuracy=99.37%\n",
      "Epoch 3/20, Batch 0/782: Loss=0.0004\n",
      "Epoch 3/20, Batch 100/782: Loss=0.0134\n",
      "Epoch 3/20, Batch 200/782: Loss=0.0138\n",
      "Epoch 3/20, Batch 300/782: Loss=0.0159\n",
      "Epoch 3/20, Batch 400/782: Loss=0.0068\n",
      "Epoch 3/20, Batch 500/782: Loss=0.0164\n",
      "Epoch 3/20, Batch 600/782: Loss=0.0114\n",
      "Epoch 3/20, Batch 700/782: Loss=0.0791\n",
      "Epoch 3/20: Avg Loss=0.0238, Accuracy=99.21%\n",
      "Epoch 4/20, Batch 0/782: Loss=0.0073\n",
      "Epoch 4/20, Batch 100/782: Loss=0.0403\n",
      "Epoch 4/20, Batch 200/782: Loss=0.0022\n",
      "Epoch 4/20, Batch 300/782: Loss=0.0099\n",
      "Epoch 4/20, Batch 400/782: Loss=0.0014\n",
      "Epoch 4/20, Batch 500/782: Loss=0.0352\n",
      "Epoch 4/20, Batch 600/782: Loss=0.0171\n",
      "Epoch 4/20, Batch 700/782: Loss=0.0295\n",
      "Epoch 4/20: Avg Loss=0.0212, Accuracy=99.29%\n",
      "Epoch 5/20, Batch 0/782: Loss=0.0116\n",
      "Epoch 5/20, Batch 100/782: Loss=0.0070\n",
      "Epoch 5/20, Batch 200/782: Loss=0.0246\n",
      "Epoch 5/20, Batch 300/782: Loss=0.0523\n",
      "Epoch 5/20, Batch 400/782: Loss=0.0020\n",
      "Epoch 5/20, Batch 500/782: Loss=0.0016\n",
      "Epoch 5/20, Batch 600/782: Loss=0.0409\n",
      "Epoch 5/20, Batch 700/782: Loss=0.0297\n",
      "Epoch 5/20: Avg Loss=0.0219, Accuracy=99.23%\n",
      "Epoch 6/20, Batch 0/782: Loss=0.0107\n",
      "Epoch 6/20, Batch 100/782: Loss=0.0010\n",
      "Epoch 6/20, Batch 200/782: Loss=0.0048\n",
      "Epoch 6/20, Batch 300/782: Loss=0.0044\n",
      "Epoch 6/20, Batch 400/782: Loss=0.0012\n",
      "Epoch 6/20, Batch 500/782: Loss=0.0027\n",
      "Epoch 6/20, Batch 600/782: Loss=0.0134\n",
      "Epoch 6/20, Batch 700/782: Loss=0.0044\n",
      "Epoch 6/20: Avg Loss=0.0131, Accuracy=99.58%\n",
      "Epoch 7/20, Batch 0/782: Loss=0.0294\n",
      "Epoch 7/20, Batch 100/782: Loss=0.0020\n",
      "Epoch 7/20, Batch 200/782: Loss=0.0881\n",
      "Epoch 7/20, Batch 300/782: Loss=0.0507\n",
      "Epoch 7/20, Batch 400/782: Loss=0.0020\n",
      "Epoch 7/20, Batch 500/782: Loss=0.0075\n",
      "Epoch 7/20, Batch 600/782: Loss=0.0354\n",
      "Epoch 7/20, Batch 700/782: Loss=0.0206\n",
      "Epoch 7/20: Avg Loss=0.0169, Accuracy=99.40%\n",
      "Epoch 8/20, Batch 0/782: Loss=0.0248\n",
      "Epoch 8/20, Batch 100/782: Loss=0.0224\n",
      "Epoch 8/20, Batch 200/782: Loss=0.0060\n",
      "Epoch 8/20, Batch 300/782: Loss=0.0003\n",
      "Epoch 8/20, Batch 400/782: Loss=0.0039\n",
      "Epoch 8/20, Batch 500/782: Loss=0.0027\n",
      "Epoch 8/20, Batch 600/782: Loss=0.0207\n",
      "Epoch 8/20, Batch 700/782: Loss=0.0036\n",
      "Epoch 8/20: Avg Loss=0.0168, Accuracy=99.42%\n",
      "Epoch 9/20, Batch 0/782: Loss=0.0015\n",
      "Epoch 9/20, Batch 100/782: Loss=0.0741\n",
      "Epoch 9/20, Batch 200/782: Loss=0.0227\n",
      "Epoch 9/20, Batch 300/782: Loss=0.0028\n",
      "Epoch 9/20, Batch 400/782: Loss=0.0022\n",
      "Epoch 9/20, Batch 500/782: Loss=0.0003\n",
      "Epoch 9/20, Batch 600/782: Loss=0.0293\n",
      "Epoch 9/20, Batch 700/782: Loss=0.0090\n",
      "Epoch 9/20: Avg Loss=0.0179, Accuracy=99.38%\n",
      "Epoch 10/20, Batch 0/782: Loss=0.0034\n",
      "Epoch 10/20, Batch 100/782: Loss=0.0068\n",
      "Epoch 10/20, Batch 200/782: Loss=0.0007\n",
      "Epoch 10/20, Batch 300/782: Loss=0.0226\n",
      "Epoch 10/20, Batch 400/782: Loss=0.0043\n",
      "Epoch 10/20, Batch 500/782: Loss=0.0042\n",
      "Epoch 10/20, Batch 600/782: Loss=0.0029\n",
      "Epoch 10/20, Batch 700/782: Loss=0.0269\n",
      "Epoch 10/20: Avg Loss=0.0141, Accuracy=99.53%\n",
      "Epoch 11/20, Batch 0/782: Loss=0.0213\n",
      "Epoch 11/20, Batch 100/782: Loss=0.0297\n",
      "Epoch 11/20, Batch 200/782: Loss=0.0013\n",
      "Epoch 11/20, Batch 300/782: Loss=0.0149\n",
      "Epoch 11/20, Batch 400/782: Loss=0.0308\n",
      "Epoch 11/20, Batch 500/782: Loss=0.0030\n",
      "Epoch 11/20, Batch 600/782: Loss=0.0065\n",
      "Epoch 11/20, Batch 700/782: Loss=0.0025\n",
      "Epoch 11/20: Avg Loss=0.0175, Accuracy=99.39%\n",
      "Epoch 12/20, Batch 0/782: Loss=0.0041\n",
      "Epoch 12/20, Batch 100/782: Loss=0.0059\n",
      "Epoch 12/20, Batch 200/782: Loss=0.0037\n",
      "Epoch 12/20, Batch 300/782: Loss=0.0040\n",
      "Epoch 12/20, Batch 400/782: Loss=0.0028\n",
      "Epoch 12/20, Batch 500/782: Loss=0.0011\n",
      "Epoch 12/20, Batch 600/782: Loss=0.0028\n",
      "Epoch 12/20, Batch 700/782: Loss=0.0144\n",
      "Epoch 12/20: Avg Loss=0.0118, Accuracy=99.64%\n",
      "Epoch 13/20, Batch 0/782: Loss=0.0020\n",
      "Epoch 13/20, Batch 100/782: Loss=0.0004\n",
      "Epoch 13/20, Batch 200/782: Loss=0.0098\n",
      "Epoch 13/20, Batch 300/782: Loss=0.0176\n",
      "Epoch 13/20, Batch 400/782: Loss=0.0006\n",
      "Epoch 13/20, Batch 500/782: Loss=0.0042\n",
      "Epoch 13/20, Batch 600/782: Loss=0.0089\n",
      "Epoch 13/20, Batch 700/782: Loss=0.0036\n",
      "Epoch 13/20: Avg Loss=0.0146, Accuracy=99.48%\n",
      "Epoch 14/20, Batch 0/782: Loss=0.0222\n",
      "Epoch 14/20, Batch 100/782: Loss=0.0083\n",
      "Epoch 14/20, Batch 200/782: Loss=0.0019\n",
      "Epoch 14/20, Batch 300/782: Loss=0.0000\n",
      "Epoch 14/20, Batch 400/782: Loss=0.0006\n",
      "Epoch 14/20, Batch 500/782: Loss=0.0884\n",
      "Epoch 14/20, Batch 600/782: Loss=0.0004\n",
      "Epoch 14/20, Batch 700/782: Loss=0.0015\n",
      "Epoch 14/20: Avg Loss=0.0118, Accuracy=99.61%\n",
      "Epoch 15/20, Batch 0/782: Loss=0.0035\n",
      "Epoch 15/20, Batch 100/782: Loss=0.0027\n",
      "Epoch 15/20, Batch 200/782: Loss=0.0177\n",
      "Epoch 15/20, Batch 300/782: Loss=0.0003\n",
      "Epoch 15/20, Batch 400/782: Loss=0.0007\n",
      "Epoch 15/20, Batch 500/782: Loss=0.0077\n",
      "Epoch 15/20, Batch 600/782: Loss=0.0040\n",
      "Epoch 15/20, Batch 700/782: Loss=0.0074\n",
      "Epoch 15/20: Avg Loss=0.0117, Accuracy=99.62%\n",
      "Epoch 16/20, Batch 0/782: Loss=0.0245\n",
      "Epoch 16/20, Batch 100/782: Loss=0.0296\n",
      "Epoch 16/20, Batch 200/782: Loss=0.0001\n",
      "Epoch 16/20, Batch 300/782: Loss=0.0203\n",
      "Epoch 16/20, Batch 400/782: Loss=0.0002\n",
      "Epoch 16/20, Batch 500/782: Loss=0.0001\n",
      "Epoch 16/20, Batch 600/782: Loss=0.0037\n",
      "Epoch 16/20, Batch 700/782: Loss=0.0024\n",
      "Epoch 16/20: Avg Loss=0.0137, Accuracy=99.57%\n",
      "Epoch 17/20, Batch 0/782: Loss=0.0026\n",
      "Epoch 17/20, Batch 100/782: Loss=0.0235\n",
      "Epoch 17/20, Batch 200/782: Loss=0.0024\n",
      "Epoch 17/20, Batch 300/782: Loss=0.0097\n",
      "Epoch 17/20, Batch 400/782: Loss=0.0002\n",
      "Epoch 17/20, Batch 500/782: Loss=0.0018\n",
      "Epoch 17/20, Batch 600/782: Loss=0.0019\n",
      "Epoch 17/20, Batch 700/782: Loss=0.0066\n",
      "Epoch 17/20: Avg Loss=0.0146, Accuracy=99.53%\n",
      "Epoch 18/20, Batch 0/782: Loss=0.0046\n",
      "Epoch 18/20, Batch 100/782: Loss=0.0049\n",
      "Epoch 18/20, Batch 200/782: Loss=0.0001\n",
      "Epoch 18/20, Batch 300/782: Loss=0.0015\n",
      "Epoch 18/20, Batch 400/782: Loss=0.0076\n",
      "Epoch 18/20, Batch 500/782: Loss=0.0009\n",
      "Epoch 18/20, Batch 600/782: Loss=0.0010\n",
      "Epoch 18/20, Batch 700/782: Loss=0.0014\n",
      "Epoch 18/20: Avg Loss=0.0119, Accuracy=99.58%\n",
      "Epoch 19/20, Batch 0/782: Loss=0.0034\n",
      "Epoch 19/20, Batch 100/782: Loss=0.0057\n",
      "Epoch 19/20, Batch 200/782: Loss=0.0001\n",
      "Epoch 19/20, Batch 300/782: Loss=0.0004\n",
      "Epoch 19/20, Batch 400/782: Loss=0.0235\n",
      "Epoch 19/20, Batch 500/782: Loss=0.0020\n",
      "Epoch 19/20, Batch 600/782: Loss=0.0037\n",
      "Epoch 19/20, Batch 700/782: Loss=0.0076\n",
      "Epoch 19/20: Avg Loss=0.0130, Accuracy=99.55%\n",
      "Epoch 20/20, Batch 0/782: Loss=0.0034\n",
      "Epoch 20/20, Batch 100/782: Loss=0.0020\n",
      "Epoch 20/20, Batch 200/782: Loss=0.0060\n",
      "Epoch 20/20, Batch 300/782: Loss=0.0001\n",
      "Epoch 20/20, Batch 400/782: Loss=0.0538\n",
      "Epoch 20/20, Batch 500/782: Loss=0.0112\n",
      "Epoch 20/20, Batch 600/782: Loss=0.0037\n",
      "Epoch 20/20, Batch 700/782: Loss=0.0002\n",
      "Epoch 20/20: Avg Loss=0.0112, Accuracy=99.62%\n",
      "\n",
      "--- Evaluating fine-tuned resnet18 (Taylor) ---\n",
      "Metrics for Taylor: {'macs': 447320962.0, 'size_mb': 36.073432, 'accuracy': 84.38}\n",
      "✅ Model saved as ONNX to ./output_resnet/resnet18/strategies/resnet18_Taylor_final.onnx\n",
      "Final fine-tuned model saved to ./output_resnet/resnet18/strategies/resnet18_Taylor_final.pth\n",
      "\n",
      "--- Pruning resnet18 with Strategy: Hessian ---\n",
      "Initial MACs: 0.557 G, Size: 42.63 MB\n",
      "Step 1/5 (Non-Taylor): MACs 0.557 G, Size 42.63 MB\n",
      "Step 2/5: No more prunable elements found for non-Taylor strategy.\n",
      "After pruning: MACs 0.557 G, Size 42.63 MB\n",
      "Warning: Pruning targets not fully achieved.\n",
      "✅ Model saved as ONNX to ./output_resnet/resnet18/strategies/resnet18_Hessian_pruned.onnx\n",
      "Pruned model saved to ./output_resnet/resnet18/strategies/resnet18_Hessian_pruned.pth\n",
      "\n",
      "--- Fine-tuning resnet18 after Hessian pruning ---\n",
      "Epoch 1/20, Batch 0/782: Loss=0.0623\n",
      "Epoch 1/20, Batch 100/782: Loss=0.0494\n",
      "Epoch 1/20, Batch 200/782: Loss=0.0383\n",
      "Epoch 1/20, Batch 300/782: Loss=0.0173\n",
      "Epoch 1/20, Batch 400/782: Loss=0.0050\n",
      "Epoch 1/20, Batch 500/782: Loss=0.1027\n",
      "Epoch 1/20, Batch 600/782: Loss=0.0310\n",
      "Epoch 1/20, Batch 700/782: Loss=0.0129\n",
      "Epoch 1/20: Avg Loss=0.0359, Accuracy=98.77%\n",
      "Epoch 2/20, Batch 0/782: Loss=0.0059\n",
      "Epoch 2/20, Batch 100/782: Loss=0.0993\n",
      "Epoch 2/20, Batch 200/782: Loss=0.0301\n",
      "Epoch 2/20, Batch 300/782: Loss=0.0188\n",
      "Epoch 2/20, Batch 400/782: Loss=0.0062\n",
      "Epoch 2/20, Batch 500/782: Loss=0.0020\n",
      "Epoch 2/20, Batch 600/782: Loss=0.0282\n",
      "Epoch 2/20, Batch 700/782: Loss=0.0133\n",
      "Epoch 2/20: Avg Loss=0.0181, Accuracy=99.44%\n",
      "Epoch 3/20, Batch 0/782: Loss=0.0516\n",
      "Epoch 3/20, Batch 100/782: Loss=0.0636\n",
      "Epoch 3/20, Batch 200/782: Loss=0.0406\n",
      "Epoch 3/20, Batch 300/782: Loss=0.0106\n",
      "Epoch 3/20, Batch 400/782: Loss=0.0195\n",
      "Epoch 3/20, Batch 500/782: Loss=0.0014\n",
      "Epoch 3/20, Batch 600/782: Loss=0.0434\n",
      "Epoch 3/20, Batch 700/782: Loss=0.0082\n",
      "Epoch 3/20: Avg Loss=0.0254, Accuracy=99.14%\n",
      "Epoch 4/20, Batch 0/782: Loss=0.0019\n",
      "Epoch 4/20, Batch 100/782: Loss=0.0329\n",
      "Epoch 4/20, Batch 200/782: Loss=0.0159\n",
      "Epoch 4/20, Batch 300/782: Loss=0.0040\n",
      "Epoch 4/20, Batch 400/782: Loss=0.0008\n",
      "Epoch 4/20, Batch 500/782: Loss=0.0358\n",
      "Epoch 4/20, Batch 600/782: Loss=0.0010\n",
      "Epoch 4/20, Batch 700/782: Loss=0.0083\n",
      "Epoch 4/20: Avg Loss=0.0144, Accuracy=99.59%\n",
      "Epoch 5/20, Batch 0/782: Loss=0.0040\n",
      "Epoch 5/20, Batch 100/782: Loss=0.0058\n",
      "Epoch 5/20, Batch 200/782: Loss=0.0475\n",
      "Epoch 5/20, Batch 300/782: Loss=0.0308\n",
      "Epoch 5/20, Batch 400/782: Loss=0.0011\n",
      "Epoch 5/20, Batch 500/782: Loss=0.0490\n",
      "Epoch 5/20, Batch 600/782: Loss=0.0055\n",
      "Epoch 5/20, Batch 700/782: Loss=0.0271\n",
      "Epoch 5/20: Avg Loss=0.0196, Accuracy=99.32%\n",
      "Epoch 6/20, Batch 0/782: Loss=0.0018\n",
      "Epoch 6/20, Batch 100/782: Loss=0.0247\n",
      "Epoch 6/20, Batch 200/782: Loss=0.0092\n",
      "Epoch 6/20, Batch 300/782: Loss=0.0268\n",
      "Epoch 6/20, Batch 400/782: Loss=0.0092\n",
      "Epoch 6/20, Batch 500/782: Loss=0.0625\n",
      "Epoch 6/20, Batch 600/782: Loss=0.0110\n",
      "Epoch 6/20, Batch 700/782: Loss=0.0315\n",
      "Epoch 6/20: Avg Loss=0.0222, Accuracy=99.25%\n",
      "Epoch 7/20, Batch 0/782: Loss=0.0024\n",
      "Epoch 7/20, Batch 100/782: Loss=0.0040\n",
      "Epoch 7/20, Batch 200/782: Loss=0.0039\n",
      "Epoch 7/20, Batch 300/782: Loss=0.0330\n",
      "Epoch 7/20, Batch 400/782: Loss=0.0407\n",
      "Epoch 7/20, Batch 500/782: Loss=0.0028\n",
      "Epoch 7/20, Batch 600/782: Loss=0.0168\n",
      "Epoch 7/20, Batch 700/782: Loss=0.0067\n",
      "Epoch 7/20: Avg Loss=0.0160, Accuracy=99.46%\n",
      "Epoch 8/20, Batch 0/782: Loss=0.0115\n",
      "Epoch 8/20, Batch 100/782: Loss=0.0044\n",
      "Epoch 8/20, Batch 200/782: Loss=0.0158\n",
      "Epoch 8/20, Batch 300/782: Loss=0.0011\n",
      "Epoch 8/20, Batch 400/782: Loss=0.0024\n",
      "Epoch 8/20, Batch 500/782: Loss=0.0045\n",
      "Epoch 8/20, Batch 600/782: Loss=0.0055\n",
      "Epoch 8/20, Batch 700/782: Loss=0.0354\n",
      "Epoch 8/20: Avg Loss=0.0143, Accuracy=99.53%\n",
      "Epoch 9/20, Batch 0/782: Loss=0.0023\n",
      "Epoch 9/20, Batch 100/782: Loss=0.0611\n",
      "Epoch 9/20, Batch 200/782: Loss=0.0066\n",
      "Epoch 9/20, Batch 300/782: Loss=0.0005\n",
      "Epoch 9/20, Batch 400/782: Loss=0.0023\n",
      "Epoch 9/20, Batch 500/782: Loss=0.0079\n",
      "Epoch 9/20, Batch 600/782: Loss=0.0025\n",
      "Epoch 9/20, Batch 700/782: Loss=0.0042\n",
      "Epoch 9/20: Avg Loss=0.0165, Accuracy=99.43%\n",
      "Epoch 10/20, Batch 0/782: Loss=0.0122\n",
      "Epoch 10/20, Batch 100/782: Loss=0.0006\n",
      "Epoch 10/20, Batch 200/782: Loss=0.0270\n",
      "Epoch 10/20, Batch 300/782: Loss=0.0038\n",
      "Epoch 10/20, Batch 400/782: Loss=0.0005\n",
      "Epoch 10/20, Batch 500/782: Loss=0.0092\n",
      "Epoch 10/20, Batch 600/782: Loss=0.0016\n",
      "Epoch 10/20, Batch 700/782: Loss=0.0007\n",
      "Epoch 10/20: Avg Loss=0.0119, Accuracy=99.61%\n",
      "Epoch 11/20, Batch 0/782: Loss=0.0047\n",
      "Epoch 11/20, Batch 100/782: Loss=0.0046\n",
      "Epoch 11/20, Batch 200/782: Loss=0.0507\n",
      "Epoch 11/20, Batch 300/782: Loss=0.0171\n",
      "Epoch 11/20, Batch 400/782: Loss=0.0125\n",
      "Epoch 11/20, Batch 500/782: Loss=0.0011\n",
      "Epoch 11/20, Batch 600/782: Loss=0.0198\n",
      "Epoch 11/20, Batch 700/782: Loss=0.0392\n",
      "Epoch 11/20: Avg Loss=0.0184, Accuracy=99.36%\n",
      "Epoch 12/20, Batch 0/782: Loss=0.0030\n",
      "Epoch 12/20, Batch 100/782: Loss=0.0718\n",
      "Epoch 12/20, Batch 200/782: Loss=0.0059\n",
      "Epoch 12/20, Batch 300/782: Loss=0.0147\n",
      "Epoch 12/20, Batch 400/782: Loss=0.0054\n",
      "Epoch 12/20, Batch 500/782: Loss=0.0174\n",
      "Epoch 12/20, Batch 600/782: Loss=0.0069\n",
      "Epoch 12/20, Batch 700/782: Loss=0.0220\n",
      "Epoch 12/20: Avg Loss=0.0148, Accuracy=99.50%\n",
      "Epoch 13/20, Batch 0/782: Loss=0.0034\n",
      "Epoch 13/20, Batch 100/782: Loss=0.0010\n",
      "Epoch 13/20, Batch 200/782: Loss=0.0045\n",
      "Epoch 13/20, Batch 300/782: Loss=0.0006\n",
      "Epoch 13/20, Batch 400/782: Loss=0.0083\n",
      "Epoch 13/20, Batch 500/782: Loss=0.0087\n",
      "Epoch 13/20, Batch 600/782: Loss=0.0047\n",
      "Epoch 13/20, Batch 700/782: Loss=0.0281\n",
      "Epoch 13/20: Avg Loss=0.0109, Accuracy=99.63%\n",
      "Epoch 14/20, Batch 0/782: Loss=0.0002\n",
      "Epoch 14/20, Batch 100/782: Loss=0.0004\n",
      "Epoch 14/20, Batch 200/782: Loss=0.0036\n",
      "Epoch 14/20, Batch 300/782: Loss=0.0120\n",
      "Epoch 14/20, Batch 400/782: Loss=0.0025\n",
      "Epoch 14/20, Batch 500/782: Loss=0.0006\n",
      "Epoch 14/20, Batch 600/782: Loss=0.0011\n",
      "Epoch 14/20, Batch 700/782: Loss=0.0995\n",
      "Epoch 14/20: Avg Loss=0.0147, Accuracy=99.52%\n",
      "Epoch 15/20, Batch 0/782: Loss=0.0015\n",
      "Epoch 15/20, Batch 100/782: Loss=0.0828\n",
      "Epoch 15/20, Batch 200/782: Loss=0.0028\n",
      "Epoch 15/20, Batch 300/782: Loss=0.1239\n",
      "Epoch 15/20, Batch 400/782: Loss=0.0006\n",
      "Epoch 15/20, Batch 500/782: Loss=0.0003\n",
      "Epoch 15/20, Batch 600/782: Loss=0.0121\n",
      "Epoch 15/20, Batch 700/782: Loss=0.0003\n",
      "Epoch 15/20: Avg Loss=0.0140, Accuracy=99.53%\n",
      "Epoch 16/20, Batch 0/782: Loss=0.0051\n",
      "Epoch 16/20, Batch 100/782: Loss=0.0025\n",
      "Epoch 16/20, Batch 200/782: Loss=0.0103\n",
      "Epoch 16/20, Batch 300/782: Loss=0.0269\n",
      "Epoch 16/20, Batch 400/782: Loss=0.0004\n",
      "Epoch 16/20, Batch 500/782: Loss=0.0023\n",
      "Epoch 16/20, Batch 600/782: Loss=0.0009\n",
      "Epoch 16/20, Batch 700/782: Loss=0.0054\n",
      "Epoch 16/20: Avg Loss=0.0107, Accuracy=99.61%\n",
      "Epoch 17/20, Batch 0/782: Loss=0.0052\n",
      "Epoch 17/20, Batch 100/782: Loss=0.0060\n",
      "Epoch 17/20, Batch 200/782: Loss=0.0009\n",
      "Epoch 17/20, Batch 300/782: Loss=0.0002\n",
      "Epoch 17/20, Batch 400/782: Loss=0.0149\n",
      "Epoch 17/20, Batch 500/782: Loss=0.0016\n",
      "Epoch 17/20, Batch 600/782: Loss=0.1694\n",
      "Epoch 17/20, Batch 700/782: Loss=0.0010\n",
      "Epoch 17/20: Avg Loss=0.0101, Accuracy=99.67%\n",
      "Epoch 18/20, Batch 0/782: Loss=0.0011\n",
      "Epoch 18/20, Batch 100/782: Loss=0.0440\n",
      "Epoch 18/20, Batch 200/782: Loss=0.0034\n",
      "Epoch 18/20, Batch 300/782: Loss=0.0090\n",
      "Epoch 18/20, Batch 400/782: Loss=0.0009\n",
      "Epoch 18/20, Batch 500/782: Loss=0.0244\n",
      "Epoch 18/20, Batch 600/782: Loss=0.0011\n",
      "Epoch 18/20, Batch 700/782: Loss=0.0005\n",
      "Epoch 18/20: Avg Loss=0.0127, Accuracy=99.54%\n",
      "Epoch 19/20, Batch 0/782: Loss=0.0121\n",
      "Epoch 19/20, Batch 100/782: Loss=0.0004\n",
      "Epoch 19/20, Batch 200/782: Loss=0.0287\n",
      "Epoch 19/20, Batch 300/782: Loss=0.0005\n",
      "Epoch 19/20, Batch 400/782: Loss=0.0037\n",
      "Epoch 19/20, Batch 500/782: Loss=0.0208\n",
      "Epoch 19/20, Batch 600/782: Loss=0.0036\n",
      "Epoch 19/20, Batch 700/782: Loss=0.0005\n",
      "Epoch 19/20: Avg Loss=0.0118, Accuracy=99.60%\n",
      "Epoch 20/20, Batch 0/782: Loss=0.0011\n",
      "Epoch 20/20, Batch 100/782: Loss=0.0009\n",
      "Epoch 20/20, Batch 200/782: Loss=0.0021\n",
      "Epoch 20/20, Batch 300/782: Loss=0.0001\n",
      "Epoch 20/20, Batch 400/782: Loss=0.0041\n",
      "Epoch 20/20, Batch 500/782: Loss=0.0127\n",
      "Epoch 20/20, Batch 600/782: Loss=0.0018\n",
      "Epoch 20/20, Batch 700/782: Loss=0.0362\n",
      "Epoch 20/20: Avg Loss=0.0130, Accuracy=99.51%\n",
      "\n",
      "--- Evaluating fine-tuned resnet18 (Hessian) ---\n",
      "Metrics for Hessian: {'macs': 556651530.0, 'size_mb': 44.695848, 'accuracy': 84.73}\n",
      "✅ Model saved as ONNX to ./output_resnet/resnet18/strategies/resnet18_Hessian_final.onnx\n",
      "Final fine-tuned model saved to ./output_resnet/resnet18/strategies/resnet18_Hessian_final.pth\n",
      "\n",
      "--- Pruning resnet18 with Strategy: lamp ---\n",
      "Initial MACs: 0.557 G, Size: 42.63 MB\n",
      "Step 1/5 (Non-Taylor): MACs 0.447 G, Size 34.40 MB\n",
      "Step 2/5 (Non-Taylor): MACs 0.354 G, Size 27.17 MB\n",
      "Step 3/5 (Non-Taylor): MACs 0.269 G, Size 20.83 MB\n",
      "Step 4/5 (Non-Taylor): MACs 0.198 G, Size 15.29 MB\n",
      "Step 5/5 (Non-Taylor): MACs 0.140 G, Size 10.67 MB\n",
      "Targets reached at step 5 for non-Taylor strategy.\n",
      "After pruning: MACs 0.140 G, Size 10.67 MB\n",
      "Pruning targets achieved.\n",
      "✅ Model saved as ONNX to ./output_resnet/resnet18/strategies/resnet18_lamp_pruned.onnx\n",
      "Pruned model saved to ./output_resnet/resnet18/strategies/resnet18_lamp_pruned.pth\n",
      "\n",
      "--- Fine-tuning resnet18 after lamp pruning ---\n",
      "Epoch 1/20, Batch 0/782: Loss=2.0090\n",
      "Epoch 1/20, Batch 100/782: Loss=0.4540\n",
      "Epoch 1/20, Batch 200/782: Loss=0.2513\n",
      "Epoch 1/20, Batch 300/782: Loss=0.2706\n",
      "Epoch 1/20, Batch 400/782: Loss=0.1710\n",
      "Epoch 1/20, Batch 500/782: Loss=0.1540\n",
      "Epoch 1/20, Batch 600/782: Loss=0.1675\n",
      "Epoch 1/20, Batch 700/782: Loss=0.1056\n",
      "Epoch 1/20: Avg Loss=0.2683, Accuracy=91.05%\n",
      "Epoch 2/20, Batch 0/782: Loss=0.0463\n",
      "Epoch 2/20, Batch 100/782: Loss=0.0732\n",
      "Epoch 2/20, Batch 200/782: Loss=0.0345\n",
      "Epoch 2/20, Batch 300/782: Loss=0.1160\n",
      "Epoch 2/20, Batch 400/782: Loss=0.0365\n",
      "Epoch 2/20, Batch 500/782: Loss=0.1210\n",
      "Epoch 2/20, Batch 600/782: Loss=0.0502\n",
      "Epoch 2/20, Batch 700/782: Loss=0.0831\n",
      "Epoch 2/20: Avg Loss=0.0671, Accuracy=97.91%\n",
      "Epoch 3/20, Batch 0/782: Loss=0.0207\n",
      "Epoch 3/20, Batch 100/782: Loss=0.1357\n",
      "Epoch 3/20, Batch 200/782: Loss=0.0995\n",
      "Epoch 3/20, Batch 300/782: Loss=0.0119\n",
      "Epoch 3/20, Batch 400/782: Loss=0.0323\n",
      "Epoch 3/20, Batch 500/782: Loss=0.0502\n",
      "Epoch 3/20, Batch 600/782: Loss=0.0370\n",
      "Epoch 3/20, Batch 700/782: Loss=0.0345\n",
      "Epoch 3/20: Avg Loss=0.0371, Accuracy=98.92%\n",
      "Epoch 4/20, Batch 0/782: Loss=0.0356\n",
      "Epoch 4/20, Batch 100/782: Loss=0.0075\n",
      "Epoch 4/20, Batch 200/782: Loss=0.0213\n",
      "Epoch 4/20, Batch 300/782: Loss=0.0231\n",
      "Epoch 4/20, Batch 400/782: Loss=0.0225\n",
      "Epoch 4/20, Batch 500/782: Loss=0.0056\n",
      "Epoch 4/20, Batch 600/782: Loss=0.0202\n",
      "Epoch 4/20, Batch 700/782: Loss=0.0537\n",
      "Epoch 4/20: Avg Loss=0.0337, Accuracy=98.84%\n",
      "Epoch 5/20, Batch 0/782: Loss=0.0740\n",
      "Epoch 5/20, Batch 100/782: Loss=0.0443\n",
      "Epoch 5/20, Batch 200/782: Loss=0.0238\n",
      "Epoch 5/20, Batch 300/782: Loss=0.0020\n",
      "Epoch 5/20, Batch 400/782: Loss=0.0418\n",
      "Epoch 5/20, Batch 500/782: Loss=0.0037\n",
      "Epoch 5/20, Batch 600/782: Loss=0.0651\n",
      "Epoch 5/20, Batch 700/782: Loss=0.0167\n",
      "Epoch 5/20: Avg Loss=0.0261, Accuracy=99.15%\n",
      "Epoch 6/20, Batch 0/782: Loss=0.0510\n",
      "Epoch 6/20, Batch 100/782: Loss=0.0116\n",
      "Epoch 6/20, Batch 200/782: Loss=0.0114\n",
      "Epoch 6/20, Batch 300/782: Loss=0.0032\n",
      "Epoch 6/20, Batch 400/782: Loss=0.0128\n",
      "Epoch 6/20, Batch 500/782: Loss=0.0408\n",
      "Epoch 6/20, Batch 600/782: Loss=0.0144\n",
      "Epoch 6/20, Batch 700/782: Loss=0.0206\n",
      "Epoch 6/20: Avg Loss=0.0290, Accuracy=99.03%\n",
      "Epoch 7/20, Batch 0/782: Loss=0.0073\n",
      "Epoch 7/20, Batch 100/782: Loss=0.0362\n",
      "Epoch 7/20, Batch 200/782: Loss=0.0113\n",
      "Epoch 7/20, Batch 300/782: Loss=0.0090\n",
      "Epoch 7/20, Batch 400/782: Loss=0.0098\n",
      "Epoch 7/20, Batch 500/782: Loss=0.0639\n",
      "Epoch 7/20, Batch 600/782: Loss=0.0171\n",
      "Epoch 7/20, Batch 700/782: Loss=0.0006\n",
      "Epoch 7/20: Avg Loss=0.0248, Accuracy=99.14%\n",
      "Epoch 8/20, Batch 0/782: Loss=0.0062\n",
      "Epoch 8/20, Batch 100/782: Loss=0.0383\n",
      "Epoch 8/20, Batch 200/782: Loss=0.0288\n",
      "Epoch 8/20, Batch 300/782: Loss=0.0032\n",
      "Epoch 8/20, Batch 400/782: Loss=0.0093\n",
      "Epoch 8/20, Batch 500/782: Loss=0.0465\n",
      "Epoch 8/20, Batch 600/782: Loss=0.0993\n",
      "Epoch 8/20, Batch 700/782: Loss=0.0129\n",
      "Epoch 8/20: Avg Loss=0.0196, Accuracy=99.35%\n",
      "Epoch 9/20, Batch 0/782: Loss=0.0622\n",
      "Epoch 9/20, Batch 100/782: Loss=0.0064\n",
      "Epoch 9/20, Batch 200/782: Loss=0.0013\n",
      "Epoch 9/20, Batch 300/782: Loss=0.0788\n",
      "Epoch 9/20, Batch 400/782: Loss=0.0019\n",
      "Epoch 9/20, Batch 500/782: Loss=0.0362\n",
      "Epoch 9/20, Batch 600/782: Loss=0.0568\n",
      "Epoch 9/20, Batch 700/782: Loss=0.0063\n",
      "Epoch 9/20: Avg Loss=0.0237, Accuracy=99.22%\n",
      "Epoch 10/20, Batch 0/782: Loss=0.0305\n",
      "Epoch 10/20, Batch 100/782: Loss=0.0119\n",
      "Epoch 10/20, Batch 200/782: Loss=0.0138\n",
      "Epoch 10/20, Batch 300/782: Loss=0.0055\n",
      "Epoch 10/20, Batch 400/782: Loss=0.0015\n",
      "Epoch 10/20, Batch 500/782: Loss=0.0178\n",
      "Epoch 10/20, Batch 600/782: Loss=0.0281\n",
      "Epoch 10/20, Batch 700/782: Loss=0.0058\n",
      "Epoch 10/20: Avg Loss=0.0206, Accuracy=99.30%\n",
      "Epoch 11/20, Batch 0/782: Loss=0.0087\n",
      "Epoch 11/20, Batch 100/782: Loss=0.0075\n",
      "Epoch 11/20, Batch 200/782: Loss=0.0326\n",
      "Epoch 11/20, Batch 300/782: Loss=0.0039\n",
      "Epoch 11/20, Batch 400/782: Loss=0.0010\n",
      "Epoch 11/20, Batch 500/782: Loss=0.0087\n",
      "Epoch 11/20, Batch 600/782: Loss=0.0174\n",
      "Epoch 11/20, Batch 700/782: Loss=0.0209\n",
      "Epoch 11/20: Avg Loss=0.0199, Accuracy=99.37%\n",
      "Epoch 12/20, Batch 0/782: Loss=0.0036\n",
      "Epoch 12/20, Batch 100/782: Loss=0.0348\n",
      "Epoch 12/20, Batch 200/782: Loss=0.0026\n",
      "Epoch 12/20, Batch 300/782: Loss=0.0125\n",
      "Epoch 12/20, Batch 400/782: Loss=0.0188\n",
      "Epoch 12/20, Batch 500/782: Loss=0.0252\n",
      "Epoch 12/20, Batch 600/782: Loss=0.0082\n",
      "Epoch 12/20, Batch 700/782: Loss=0.0005\n",
      "Epoch 12/20: Avg Loss=0.0231, Accuracy=99.23%\n",
      "Epoch 13/20, Batch 0/782: Loss=0.0465\n",
      "Epoch 13/20, Batch 100/782: Loss=0.0010\n",
      "Epoch 13/20, Batch 200/782: Loss=0.0034\n",
      "Epoch 13/20, Batch 300/782: Loss=0.0023\n",
      "Epoch 13/20, Batch 400/782: Loss=0.0006\n",
      "Epoch 13/20, Batch 500/782: Loss=0.0038\n",
      "Epoch 13/20, Batch 600/782: Loss=0.0354\n",
      "Epoch 13/20, Batch 700/782: Loss=0.0808\n",
      "Epoch 13/20: Avg Loss=0.0177, Accuracy=99.41%\n",
      "Epoch 14/20, Batch 0/782: Loss=0.0016\n",
      "Epoch 14/20, Batch 100/782: Loss=0.0005\n",
      "Epoch 14/20, Batch 200/782: Loss=0.0002\n",
      "Epoch 14/20, Batch 300/782: Loss=0.0055\n",
      "Epoch 14/20, Batch 400/782: Loss=0.0184\n",
      "Epoch 14/20, Batch 500/782: Loss=0.0083\n",
      "Epoch 14/20, Batch 600/782: Loss=0.0065\n",
      "Epoch 14/20, Batch 700/782: Loss=0.0008\n",
      "Epoch 14/20: Avg Loss=0.0127, Accuracy=99.58%\n",
      "Epoch 15/20, Batch 0/782: Loss=0.0024\n",
      "Epoch 15/20, Batch 100/782: Loss=0.0193\n",
      "Epoch 15/20, Batch 200/782: Loss=0.0035\n",
      "Epoch 15/20, Batch 300/782: Loss=0.0232\n",
      "Epoch 15/20, Batch 400/782: Loss=0.0021\n",
      "Epoch 15/20, Batch 500/782: Loss=0.0230\n",
      "Epoch 15/20, Batch 600/782: Loss=0.0117\n",
      "Epoch 15/20, Batch 700/782: Loss=0.0559\n",
      "Epoch 15/20: Avg Loss=0.0175, Accuracy=99.37%\n",
      "Epoch 16/20, Batch 0/782: Loss=0.0136\n",
      "Epoch 16/20, Batch 100/782: Loss=0.0451\n",
      "Epoch 16/20, Batch 200/782: Loss=0.0200\n",
      "Epoch 16/20, Batch 300/782: Loss=0.0013\n",
      "Epoch 16/20, Batch 400/782: Loss=0.0086\n",
      "Epoch 16/20, Batch 500/782: Loss=0.0031\n",
      "Epoch 16/20, Batch 600/782: Loss=0.0060\n",
      "Epoch 16/20, Batch 700/782: Loss=0.1485\n",
      "Epoch 16/20: Avg Loss=0.0209, Accuracy=99.27%\n",
      "Epoch 17/20, Batch 0/782: Loss=0.0207\n",
      "Epoch 17/20, Batch 100/782: Loss=0.0205\n",
      "Epoch 17/20, Batch 200/782: Loss=0.0040\n",
      "Epoch 17/20, Batch 300/782: Loss=0.0158\n",
      "Epoch 17/20, Batch 400/782: Loss=0.0472\n",
      "Epoch 17/20, Batch 500/782: Loss=0.0072\n",
      "Epoch 17/20, Batch 600/782: Loss=0.0025\n",
      "Epoch 17/20, Batch 700/782: Loss=0.0289\n",
      "Epoch 17/20: Avg Loss=0.0143, Accuracy=99.54%\n",
      "Epoch 18/20, Batch 0/782: Loss=0.0030\n",
      "Epoch 18/20, Batch 100/782: Loss=0.0057\n",
      "Epoch 18/20, Batch 200/782: Loss=0.0075\n",
      "Epoch 18/20, Batch 300/782: Loss=0.0023\n",
      "Epoch 18/20, Batch 400/782: Loss=0.0123\n",
      "Epoch 18/20, Batch 500/782: Loss=0.0009\n",
      "Epoch 18/20, Batch 600/782: Loss=0.0074\n",
      "Epoch 18/20, Batch 700/782: Loss=0.0012\n",
      "Epoch 18/20: Avg Loss=0.0143, Accuracy=99.52%\n",
      "Epoch 19/20, Batch 0/782: Loss=0.0358\n",
      "Epoch 19/20, Batch 100/782: Loss=0.0017\n",
      "Epoch 19/20, Batch 200/782: Loss=0.0125\n",
      "Epoch 19/20, Batch 300/782: Loss=0.0059\n",
      "Epoch 19/20, Batch 400/782: Loss=0.0686\n",
      "Epoch 19/20, Batch 500/782: Loss=0.0049\n",
      "Epoch 19/20, Batch 600/782: Loss=0.0056\n",
      "Epoch 19/20, Batch 700/782: Loss=0.0164\n",
      "Epoch 19/20: Avg Loss=0.0186, Accuracy=99.37%\n",
      "Epoch 20/20, Batch 0/782: Loss=0.0118\n",
      "Epoch 20/20, Batch 100/782: Loss=0.0056\n",
      "Epoch 20/20, Batch 200/782: Loss=0.0205\n",
      "Epoch 20/20, Batch 300/782: Loss=0.0132\n",
      "Epoch 20/20, Batch 400/782: Loss=0.0048\n",
      "Epoch 20/20, Batch 500/782: Loss=0.0158\n",
      "Epoch 20/20, Batch 600/782: Loss=0.0015\n",
      "Epoch 20/20, Batch 700/782: Loss=0.0786\n",
      "Epoch 20/20: Avg Loss=0.0122, Accuracy=99.57%\n",
      "\n",
      "--- Evaluating fine-tuned resnet18 (lamp) ---\n",
      "Metrics for lamp: {'macs': 139913738.0, 'size_mb': 11.19044, 'accuracy': 84.77}\n",
      "✅ Model saved as ONNX to ./output_resnet/resnet18/strategies/resnet18_lamp_final.onnx\n",
      "Final fine-tuned model saved to ./output_resnet/resnet18/strategies/resnet18_lamp_final.pth\n",
      "\n",
      "--- Pruning resnet18 with Strategy: geometry ---\n",
      "Initial MACs: 0.557 G, Size: 42.63 MB\n",
      "Step 1/5 (Non-Taylor): MACs 0.447 G, Size 34.40 MB\n",
      "Step 2/5 (Non-Taylor): MACs 0.354 G, Size 27.17 MB\n",
      "Step 3/5 (Non-Taylor): MACs 0.269 G, Size 20.83 MB\n",
      "Step 4/5 (Non-Taylor): MACs 0.198 G, Size 15.29 MB\n",
      "Step 5/5 (Non-Taylor): MACs 0.140 G, Size 10.67 MB\n",
      "Targets reached at step 5 for non-Taylor strategy.\n",
      "After pruning: MACs 0.140 G, Size 10.67 MB\n",
      "Pruning targets achieved.\n",
      "✅ Model saved as ONNX to ./output_resnet/resnet18/strategies/resnet18_geometry_pruned.onnx\n",
      "Pruned model saved to ./output_resnet/resnet18/strategies/resnet18_geometry_pruned.pth\n",
      "\n",
      "--- Fine-tuning resnet18 after geometry pruning ---\n",
      "Epoch 1/20, Batch 0/782: Loss=2.2596\n",
      "Epoch 1/20, Batch 100/782: Loss=0.4089\n",
      "Epoch 1/20, Batch 200/782: Loss=0.1664\n",
      "Epoch 1/20, Batch 300/782: Loss=0.2841\n",
      "Epoch 1/20, Batch 400/782: Loss=0.2526\n",
      "Epoch 1/20, Batch 500/782: Loss=0.1938\n",
      "Epoch 1/20, Batch 600/782: Loss=0.1146\n",
      "Epoch 1/20, Batch 700/782: Loss=0.1392\n",
      "Epoch 1/20: Avg Loss=0.2401, Accuracy=92.22%\n",
      "Epoch 2/20, Batch 0/782: Loss=0.0115\n",
      "Epoch 2/20, Batch 100/782: Loss=0.0694\n",
      "Epoch 2/20, Batch 200/782: Loss=0.0258\n",
      "Epoch 2/20, Batch 300/782: Loss=0.0314\n",
      "Epoch 2/20, Batch 400/782: Loss=0.0361\n",
      "Epoch 2/20, Batch 500/782: Loss=0.0185\n",
      "Epoch 2/20, Batch 600/782: Loss=0.0571\n",
      "Epoch 2/20, Batch 700/782: Loss=0.0440\n",
      "Epoch 2/20: Avg Loss=0.0595, Accuracy=98.24%\n",
      "Epoch 3/20, Batch 0/782: Loss=0.0166\n",
      "Epoch 3/20, Batch 100/782: Loss=0.0294\n",
      "Epoch 3/20, Batch 200/782: Loss=0.0280\n",
      "Epoch 3/20, Batch 300/782: Loss=0.0413\n",
      "Epoch 3/20, Batch 400/782: Loss=0.0115\n",
      "Epoch 3/20, Batch 500/782: Loss=0.0316\n",
      "Epoch 3/20, Batch 600/782: Loss=0.0323\n",
      "Epoch 3/20, Batch 700/782: Loss=0.0960\n",
      "Epoch 3/20: Avg Loss=0.0325, Accuracy=99.01%\n",
      "Epoch 4/20, Batch 0/782: Loss=0.0151\n",
      "Epoch 4/20, Batch 100/782: Loss=0.0275\n",
      "Epoch 4/20, Batch 200/782: Loss=0.0369\n",
      "Epoch 4/20, Batch 300/782: Loss=0.0228\n",
      "Epoch 4/20, Batch 400/782: Loss=0.0693\n",
      "Epoch 4/20, Batch 500/782: Loss=0.0279\n",
      "Epoch 4/20, Batch 600/782: Loss=0.0387\n",
      "Epoch 4/20, Batch 700/782: Loss=0.1627\n",
      "Epoch 4/20: Avg Loss=0.0318, Accuracy=98.96%\n",
      "Epoch 5/20, Batch 0/782: Loss=0.0272\n",
      "Epoch 5/20, Batch 100/782: Loss=0.0062\n",
      "Epoch 5/20, Batch 200/782: Loss=0.0111\n",
      "Epoch 5/20, Batch 300/782: Loss=0.0075\n",
      "Epoch 5/20, Batch 400/782: Loss=0.0706\n",
      "Epoch 5/20, Batch 500/782: Loss=0.0661\n",
      "Epoch 5/20, Batch 600/782: Loss=0.0239\n",
      "Epoch 5/20, Batch 700/782: Loss=0.0440\n",
      "Epoch 5/20: Avg Loss=0.0341, Accuracy=98.85%\n",
      "Epoch 6/20, Batch 0/782: Loss=0.0049\n",
      "Epoch 6/20, Batch 100/782: Loss=0.0472\n",
      "Epoch 6/20, Batch 200/782: Loss=0.0164\n",
      "Epoch 6/20, Batch 300/782: Loss=0.0044\n",
      "Epoch 6/20, Batch 400/782: Loss=0.0037\n",
      "Epoch 6/20, Batch 500/782: Loss=0.0085\n",
      "Epoch 6/20, Batch 600/782: Loss=0.0216\n",
      "Epoch 6/20, Batch 700/782: Loss=0.0763\n",
      "Epoch 6/20: Avg Loss=0.0242, Accuracy=99.21%\n",
      "Epoch 7/20, Batch 0/782: Loss=0.0071\n",
      "Epoch 7/20, Batch 100/782: Loss=0.0012\n",
      "Epoch 7/20, Batch 200/782: Loss=0.0293\n",
      "Epoch 7/20, Batch 300/782: Loss=0.0291\n",
      "Epoch 7/20, Batch 400/782: Loss=0.0117\n",
      "Epoch 7/20, Batch 500/782: Loss=0.0146\n",
      "Epoch 7/20, Batch 600/782: Loss=0.0974\n",
      "Epoch 7/20, Batch 700/782: Loss=0.0056\n",
      "Epoch 7/20: Avg Loss=0.0252, Accuracy=99.16%\n",
      "Epoch 8/20, Batch 0/782: Loss=0.0268\n",
      "Epoch 8/20, Batch 100/782: Loss=0.0077\n",
      "Epoch 8/20, Batch 200/782: Loss=0.0014\n",
      "Epoch 8/20, Batch 300/782: Loss=0.0048\n",
      "Epoch 8/20, Batch 400/782: Loss=0.0169\n",
      "Epoch 8/20, Batch 500/782: Loss=0.0117\n",
      "Epoch 8/20, Batch 600/782: Loss=0.0199\n",
      "Epoch 8/20, Batch 700/782: Loss=0.0059\n",
      "Epoch 8/20: Avg Loss=0.0207, Accuracy=99.33%\n",
      "Epoch 9/20, Batch 0/782: Loss=0.0066\n",
      "Epoch 9/20, Batch 100/782: Loss=0.0072\n",
      "Epoch 9/20, Batch 200/782: Loss=0.0080\n",
      "Epoch 9/20, Batch 300/782: Loss=0.0135\n",
      "Epoch 9/20, Batch 400/782: Loss=0.0240\n",
      "Epoch 9/20, Batch 500/782: Loss=0.0051\n",
      "Epoch 9/20, Batch 600/782: Loss=0.0094\n",
      "Epoch 9/20, Batch 700/782: Loss=0.0131\n",
      "Epoch 9/20: Avg Loss=0.0211, Accuracy=99.24%\n",
      "Epoch 10/20, Batch 0/782: Loss=0.0021\n",
      "Epoch 10/20, Batch 100/782: Loss=0.0011\n",
      "Epoch 10/20, Batch 200/782: Loss=0.0224\n",
      "Epoch 10/20, Batch 300/782: Loss=0.0379\n",
      "Epoch 10/20, Batch 400/782: Loss=0.0049\n",
      "Epoch 10/20, Batch 500/782: Loss=0.0180\n",
      "Epoch 10/20, Batch 600/782: Loss=0.0111\n",
      "Epoch 10/20, Batch 700/782: Loss=0.0113\n",
      "Epoch 10/20: Avg Loss=0.0223, Accuracy=99.30%\n",
      "Epoch 11/20, Batch 0/782: Loss=0.0153\n",
      "Epoch 11/20, Batch 100/782: Loss=0.0037\n",
      "Epoch 11/20, Batch 200/782: Loss=0.0022\n",
      "Epoch 11/20, Batch 300/782: Loss=0.0034\n",
      "Epoch 11/20, Batch 400/782: Loss=0.0147\n",
      "Epoch 11/20, Batch 500/782: Loss=0.0414\n",
      "Epoch 11/20, Batch 600/782: Loss=0.0021\n",
      "Epoch 11/20, Batch 700/782: Loss=0.0367\n",
      "Epoch 11/20: Avg Loss=0.0200, Accuracy=99.34%\n",
      "Epoch 12/20, Batch 0/782: Loss=0.0331\n",
      "Epoch 12/20, Batch 100/782: Loss=0.0327\n",
      "Epoch 12/20, Batch 200/782: Loss=0.0093\n",
      "Epoch 12/20, Batch 300/782: Loss=0.0009\n",
      "Epoch 12/20, Batch 400/782: Loss=0.0017\n",
      "Epoch 12/20, Batch 500/782: Loss=0.0406\n",
      "Epoch 12/20, Batch 600/782: Loss=0.0023\n",
      "Epoch 12/20, Batch 700/782: Loss=0.0051\n",
      "Epoch 12/20: Avg Loss=0.0189, Accuracy=99.40%\n",
      "Epoch 13/20, Batch 0/782: Loss=0.0199\n",
      "Epoch 13/20, Batch 100/782: Loss=0.0496\n",
      "Epoch 13/20, Batch 200/782: Loss=0.0029\n",
      "Epoch 13/20, Batch 300/782: Loss=0.0001\n",
      "Epoch 13/20, Batch 400/782: Loss=0.0045\n",
      "Epoch 13/20, Batch 500/782: Loss=0.0052\n",
      "Epoch 13/20, Batch 600/782: Loss=0.0114\n",
      "Epoch 13/20, Batch 700/782: Loss=0.0027\n",
      "Epoch 13/20: Avg Loss=0.0167, Accuracy=99.45%\n",
      "Epoch 14/20, Batch 0/782: Loss=0.0411\n",
      "Epoch 14/20, Batch 100/782: Loss=0.0118\n",
      "Epoch 14/20, Batch 200/782: Loss=0.0379\n",
      "Epoch 14/20, Batch 300/782: Loss=0.0057\n",
      "Epoch 14/20, Batch 400/782: Loss=0.0038\n",
      "Epoch 14/20, Batch 500/782: Loss=0.0763\n",
      "Epoch 14/20, Batch 600/782: Loss=0.0319\n",
      "Epoch 14/20, Batch 700/782: Loss=0.0713\n",
      "Epoch 14/20: Avg Loss=0.0180, Accuracy=99.36%\n",
      "Epoch 15/20, Batch 0/782: Loss=0.0077\n",
      "Epoch 15/20, Batch 100/782: Loss=0.0195\n",
      "Epoch 15/20, Batch 200/782: Loss=0.0050\n",
      "Epoch 15/20, Batch 300/782: Loss=0.0084\n",
      "Epoch 15/20, Batch 400/782: Loss=0.0403\n",
      "Epoch 15/20, Batch 500/782: Loss=0.0238\n",
      "Epoch 15/20, Batch 600/782: Loss=0.0029\n",
      "Epoch 15/20, Batch 700/782: Loss=0.0004\n",
      "Epoch 15/20: Avg Loss=0.0161, Accuracy=99.49%\n",
      "Epoch 16/20, Batch 0/782: Loss=0.0428\n",
      "Epoch 16/20, Batch 100/782: Loss=0.0010\n",
      "Epoch 16/20, Batch 200/782: Loss=0.0202\n",
      "Epoch 16/20, Batch 300/782: Loss=0.0431\n",
      "Epoch 16/20, Batch 400/782: Loss=0.0078\n",
      "Epoch 16/20, Batch 500/782: Loss=0.0302\n",
      "Epoch 16/20, Batch 600/782: Loss=0.0462\n",
      "Epoch 16/20, Batch 700/782: Loss=0.0040\n",
      "Epoch 16/20: Avg Loss=0.0160, Accuracy=99.47%\n",
      "Epoch 17/20, Batch 0/782: Loss=0.0019\n",
      "Epoch 17/20, Batch 100/782: Loss=0.0185\n",
      "Epoch 17/20, Batch 200/782: Loss=0.0042\n",
      "Epoch 17/20, Batch 300/782: Loss=0.0005\n",
      "Epoch 17/20, Batch 400/782: Loss=0.0014\n",
      "Epoch 17/20, Batch 500/782: Loss=0.0298\n",
      "Epoch 17/20, Batch 600/782: Loss=0.0112\n",
      "Epoch 17/20, Batch 700/782: Loss=0.0181\n",
      "Epoch 17/20: Avg Loss=0.0152, Accuracy=99.50%\n",
      "Epoch 18/20, Batch 0/782: Loss=0.0016\n",
      "Epoch 18/20, Batch 100/782: Loss=0.0002\n",
      "Epoch 18/20, Batch 200/782: Loss=0.0011\n",
      "Epoch 18/20, Batch 300/782: Loss=0.0027\n",
      "Epoch 18/20, Batch 400/782: Loss=0.0263\n",
      "Epoch 18/20, Batch 500/782: Loss=0.0082\n",
      "Epoch 18/20, Batch 600/782: Loss=0.0065\n",
      "Epoch 18/20, Batch 700/782: Loss=0.0209\n",
      "Epoch 18/20: Avg Loss=0.0117, Accuracy=99.63%\n",
      "Epoch 19/20, Batch 0/782: Loss=0.0153\n",
      "Epoch 19/20, Batch 100/782: Loss=0.0116\n",
      "Epoch 19/20, Batch 200/782: Loss=0.0006\n",
      "Epoch 19/20, Batch 300/782: Loss=0.0032\n",
      "Epoch 19/20, Batch 400/782: Loss=0.0562\n",
      "Epoch 19/20, Batch 500/782: Loss=0.0014\n",
      "Epoch 19/20, Batch 600/782: Loss=0.0038\n",
      "Epoch 19/20, Batch 700/782: Loss=0.0177\n",
      "Epoch 19/20: Avg Loss=0.0185, Accuracy=99.35%\n",
      "Epoch 20/20, Batch 0/782: Loss=0.0210\n",
      "Epoch 20/20, Batch 100/782: Loss=0.0015\n",
      "Epoch 20/20, Batch 200/782: Loss=0.0015\n",
      "Epoch 20/20, Batch 300/782: Loss=0.0266\n",
      "Epoch 20/20, Batch 400/782: Loss=0.0036\n",
      "Epoch 20/20, Batch 500/782: Loss=0.0970\n",
      "Epoch 20/20, Batch 600/782: Loss=0.0062\n",
      "Epoch 20/20, Batch 700/782: Loss=0.0402\n",
      "Epoch 20/20: Avg Loss=0.0152, Accuracy=99.48%\n",
      "\n",
      "--- Evaluating fine-tuned resnet18 (geometry) ---\n",
      "Metrics for geometry: {'macs': 139913738.0, 'size_mb': 11.19044, 'accuracy': 84.02}\n",
      "✅ Model saved as ONNX to ./output_resnet/resnet18/strategies/resnet18_geometry_final.onnx\n",
      "Final fine-tuned model saved to ./output_resnet/resnet18/strategies/resnet18_geometry_final.pth\n",
      "\n",
      "=== Pruning Strategy Comparison ===\n",
      "Strategy     | MACs         | Size (MB)  | Accuracy (%)\n",
      "-------------------------------------------------------\n",
      "initial      | 5.57e+08 |     44.70 |        84.55\n",
      "magnitude    | 1.40e+08 |     11.19 |        83.86\n",
      "bn_scale     | 1.40e+08 |     11.19 |        84.27\n",
      "group_norm   | 1.40e+08 |     11.19 |        83.83\n",
      "random       | 1.40e+08 |     11.19 |        84.42\n",
      "Taylor       | 4.47e+08 |     36.07 |        84.38\n",
      "Hessian      | 5.57e+08 |     44.70 |        84.73\n",
      "lamp         | 1.40e+08 |     11.19 |        84.77\n",
      "geometry     | 1.40e+08 |     11.19 |        84.02\n",
      "ResNet pruning workflow completed successfully!\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
