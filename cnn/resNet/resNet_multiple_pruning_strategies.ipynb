{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-09T18:20:20.488637Z",
     "start_time": "2025-05-09T18:20:18.537687Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_pruning as tp\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau # Added LR Scheduler\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import copy # For deepcopying model state\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler # Ensure SubsetRandomSampler is imported\n",
    "import os\n",
    "import numpy as np # For shuffling indices if not using torch.random.shuffle directly\n",
    "#from cnn.resNet.resnet_example import get_data_loaders"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data loaders",
   "id": "ccfef59ede67787b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T18:20:20.502744Z",
     "start_time": "2025-05-09T18:20:20.495872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_data_loaders(data_dir, batch_size=64, validation_split=0.1, num_workers=4, pin_memory=True):\n",
    "    \"\"\"\n",
    "    Provides CIFAR-10 data loaders from a pre-downloaded dataset.\n",
    "    Splits the training set into train and validation if validation_split > 0.\n",
    "    \"\"\"\n",
    "    abs_data_dir = os.path.abspath(data_dir)\n",
    "    print(f\"Using dataset directory: {abs_data_dir}\")\n",
    "\n",
    "    # Using common CIFAR-10 augmentations for training, simple ToTensor for validation/test\n",
    "    # Note: Your original transform was ((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "    # Standard CIFAR-10 normalization is usually different, like:\n",
    "    # Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    # If you stick with 0.5,0.5,0.5, make sure your pretrained models (if any) used the same.\n",
    "    # For now, I'll use the more common ones for train_transform and a simple one for val/test\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    val_test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    # Load the full training dataset\n",
    "    # Assign appropriate transforms based on whether we'll split or not\n",
    "    # If splitting, the full_trainset should use train_transform for its training part\n",
    "    # and val_test_transform for its validation part. This is handled by creating\n",
    "    # Subset instances with different transforms, but torchvision.datasets.CIFAR10 takes one transform.\n",
    "    # A common approach is to apply train_transform to the full dataset and then subsets\n",
    "    # will inherit it. If validation needs different transform, it's more complex without\n",
    "    # re-instantiating datasets or custom Subset classes.\n",
    "    # For simplicity here, we apply train_transform to the full set, which might be slightly\n",
    "    # suboptimal for validation (as it gets augmentations meant for training).\n",
    "    # A more robust way would be separate datasets or a wrapper.\n",
    "    # Let's keep your original transform for the initial dataset load if validation_split is used\n",
    "    # to avoid augmenting the validation set during the split.\n",
    "\n",
    "    initial_transform = val_test_transform if validation_split > 0 else train_transform\n",
    "\n",
    "    full_train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir,\n",
    "        train=True,\n",
    "        download=False, # Respecting your setting\n",
    "        transform=initial_transform # Use val_test_transform if splitting, else train_transform\n",
    "    )\n",
    "\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir,\n",
    "        train=False,\n",
    "        download=False, # Respecting your setting\n",
    "        transform=val_test_transform\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(os.path.join(abs_data_dir, 'cifar-10-batches-py')):\n",
    "        print(f\"ERROR: CIFAR-10 data not found in {abs_data_dir}. \"\n",
    "              \"Please ensure it's pre-downloaded and extracted correctly.\")\n",
    "        # Depending on how strict you want this:\n",
    "        # return None, None, None\n",
    "        # raise FileNotFoundError(f\"CIFAR-10 data not found in {abs_data_dir}\")\n",
    "\n",
    "\n",
    "    if validation_split > 0.0 and 0.0 < validation_split < 1.0:\n",
    "        num_train = len(full_train_dataset)\n",
    "        indices = list(range(num_train))\n",
    "        split = int(np.floor(validation_split * num_train))\n",
    "\n",
    "        # For reproducibility of the split\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        train_idx, val_idx = indices[split:], indices[:split]\n",
    "\n",
    "        # If we used val_test_transform initially for full_train_dataset,\n",
    "        # now we want the training subset to use train_transform.\n",
    "        # We create new Subset objects and can assign specific transforms to DataLoaders if needed,\n",
    "        # but it's cleaner if the Subset itself 'knows' its transform.\n",
    "        # A simple way is to re-wrap datasets with desired transforms after splitting indices.\n",
    "        # However, a Subset directly uses the transform of its parent dataset.\n",
    "\n",
    "        # To have different transforms for train and val subsets derived from the same original dataset\n",
    "        # without augmentations on the val set:\n",
    "        # 1. Load full_train_dataset with NO transform (or ToTensor only).\n",
    "        # 2. Create two wrapper datasets (e.g., using `torch.utils.data.Dataset` subclass) for train and val\n",
    "        #    that take a subset of indices and apply the correct transform in their __getitem__.\n",
    "        # For simplicity here, if `initial_transform` was `val_test_transform`, the training data won't have augmentation.\n",
    "        # To fix this while keeping it relatively simple:\n",
    "        if initial_transform == val_test_transform: # Means we are splitting\n",
    "             # We want the actual training part to have training augmentations.\n",
    "             # Create a new dataset instance JUST FOR TRAINING with the train_transform.\n",
    "             # This is a bit inefficient (re-scans dataset metadata) but clear.\n",
    "            train_dataset_for_loader = torchvision.datasets.CIFAR10(\n",
    "                root=data_dir, train=True, download=False, transform=train_transform\n",
    "            )\n",
    "            # val_dataset_for_loader will use full_train_dataset which has val_test_transform\n",
    "            val_dataset_for_loader = full_train_dataset\n",
    "        else: # No split, or initial_transform was already train_transform\n",
    "            train_dataset_for_loader = full_train_dataset\n",
    "            val_dataset_for_loader = test_dataset # Fallback for val_loader\n",
    "\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        val_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            dataset=train_dataset_for_loader, # This dataset should have train_transform\n",
    "            batch_size=batch_size,\n",
    "            sampler=train_sampler,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            dataset=val_dataset_for_loader, # This dataset should have val_test_transform\n",
    "            batch_size=batch_size,\n",
    "            sampler=val_sampler,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory\n",
    "        )\n",
    "        print(f\"Splitting training data: {len(train_idx)} for training (with augmentation), {len(val_idx)} for validation.\")\n",
    "\n",
    "    else: # No validation split, or invalid split ratio\n",
    "        if validation_split != 0:\n",
    "             print(f\"Warning: Invalid validation_split value ({validation_split}). Using full training set for training and test set for validation.\")\n",
    "        # Standard train loader with full training data\n",
    "        train_loader = DataLoader(\n",
    "            dataset=full_train_dataset, # full_train_dataset has train_transform if no split\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory\n",
    "        )\n",
    "        # Use test_loader as val_loader\n",
    "        # This is okay for quick experiments but not for final model selection / reporting\n",
    "        val_loader = DataLoader(\n",
    "            dataset=test_dataset, # test_dataset has val_test_transform\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory\n",
    "        )\n",
    "        print(\"Using full training set for training. Using test set as validation set (not recommended for final evaluation).\")\n",
    "\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ],
   "id": "a5ceb8152d9daf42",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ResNet Blocks and Model Definition",
   "id": "ff63c1511dde0fd2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T18:20:24.955383Z",
     "start_time": "2025-05-09T18:20:24.944322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        # Adapted for CIFAR-10: kernel_size 3, stride 1\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        # The final FC layer name is 'linear'\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride_val in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride_val))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        # For CIFAR-10, output of layer4 will be 4x4 if input is 32x32\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18(num_classes=10):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
    "\n",
    "def ResNet34(num_classes=10):\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes)\n",
    "\n",
    "def ResNet50(num_classes=10):\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes)\n"
   ],
   "id": "7604bf46058c7c78",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T18:20:33.001658Z",
     "start_time": "2025-05-09T18:20:32.950031Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
   "id": "9b43e70684af2e8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T18:20:34.834194Z",
     "start_time": "2025-05-09T18:20:34.829488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_model_as_onnx(model, example_input, output_path):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    # Export to ONNX\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        example_input,\n",
    "        output_path,\n",
    "        export_params=True,\n",
    "        opset_version=13,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "    )\n",
    "    print(f\"âœ… Model saved as ONNX to {output_path}\")"
   ],
   "id": "5c71b687c1f36104",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T18:20:36.943816Z",
     "start_time": "2025-05-09T18:20:36.939602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_macs(model, example_input):\n",
    "    macs, params = tp.utils.count_ops_and_params(model, example_input)\n",
    "    return macs, params"
   ],
   "id": "45ac3df7fad6d230",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### compare results of different pruning strategies",
   "id": "13b3277bfa392f82"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T18:20:38.853123Z",
     "start_time": "2025-05-09T18:20:38.849571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compare_results(results):\n",
    "    print(\"\\n=== Pruning Strategy Comparison ===\")\n",
    "    print(f\"{'Strategy':<12} | {'MACs':<12} | {'Size (MB)':<10} | {'Accuracy (%)':<12}\")\n",
    "    print(\"-\" * 55)\n",
    "    for strategy, metrics in results.items():\n",
    "        print(f\"{strategy:<12} | {metrics['macs']:.2e} | {metrics['size_mb']:>9.2f} | {metrics['accuracy']:>12.2f}\")"
   ],
   "id": "95f0f63179a4d2d2",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### compare and plot results",
   "id": "be0aec494fcb0c5b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T18:20:40.754167Z",
     "start_time": "2025-05-09T18:20:40.746517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compare_results_and_plot(results, output_dir='output'):\n",
    "    \"\"\"\n",
    "    Print a comparison table and generate bar charts for MACs, model size, and accuracy\n",
    "    for each pruning strategy, including the initial model.\n",
    "    \"\"\"\n",
    "    # Print comparison table\n",
    "    print(\"\\n=== Pruning Strategy Comparison ===\")\n",
    "    print(f\"{'Strategy':<12} | {'MACs':<12} | {'Size (MB)':<10} | {'Accuracy (%)':<12}\")\n",
    "    print(\"-\" * 55)\n",
    "    for strategy, metrics in results.items():\n",
    "        print(f\"{strategy:<12} | {metrics['macs']:.2e} | {metrics['size_mb']:>9.2f} | {metrics['accuracy']:>12.2f}\")\n",
    "\n",
    "    # Generate bar charts\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    strategies = ['initial'] + [s for s in results if s != 'initial']\n",
    "    metrics_to_plot = ['macs', 'size_mb', 'accuracy'] # Renamed to avoid conflict\n",
    "    titles = {\n",
    "        'macs': 'MACs Comparison',\n",
    "        'size_mb': 'Model Size (MB) Comparison',\n",
    "        'accuracy': 'Accuracy (%) Comparison'\n",
    "    }\n",
    "    y_labels = {\n",
    "        'macs': 'MACs (Millions)',\n",
    "        'size_mb': 'Size (MB)',\n",
    "        'accuracy': 'Accuracy (%)'\n",
    "    }\n",
    "\n",
    "    colors = plt.cm.tab10(range(len(strategies)))\n",
    "    for metric_name in metrics_to_plot:\n",
    "        values = [results[strategy][metric_name] / 1e6 if metric_name == 'macs' else results[strategy][metric_name]\n",
    "                  for strategy in strategies]  # Convert MACs to millions\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bars = plt.bar(strategies, values, color=colors)\n",
    "        plt.xlabel('Strategy')\n",
    "        plt.ylabel(y_labels[metric_name])\n",
    "        plt.title(titles[metric_name])\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "        # Add value labels\n",
    "        for bar in bars:\n",
    "            yval = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., yval, f'{yval:.2f}', ha='center', va='bottom')\n",
    "\n",
    "        # Add initial model reference line\n",
    "        if 'initial' in results: # Ensure initial results exist\n",
    "            initial_value = results['initial'][metric_name] / 1e6 if metric_name == 'macs' else results['initial'][metric_name]\n",
    "            plt.axhline(y=initial_value, color='r', linestyle='--', label='Initial')\n",
    "            plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f'{metric_name}_comparison.png'))\n",
    "        plt.close()"
   ],
   "id": "a66fc2df36581084",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Model",
   "id": "839fd8aec0102217"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T18:20:43.594706Z",
     "start_time": "2025-05-09T18:20:43.591509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_model_state(model_class, num_classes, path, device_to_load): # Renamed for clarity\n",
    "    model = model_class(num_classes=num_classes).to(device_to_load)\n",
    "    model.load_state_dict(torch.load(path, map_location=device_to_load))\n",
    "    return model"
   ],
   "id": "9e6901cb385a9827",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Utility function to save the model",
   "id": "f04222e9289a359b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T18:20:45.660957Z",
     "start_time": "2025-05-09T18:20:45.657041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_model_state(model, path, example_input=None): # Renamed for clarity\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), path)\n",
    "    if example_input is not None:\n",
    "        onnx_path = path.replace('.pth', '.onnx')\n",
    "        save_model_as_onnx(model, example_input, onnx_path)"
   ],
   "id": "9a218511c4dbb077",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluate the model",
   "id": "cdabff4c285236b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T18:20:48.264438Z",
     "start_time": "2025-05-09T18:20:48.258898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, data_loader, example_input, device_to_eval, eval_mode_str=\"Test\"): # Added eval_mode_str\n",
    "    model.eval()\n",
    "    macs, _ = calculate_macs(model, example_input.to(device_to_eval))\n",
    "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    size_mb = params * 4 / 1e6\n",
    "    correct = 0; total = 0\n",
    "    with torch.no_grad():\n",
    "        for data_batch in data_loader: # Renamed data to data_batch\n",
    "            inputs, labels = [d.to(device_to_eval) for d in data_batch]\n",
    "            outputs = model(inputs); _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0); correct += (predicted == labels).sum().item()\n",
    "    accuracy_val = 0\n",
    "    if total > 0: accuracy_val = 100 * correct / total\n",
    "    print(f\"{eval_mode_str} Accuracy: {accuracy_val:.2f}%\")\n",
    "    return {'macs': macs, 'size_mb': size_mb, 'accuracy': accuracy_val}"
   ],
   "id": "bc63e176a8657f07",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prune the model\n",
   "id": "559bdffab16bafab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T18:22:56.889889Z",
     "start_time": "2025-05-09T18:22:56.884524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prune_model(model, example_input, target_macs_unused, strategy, iterative_steps=5): # target_macs unused currently\n",
    "    pruning_ratio = 0.1 if isinstance(strategy['importance'], tp.importance.TaylorImportance) else 0.5\n",
    "    ignored_layers_list = [model.linear] if hasattr(model, 'linear') else ([model.fc] if hasattr(model, 'fc') else [])\n",
    "    pruner = strategy['pruner'](\n",
    "        model, example_input, importance=strategy['importance'], iterative_steps=iterative_steps,\n",
    "        ch_sparsity=pruning_ratio, root_module_types=[nn.Conv2d], ignored_layers=ignored_layers_list,\n",
    "    )\n",
    "    current_macs, base_nparams = calculate_macs(model, example_input); initial_macs_log = current_macs\n",
    "    for i in range(iterative_steps):\n",
    "        if isinstance(strategy['importance'], tp.importance.TaylorImportance):\n",
    "            is_training = model.training; model.train()\n",
    "            loss = model(example_input).sum(); loss.backward()\n",
    "            if not is_training: model.eval()\n",
    "        pruning_occurred = False\n",
    "        for g in pruner.step(interactive=True): g.prune(); pruning_occurred = True\n",
    "        if not pruning_occurred: print(f\"  Iter {i+1}/{iterative_steps}, No prunable groups. Stopping.\"); break\n",
    "        macs, nparams = calculate_macs(model, example_input)\n",
    "        print(f\"  Iter {i+1}/{iterative_steps}, Params: {base_nparams/1e6:.2f}M->{nparams/1e6:.2f}M, MACs: {initial_macs_log/1e9:.2f}G->{macs/1e9:.2f}G\")\n",
    "        initial_macs_log = macs; base_nparams = nparams\n",
    "    return model"
   ],
   "id": "b7bd2fe38bb39c2c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train the model",
   "id": "1bbe7172b741a9b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T18:23:01.723024Z",
     "start_time": "2025-05-09T18:23:01.710108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy # Ensure copy is imported\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
    "                device, num_epochs, model_name=\"Model\", patience=10): # Added patience parameter\n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    epochs_no_improve = 0  # Counter for epochs without validation improvement\n",
    "\n",
    "    print(f\"Starting training for {model_name} with patience={patience}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Optional: More frequent logging if needed\n",
    "            # if batch_idx % 100 == 0 and batch_idx > 0:\n",
    "            #     print(f\"{model_name} - Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, \"\n",
    "            #           f\"Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "        epoch_train_loss = running_loss / len(train_loader)\n",
    "        epoch_train_acc = 100 * correct_train / total_train\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss_val_batch = criterion(outputs, labels) # Use a different var name for batch val loss\n",
    "                val_loss += loss_val_batch.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_val_loss = val_loss / len(val_loader)\n",
    "        epoch_val_acc = 100 * correct_val / total_val\n",
    "\n",
    "        print(f\"{model_name} - Epoch {epoch+1}/{num_epochs}: Train Loss={epoch_train_loss:.4f}, Train Acc={epoch_train_acc:.2f}% | \"\n",
    "              f\"Val Loss={epoch_val_loss:.4f}, Val Acc={epoch_val_acc:.2f}% | LR: {optimizer.param_groups[0]['lr']:.1e}\")\n",
    "\n",
    "        if scheduler:\n",
    "            if isinstance(scheduler, ReduceLROnPlateau):\n",
    "                scheduler.step(epoch_val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "\n",
    "        # Early stopping logic\n",
    "        if epoch_val_acc > best_val_acc:\n",
    "            best_val_acc = epoch_val_acc\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            epochs_no_improve = 0 # Reset counter\n",
    "            print(f\"****** New best validation accuracy for {model_name}: {best_val_acc:.2f}% (Epoch {epoch+1}) ******\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"Validation accuracy did not improve for {epochs_no_improve} epoch(s). Best: {best_val_acc:.2f}%\")\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered for {model_name} after {patience} epochs without improvement.\")\n",
    "            break # Exit the training loop\n",
    "\n",
    "    if best_model_state:\n",
    "        print(f\"Finished training {model_name}. Best validation accuracy achieved: {best_val_acc:.2f}%\")\n",
    "        model.load_state_dict(best_model_state)\n",
    "    else:\n",
    "        print(f\"Warning: {model_name} - No best model state recorded. This might happen if training stops very early or val_acc never improves.\")\n",
    "\n",
    "    return model"
   ],
   "id": "d69fc3f1c05ff087",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Main workflow",
   "id": "507a033ad415de65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T18:23:06.755318Z",
     "start_time": "2025-05-09T18:23:06.743176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    CURRENT_MODEL_NAME = \"resnet18\"\n",
    "    MODEL_CLASS = ResNet18\n",
    "    NUM_CLASSES = 10\n",
    "\n",
    "    config = {\n",
    "        'strategies': {\n",
    "            'magnitude': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.MagnitudeImportance(p=2)},\n",
    "            'bn_scale': {'pruner': tp.pruner.BNScalePruner, 'importance': tp.importance.BNScaleImportance()},\n",
    "            'group_norm': {'pruner': tp.pruner.GroupNormPruner, 'importance': tp.importance.GroupMagnitudeImportance(p=1)},\n",
    "            'random': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.RandomImportance()},\n",
    "            'Taylor': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.TaylorImportance()},\n",
    "            # 'Hessian': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.GroupHessianImportance()}, # Can be slow\n",
    "            'lamp': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.LAMPImportance(p=2)},\n",
    "            'geometry': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.FPGMImportance()}\n",
    "        },\n",
    "        'target_macs_sparsity': 0.5,\n",
    "        'train_epochs': 30,  # Increased initial training epochs\n",
    "        'fine_tune_epochs': 50, # Significantly increased fine-tune epochs\n",
    "        'data_dir': './data',\n",
    "        'output_dir': f'./output_resnet_finetuned/{CURRENT_MODEL_NAME}/strategies',\n",
    "        'iterative_steps': 15,\n",
    "        'learning_rate_initial': 0.001,\n",
    "        'learning_rate_finetune': 0.0005, # Initial LR for fine-tuning, scheduler will adjust\n",
    "        'validation_split_ratio': 0.1, # Use 10% of training data for validation\n",
    "        'early_stopping_patience': 10,\n",
    "    }\n",
    "    os.makedirs(config['output_dir'], exist_ok=True)\n",
    "\n",
    "    model = MODEL_CLASS(num_classes=NUM_CLASSES).to(device)\n",
    "    train_loader, val_loader, test_loader = get_data_loaders(\n",
    "        config['data_dir'], validation_split=config['validation_split_ratio']\n",
    "    )\n",
    "    if train_loader is None: # In case get_data_loaders returns None on error\n",
    "        print(\"Failed to load data. Exiting.\")\n",
    "        return\n",
    "    example_input = torch.randn(1, 3, 32, 32).to(device)\n",
    "\n",
    "    initial_model_path = os.path.join(config['output_dir'], f\"{CURRENT_MODEL_NAME}_initial.pth\")\n",
    "\n",
    "    if not os.path.exists(initial_model_path):\n",
    "        print(f\"--- Initial Training for {CURRENT_MODEL_NAME} ---\")\n",
    "        optimizer_initial = optim.Adam(model.parameters(), lr=config['learning_rate_initial'])\n",
    "        # Scheduler for initial training (optional, but can be good)\n",
    "        scheduler_initial = ReduceLROnPlateau(optimizer_initial, mode='max', factor=0.1, patience=5, verbose=True)\n",
    "        model = train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader, # Use val_loader\n",
    "            criterion=nn.CrossEntropyLoss().to(device),\n",
    "            optimizer=optimizer_initial,\n",
    "            scheduler=scheduler_initial, # Pass scheduler\n",
    "            device=device,\n",
    "            num_epochs=config['train_epochs'],\n",
    "            patience=config['early_stopping_patience'], # Pass patience\n",
    "            model_name=f\"{CURRENT_MODEL_NAME} Initial\"\n",
    "        )\n",
    "        save_model_state(model, initial_model_path, example_input)\n",
    "    else:\n",
    "        print(f\"Loading existing initial model from {initial_model_path}\")\n",
    "        model = load_model_state(MODEL_CLASS, NUM_CLASSES, initial_model_path, device)\n",
    "\n",
    "    results = {}\n",
    "    print(f\"\\n--- Evaluating Initial {CURRENT_MODEL_NAME} on Test Set ---\")\n",
    "    # Evaluate initial model on the actual test set\n",
    "    initial_metrics = evaluate_model(model, test_loader, example_input, device, eval_mode_str=f\"{CURRENT_MODEL_NAME} Initial Test\")\n",
    "    results['initial'] = initial_metrics\n",
    "    print(f\"Initial Test Metrics: {initial_metrics}\")\n",
    "\n",
    "    initial_macs, _ = calculate_macs(model, example_input)\n",
    "    target_macs_for_prune = initial_macs * config['target_macs_sparsity']\n",
    "\n",
    "    # Pruning targets for GR and GEM (can be made more dynamic)\n",
    "    target_macs_gr_gem = initial_macs * 0.3\n",
    "    target_size_mb_gr = results['initial']['size_mb'] * 0.3\n",
    "    target_params_gem = sum(p.numel() for p in model.parameters()) * 0.3\n",
    "    CHOSEN_PRUNING_FUNCTION = \"gr\" # \"simple\", \"gr\", or \"gem\"\n",
    "\n",
    "    for strategy_name, strategy_details in config['strategies'].items():\n",
    "        print(f\"\\n--- Pruning {CURRENT_MODEL_NAME} with Strategy: {strategy_name} ---\")\n",
    "        model_copy = load_model_state(MODEL_CLASS, NUM_CLASSES, initial_model_path, device)\n",
    "\n",
    "        if CHOSEN_PRUNING_FUNCTION == \"simple\":\n",
    "            pruned_model = prune_model(\n",
    "                model=model_copy,\n",
    "                example_input=example_input,\n",
    "                target_macs_unused=target_macs_for_prune,\n",
    "                strategy=strategy_details,\n",
    "                iterative_steps=config['iterative_steps'],\n",
    "            )\n",
    "        elif CHOSEN_PRUNING_FUNCTION == \"gr\":\n",
    "            pruned_model = gr_prune_model_with_threshold(\n",
    "                model=model_copy,\n",
    "                example_input=example_input,\n",
    "                target_macs=target_macs_gr_gem,\n",
    "                target_size_mb=target_size_mb_gr,\n",
    "                strategy=strategy_details,\n",
    "                iterative_steps=config['iterative_steps']\n",
    "            )\n",
    "        elif CHOSEN_PRUNING_FUNCTION == \"gem\":\n",
    "            pruned_model = gem_prune_model_by_threshold(\n",
    "                model=model_copy,\n",
    "                example_input=example_input,\n",
    "                target_macs=target_macs_gr_gem,\n",
    "                target_params=target_params_gem,\n",
    "                strategy=strategy_details,\n",
    "                max_iterations=20,\n",
    "                step_ch_sparsity=0.2\n",
    "            )\n",
    "        else: raise ValueError(f\"Unknown CHOSEN_PRUNING_FUNCTION: {CHOSEN_PRUNING_FUNCTION}\")\n",
    "\n",
    "\n",
    "        pruned_path = os.path.join(config['output_dir'], f\"{CURRENT_MODEL_NAME}_{strategy_name}_pruned.pth\")\n",
    "        save_model_state(pruned_model, pruned_path, example_input)\n",
    "\n",
    "        print(f\"\\n--- Fine-tuning {CURRENT_MODEL_NAME} after {strategy_name} pruning ---\")\n",
    "        # Use a new optimizer and scheduler for fine-tuning each pruned model\n",
    "        optimizer_ft = optim.Adam(pruned_model.parameters(), lr=config['learning_rate_finetune'])\n",
    "        scheduler_ft = ReduceLROnPlateau(optimizer_ft, mode='max', factor=0.2, patience=5, verbose=True, min_lr=1e-6)\n",
    "\n",
    "        fine_tuned_model = train_model(\n",
    "            model=pruned_model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader, # Use val_loader\n",
    "            criterion=nn.CrossEntropyLoss().to(device),\n",
    "            optimizer=optimizer_ft,\n",
    "            scheduler=scheduler_ft, # Pass scheduler\n",
    "            device=device,\n",
    "            num_epochs=config['fine_tune_epochs'],\n",
    "            patience=config['early_stopping_patience'], # Pass patience\n",
    "            model_name=f\"{CURRENT_MODEL_NAME} Pruned-{strategy_name}\"\n",
    "        )\n",
    "\n",
    "        print(f\"\\n--- Evaluating fine-tuned {CURRENT_MODEL_NAME} ({strategy_name}) on Test Set ---\")\n",
    "        # Evaluate the fine-tuned model (which has the best validation weights) on the test set\n",
    "        results[strategy_name] = evaluate_model(\n",
    "            model=fine_tuned_model,\n",
    "            data_loader=test_loader, # Use actual test_loader for final eval\n",
    "            example_input=example_input,\n",
    "            device_to_eval=device,\n",
    "            eval_mode_str=f\"{CURRENT_MODEL_NAME} FineTuned-{strategy_name} Test\"\n",
    "        )\n",
    "        print(f\"Test Metrics for {strategy_name}: {results[strategy_name]}\")\n",
    "\n",
    "        final_path = os.path.join(config['output_dir'], f\"{CURRENT_MODEL_NAME}_{strategy_name}_final_best_val.pth\")\n",
    "        save_model_state(fine_tuned_model, final_path, example_input) # Model already has best val weights\n",
    "\n",
    "    compare_results_and_plot(results, output_dir=config['output_dir'])\n",
    "    print(\"ResNet pruning and enhanced fine-tuning workflow completed successfully!\")"
   ],
   "id": "15165bf722d638e1",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### GR Pruning Model",
   "id": "1398be86380afb3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T18:23:12.300623Z",
     "start_time": "2025-05-09T18:23:12.290389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gr_prune_model_with_threshold(model, example_input, target_macs, target_size_mb, strategy, iterative_steps=5):\n",
    "    current_macs, nparams = tp.utils.count_ops_and_params(model, example_input)\n",
    "    current_size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 ** 2)\n",
    "    print(f\"Initial MACs: {current_macs / 1e9:.3f} G, Size: {current_size_mb:.2f} MB\")\n",
    "\n",
    "    pruning_ratio = 0.5\n",
    "    ignored_layers_list = [model.linear] if hasattr(model, 'linear') else []\n",
    "    if hasattr(model, 'fc') and model.fc not in ignored_layers_list:\n",
    "        ignored_layers_list.append(model.fc)\n",
    "\n",
    "\n",
    "    if isinstance(strategy['importance'], tp.importance.TaylorImportance):\n",
    "        pruner = strategy['pruner'](\n",
    "            model, example_input, importance=strategy['importance'],\n",
    "            iterative_steps=1, ch_sparsity=pruning_ratio / iterative_steps if iterative_steps > 0 else pruning_ratio,\n",
    "            root_module_types=[nn.Conv2d], ignored_layers=ignored_layers_list\n",
    "        )\n",
    "        for i in range(iterative_steps):\n",
    "            is_training = model.training\n",
    "            model.train()\n",
    "            model.zero_grad()\n",
    "            loss = model(example_input).sum()\n",
    "            loss.backward()\n",
    "            if not is_training: model.eval()\n",
    "\n",
    "            pruned_something = False\n",
    "            for group in pruner.step(interactive=True):\n",
    "                group.prune()\n",
    "                pruned_something = True\n",
    "\n",
    "            if not pruned_something and i > 0 : # Don't break on first iter if nothing found\n",
    "                print(f\"Step {i+1}/{iterative_steps}: No more prunable elements found.\")\n",
    "                break\n",
    "\n",
    "\n",
    "            current_macs, _ = tp.utils.count_ops_and_params(model, example_input)\n",
    "            current_size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 ** 2)\n",
    "            print(f\"Step {i+1}/{iterative_steps}: MACs {current_macs / 1e9:.3f} G, Size {current_size_mb:.2f} MB\")\n",
    "            if current_macs <= target_macs and current_size_mb <= target_size_mb:\n",
    "                print(f\"Targets reached at step {i+1}\")\n",
    "                break\n",
    "            model.zero_grad() # important for Taylor if used in loop without re-instantiating pruner\n",
    "    else:\n",
    "        pruner = strategy['pruner'](\n",
    "            model, example_input, importance=strategy['importance'],\n",
    "            iterative_steps=iterative_steps, ch_sparsity=pruning_ratio,\n",
    "            root_module_types=[nn.Conv2d], ignored_layers=ignored_layers_list\n",
    "        )\n",
    "        # pruner.step() # This typically means run all iterative steps internally.\n",
    "        # If you want fine-grained control like Taylor above, loop pruner.step()\n",
    "        for i in range(iterative_steps):\n",
    "            pruned_something = False\n",
    "            for group in pruner.step(interactive=True): # Use interactive for explicit pruning\n",
    "                group.prune()\n",
    "                pruned_something = True\n",
    "\n",
    "            if not pruned_something and i > 0:\n",
    "                 print(f\"Step {i+1}/{iterative_steps}: No more prunable elements found for non-Taylor strategy.\")\n",
    "                 break\n",
    "\n",
    "            current_macs_step, _ = tp.utils.count_ops_and_params(model, example_input)\n",
    "            current_size_mb_step = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 ** 2)\n",
    "            print(f\"Step {i+1}/{iterative_steps} (Non-Taylor): MACs {current_macs_step / 1e9:.3f} G, Size {current_size_mb_step:.2f} MB\")\n",
    "            if current_macs_step <= target_macs and current_size_mb_step <= target_size_mb:\n",
    "                print(f\"Targets reached at step {i+1} for non-Taylor strategy.\")\n",
    "                break\n",
    "\n",
    "\n",
    "    final_macs, _ = tp.utils.count_ops_and_params(model, example_input)\n",
    "    final_size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 ** 2)\n",
    "    print(f\"After pruning: MACs {final_macs / 1e9:.3f} G, Size {final_size_mb:.2f} MB\")\n",
    "\n",
    "    if final_macs <= target_macs and final_size_mb <= target_size_mb:\n",
    "        print(\"Pruning targets achieved.\")\n",
    "    else:\n",
    "        print(\"Warning: Pruning targets not fully achieved.\")\n",
    "    return model"
   ],
   "id": "6b7eb134043b7290",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### gem pruning model",
   "id": "40607f616072024d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T18:23:16.627955Z",
     "start_time": "2025-05-09T18:23:16.617213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gem_prune_model_by_threshold(model, example_input, target_macs, target_params, strategy, max_iterations=100, step_ch_sparsity=0.1):\n",
    "    device = example_input.device\n",
    "    model.to(device)\n",
    "\n",
    "    print(f\"--- Starting Pruning (Strategy: {strategy['importance'].__class__.__name__}) ---\")\n",
    "    print(f\"Target MACs: {target_macs:,.0f}, Target Params: {target_params:,.0f}\")\n",
    "\n",
    "    ignored_layers_list = [model.linear] if hasattr(model, 'linear') else []\n",
    "    if hasattr(model, 'fc') and model.fc not in ignored_layers_list:\n",
    "        ignored_layers_list.append(model.fc)\n",
    "\n",
    "    pruner = strategy['pruner'](\n",
    "        model, example_input, importance=strategy['importance'],\n",
    "        ch_sparsity=step_ch_sparsity, root_module_types=[nn.Conv2d],\n",
    "        ignored_layers=ignored_layers_list,\n",
    "        # round_to=8, # Optional\n",
    "    )\n",
    "\n",
    "    current_macs, current_params = calculate_macs(model, example_input)\n",
    "    initial_macs, initial_params = current_macs, current_params\n",
    "    print(f\"Initial | MACs: {current_macs:,.0f}, Params: {current_params:,.0f}\")\n",
    "\n",
    "    iteration = 0\n",
    "    # model.eval() # Moved inside loop for Taylor/Hessian\n",
    "\n",
    "    while (current_macs > target_macs or current_params > target_params) and iteration < max_iterations:\n",
    "        iteration += 1\n",
    "        macs_before_step = current_macs\n",
    "        params_before_step = current_params\n",
    "\n",
    "        original_mode_is_train = model.training # Store original mode\n",
    "\n",
    "        if isinstance(strategy['importance'], (tp.importance.TaylorImportance, tp.importance.GroupHessianImportance)):\n",
    "             model.train()\n",
    "             for param in model.parameters():\n",
    "                 param.requires_grad_(True)\n",
    "             loss = model(example_input).mean()\n",
    "             try:\n",
    "                 model.zero_grad() # Zero gradients before backward\n",
    "                 loss.backward()\n",
    "             except Exception as e:\n",
    "                 print(f\"Error during backward pass for importance calc (Iter {iteration}): {e}\")\n",
    "                 if not original_mode_is_train: model.eval() # Restore mode\n",
    "                 break\n",
    "        else: # Ensure eval mode for other strategies\n",
    "            model.eval()\n",
    "\n",
    "\n",
    "        try:\n",
    "            pruning_groups = list(pruner.step(interactive=True))\n",
    "        except Exception as e:\n",
    "             print(f\"Error during pruner.step() (Iter {iteration}): {e}\")\n",
    "             if not original_mode_is_train and isinstance(strategy['importance'], (tp.importance.TaylorImportance, tp.importance.GroupHessianImportance)):\n",
    "                 model.eval() # Restore eval mode\n",
    "             elif original_mode_is_train and not isinstance(strategy['importance'], (tp.importance.TaylorImportance, tp.importance.GroupHessianImportance)):\n",
    "                 model.train() # Restore train mode if it was initially training\n",
    "             break\n",
    "\n",
    "        if not pruning_groups:\n",
    "            print(f\"Iteration {iteration}: Pruner found no more candidates. Stopping.\")\n",
    "            break\n",
    "\n",
    "        for group in pruning_groups:\n",
    "            group.prune()\n",
    "\n",
    "        if isinstance(strategy['importance'], (tp.importance.TaylorImportance, tp.importance.GroupHessianImportance)):\n",
    "            model.zero_grad() # Clean up gradients\n",
    "            if not original_mode_is_train: # If model was originally in eval, set it back\n",
    "                 model.eval()\n",
    "        elif original_mode_is_train: # If it was originally training and not Taylor/Hessian\n",
    "            model.train()\n",
    "\n",
    "\n",
    "        current_macs, current_params = calculate_macs(model, example_input)\n",
    "        print(\n",
    "            f\"Iter {iteration: >3}/{max_iterations} | \"\n",
    "            f\"MACs: {macs_before_step:,.0f} -> {current_macs:,.0f} \"\n",
    "            f\"({(macs_before_step-current_macs)/macs_before_step*100 if macs_before_step > 0 else 0:+.1f}%) | \"\n",
    "            f\"Params: {params_before_step:,.0f} -> {current_params:,.0f} \"\n",
    "            f\"({(params_before_step-current_params)/params_before_step*100 if params_before_step > 0 else 0:+.1f}%)\"\n",
    "        )\n",
    "\n",
    "        if current_macs >= macs_before_step and current_params >= params_before_step and iteration > 1:\n",
    "            print(f\"Iteration {iteration}: No reduction. Stopping.\")\n",
    "            break\n",
    "\n",
    "        if not original_mode_is_train : model.eval() # Ensure eval mode if it was initially\n",
    "\n",
    "    print(f\"--- Finished Pruning (Strategy: {strategy['importance'].__class__.__name__}) ---\")\n",
    "    if iteration >= max_iterations:\n",
    "        print(f\"Warning: Reached maximum pruning iterations ({max_iterations}).\")\n",
    "\n",
    "    final_macs, final_params = calculate_macs(model, example_input)\n",
    "    print(f\"Initial | MACs: {initial_macs:,.0f}, Params: {initial_params:,.0f}\")\n",
    "    print(f\"Final   | MACs: {final_macs:,.0f}, Params: {final_params:,.0f}\")\n",
    "    print(f\"Target  | MACs: {target_macs:,.0f}, Params: {target_params:,.0f}\")\n",
    "\n",
    "    macs_reduction = (initial_macs - final_macs) / initial_macs * 100 if initial_macs > 0 else 0\n",
    "    params_reduction = (initial_params - final_params) / initial_params * 100 if initial_params > 0 else 0\n",
    "    print(f\"Reduction | MACs: {macs_reduction:.2f}%, Params: {params_reduction:.2f}%\")\n",
    "\n",
    "    if final_macs > target_macs or final_params > target_params:\n",
    "         print(\"Warning: Pruning finished, but target threshold(s) were not fully met.\")\n",
    "\n",
    "    model.eval() # Ensure final model state is eval\n",
    "    return model\n"
   ],
   "id": "69e1ec0dac84318e",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T18:46:35.725887Z",
     "start_time": "2025-05-09T18:23:29.319611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "abe4e338df2d4e9f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset directory: /home/muis/thesis/github-repo/master-thesis/cnn/resNet/data\n",
      "Splitting training data: 45000 for training (with augmentation), 5000 for validation.\n",
      "--- Initial Training for resnet18 ---\n",
      "Starting training for resnet18 Initial with patience=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet18 Initial - Epoch 1/30: Train Loss=1.5045, Train Acc=44.74% | Val Loss=1.2184, Val Acc=55.32% | LR: 1.0e-03\n",
      "****** New best validation accuracy for resnet18 Initial: 55.32% (Epoch 1) ******\n",
      "resnet18 Initial - Epoch 2/30: Train Loss=1.0273, Train Acc=63.27% | Val Loss=0.9467, Val Acc=66.22% | LR: 1.0e-03\n",
      "****** New best validation accuracy for resnet18 Initial: 66.22% (Epoch 2) ******\n",
      "resnet18 Initial - Epoch 3/30: Train Loss=0.8240, Train Acc=70.76% | Val Loss=0.7191, Val Acc=76.20% | LR: 1.0e-03\n",
      "****** New best validation accuracy for resnet18 Initial: 76.20% (Epoch 3) ******\n",
      "resnet18 Initial - Epoch 4/30: Train Loss=0.6754, Train Acc=76.54% | Val Loss=0.7447, Val Acc=75.12% | LR: 1.0e-03\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 76.20%\n",
      "resnet18 Initial - Epoch 5/30: Train Loss=0.5830, Train Acc=79.77% | Val Loss=0.5466, Val Acc=81.22% | LR: 1.0e-03\n",
      "****** New best validation accuracy for resnet18 Initial: 81.22% (Epoch 5) ******\n",
      "resnet18 Initial - Epoch 6/30: Train Loss=0.5088, Train Acc=82.35% | Val Loss=0.5279, Val Acc=82.02% | LR: 1.0e-03\n",
      "****** New best validation accuracy for resnet18 Initial: 82.02% (Epoch 6) ******\n",
      "resnet18 Initial - Epoch 7/30: Train Loss=0.4593, Train Acc=84.33% | Val Loss=0.4818, Val Acc=84.06% | LR: 1.0e-03\n",
      "****** New best validation accuracy for resnet18 Initial: 84.06% (Epoch 7) ******\n",
      "resnet18 Initial - Epoch 8/30: Train Loss=0.3252, Train Acc=88.86% | Val Loss=0.3452, Val Acc=88.04% | LR: 1.0e-04\n",
      "****** New best validation accuracy for resnet18 Initial: 88.04% (Epoch 8) ******\n",
      "resnet18 Initial - Epoch 9/30: Train Loss=0.2920, Train Acc=89.90% | Val Loss=0.3290, Val Acc=88.34% | LR: 1.0e-04\n",
      "****** New best validation accuracy for resnet18 Initial: 88.34% (Epoch 9) ******\n",
      "resnet18 Initial - Epoch 10/30: Train Loss=0.2777, Train Acc=90.29% | Val Loss=0.3337, Val Acc=88.92% | LR: 1.0e-04\n",
      "****** New best validation accuracy for resnet18 Initial: 88.92% (Epoch 10) ******\n",
      "resnet18 Initial - Epoch 11/30: Train Loss=0.2629, Train Acc=90.88% | Val Loss=0.3389, Val Acc=88.62% | LR: 1.0e-04\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 88.92%\n",
      "resnet18 Initial - Epoch 12/30: Train Loss=0.2505, Train Acc=91.38% | Val Loss=0.3189, Val Acc=88.86% | LR: 1.0e-04\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 88.92%\n",
      "resnet18 Initial - Epoch 13/30: Train Loss=0.2363, Train Acc=91.66% | Val Loss=0.3101, Val Acc=89.28% | LR: 1.0e-04\n",
      "****** New best validation accuracy for resnet18 Initial: 89.28% (Epoch 13) ******\n",
      "resnet18 Initial - Epoch 14/30: Train Loss=0.2148, Train Acc=92.59% | Val Loss=0.3077, Val Acc=89.66% | LR: 1.0e-05\n",
      "****** New best validation accuracy for resnet18 Initial: 89.66% (Epoch 14) ******\n",
      "resnet18 Initial - Epoch 15/30: Train Loss=0.2118, Train Acc=92.59% | Val Loss=0.3076, Val Acc=89.72% | LR: 1.0e-05\n",
      "****** New best validation accuracy for resnet18 Initial: 89.72% (Epoch 15) ******\n",
      "resnet18 Initial - Epoch 16/30: Train Loss=0.2096, Train Acc=92.76% | Val Loss=0.3074, Val Acc=89.38% | LR: 1.0e-05\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 89.72%\n",
      "resnet18 Initial - Epoch 17/30: Train Loss=0.2064, Train Acc=92.82% | Val Loss=0.3070, Val Acc=89.64% | LR: 1.0e-05\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 89.72%\n",
      "resnet18 Initial - Epoch 18/30: Train Loss=0.2046, Train Acc=92.88% | Val Loss=0.3165, Val Acc=89.62% | LR: 1.0e-05\n",
      "Validation accuracy did not improve for 3 epoch(s). Best: 89.72%\n",
      "resnet18 Initial - Epoch 19/30: Train Loss=0.2069, Train Acc=92.84% | Val Loss=0.3104, Val Acc=89.76% | LR: 1.0e-05\n",
      "****** New best validation accuracy for resnet18 Initial: 89.76% (Epoch 19) ******\n",
      "resnet18 Initial - Epoch 20/30: Train Loss=0.2029, Train Acc=92.95% | Val Loss=0.3143, Val Acc=89.72% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 89.76%\n",
      "resnet18 Initial - Epoch 21/30: Train Loss=0.1996, Train Acc=93.09% | Val Loss=0.3134, Val Acc=89.72% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 89.76%\n",
      "resnet18 Initial - Epoch 22/30: Train Loss=0.2011, Train Acc=93.09% | Val Loss=0.3084, Val Acc=89.78% | LR: 1.0e-06\n",
      "****** New best validation accuracy for resnet18 Initial: 89.78% (Epoch 22) ******\n",
      "resnet18 Initial - Epoch 23/30: Train Loss=0.2018, Train Acc=93.02% | Val Loss=0.3091, Val Acc=89.70% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 89.78%\n",
      "resnet18 Initial - Epoch 24/30: Train Loss=0.2013, Train Acc=93.04% | Val Loss=0.3112, Val Acc=89.82% | LR: 1.0e-06\n",
      "****** New best validation accuracy for resnet18 Initial: 89.82% (Epoch 24) ******\n",
      "resnet18 Initial - Epoch 25/30: Train Loss=0.1994, Train Acc=93.08% | Val Loss=0.3109, Val Acc=89.72% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 89.82%\n",
      "resnet18 Initial - Epoch 26/30: Train Loss=0.2019, Train Acc=92.96% | Val Loss=0.3121, Val Acc=89.94% | LR: 1.0e-07\n",
      "****** New best validation accuracy for resnet18 Initial: 89.94% (Epoch 26) ******\n",
      "resnet18 Initial - Epoch 27/30: Train Loss=0.1975, Train Acc=93.09% | Val Loss=0.3060, Val Acc=89.94% | LR: 1.0e-07\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 89.94%\n",
      "resnet18 Initial - Epoch 28/30: Train Loss=0.2014, Train Acc=93.03% | Val Loss=0.3133, Val Acc=89.88% | LR: 1.0e-07\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 89.94%\n",
      "resnet18 Initial - Epoch 29/30: Train Loss=0.1957, Train Acc=93.22% | Val Loss=0.3047, Val Acc=89.60% | LR: 1.0e-07\n",
      "Validation accuracy did not improve for 3 epoch(s). Best: 89.94%\n",
      "resnet18 Initial - Epoch 30/30: Train Loss=0.2004, Train Acc=92.94% | Val Loss=0.3084, Val Acc=89.66% | LR: 1.0e-07\n",
      "Validation accuracy did not improve for 4 epoch(s). Best: 89.94%\n",
      "Finished training resnet18 Initial. Best validation accuracy achieved: 89.94%\n",
      "âœ… Model saved as ONNX to ./output_resnet_finetuned/resnet18/strategies/resnet18_initial.onnx\n",
      "\n",
      "--- Evaluating Initial resnet18 on Test Set ---\n",
      "resnet18 Initial Test Accuracy: 89.47%\n",
      "Initial Test Metrics: {'macs': 556651530.0, 'size_mb': 44.695848, 'accuracy': 89.47}\n",
      "\n",
      "--- Pruning resnet18 with Strategy: magnitude ---\n",
      "Initial MACs: 0.557 G, Size: 42.63 MB\n",
      "Step 1/15 (Non-Taylor): MACs 0.514 G, Size 39.65 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch_pruning/pruner/algorithms/base_pruner.py:87: UserWarning: ch_sparsity is deprecated in v1.3.0. Please use pruning_ratio.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2/15 (Non-Taylor): MACs 0.479 G, Size 36.94 MB\n",
      "Step 3/15 (Non-Taylor): MACs 0.447 G, Size 34.40 MB\n",
      "Step 4/15 (Non-Taylor): MACs 0.414 G, Size 31.85 MB\n",
      "Step 5/15 (Non-Taylor): MACs 0.384 G, Size 29.50 MB\n",
      "Step 6/15 (Non-Taylor): MACs 0.354 G, Size 27.17 MB\n",
      "Step 7/15 (Non-Taylor): MACs 0.327 G, Size 25.00 MB\n",
      "Step 8/15 (Non-Taylor): MACs 0.294 G, Size 22.81 MB\n",
      "Step 9/15 (Non-Taylor): MACs 0.269 G, Size 20.83 MB\n",
      "Step 10/15 (Non-Taylor): MACs 0.245 G, Size 18.88 MB\n",
      "Step 11/15 (Non-Taylor): MACs 0.222 G, Size 17.08 MB\n",
      "Step 12/15 (Non-Taylor): MACs 0.198 G, Size 15.29 MB\n",
      "Step 13/15 (Non-Taylor): MACs 0.178 G, Size 13.68 MB\n",
      "Step 14/15 (Non-Taylor): MACs 0.158 G, Size 12.11 MB\n",
      "Targets reached at step 14 for non-Taylor strategy.\n",
      "After pruning: MACs 0.158 G, Size 12.11 MB\n",
      "Pruning targets achieved.\n",
      "âœ… Model saved as ONNX to ./output_resnet_finetuned/resnet18/strategies/resnet18_magnitude_pruned.onnx\n",
      "\n",
      "--- Fine-tuning resnet18 after magnitude pruning ---\n",
      "Starting training for resnet18 Pruned-magnitude with patience=10\n",
      "resnet18 Pruned-magnitude - Epoch 1/50: Train Loss=0.4511, Train Acc=84.62% | Val Loss=0.4173, Val Acc=86.38% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-magnitude: 86.38% (Epoch 1) ******\n",
      "resnet18 Pruned-magnitude - Epoch 2/50: Train Loss=0.3535, Train Acc=87.92% | Val Loss=0.4023, Val Acc=86.50% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-magnitude: 86.50% (Epoch 2) ******\n",
      "resnet18 Pruned-magnitude - Epoch 3/50: Train Loss=0.3242, Train Acc=88.85% | Val Loss=0.3871, Val Acc=87.30% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-magnitude: 87.30% (Epoch 3) ******\n",
      "resnet18 Pruned-magnitude - Epoch 4/50: Train Loss=0.3027, Train Acc=89.42% | Val Loss=0.3835, Val Acc=87.10% | LR: 5.0e-04\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 87.30%\n",
      "resnet18 Pruned-magnitude - Epoch 5/50: Train Loss=0.2865, Train Acc=90.03% | Val Loss=0.3774, Val Acc=87.82% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-magnitude: 87.82% (Epoch 5) ******\n",
      "resnet18 Pruned-magnitude - Epoch 6/50: Train Loss=0.2662, Train Acc=90.77% | Val Loss=0.3648, Val Acc=87.98% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-magnitude: 87.98% (Epoch 6) ******\n",
      "resnet18 Pruned-magnitude - Epoch 7/50: Train Loss=0.2547, Train Acc=91.05% | Val Loss=0.3748, Val Acc=88.06% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-magnitude: 88.06% (Epoch 7) ******\n",
      "resnet18 Pruned-magnitude - Epoch 8/50: Train Loss=0.1864, Train Acc=93.50% | Val Loss=0.3019, Val Acc=89.94% | LR: 1.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-magnitude: 89.94% (Epoch 8) ******\n",
      "resnet18 Pruned-magnitude - Epoch 9/50: Train Loss=0.1677, Train Acc=94.12% | Val Loss=0.3051, Val Acc=90.38% | LR: 1.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-magnitude: 90.38% (Epoch 9) ******\n",
      "resnet18 Pruned-magnitude - Epoch 10/50: Train Loss=0.1568, Train Acc=94.55% | Val Loss=0.3335, Val Acc=89.58% | LR: 1.0e-04\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 90.38%\n",
      "resnet18 Pruned-magnitude - Epoch 11/50: Train Loss=0.1530, Train Acc=94.64% | Val Loss=0.3197, Val Acc=89.88% | LR: 1.0e-04\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 90.38%\n",
      "resnet18 Pruned-magnitude - Epoch 12/50: Train Loss=0.1474, Train Acc=94.85% | Val Loss=0.3109, Val Acc=90.38% | LR: 1.0e-04\n",
      "Validation accuracy did not improve for 3 epoch(s). Best: 90.38%\n",
      "resnet18 Pruned-magnitude - Epoch 13/50: Train Loss=0.1385, Train Acc=95.17% | Val Loss=0.3199, Val Acc=90.20% | LR: 1.0e-04\n",
      "Validation accuracy did not improve for 4 epoch(s). Best: 90.38%\n",
      "resnet18 Pruned-magnitude - Epoch 14/50: Train Loss=0.1280, Train Acc=95.61% | Val Loss=0.3141, Val Acc=90.74% | LR: 2.0e-05\n",
      "****** New best validation accuracy for resnet18 Pruned-magnitude: 90.74% (Epoch 14) ******\n",
      "resnet18 Pruned-magnitude - Epoch 15/50: Train Loss=0.1228, Train Acc=95.63% | Val Loss=0.3014, Val Acc=90.64% | LR: 2.0e-05\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 90.74%\n",
      "resnet18 Pruned-magnitude - Epoch 16/50: Train Loss=0.1210, Train Acc=95.76% | Val Loss=0.3171, Val Acc=90.80% | LR: 2.0e-05\n",
      "****** New best validation accuracy for resnet18 Pruned-magnitude: 90.80% (Epoch 16) ******\n",
      "resnet18 Pruned-magnitude - Epoch 17/50: Train Loss=0.1168, Train Acc=95.97% | Val Loss=0.3234, Val Acc=90.60% | LR: 2.0e-05\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 90.80%\n",
      "resnet18 Pruned-magnitude - Epoch 18/50: Train Loss=0.1165, Train Acc=95.99% | Val Loss=0.3139, Val Acc=90.34% | LR: 2.0e-05\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 90.80%\n",
      "resnet18 Pruned-magnitude - Epoch 19/50: Train Loss=0.1147, Train Acc=96.07% | Val Loss=0.3070, Val Acc=90.72% | LR: 2.0e-05\n",
      "Validation accuracy did not improve for 3 epoch(s). Best: 90.80%\n",
      "resnet18 Pruned-magnitude - Epoch 20/50: Train Loss=0.1129, Train Acc=96.13% | Val Loss=0.3147, Val Acc=90.56% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 4 epoch(s). Best: 90.80%\n",
      "resnet18 Pruned-magnitude - Epoch 21/50: Train Loss=0.1119, Train Acc=96.09% | Val Loss=0.3116, Val Acc=90.68% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 5 epoch(s). Best: 90.80%\n",
      "resnet18 Pruned-magnitude - Epoch 22/50: Train Loss=0.1107, Train Acc=96.12% | Val Loss=0.3081, Val Acc=90.58% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 6 epoch(s). Best: 90.80%\n",
      "resnet18 Pruned-magnitude - Epoch 23/50: Train Loss=0.1094, Train Acc=96.18% | Val Loss=0.3110, Val Acc=90.56% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 7 epoch(s). Best: 90.80%\n",
      "resnet18 Pruned-magnitude - Epoch 24/50: Train Loss=0.1123, Train Acc=96.18% | Val Loss=0.3258, Val Acc=90.62% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 8 epoch(s). Best: 90.80%\n",
      "resnet18 Pruned-magnitude - Epoch 25/50: Train Loss=0.1103, Train Acc=96.09% | Val Loss=0.3150, Val Acc=90.48% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 9 epoch(s). Best: 90.80%\n",
      "resnet18 Pruned-magnitude - Epoch 26/50: Train Loss=0.1111, Train Acc=96.27% | Val Loss=0.3183, Val Acc=90.60% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 10 epoch(s). Best: 90.80%\n",
      "Early stopping triggered for resnet18 Pruned-magnitude after 10 epochs without improvement.\n",
      "Finished training resnet18 Pruned-magnitude. Best validation accuracy achieved: 90.80%\n",
      "\n",
      "--- Evaluating fine-tuned resnet18 (magnitude) on Test Set ---\n",
      "resnet18 FineTuned-magnitude Test Accuracy: 90.07%\n",
      "Test Metrics for magnitude: {'macs': 158107140.0, 'size_mb': 12.695148, 'accuracy': 90.07}\n",
      "âœ… Model saved as ONNX to ./output_resnet_finetuned/resnet18/strategies/resnet18_magnitude_final_best_val.onnx\n",
      "\n",
      "--- Pruning resnet18 with Strategy: bn_scale ---\n",
      "Initial MACs: 0.557 G, Size: 42.63 MB\n",
      "Step 1/15 (Non-Taylor): MACs 0.514 G, Size 39.65 MB\n",
      "Step 2/15 (Non-Taylor): MACs 0.479 G, Size 36.94 MB\n",
      "Step 3/15 (Non-Taylor): MACs 0.447 G, Size 34.40 MB\n",
      "Step 4/15 (Non-Taylor): MACs 0.414 G, Size 31.85 MB\n",
      "Step 5/15 (Non-Taylor): MACs 0.384 G, Size 29.50 MB\n",
      "Step 6/15 (Non-Taylor): MACs 0.354 G, Size 27.17 MB\n",
      "Step 7/15 (Non-Taylor): MACs 0.327 G, Size 25.00 MB\n",
      "Step 8/15 (Non-Taylor): MACs 0.294 G, Size 22.81 MB\n",
      "Step 9/15 (Non-Taylor): MACs 0.269 G, Size 20.83 MB\n",
      "Step 10/15 (Non-Taylor): MACs 0.245 G, Size 18.88 MB\n",
      "Step 11/15 (Non-Taylor): MACs 0.222 G, Size 17.08 MB\n",
      "Step 12/15 (Non-Taylor): MACs 0.198 G, Size 15.29 MB\n",
      "Step 13/15 (Non-Taylor): MACs 0.178 G, Size 13.68 MB\n",
      "Step 14/15 (Non-Taylor): MACs 0.158 G, Size 12.11 MB\n",
      "Targets reached at step 14 for non-Taylor strategy.\n",
      "After pruning: MACs 0.158 G, Size 12.11 MB\n",
      "Pruning targets achieved.\n",
      "âœ… Model saved as ONNX to ./output_resnet_finetuned/resnet18/strategies/resnet18_bn_scale_pruned.onnx\n",
      "\n",
      "--- Fine-tuning resnet18 after bn_scale pruning ---\n",
      "Starting training for resnet18 Pruned-bn_scale with patience=10\n",
      "resnet18 Pruned-bn_scale - Epoch 1/50: Train Loss=0.4441, Train Acc=84.76% | Val Loss=0.4406, Val Acc=84.76% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-bn_scale: 84.76% (Epoch 1) ******\n",
      "resnet18 Pruned-bn_scale - Epoch 2/50: Train Loss=0.3563, Train Acc=87.63% | Val Loss=0.3840, Val Acc=87.30% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-bn_scale: 87.30% (Epoch 2) ******\n",
      "resnet18 Pruned-bn_scale - Epoch 3/50: Train Loss=0.3237, Train Acc=88.82% | Val Loss=0.3786, Val Acc=87.34% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-bn_scale: 87.34% (Epoch 3) ******\n",
      "resnet18 Pruned-bn_scale - Epoch 4/50: Train Loss=0.3045, Train Acc=89.34% | Val Loss=0.3775, Val Acc=87.52% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-bn_scale: 87.52% (Epoch 4) ******\n",
      "resnet18 Pruned-bn_scale - Epoch 5/50: Train Loss=0.2844, Train Acc=89.95% | Val Loss=0.4030, Val Acc=86.96% | LR: 5.0e-04\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 87.52%\n",
      "resnet18 Pruned-bn_scale - Epoch 6/50: Train Loss=0.2699, Train Acc=90.43% | Val Loss=0.3665, Val Acc=88.14% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-bn_scale: 88.14% (Epoch 6) ******\n",
      "resnet18 Pruned-bn_scale - Epoch 7/50: Train Loss=0.2553, Train Acc=91.09% | Val Loss=0.3715, Val Acc=87.86% | LR: 5.0e-04\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 88.14%\n",
      "resnet18 Pruned-bn_scale - Epoch 8/50: Train Loss=0.1900, Train Acc=93.46% | Val Loss=0.2975, Val Acc=90.62% | LR: 1.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-bn_scale: 90.62% (Epoch 8) ******\n",
      "resnet18 Pruned-bn_scale - Epoch 9/50: Train Loss=0.1698, Train Acc=94.12% | Val Loss=0.3184, Val Acc=90.70% | LR: 1.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-bn_scale: 90.70% (Epoch 9) ******\n",
      "resnet18 Pruned-bn_scale - Epoch 10/50: Train Loss=0.1622, Train Acc=94.29% | Val Loss=0.3086, Val Acc=90.54% | LR: 1.0e-04\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 90.70%\n",
      "resnet18 Pruned-bn_scale - Epoch 11/50: Train Loss=0.1530, Train Acc=94.70% | Val Loss=0.3005, Val Acc=90.84% | LR: 1.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-bn_scale: 90.84% (Epoch 11) ******\n",
      "resnet18 Pruned-bn_scale - Epoch 12/50: Train Loss=0.1441, Train Acc=94.98% | Val Loss=0.3120, Val Acc=90.82% | LR: 1.0e-04\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 90.84%\n",
      "resnet18 Pruned-bn_scale - Epoch 13/50: Train Loss=0.1419, Train Acc=95.05% | Val Loss=0.3029, Val Acc=91.18% | LR: 1.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-bn_scale: 91.18% (Epoch 13) ******\n",
      "resnet18 Pruned-bn_scale - Epoch 14/50: Train Loss=0.1252, Train Acc=95.66% | Val Loss=0.2978, Val Acc=91.04% | LR: 2.0e-05\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 91.18%\n",
      "resnet18 Pruned-bn_scale - Epoch 15/50: Train Loss=0.1235, Train Acc=95.62% | Val Loss=0.3011, Val Acc=91.00% | LR: 2.0e-05\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 91.18%\n",
      "resnet18 Pruned-bn_scale - Epoch 16/50: Train Loss=0.1218, Train Acc=95.82% | Val Loss=0.3003, Val Acc=91.14% | LR: 2.0e-05\n",
      "Validation accuracy did not improve for 3 epoch(s). Best: 91.18%\n",
      "resnet18 Pruned-bn_scale - Epoch 17/50: Train Loss=0.1184, Train Acc=95.78% | Val Loss=0.3034, Val Acc=91.28% | LR: 2.0e-05\n",
      "****** New best validation accuracy for resnet18 Pruned-bn_scale: 91.28% (Epoch 17) ******\n",
      "resnet18 Pruned-bn_scale - Epoch 18/50: Train Loss=0.1167, Train Acc=95.97% | Val Loss=0.3082, Val Acc=91.20% | LR: 2.0e-05\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 91.28%\n",
      "resnet18 Pruned-bn_scale - Epoch 19/50: Train Loss=0.1155, Train Acc=96.03% | Val Loss=0.3026, Val Acc=91.22% | LR: 2.0e-05\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 91.28%\n",
      "resnet18 Pruned-bn_scale - Epoch 20/50: Train Loss=0.1134, Train Acc=96.06% | Val Loss=0.3026, Val Acc=91.14% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 3 epoch(s). Best: 91.28%\n",
      "resnet18 Pruned-bn_scale - Epoch 21/50: Train Loss=0.1124, Train Acc=96.14% | Val Loss=0.3055, Val Acc=91.38% | LR: 4.0e-06\n",
      "****** New best validation accuracy for resnet18 Pruned-bn_scale: 91.38% (Epoch 21) ******\n",
      "resnet18 Pruned-bn_scale - Epoch 22/50: Train Loss=0.1129, Train Acc=95.99% | Val Loss=0.3064, Val Acc=91.16% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 91.38%\n",
      "resnet18 Pruned-bn_scale - Epoch 23/50: Train Loss=0.1116, Train Acc=96.16% | Val Loss=0.3215, Val Acc=91.26% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 91.38%\n",
      "resnet18 Pruned-bn_scale - Epoch 24/50: Train Loss=0.1091, Train Acc=96.23% | Val Loss=0.3091, Val Acc=91.20% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 3 epoch(s). Best: 91.38%\n",
      "resnet18 Pruned-bn_scale - Epoch 25/50: Train Loss=0.1093, Train Acc=96.29% | Val Loss=0.3101, Val Acc=91.32% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 4 epoch(s). Best: 91.38%\n",
      "resnet18 Pruned-bn_scale - Epoch 26/50: Train Loss=0.1113, Train Acc=96.19% | Val Loss=0.3137, Val Acc=91.38% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 5 epoch(s). Best: 91.38%\n",
      "resnet18 Pruned-bn_scale - Epoch 27/50: Train Loss=0.1091, Train Acc=96.21% | Val Loss=0.3077, Val Acc=91.32% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 6 epoch(s). Best: 91.38%\n",
      "resnet18 Pruned-bn_scale - Epoch 28/50: Train Loss=0.1126, Train Acc=96.20% | Val Loss=0.3045, Val Acc=91.18% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 7 epoch(s). Best: 91.38%\n",
      "resnet18 Pruned-bn_scale - Epoch 29/50: Train Loss=0.1095, Train Acc=96.20% | Val Loss=0.3027, Val Acc=91.50% | LR: 1.0e-06\n",
      "****** New best validation accuracy for resnet18 Pruned-bn_scale: 91.50% (Epoch 29) ******\n",
      "resnet18 Pruned-bn_scale - Epoch 30/50: Train Loss=0.1092, Train Acc=96.18% | Val Loss=0.3115, Val Acc=91.30% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 91.50%\n",
      "resnet18 Pruned-bn_scale - Epoch 31/50: Train Loss=0.1093, Train Acc=96.20% | Val Loss=0.3167, Val Acc=91.42% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 91.50%\n",
      "resnet18 Pruned-bn_scale - Epoch 32/50: Train Loss=0.1052, Train Acc=96.39% | Val Loss=0.3062, Val Acc=91.52% | LR: 1.0e-06\n",
      "****** New best validation accuracy for resnet18 Pruned-bn_scale: 91.52% (Epoch 32) ******\n",
      "resnet18 Pruned-bn_scale - Epoch 33/50: Train Loss=0.1110, Train Acc=96.09% | Val Loss=0.3085, Val Acc=91.28% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 91.52%\n",
      "resnet18 Pruned-bn_scale - Epoch 34/50: Train Loss=0.1080, Train Acc=96.29% | Val Loss=0.3082, Val Acc=91.20% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 91.52%\n",
      "resnet18 Pruned-bn_scale - Epoch 35/50: Train Loss=0.1087, Train Acc=96.22% | Val Loss=0.3252, Val Acc=91.40% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 3 epoch(s). Best: 91.52%\n",
      "resnet18 Pruned-bn_scale - Epoch 36/50: Train Loss=0.1075, Train Acc=96.31% | Val Loss=0.3050, Val Acc=91.18% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 4 epoch(s). Best: 91.52%\n",
      "resnet18 Pruned-bn_scale - Epoch 37/50: Train Loss=0.1077, Train Acc=96.23% | Val Loss=0.3042, Val Acc=91.22% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 5 epoch(s). Best: 91.52%\n",
      "resnet18 Pruned-bn_scale - Epoch 38/50: Train Loss=0.1083, Train Acc=96.32% | Val Loss=0.3105, Val Acc=91.26% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 6 epoch(s). Best: 91.52%\n",
      "resnet18 Pruned-bn_scale - Epoch 39/50: Train Loss=0.1086, Train Acc=96.36% | Val Loss=0.3074, Val Acc=91.26% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 7 epoch(s). Best: 91.52%\n",
      "resnet18 Pruned-bn_scale - Epoch 40/50: Train Loss=0.1083, Train Acc=96.21% | Val Loss=0.3064, Val Acc=91.22% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 8 epoch(s). Best: 91.52%\n",
      "resnet18 Pruned-bn_scale - Epoch 41/50: Train Loss=0.1098, Train Acc=96.17% | Val Loss=0.3060, Val Acc=91.16% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 9 epoch(s). Best: 91.52%\n",
      "resnet18 Pruned-bn_scale - Epoch 42/50: Train Loss=0.1062, Train Acc=96.35% | Val Loss=0.3077, Val Acc=90.88% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 10 epoch(s). Best: 91.52%\n",
      "Early stopping triggered for resnet18 Pruned-bn_scale after 10 epochs without improvement.\n",
      "Finished training resnet18 Pruned-bn_scale. Best validation accuracy achieved: 91.52%\n",
      "\n",
      "--- Evaluating fine-tuned resnet18 (bn_scale) on Test Set ---\n",
      "resnet18 FineTuned-bn_scale Test Accuracy: 90.14%\n",
      "Test Metrics for bn_scale: {'macs': 158107140.0, 'size_mb': 12.695148, 'accuracy': 90.14}\n",
      "âœ… Model saved as ONNX to ./output_resnet_finetuned/resnet18/strategies/resnet18_bn_scale_final_best_val.onnx\n",
      "\n",
      "--- Pruning resnet18 with Strategy: group_norm ---\n",
      "Initial MACs: 0.557 G, Size: 42.63 MB\n",
      "Step 1/15 (Non-Taylor): MACs 0.514 G, Size 39.65 MB\n",
      "Step 2/15 (Non-Taylor): MACs 0.479 G, Size 36.94 MB\n",
      "Step 3/15 (Non-Taylor): MACs 0.447 G, Size 34.40 MB\n",
      "Step 4/15 (Non-Taylor): MACs 0.414 G, Size 31.85 MB\n",
      "Step 5/15 (Non-Taylor): MACs 0.384 G, Size 29.50 MB\n",
      "Step 6/15 (Non-Taylor): MACs 0.354 G, Size 27.17 MB\n",
      "Step 7/15 (Non-Taylor): MACs 0.327 G, Size 25.00 MB\n",
      "Step 8/15 (Non-Taylor): MACs 0.294 G, Size 22.81 MB\n",
      "Step 9/15 (Non-Taylor): MACs 0.269 G, Size 20.83 MB\n",
      "Step 10/15 (Non-Taylor): MACs 0.245 G, Size 18.88 MB\n",
      "Step 11/15 (Non-Taylor): MACs 0.222 G, Size 17.08 MB\n",
      "Step 12/15 (Non-Taylor): MACs 0.198 G, Size 15.29 MB\n",
      "Step 13/15 (Non-Taylor): MACs 0.178 G, Size 13.68 MB\n",
      "Step 14/15 (Non-Taylor): MACs 0.158 G, Size 12.11 MB\n",
      "Targets reached at step 14 for non-Taylor strategy.\n",
      "After pruning: MACs 0.158 G, Size 12.11 MB\n",
      "Pruning targets achieved.\n",
      "âœ… Model saved as ONNX to ./output_resnet_finetuned/resnet18/strategies/resnet18_group_norm_pruned.onnx\n",
      "\n",
      "--- Fine-tuning resnet18 after group_norm pruning ---\n",
      "Starting training for resnet18 Pruned-group_norm with patience=10\n",
      "resnet18 Pruned-group_norm - Epoch 1/50: Train Loss=0.4596, Train Acc=84.14% | Val Loss=0.4043, Val Acc=86.14% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-group_norm: 86.14% (Epoch 1) ******\n",
      "resnet18 Pruned-group_norm - Epoch 2/50: Train Loss=0.3602, Train Acc=87.51% | Val Loss=0.4119, Val Acc=86.30% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-group_norm: 86.30% (Epoch 2) ******\n",
      "resnet18 Pruned-group_norm - Epoch 3/50: Train Loss=0.3296, Train Acc=88.52% | Val Loss=0.3885, Val Acc=86.98% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-group_norm: 86.98% (Epoch 3) ******\n",
      "resnet18 Pruned-group_norm - Epoch 4/50: Train Loss=0.3049, Train Acc=89.37% | Val Loss=0.3926, Val Acc=87.02% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-group_norm: 87.02% (Epoch 4) ******\n",
      "resnet18 Pruned-group_norm - Epoch 5/50: Train Loss=0.2855, Train Acc=90.06% | Val Loss=0.3653, Val Acc=88.02% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-group_norm: 88.02% (Epoch 5) ******\n",
      "resnet18 Pruned-group_norm - Epoch 6/50: Train Loss=0.2720, Train Acc=90.61% | Val Loss=0.3781, Val Acc=87.28% | LR: 5.0e-04\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 88.02%\n",
      "resnet18 Pruned-group_norm - Epoch 7/50: Train Loss=0.2539, Train Acc=91.23% | Val Loss=0.3425, Val Acc=89.00% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-group_norm: 89.00% (Epoch 7) ******\n",
      "resnet18 Pruned-group_norm - Epoch 8/50: Train Loss=0.2411, Train Acc=91.64% | Val Loss=0.3355, Val Acc=89.02% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-group_norm: 89.02% (Epoch 8) ******\n",
      "resnet18 Pruned-group_norm - Epoch 9/50: Train Loss=0.1762, Train Acc=93.84% | Val Loss=0.2923, Val Acc=90.34% | LR: 1.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-group_norm: 90.34% (Epoch 9) ******\n",
      "resnet18 Pruned-group_norm - Epoch 10/50: Train Loss=0.1584, Train Acc=94.65% | Val Loss=0.3058, Val Acc=90.42% | LR: 1.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-group_norm: 90.42% (Epoch 10) ******\n",
      "resnet18 Pruned-group_norm - Epoch 11/50: Train Loss=0.1535, Train Acc=94.67% | Val Loss=0.3063, Val Acc=90.22% | LR: 1.0e-04\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 90.42%\n",
      "resnet18 Pruned-group_norm - Epoch 12/50: Train Loss=0.1444, Train Acc=95.09% | Val Loss=0.2955, Val Acc=90.40% | LR: 1.0e-04\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 90.42%\n",
      "resnet18 Pruned-group_norm - Epoch 13/50: Train Loss=0.1368, Train Acc=95.26% | Val Loss=0.3101, Val Acc=90.20% | LR: 1.0e-04\n",
      "Validation accuracy did not improve for 3 epoch(s). Best: 90.42%\n",
      "resnet18 Pruned-group_norm - Epoch 14/50: Train Loss=0.1312, Train Acc=95.52% | Val Loss=0.2998, Val Acc=90.68% | LR: 1.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-group_norm: 90.68% (Epoch 14) ******\n",
      "resnet18 Pruned-group_norm - Epoch 15/50: Train Loss=0.1154, Train Acc=96.04% | Val Loss=0.2964, Val Acc=90.68% | LR: 2.0e-05\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 90.68%\n",
      "resnet18 Pruned-group_norm - Epoch 16/50: Train Loss=0.1166, Train Acc=95.87% | Val Loss=0.2944, Val Acc=90.82% | LR: 2.0e-05\n",
      "****** New best validation accuracy for resnet18 Pruned-group_norm: 90.82% (Epoch 16) ******\n",
      "resnet18 Pruned-group_norm - Epoch 17/50: Train Loss=0.1155, Train Acc=96.06% | Val Loss=0.2939, Val Acc=90.70% | LR: 2.0e-05\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 90.82%\n",
      "resnet18 Pruned-group_norm - Epoch 18/50: Train Loss=0.1110, Train Acc=96.18% | Val Loss=0.3091, Val Acc=90.80% | LR: 2.0e-05\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 90.82%\n",
      "resnet18 Pruned-group_norm - Epoch 19/50: Train Loss=0.1066, Train Acc=96.33% | Val Loss=0.3044, Val Acc=90.66% | LR: 2.0e-05\n",
      "Validation accuracy did not improve for 3 epoch(s). Best: 90.82%\n",
      "resnet18 Pruned-group_norm - Epoch 20/50: Train Loss=0.1111, Train Acc=96.24% | Val Loss=0.3027, Val Acc=90.76% | LR: 2.0e-05\n",
      "Validation accuracy did not improve for 4 epoch(s). Best: 90.82%\n",
      "resnet18 Pruned-group_norm - Epoch 21/50: Train Loss=0.1057, Train Acc=96.43% | Val Loss=0.3055, Val Acc=90.80% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 5 epoch(s). Best: 90.82%\n",
      "resnet18 Pruned-group_norm - Epoch 22/50: Train Loss=0.1064, Train Acc=96.35% | Val Loss=0.3031, Val Acc=90.70% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 6 epoch(s). Best: 90.82%\n",
      "resnet18 Pruned-group_norm - Epoch 23/50: Train Loss=0.1044, Train Acc=96.42% | Val Loss=0.3028, Val Acc=90.74% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 7 epoch(s). Best: 90.82%\n",
      "resnet18 Pruned-group_norm - Epoch 24/50: Train Loss=0.1059, Train Acc=96.40% | Val Loss=0.3156, Val Acc=90.74% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 8 epoch(s). Best: 90.82%\n",
      "resnet18 Pruned-group_norm - Epoch 25/50: Train Loss=0.1037, Train Acc=96.36% | Val Loss=0.3007, Val Acc=90.90% | LR: 4.0e-06\n",
      "****** New best validation accuracy for resnet18 Pruned-group_norm: 90.90% (Epoch 25) ******\n",
      "resnet18 Pruned-group_norm - Epoch 26/50: Train Loss=0.1030, Train Acc=96.51% | Val Loss=0.2989, Val Acc=90.78% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 90.90%\n",
      "resnet18 Pruned-group_norm - Epoch 27/50: Train Loss=0.1014, Train Acc=96.64% | Val Loss=0.3082, Val Acc=90.64% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 90.90%\n",
      "resnet18 Pruned-group_norm - Epoch 28/50: Train Loss=0.1028, Train Acc=96.56% | Val Loss=0.2982, Val Acc=90.90% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 3 epoch(s). Best: 90.90%\n",
      "resnet18 Pruned-group_norm - Epoch 29/50: Train Loss=0.1033, Train Acc=96.39% | Val Loss=0.3084, Val Acc=90.60% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 4 epoch(s). Best: 90.90%\n",
      "resnet18 Pruned-group_norm - Epoch 30/50: Train Loss=0.1027, Train Acc=96.50% | Val Loss=0.3156, Val Acc=90.84% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 5 epoch(s). Best: 90.90%\n",
      "resnet18 Pruned-group_norm - Epoch 31/50: Train Loss=0.1016, Train Acc=96.46% | Val Loss=0.3036, Val Acc=90.82% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 6 epoch(s). Best: 90.90%\n",
      "resnet18 Pruned-group_norm - Epoch 32/50: Train Loss=0.1033, Train Acc=96.47% | Val Loss=0.3061, Val Acc=90.94% | LR: 1.0e-06\n",
      "****** New best validation accuracy for resnet18 Pruned-group_norm: 90.94% (Epoch 32) ******\n",
      "resnet18 Pruned-group_norm - Epoch 33/50: Train Loss=0.1019, Train Acc=96.57% | Val Loss=0.3113, Val Acc=90.90% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 90.94%\n",
      "resnet18 Pruned-group_norm - Epoch 34/50: Train Loss=0.1033, Train Acc=96.38% | Val Loss=0.3072, Val Acc=90.94% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 90.94%\n",
      "resnet18 Pruned-group_norm - Epoch 35/50: Train Loss=0.1033, Train Acc=96.42% | Val Loss=0.3066, Val Acc=90.76% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 3 epoch(s). Best: 90.94%\n",
      "resnet18 Pruned-group_norm - Epoch 36/50: Train Loss=0.1020, Train Acc=96.51% | Val Loss=0.3049, Val Acc=91.00% | LR: 1.0e-06\n",
      "****** New best validation accuracy for resnet18 Pruned-group_norm: 91.00% (Epoch 36) ******\n",
      "resnet18 Pruned-group_norm - Epoch 37/50: Train Loss=0.1034, Train Acc=96.52% | Val Loss=0.3149, Val Acc=90.88% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 91.00%\n",
      "resnet18 Pruned-group_norm - Epoch 38/50: Train Loss=0.1019, Train Acc=96.52% | Val Loss=0.3003, Val Acc=90.90% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 91.00%\n",
      "resnet18 Pruned-group_norm - Epoch 39/50: Train Loss=0.1034, Train Acc=96.50% | Val Loss=0.3029, Val Acc=90.78% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 3 epoch(s). Best: 91.00%\n",
      "resnet18 Pruned-group_norm - Epoch 40/50: Train Loss=0.1019, Train Acc=96.58% | Val Loss=0.2999, Val Acc=90.84% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 4 epoch(s). Best: 91.00%\n",
      "resnet18 Pruned-group_norm - Epoch 41/50: Train Loss=0.1029, Train Acc=96.42% | Val Loss=0.3199, Val Acc=90.72% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 5 epoch(s). Best: 91.00%\n",
      "resnet18 Pruned-group_norm - Epoch 42/50: Train Loss=0.1019, Train Acc=96.45% | Val Loss=0.3109, Val Acc=90.76% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 6 epoch(s). Best: 91.00%\n",
      "resnet18 Pruned-group_norm - Epoch 43/50: Train Loss=0.1018, Train Acc=96.45% | Val Loss=0.3065, Val Acc=90.70% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 7 epoch(s). Best: 91.00%\n",
      "resnet18 Pruned-group_norm - Epoch 44/50: Train Loss=0.1035, Train Acc=96.45% | Val Loss=0.3015, Val Acc=90.54% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 8 epoch(s). Best: 91.00%\n",
      "resnet18 Pruned-group_norm - Epoch 45/50: Train Loss=0.1006, Train Acc=96.47% | Val Loss=0.3039, Val Acc=90.90% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 9 epoch(s). Best: 91.00%\n",
      "resnet18 Pruned-group_norm - Epoch 46/50: Train Loss=0.1002, Train Acc=96.63% | Val Loss=0.3117, Val Acc=90.84% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 10 epoch(s). Best: 91.00%\n",
      "Early stopping triggered for resnet18 Pruned-group_norm after 10 epochs without improvement.\n",
      "Finished training resnet18 Pruned-group_norm. Best validation accuracy achieved: 91.00%\n",
      "\n",
      "--- Evaluating fine-tuned resnet18 (group_norm) on Test Set ---\n",
      "resnet18 FineTuned-group_norm Test Accuracy: 90.36%\n",
      "Test Metrics for group_norm: {'macs': 158107140.0, 'size_mb': 12.695148, 'accuracy': 90.36}\n",
      "âœ… Model saved as ONNX to ./output_resnet_finetuned/resnet18/strategies/resnet18_group_norm_final_best_val.onnx\n",
      "\n",
      "--- Pruning resnet18 with Strategy: random ---\n",
      "Initial MACs: 0.557 G, Size: 42.63 MB\n",
      "Step 1/15 (Non-Taylor): MACs 0.514 G, Size 39.65 MB\n",
      "Step 2/15 (Non-Taylor): MACs 0.479 G, Size 36.94 MB\n",
      "Step 3/15 (Non-Taylor): MACs 0.447 G, Size 34.40 MB\n",
      "Step 4/15 (Non-Taylor): MACs 0.414 G, Size 31.85 MB\n",
      "Step 5/15 (Non-Taylor): MACs 0.384 G, Size 29.50 MB\n",
      "Step 6/15 (Non-Taylor): MACs 0.354 G, Size 27.17 MB\n",
      "Step 7/15 (Non-Taylor): MACs 0.327 G, Size 25.00 MB\n",
      "Step 8/15 (Non-Taylor): MACs 0.294 G, Size 22.81 MB\n",
      "Step 9/15 (Non-Taylor): MACs 0.269 G, Size 20.83 MB\n",
      "Step 10/15 (Non-Taylor): MACs 0.245 G, Size 18.88 MB\n",
      "Step 11/15 (Non-Taylor): MACs 0.222 G, Size 17.08 MB\n",
      "Step 12/15 (Non-Taylor): MACs 0.198 G, Size 15.29 MB\n",
      "Step 13/15 (Non-Taylor): MACs 0.178 G, Size 13.68 MB\n",
      "Step 14/15 (Non-Taylor): MACs 0.158 G, Size 12.11 MB\n",
      "Targets reached at step 14 for non-Taylor strategy.\n",
      "After pruning: MACs 0.158 G, Size 12.11 MB\n",
      "Pruning targets achieved.\n",
      "âœ… Model saved as ONNX to ./output_resnet_finetuned/resnet18/strategies/resnet18_random_pruned.onnx\n",
      "\n",
      "--- Fine-tuning resnet18 after random pruning ---\n",
      "Starting training for resnet18 Pruned-random with patience=10\n",
      "resnet18 Pruned-random - Epoch 1/50: Train Loss=0.4514, Train Acc=84.54% | Val Loss=0.4157, Val Acc=85.66% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-random: 85.66% (Epoch 1) ******\n",
      "resnet18 Pruned-random - Epoch 2/50: Train Loss=0.3617, Train Acc=87.39% | Val Loss=0.4171, Val Acc=85.56% | LR: 5.0e-04\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 85.66%\n",
      "resnet18 Pruned-random - Epoch 3/50: Train Loss=0.3345, Train Acc=88.38% | Val Loss=0.3904, Val Acc=86.50% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-random: 86.50% (Epoch 3) ******\n",
      "resnet18 Pruned-random - Epoch 4/50: Train Loss=0.3098, Train Acc=89.16% | Val Loss=0.3663, Val Acc=87.86% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-random: 87.86% (Epoch 4) ******\n",
      "resnet18 Pruned-random - Epoch 5/50: Train Loss=0.2922, Train Acc=89.76% | Val Loss=0.4116, Val Acc=86.00% | LR: 5.0e-04\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 87.86%\n",
      "resnet18 Pruned-random - Epoch 6/50: Train Loss=0.2749, Train Acc=90.45% | Val Loss=0.3579, Val Acc=88.68% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-random: 88.68% (Epoch 6) ******\n",
      "resnet18 Pruned-random - Epoch 7/50: Train Loss=0.2594, Train Acc=90.95% | Val Loss=0.3713, Val Acc=87.66% | LR: 5.0e-04\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 88.68%\n",
      "resnet18 Pruned-random - Epoch 8/50: Train Loss=0.2450, Train Acc=91.40% | Val Loss=0.3520, Val Acc=88.32% | LR: 5.0e-04\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 88.68%\n",
      "resnet18 Pruned-random - Epoch 9/50: Train Loss=0.1725, Train Acc=94.18% | Val Loss=0.3011, Val Acc=90.50% | LR: 1.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-random: 90.50% (Epoch 9) ******\n",
      "resnet18 Pruned-random - Epoch 10/50: Train Loss=0.1569, Train Acc=94.51% | Val Loss=0.2956, Val Acc=90.54% | LR: 1.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-random: 90.54% (Epoch 10) ******\n",
      "resnet18 Pruned-random - Epoch 11/50: Train Loss=0.1517, Train Acc=94.78% | Val Loss=0.3015, Val Acc=90.60% | LR: 1.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-random: 90.60% (Epoch 11) ******\n",
      "resnet18 Pruned-random - Epoch 12/50: Train Loss=0.1410, Train Acc=95.13% | Val Loss=0.2990, Val Acc=90.50% | LR: 1.0e-04\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 90.60%\n",
      "resnet18 Pruned-random - Epoch 13/50: Train Loss=0.1338, Train Acc=95.39% | Val Loss=0.3011, Val Acc=90.84% | LR: 1.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-random: 90.84% (Epoch 13) ******\n",
      "resnet18 Pruned-random - Epoch 14/50: Train Loss=0.1291, Train Acc=95.51% | Val Loss=0.3106, Val Acc=90.84% | LR: 1.0e-04\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 90.84%\n",
      "resnet18 Pruned-random - Epoch 15/50: Train Loss=0.1137, Train Acc=96.12% | Val Loss=0.3131, Val Acc=91.06% | LR: 2.0e-05\n",
      "****** New best validation accuracy for resnet18 Pruned-random: 91.06% (Epoch 15) ******\n",
      "resnet18 Pruned-random - Epoch 16/50: Train Loss=0.1085, Train Acc=96.29% | Val Loss=0.3085, Val Acc=91.04% | LR: 2.0e-05\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 91.06%\n",
      "resnet18 Pruned-random - Epoch 17/50: Train Loss=0.1065, Train Acc=96.34% | Val Loss=0.3059, Val Acc=91.12% | LR: 2.0e-05\n",
      "****** New best validation accuracy for resnet18 Pruned-random: 91.12% (Epoch 17) ******\n",
      "resnet18 Pruned-random - Epoch 18/50: Train Loss=0.1053, Train Acc=96.37% | Val Loss=0.3122, Val Acc=91.02% | LR: 2.0e-05\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 91.12%\n",
      "resnet18 Pruned-random - Epoch 19/50: Train Loss=0.1035, Train Acc=96.49% | Val Loss=0.3077, Val Acc=91.16% | LR: 2.0e-05\n",
      "****** New best validation accuracy for resnet18 Pruned-random: 91.16% (Epoch 19) ******\n",
      "resnet18 Pruned-random - Epoch 20/50: Train Loss=0.0995, Train Acc=96.59% | Val Loss=0.3084, Val Acc=91.16% | LR: 2.0e-05\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 91.16%\n",
      "resnet18 Pruned-random - Epoch 21/50: Train Loss=0.1011, Train Acc=96.59% | Val Loss=0.3101, Val Acc=91.16% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 91.16%\n",
      "resnet18 Pruned-random - Epoch 22/50: Train Loss=0.0993, Train Acc=96.57% | Val Loss=0.3121, Val Acc=91.12% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 3 epoch(s). Best: 91.16%\n",
      "resnet18 Pruned-random - Epoch 23/50: Train Loss=0.0970, Train Acc=96.75% | Val Loss=0.3071, Val Acc=91.12% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 4 epoch(s). Best: 91.16%\n",
      "resnet18 Pruned-random - Epoch 24/50: Train Loss=0.0979, Train Acc=96.62% | Val Loss=0.3090, Val Acc=91.26% | LR: 4.0e-06\n",
      "****** New best validation accuracy for resnet18 Pruned-random: 91.26% (Epoch 24) ******\n",
      "resnet18 Pruned-random - Epoch 25/50: Train Loss=0.0990, Train Acc=96.66% | Val Loss=0.3153, Val Acc=91.12% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 91.26%\n",
      "resnet18 Pruned-random - Epoch 26/50: Train Loss=0.0971, Train Acc=96.63% | Val Loss=0.3204, Val Acc=91.24% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 91.26%\n",
      "resnet18 Pruned-random - Epoch 27/50: Train Loss=0.0946, Train Acc=96.79% | Val Loss=0.3119, Val Acc=90.94% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 3 epoch(s). Best: 91.26%\n",
      "resnet18 Pruned-random - Epoch 28/50: Train Loss=0.0946, Train Acc=96.84% | Val Loss=0.3139, Val Acc=91.14% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 4 epoch(s). Best: 91.26%\n",
      "resnet18 Pruned-random - Epoch 29/50: Train Loss=0.0973, Train Acc=96.68% | Val Loss=0.3123, Val Acc=91.28% | LR: 1.0e-06\n",
      "****** New best validation accuracy for resnet18 Pruned-random: 91.28% (Epoch 29) ******\n",
      "resnet18 Pruned-random - Epoch 30/50: Train Loss=0.0969, Train Acc=96.69% | Val Loss=0.3104, Val Acc=91.28% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 91.28%\n",
      "resnet18 Pruned-random - Epoch 31/50: Train Loss=0.0969, Train Acc=96.77% | Val Loss=0.3129, Val Acc=91.18% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 91.28%\n",
      "resnet18 Pruned-random - Epoch 32/50: Train Loss=0.0944, Train Acc=96.74% | Val Loss=0.3141, Val Acc=91.18% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 3 epoch(s). Best: 91.28%\n",
      "resnet18 Pruned-random - Epoch 33/50: Train Loss=0.0952, Train Acc=96.82% | Val Loss=0.3088, Val Acc=91.24% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 4 epoch(s). Best: 91.28%\n",
      "resnet18 Pruned-random - Epoch 34/50: Train Loss=0.0975, Train Acc=96.76% | Val Loss=0.3140, Val Acc=91.38% | LR: 1.0e-06\n",
      "****** New best validation accuracy for resnet18 Pruned-random: 91.38% (Epoch 34) ******\n",
      "resnet18 Pruned-random - Epoch 35/50: Train Loss=0.0959, Train Acc=96.68% | Val Loss=0.3154, Val Acc=91.36% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 91.38%\n",
      "resnet18 Pruned-random - Epoch 36/50: Train Loss=0.0946, Train Acc=96.78% | Val Loss=0.3050, Val Acc=91.32% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 91.38%\n",
      "resnet18 Pruned-random - Epoch 37/50: Train Loss=0.0953, Train Acc=96.82% | Val Loss=0.3094, Val Acc=91.18% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 3 epoch(s). Best: 91.38%\n",
      "resnet18 Pruned-random - Epoch 38/50: Train Loss=0.0942, Train Acc=96.80% | Val Loss=0.3070, Val Acc=91.26% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 4 epoch(s). Best: 91.38%\n",
      "resnet18 Pruned-random - Epoch 39/50: Train Loss=0.0928, Train Acc=96.82% | Val Loss=0.3089, Val Acc=91.18% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 5 epoch(s). Best: 91.38%\n",
      "resnet18 Pruned-random - Epoch 40/50: Train Loss=0.0958, Train Acc=96.66% | Val Loss=0.3093, Val Acc=91.16% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 6 epoch(s). Best: 91.38%\n",
      "resnet18 Pruned-random - Epoch 41/50: Train Loss=0.0961, Train Acc=96.70% | Val Loss=0.3069, Val Acc=91.32% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 7 epoch(s). Best: 91.38%\n",
      "resnet18 Pruned-random - Epoch 42/50: Train Loss=0.0940, Train Acc=96.79% | Val Loss=0.3092, Val Acc=91.22% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 8 epoch(s). Best: 91.38%\n",
      "resnet18 Pruned-random - Epoch 43/50: Train Loss=0.0945, Train Acc=96.79% | Val Loss=0.3064, Val Acc=91.38% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 9 epoch(s). Best: 91.38%\n",
      "resnet18 Pruned-random - Epoch 44/50: Train Loss=0.0926, Train Acc=96.82% | Val Loss=0.3025, Val Acc=91.24% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 10 epoch(s). Best: 91.38%\n",
      "Early stopping triggered for resnet18 Pruned-random after 10 epochs without improvement.\n",
      "Finished training resnet18 Pruned-random. Best validation accuracy achieved: 91.38%\n",
      "\n",
      "--- Evaluating fine-tuned resnet18 (random) on Test Set ---\n",
      "resnet18 FineTuned-random Test Accuracy: 90.72%\n",
      "Test Metrics for random: {'macs': 158107140.0, 'size_mb': 12.695148, 'accuracy': 90.72}\n",
      "âœ… Model saved as ONNX to ./output_resnet_finetuned/resnet18/strategies/resnet18_random_final_best_val.onnx\n",
      "\n",
      "--- Pruning resnet18 with Strategy: Taylor ---\n",
      "Initial MACs: 0.557 G, Size: 42.63 MB\n",
      "Step 1/15: MACs 0.514 G, Size 39.65 MB\n",
      "Step 2/15: No more prunable elements found.\n",
      "After pruning: MACs 0.514 G, Size 39.65 MB\n",
      "Warning: Pruning targets not fully achieved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch_pruning/pruner/algorithms/base_pruner.py:407: UserWarning: Pruning exceed the maximum iterative steps, no pruning will be performed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model saved as ONNX to ./output_resnet_finetuned/resnet18/strategies/resnet18_Taylor_pruned.onnx\n",
      "\n",
      "--- Fine-tuning resnet18 after Taylor pruning ---\n",
      "Starting training for resnet18 Pruned-Taylor with patience=10\n",
      "resnet18 Pruned-Taylor - Epoch 1/50: Train Loss=0.2950, Train Acc=89.78% | Val Loss=0.3682, Val Acc=87.52% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-Taylor: 87.52% (Epoch 1) ******\n",
      "resnet18 Pruned-Taylor - Epoch 2/50: Train Loss=0.2817, Train Acc=90.17% | Val Loss=0.3832, Val Acc=87.20% | LR: 5.0e-04\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 87.52%\n",
      "resnet18 Pruned-Taylor - Epoch 3/50: Train Loss=0.2598, Train Acc=90.90% | Val Loss=0.3390, Val Acc=88.68% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-Taylor: 88.68% (Epoch 3) ******\n",
      "resnet18 Pruned-Taylor - Epoch 4/50: Train Loss=0.2442, Train Acc=91.58% | Val Loss=0.3223, Val Acc=89.24% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-Taylor: 89.24% (Epoch 4) ******\n",
      "resnet18 Pruned-Taylor - Epoch 5/50: Train Loss=0.2330, Train Acc=91.87% | Val Loss=0.3906, Val Acc=87.52% | LR: 5.0e-04\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 89.24%\n",
      "resnet18 Pruned-Taylor - Epoch 6/50: Train Loss=0.2190, Train Acc=92.45% | Val Loss=0.3596, Val Acc=88.44% | LR: 5.0e-04\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 89.24%\n",
      "resnet18 Pruned-Taylor - Epoch 7/50: Train Loss=0.2019, Train Acc=92.92% | Val Loss=0.3633, Val Acc=88.20% | LR: 5.0e-04\n",
      "Validation accuracy did not improve for 3 epoch(s). Best: 89.24%\n",
      "resnet18 Pruned-Taylor - Epoch 8/50: Train Loss=0.1870, Train Acc=93.49% | Val Loss=0.3393, Val Acc=89.54% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-Taylor: 89.54% (Epoch 8) ******\n",
      "resnet18 Pruned-Taylor - Epoch 9/50: Train Loss=0.1769, Train Acc=93.82% | Val Loss=0.3320, Val Acc=89.62% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-Taylor: 89.62% (Epoch 9) ******\n",
      "resnet18 Pruned-Taylor - Epoch 10/50: Train Loss=0.1657, Train Acc=94.13% | Val Loss=0.3497, Val Acc=89.86% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-Taylor: 89.86% (Epoch 10) ******\n",
      "resnet18 Pruned-Taylor - Epoch 11/50: Train Loss=0.1532, Train Acc=94.62% | Val Loss=0.3566, Val Acc=88.80% | LR: 5.0e-04\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 89.86%\n",
      "resnet18 Pruned-Taylor - Epoch 12/50: Train Loss=0.0983, Train Acc=96.62% | Val Loss=0.2827, Val Acc=91.80% | LR: 1.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-Taylor: 91.80% (Epoch 12) ******\n",
      "resnet18 Pruned-Taylor - Epoch 13/50: Train Loss=0.0791, Train Acc=97.41% | Val Loss=0.2942, Val Acc=92.04% | LR: 1.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-Taylor: 92.04% (Epoch 13) ******\n",
      "resnet18 Pruned-Taylor - Epoch 14/50: Train Loss=0.0693, Train Acc=97.63% | Val Loss=0.2977, Val Acc=92.16% | LR: 1.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-Taylor: 92.16% (Epoch 14) ******\n",
      "resnet18 Pruned-Taylor - Epoch 15/50: Train Loss=0.0668, Train Acc=97.70% | Val Loss=0.3381, Val Acc=91.62% | LR: 1.0e-04\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 92.16%\n",
      "resnet18 Pruned-Taylor - Epoch 16/50: Train Loss=0.0608, Train Acc=97.86% | Val Loss=0.3190, Val Acc=91.94% | LR: 1.0e-04\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 92.16%\n",
      "resnet18 Pruned-Taylor - Epoch 17/50: Train Loss=0.0536, Train Acc=98.13% | Val Loss=0.3345, Val Acc=92.12% | LR: 1.0e-04\n",
      "Validation accuracy did not improve for 3 epoch(s). Best: 92.16%\n",
      "resnet18 Pruned-Taylor - Epoch 18/50: Train Loss=0.0445, Train Acc=98.54% | Val Loss=0.3214, Val Acc=92.00% | LR: 2.0e-05\n",
      "Validation accuracy did not improve for 4 epoch(s). Best: 92.16%\n",
      "resnet18 Pruned-Taylor - Epoch 19/50: Train Loss=0.0414, Train Acc=98.67% | Val Loss=0.3170, Val Acc=92.38% | LR: 2.0e-05\n",
      "****** New best validation accuracy for resnet18 Pruned-Taylor: 92.38% (Epoch 19) ******\n",
      "resnet18 Pruned-Taylor - Epoch 20/50: Train Loss=0.0382, Train Acc=98.72% | Val Loss=0.3189, Val Acc=92.36% | LR: 2.0e-05\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 92.38%\n",
      "resnet18 Pruned-Taylor - Epoch 21/50: Train Loss=0.0380, Train Acc=98.74% | Val Loss=0.3364, Val Acc=92.16% | LR: 2.0e-05\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 92.38%\n",
      "resnet18 Pruned-Taylor - Epoch 22/50: Train Loss=0.0371, Train Acc=98.76% | Val Loss=0.3266, Val Acc=92.24% | LR: 2.0e-05\n",
      "Validation accuracy did not improve for 3 epoch(s). Best: 92.38%\n",
      "resnet18 Pruned-Taylor - Epoch 23/50: Train Loss=0.0367, Train Acc=98.73% | Val Loss=0.3321, Val Acc=92.20% | LR: 2.0e-05\n",
      "Validation accuracy did not improve for 4 epoch(s). Best: 92.38%\n",
      "resnet18 Pruned-Taylor - Epoch 24/50: Train Loss=0.0344, Train Acc=98.88% | Val Loss=0.3339, Val Acc=92.30% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 5 epoch(s). Best: 92.38%\n",
      "resnet18 Pruned-Taylor - Epoch 25/50: Train Loss=0.0328, Train Acc=98.90% | Val Loss=0.3263, Val Acc=92.20% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 6 epoch(s). Best: 92.38%\n",
      "resnet18 Pruned-Taylor - Epoch 26/50: Train Loss=0.0343, Train Acc=98.87% | Val Loss=0.3216, Val Acc=92.16% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 7 epoch(s). Best: 92.38%\n",
      "resnet18 Pruned-Taylor - Epoch 27/50: Train Loss=0.0333, Train Acc=98.91% | Val Loss=0.3325, Val Acc=92.30% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 8 epoch(s). Best: 92.38%\n",
      "resnet18 Pruned-Taylor - Epoch 28/50: Train Loss=0.0317, Train Acc=98.93% | Val Loss=0.3380, Val Acc=92.26% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 9 epoch(s). Best: 92.38%\n",
      "resnet18 Pruned-Taylor - Epoch 29/50: Train Loss=0.0336, Train Acc=98.88% | Val Loss=0.3285, Val Acc=92.14% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 10 epoch(s). Best: 92.38%\n",
      "Early stopping triggered for resnet18 Pruned-Taylor after 10 epochs without improvement.\n",
      "Finished training resnet18 Pruned-Taylor. Best validation accuracy achieved: 92.38%\n",
      "\n",
      "--- Evaluating fine-tuned resnet18 (Taylor) on Test Set ---\n",
      "resnet18 FineTuned-Taylor Test Accuracy: 91.54%\n",
      "Test Metrics for Taylor: {'macs': 513632214.0, 'size_mb': 41.574084, 'accuracy': 91.54}\n",
      "âœ… Model saved as ONNX to ./output_resnet_finetuned/resnet18/strategies/resnet18_Taylor_final_best_val.onnx\n",
      "\n",
      "--- Pruning resnet18 with Strategy: lamp ---\n",
      "Initial MACs: 0.557 G, Size: 42.63 MB\n",
      "Step 1/15 (Non-Taylor): MACs 0.514 G, Size 39.65 MB\n",
      "Step 2/15 (Non-Taylor): MACs 0.479 G, Size 36.94 MB\n",
      "Step 3/15 (Non-Taylor): MACs 0.447 G, Size 34.40 MB\n",
      "Step 4/15 (Non-Taylor): MACs 0.414 G, Size 31.85 MB\n",
      "Step 5/15 (Non-Taylor): MACs 0.384 G, Size 29.50 MB\n",
      "Step 6/15 (Non-Taylor): MACs 0.354 G, Size 27.17 MB\n",
      "Step 7/15 (Non-Taylor): MACs 0.327 G, Size 25.00 MB\n",
      "Step 8/15 (Non-Taylor): MACs 0.294 G, Size 22.81 MB\n",
      "Step 9/15 (Non-Taylor): MACs 0.269 G, Size 20.83 MB\n",
      "Step 10/15 (Non-Taylor): MACs 0.245 G, Size 18.88 MB\n",
      "Step 11/15 (Non-Taylor): MACs 0.222 G, Size 17.08 MB\n",
      "Step 12/15 (Non-Taylor): MACs 0.198 G, Size 15.29 MB\n",
      "Step 13/15 (Non-Taylor): MACs 0.178 G, Size 13.68 MB\n",
      "Step 14/15 (Non-Taylor): MACs 0.158 G, Size 12.11 MB\n",
      "Targets reached at step 14 for non-Taylor strategy.\n",
      "After pruning: MACs 0.158 G, Size 12.11 MB\n",
      "Pruning targets achieved.\n",
      "âœ… Model saved as ONNX to ./output_resnet_finetuned/resnet18/strategies/resnet18_lamp_pruned.onnx\n",
      "\n",
      "--- Fine-tuning resnet18 after lamp pruning ---\n",
      "Starting training for resnet18 Pruned-lamp with patience=10\n",
      "resnet18 Pruned-lamp - Epoch 1/50: Train Loss=0.4466, Train Acc=84.79% | Val Loss=0.3847, Val Acc=86.36% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-lamp: 86.36% (Epoch 1) ******\n",
      "resnet18 Pruned-lamp - Epoch 2/50: Train Loss=0.3556, Train Acc=87.80% | Val Loss=0.4115, Val Acc=85.92% | LR: 5.0e-04\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 86.36%\n",
      "resnet18 Pruned-lamp - Epoch 3/50: Train Loss=0.3293, Train Acc=88.54% | Val Loss=0.3713, Val Acc=87.24% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-lamp: 87.24% (Epoch 3) ******\n",
      "resnet18 Pruned-lamp - Epoch 4/50: Train Loss=0.3065, Train Acc=89.44% | Val Loss=0.3643, Val Acc=88.04% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-lamp: 88.04% (Epoch 4) ******\n",
      "resnet18 Pruned-lamp - Epoch 5/50: Train Loss=0.2841, Train Acc=90.15% | Val Loss=0.4014, Val Acc=86.84% | LR: 5.0e-04\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 88.04%\n",
      "resnet18 Pruned-lamp - Epoch 6/50: Train Loss=0.2658, Train Acc=90.72% | Val Loss=0.3500, Val Acc=88.60% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-lamp: 88.60% (Epoch 6) ******\n",
      "resnet18 Pruned-lamp - Epoch 7/50: Train Loss=0.2516, Train Acc=91.18% | Val Loss=0.3445, Val Acc=88.74% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-lamp: 88.74% (Epoch 7) ******\n",
      "resnet18 Pruned-lamp - Epoch 8/50: Train Loss=0.2391, Train Acc=91.70% | Val Loss=0.3364, Val Acc=88.88% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-lamp: 88.88% (Epoch 8) ******\n",
      "resnet18 Pruned-lamp - Epoch 9/50: Train Loss=0.1741, Train Acc=94.03% | Val Loss=0.2943, Val Acc=90.84% | LR: 1.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-lamp: 90.84% (Epoch 9) ******\n",
      "resnet18 Pruned-lamp - Epoch 10/50: Train Loss=0.1584, Train Acc=94.57% | Val Loss=0.2969, Val Acc=90.52% | LR: 1.0e-04\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 90.84%\n",
      "resnet18 Pruned-lamp - Epoch 11/50: Train Loss=0.1535, Train Acc=94.69% | Val Loss=0.2989, Val Acc=90.58% | LR: 1.0e-04\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 90.84%\n",
      "resnet18 Pruned-lamp - Epoch 12/50: Train Loss=0.1413, Train Acc=95.12% | Val Loss=0.3054, Val Acc=90.72% | LR: 1.0e-04\n",
      "Validation accuracy did not improve for 3 epoch(s). Best: 90.84%\n",
      "resnet18 Pruned-lamp - Epoch 13/50: Train Loss=0.1342, Train Acc=95.28% | Val Loss=0.3132, Val Acc=90.64% | LR: 1.0e-04\n",
      "Validation accuracy did not improve for 4 epoch(s). Best: 90.84%\n",
      "resnet18 Pruned-lamp - Epoch 14/50: Train Loss=0.1298, Train Acc=95.52% | Val Loss=0.2992, Val Acc=91.04% | LR: 1.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-lamp: 91.04% (Epoch 14) ******\n",
      "resnet18 Pruned-lamp - Epoch 15/50: Train Loss=0.1180, Train Acc=95.93% | Val Loss=0.3160, Val Acc=90.92% | LR: 2.0e-05\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 91.04%\n",
      "resnet18 Pruned-lamp - Epoch 16/50: Train Loss=0.1108, Train Acc=96.22% | Val Loss=0.3045, Val Acc=90.80% | LR: 2.0e-05\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 91.04%\n",
      "resnet18 Pruned-lamp - Epoch 17/50: Train Loss=0.1115, Train Acc=96.14% | Val Loss=0.3055, Val Acc=91.16% | LR: 2.0e-05\n",
      "****** New best validation accuracy for resnet18 Pruned-lamp: 91.16% (Epoch 17) ******\n",
      "resnet18 Pruned-lamp - Epoch 18/50: Train Loss=0.1104, Train Acc=96.19% | Val Loss=0.3088, Val Acc=91.02% | LR: 2.0e-05\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 91.16%\n",
      "resnet18 Pruned-lamp - Epoch 19/50: Train Loss=0.1084, Train Acc=96.26% | Val Loss=0.3036, Val Acc=91.20% | LR: 2.0e-05\n",
      "****** New best validation accuracy for resnet18 Pruned-lamp: 91.20% (Epoch 19) ******\n",
      "resnet18 Pruned-lamp - Epoch 20/50: Train Loss=0.1050, Train Acc=96.27% | Val Loss=0.3099, Val Acc=91.24% | LR: 2.0e-05\n",
      "****** New best validation accuracy for resnet18 Pruned-lamp: 91.24% (Epoch 20) ******\n",
      "resnet18 Pruned-lamp - Epoch 21/50: Train Loss=0.1034, Train Acc=96.37% | Val Loss=0.3056, Val Acc=91.02% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 91.24%\n",
      "resnet18 Pruned-lamp - Epoch 22/50: Train Loss=0.1044, Train Acc=96.35% | Val Loss=0.3095, Val Acc=91.10% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 91.24%\n",
      "resnet18 Pruned-lamp - Epoch 23/50: Train Loss=0.1024, Train Acc=96.42% | Val Loss=0.3161, Val Acc=91.20% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 3 epoch(s). Best: 91.24%\n",
      "resnet18 Pruned-lamp - Epoch 24/50: Train Loss=0.1038, Train Acc=96.42% | Val Loss=0.3054, Val Acc=91.04% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 4 epoch(s). Best: 91.24%\n",
      "resnet18 Pruned-lamp - Epoch 25/50: Train Loss=0.1015, Train Acc=96.46% | Val Loss=0.3072, Val Acc=91.08% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 5 epoch(s). Best: 91.24%\n",
      "resnet18 Pruned-lamp - Epoch 26/50: Train Loss=0.1023, Train Acc=96.45% | Val Loss=0.3067, Val Acc=90.94% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 6 epoch(s). Best: 91.24%\n",
      "resnet18 Pruned-lamp - Epoch 27/50: Train Loss=0.0988, Train Acc=96.59% | Val Loss=0.3117, Val Acc=91.30% | LR: 1.0e-06\n",
      "****** New best validation accuracy for resnet18 Pruned-lamp: 91.30% (Epoch 27) ******\n",
      "resnet18 Pruned-lamp - Epoch 28/50: Train Loss=0.1011, Train Acc=96.54% | Val Loss=0.3058, Val Acc=91.20% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 91.30%\n",
      "resnet18 Pruned-lamp - Epoch 29/50: Train Loss=0.1006, Train Acc=96.55% | Val Loss=0.3078, Val Acc=91.14% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 91.30%\n",
      "resnet18 Pruned-lamp - Epoch 30/50: Train Loss=0.1004, Train Acc=96.54% | Val Loss=0.3157, Val Acc=90.98% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 3 epoch(s). Best: 91.30%\n",
      "resnet18 Pruned-lamp - Epoch 31/50: Train Loss=0.1015, Train Acc=96.52% | Val Loss=0.3075, Val Acc=90.90% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 4 epoch(s). Best: 91.30%\n",
      "resnet18 Pruned-lamp - Epoch 32/50: Train Loss=0.0992, Train Acc=96.64% | Val Loss=0.3051, Val Acc=90.94% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 5 epoch(s). Best: 91.30%\n",
      "resnet18 Pruned-lamp - Epoch 33/50: Train Loss=0.0993, Train Acc=96.52% | Val Loss=0.3080, Val Acc=91.06% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 6 epoch(s). Best: 91.30%\n",
      "resnet18 Pruned-lamp - Epoch 34/50: Train Loss=0.0993, Train Acc=96.56% | Val Loss=0.3094, Val Acc=91.08% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 7 epoch(s). Best: 91.30%\n",
      "resnet18 Pruned-lamp - Epoch 35/50: Train Loss=0.0989, Train Acc=96.56% | Val Loss=0.3068, Val Acc=91.18% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 8 epoch(s). Best: 91.30%\n",
      "resnet18 Pruned-lamp - Epoch 36/50: Train Loss=0.0987, Train Acc=96.52% | Val Loss=0.3084, Val Acc=91.06% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 9 epoch(s). Best: 91.30%\n",
      "resnet18 Pruned-lamp - Epoch 37/50: Train Loss=0.1005, Train Acc=96.57% | Val Loss=0.3157, Val Acc=91.18% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 10 epoch(s). Best: 91.30%\n",
      "Early stopping triggered for resnet18 Pruned-lamp after 10 epochs without improvement.\n",
      "Finished training resnet18 Pruned-lamp. Best validation accuracy achieved: 91.30%\n",
      "\n",
      "--- Evaluating fine-tuned resnet18 (lamp) on Test Set ---\n",
      "resnet18 FineTuned-lamp Test Accuracy: 89.88%\n",
      "Test Metrics for lamp: {'macs': 158107140.0, 'size_mb': 12.695148, 'accuracy': 89.88}\n",
      "âœ… Model saved as ONNX to ./output_resnet_finetuned/resnet18/strategies/resnet18_lamp_final_best_val.onnx\n",
      "\n",
      "--- Pruning resnet18 with Strategy: geometry ---\n",
      "Initial MACs: 0.557 G, Size: 42.63 MB\n",
      "Step 1/15 (Non-Taylor): MACs 0.514 G, Size 39.65 MB\n",
      "Step 2/15 (Non-Taylor): MACs 0.479 G, Size 36.94 MB\n",
      "Step 3/15 (Non-Taylor): MACs 0.447 G, Size 34.40 MB\n",
      "Step 4/15 (Non-Taylor): MACs 0.414 G, Size 31.85 MB\n",
      "Step 5/15 (Non-Taylor): MACs 0.384 G, Size 29.50 MB\n",
      "Step 6/15 (Non-Taylor): MACs 0.354 G, Size 27.17 MB\n",
      "Step 7/15 (Non-Taylor): MACs 0.327 G, Size 25.00 MB\n",
      "Step 8/15 (Non-Taylor): MACs 0.294 G, Size 22.81 MB\n",
      "Step 9/15 (Non-Taylor): MACs 0.269 G, Size 20.83 MB\n",
      "Step 10/15 (Non-Taylor): MACs 0.245 G, Size 18.88 MB\n",
      "Step 11/15 (Non-Taylor): MACs 0.222 G, Size 17.08 MB\n",
      "Step 12/15 (Non-Taylor): MACs 0.198 G, Size 15.29 MB\n",
      "Step 13/15 (Non-Taylor): MACs 0.178 G, Size 13.68 MB\n",
      "Step 14/15 (Non-Taylor): MACs 0.158 G, Size 12.11 MB\n",
      "Targets reached at step 14 for non-Taylor strategy.\n",
      "After pruning: MACs 0.158 G, Size 12.11 MB\n",
      "Pruning targets achieved.\n",
      "âœ… Model saved as ONNX to ./output_resnet_finetuned/resnet18/strategies/resnet18_geometry_pruned.onnx\n",
      "\n",
      "--- Fine-tuning resnet18 after geometry pruning ---\n",
      "Starting training for resnet18 Pruned-geometry with patience=10\n",
      "resnet18 Pruned-geometry - Epoch 1/50: Train Loss=0.4366, Train Acc=85.16% | Val Loss=0.3893, Val Acc=86.90% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-geometry: 86.90% (Epoch 1) ******\n",
      "resnet18 Pruned-geometry - Epoch 2/50: Train Loss=0.3526, Train Acc=87.71% | Val Loss=0.4186, Val Acc=85.76% | LR: 5.0e-04\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 86.90%\n",
      "resnet18 Pruned-geometry - Epoch 3/50: Train Loss=0.3237, Train Acc=88.75% | Val Loss=0.4064, Val Acc=86.04% | LR: 5.0e-04\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 86.90%\n",
      "resnet18 Pruned-geometry - Epoch 4/50: Train Loss=0.3032, Train Acc=89.37% | Val Loss=0.3510, Val Acc=88.24% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-geometry: 88.24% (Epoch 4) ******\n",
      "resnet18 Pruned-geometry - Epoch 5/50: Train Loss=0.2849, Train Acc=90.14% | Val Loss=0.3468, Val Acc=88.38% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-geometry: 88.38% (Epoch 5) ******\n",
      "resnet18 Pruned-geometry - Epoch 6/50: Train Loss=0.2697, Train Acc=90.60% | Val Loss=0.3618, Val Acc=88.64% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-geometry: 88.64% (Epoch 6) ******\n",
      "resnet18 Pruned-geometry - Epoch 7/50: Train Loss=0.2523, Train Acc=91.20% | Val Loss=0.3496, Val Acc=88.88% | LR: 5.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-geometry: 88.88% (Epoch 7) ******\n",
      "resnet18 Pruned-geometry - Epoch 8/50: Train Loss=0.2359, Train Acc=91.60% | Val Loss=0.3593, Val Acc=88.62% | LR: 5.0e-04\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 88.88%\n",
      "resnet18 Pruned-geometry - Epoch 9/50: Train Loss=0.1756, Train Acc=94.01% | Val Loss=0.3065, Val Acc=90.28% | LR: 1.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-geometry: 90.28% (Epoch 9) ******\n",
      "resnet18 Pruned-geometry - Epoch 10/50: Train Loss=0.1588, Train Acc=94.48% | Val Loss=0.3117, Val Acc=90.60% | LR: 1.0e-04\n",
      "****** New best validation accuracy for resnet18 Pruned-geometry: 90.60% (Epoch 10) ******\n",
      "resnet18 Pruned-geometry - Epoch 11/50: Train Loss=0.1479, Train Acc=94.77% | Val Loss=0.3085, Val Acc=90.24% | LR: 1.0e-04\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 90.60%\n",
      "resnet18 Pruned-geometry - Epoch 12/50: Train Loss=0.1428, Train Acc=95.04% | Val Loss=0.3169, Val Acc=90.44% | LR: 1.0e-04\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 90.60%\n",
      "resnet18 Pruned-geometry - Epoch 13/50: Train Loss=0.1358, Train Acc=95.35% | Val Loss=0.3233, Val Acc=90.40% | LR: 1.0e-04\n",
      "Validation accuracy did not improve for 3 epoch(s). Best: 90.60%\n",
      "resnet18 Pruned-geometry - Epoch 14/50: Train Loss=0.1345, Train Acc=95.41% | Val Loss=0.3175, Val Acc=90.38% | LR: 1.0e-04\n",
      "Validation accuracy did not improve for 4 epoch(s). Best: 90.60%\n",
      "resnet18 Pruned-geometry - Epoch 15/50: Train Loss=0.1184, Train Acc=95.92% | Val Loss=0.3134, Val Acc=90.60% | LR: 2.0e-05\n",
      "Validation accuracy did not improve for 5 epoch(s). Best: 90.60%\n",
      "resnet18 Pruned-geometry - Epoch 16/50: Train Loss=0.1135, Train Acc=96.09% | Val Loss=0.3088, Val Acc=90.64% | LR: 2.0e-05\n",
      "****** New best validation accuracy for resnet18 Pruned-geometry: 90.64% (Epoch 16) ******\n",
      "resnet18 Pruned-geometry - Epoch 17/50: Train Loss=0.1126, Train Acc=96.09% | Val Loss=0.3196, Val Acc=90.60% | LR: 2.0e-05\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 90.64%\n",
      "resnet18 Pruned-geometry - Epoch 18/50: Train Loss=0.1104, Train Acc=96.17% | Val Loss=0.3307, Val Acc=90.54% | LR: 2.0e-05\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 90.64%\n",
      "resnet18 Pruned-geometry - Epoch 19/50: Train Loss=0.1080, Train Acc=96.31% | Val Loss=0.3126, Val Acc=90.56% | LR: 2.0e-05\n",
      "Validation accuracy did not improve for 3 epoch(s). Best: 90.64%\n",
      "resnet18 Pruned-geometry - Epoch 20/50: Train Loss=0.1044, Train Acc=96.35% | Val Loss=0.3170, Val Acc=90.60% | LR: 2.0e-05\n",
      "Validation accuracy did not improve for 4 epoch(s). Best: 90.64%\n",
      "resnet18 Pruned-geometry - Epoch 21/50: Train Loss=0.1027, Train Acc=96.52% | Val Loss=0.3127, Val Acc=90.74% | LR: 4.0e-06\n",
      "****** New best validation accuracy for resnet18 Pruned-geometry: 90.74% (Epoch 21) ******\n",
      "resnet18 Pruned-geometry - Epoch 22/50: Train Loss=0.1031, Train Acc=96.50% | Val Loss=0.3109, Val Acc=90.54% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 90.74%\n",
      "resnet18 Pruned-geometry - Epoch 23/50: Train Loss=0.1023, Train Acc=96.41% | Val Loss=0.3142, Val Acc=90.58% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 90.74%\n",
      "resnet18 Pruned-geometry - Epoch 24/50: Train Loss=0.1038, Train Acc=96.41% | Val Loss=0.3169, Val Acc=90.58% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 3 epoch(s). Best: 90.74%\n",
      "resnet18 Pruned-geometry - Epoch 25/50: Train Loss=0.1031, Train Acc=96.45% | Val Loss=0.3133, Val Acc=90.74% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 4 epoch(s). Best: 90.74%\n",
      "resnet18 Pruned-geometry - Epoch 26/50: Train Loss=0.1019, Train Acc=96.47% | Val Loss=0.3147, Val Acc=90.62% | LR: 4.0e-06\n",
      "Validation accuracy did not improve for 5 epoch(s). Best: 90.74%\n",
      "resnet18 Pruned-geometry - Epoch 27/50: Train Loss=0.1048, Train Acc=96.34% | Val Loss=0.3204, Val Acc=90.72% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 6 epoch(s). Best: 90.74%\n",
      "resnet18 Pruned-geometry - Epoch 28/50: Train Loss=0.1044, Train Acc=96.42% | Val Loss=0.3173, Val Acc=90.82% | LR: 1.0e-06\n",
      "****** New best validation accuracy for resnet18 Pruned-geometry: 90.82% (Epoch 28) ******\n",
      "resnet18 Pruned-geometry - Epoch 29/50: Train Loss=0.1015, Train Acc=96.46% | Val Loss=0.3197, Val Acc=90.88% | LR: 1.0e-06\n",
      "****** New best validation accuracy for resnet18 Pruned-geometry: 90.88% (Epoch 29) ******\n",
      "resnet18 Pruned-geometry - Epoch 30/50: Train Loss=0.1008, Train Acc=96.47% | Val Loss=0.3231, Val Acc=90.60% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 1 epoch(s). Best: 90.88%\n",
      "resnet18 Pruned-geometry - Epoch 31/50: Train Loss=0.1006, Train Acc=96.57% | Val Loss=0.3173, Val Acc=90.68% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 2 epoch(s). Best: 90.88%\n",
      "resnet18 Pruned-geometry - Epoch 32/50: Train Loss=0.1010, Train Acc=96.41% | Val Loss=0.3134, Val Acc=90.78% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 3 epoch(s). Best: 90.88%\n",
      "resnet18 Pruned-geometry - Epoch 33/50: Train Loss=0.1019, Train Acc=96.43% | Val Loss=0.3189, Val Acc=90.70% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 4 epoch(s). Best: 90.88%\n",
      "resnet18 Pruned-geometry - Epoch 34/50: Train Loss=0.1029, Train Acc=96.53% | Val Loss=0.3158, Val Acc=90.58% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 5 epoch(s). Best: 90.88%\n",
      "resnet18 Pruned-geometry - Epoch 35/50: Train Loss=0.1000, Train Acc=96.57% | Val Loss=0.3150, Val Acc=90.58% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 6 epoch(s). Best: 90.88%\n",
      "resnet18 Pruned-geometry - Epoch 36/50: Train Loss=0.0979, Train Acc=96.68% | Val Loss=0.3205, Val Acc=90.66% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 7 epoch(s). Best: 90.88%\n",
      "resnet18 Pruned-geometry - Epoch 37/50: Train Loss=0.0993, Train Acc=96.62% | Val Loss=0.3177, Val Acc=90.82% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 8 epoch(s). Best: 90.88%\n",
      "resnet18 Pruned-geometry - Epoch 38/50: Train Loss=0.1001, Train Acc=96.53% | Val Loss=0.3163, Val Acc=90.72% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 9 epoch(s). Best: 90.88%\n",
      "resnet18 Pruned-geometry - Epoch 39/50: Train Loss=0.1018, Train Acc=96.53% | Val Loss=0.3190, Val Acc=90.68% | LR: 1.0e-06\n",
      "Validation accuracy did not improve for 10 epoch(s). Best: 90.88%\n",
      "Early stopping triggered for resnet18 Pruned-geometry after 10 epochs without improvement.\n",
      "Finished training resnet18 Pruned-geometry. Best validation accuracy achieved: 90.88%\n",
      "\n",
      "--- Evaluating fine-tuned resnet18 (geometry) on Test Set ---\n",
      "resnet18 FineTuned-geometry Test Accuracy: 90.71%\n",
      "Test Metrics for geometry: {'macs': 158107140.0, 'size_mb': 12.695148, 'accuracy': 90.71}\n",
      "âœ… Model saved as ONNX to ./output_resnet_finetuned/resnet18/strategies/resnet18_geometry_final_best_val.onnx\n",
      "\n",
      "=== Pruning Strategy Comparison ===\n",
      "Strategy     | MACs         | Size (MB)  | Accuracy (%)\n",
      "-------------------------------------------------------\n",
      "initial      | 5.57e+08 |     44.70 |        89.47\n",
      "magnitude    | 1.58e+08 |     12.70 |        90.07\n",
      "bn_scale     | 1.58e+08 |     12.70 |        90.14\n",
      "group_norm   | 1.58e+08 |     12.70 |        90.36\n",
      "random       | 1.58e+08 |     12.70 |        90.72\n",
      "Taylor       | 5.14e+08 |     41.57 |        91.54\n",
      "lamp         | 1.58e+08 |     12.70 |        89.88\n",
      "geometry     | 1.58e+08 |     12.70 |        90.71\n",
      "ResNet pruning and enhanced fine-tuning workflow completed successfully!\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
