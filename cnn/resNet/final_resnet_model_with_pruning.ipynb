{
 "cells": [
  {
   "cell_type": "code",
   "id": "e436012c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T10:58:29.943588Z",
     "start_time": "2025-05-30T10:50:13.986829Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_pruning as tp\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_BASE_NAME = \"resnet18\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "def get_data_loaders(data_dir_path='./data', batch_size=128, val_split=0.1, seed=42):\n",
    "    \"\"\"Load CIFAR-10 dataset with train/val/test splits and data augmentation\"\"\"\n",
    "    abs_data_dir = os.path.abspath(data_dir_path)\n",
    "    print(f\"Loading CIFAR-10 from: {abs_data_dir}\")\n",
    "\n",
    "    # Data augmentation for training\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "\n",
    "    # No augmentation for val/test\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "\n",
    "    # Load datasets (assuming pre-downloaded)\n",
    "    full_train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=abs_data_dir, train=True, download=False, transform=transform_train\n",
    "    )\n",
    "\n",
    "    # Create a version with test transforms for validation\n",
    "    full_train_dataset_val = torchvision.datasets.CIFAR10(\n",
    "        root=abs_data_dir, train=True, download=False, transform=transform_test\n",
    "    )\n",
    "\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=abs_data_dir, train=False, download=False, transform=transform_test\n",
    "    )\n",
    "\n",
    "    # Create train/validation split\n",
    "    val_size = int(len(full_train_dataset) * val_split)\n",
    "    train_size = len(full_train_dataset) - val_size\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "\n",
    "    train_indices, val_indices = torch.utils.data.random_split(\n",
    "        range(len(full_train_dataset)), [train_size, val_size], generator=generator\n",
    "    )\n",
    "\n",
    "    # Create subset datasets\n",
    "    train_dataset = torch.utils.data.Subset(full_train_dataset, train_indices.indices)\n",
    "    val_dataset = torch.utils.data.Subset(full_train_dataset_val, val_indices.indices)\n",
    "\n",
    "    # Create data loaders\n",
    "    num_workers = min(4, os.cpu_count() or 2)\n",
    "    pin_memory = True if DEVICE.type == 'cuda' else False\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                             num_workers=num_workers, pin_memory=pin_memory)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                           num_workers=num_workers, pin_memory=pin_memory)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                            num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    print(f\"DataLoaders created - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def get_resnet18_model(num_classes=10, use_pretrained=True, pretrained_path='./base/resnet18-f37072fd.pth'):\n",
    "    \"\"\"Get ResNet-18 model adapted for CIFAR-10\"\"\"\n",
    "    # Always create model without weights first\n",
    "    model = models.resnet18(weights=None)\n",
    "    \n",
    "    if use_pretrained and os.path.exists(pretrained_path):\n",
    "        # Load pre-downloaded weights from local file\n",
    "        print(f\"Loading pre-trained weights from: {pretrained_path}\")\n",
    "        pretrained_state_dict = torch.load(pretrained_path, map_location=DEVICE)\n",
    "        \n",
    "        # Load the weights, ignoring the fc layer if it doesn't match\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_state_dict.items() \n",
    "                          if k in model_dict and model_dict[k].shape == v.shape}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict, strict=False)\n",
    "        print(\"‚úÖ Loaded ResNet-18 with pre-downloaded ImageNet weights\")\n",
    "    else:\n",
    "        if use_pretrained:\n",
    "            print(f\"Warning: Pre-trained weights not found at {pretrained_path}\")\n",
    "        print(\"‚úÖ Created ResNet-18 without pretrained weights\")\n",
    "\n",
    "    # Adapt final fc layer for CIFAR-10\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features, num_classes)\n",
    "    print(f\"‚úÖ Adapted classifier for {num_classes} classes\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_ignored_layers(model):\n",
    "    \"\"\"Get layers to ignore during pruning (typically final classifier)\"\"\"\n",
    "    ignored_layers = []\n",
    "    # For ResNet, the final classifier is the 'fc' layer\n",
    "    if hasattr(model, 'fc'):\n",
    "        ignored_layers.append(model.fc)\n",
    "    return ignored_layers\n",
    "\n",
    "\n",
    "def calculate_macs_params(model, example_input):\n",
    "    \"\"\"Calculate MACs and parameters using torch_pruning\"\"\"\n",
    "    model.eval()\n",
    "    target_device = example_input.device\n",
    "    model_on_device = model.to(target_device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        macs, params = tp.utils.count_ops_and_params(model_on_device, example_input)\n",
    "\n",
    "    return macs, params\n",
    "\n",
    "\n",
    "def save_model(model, save_path, example_input_cpu=None):\n",
    "    \"\"\"Save model state dict and optionally ONNX\"\"\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"‚úÖ Model saved to {save_path}\")\n",
    "\n",
    "    if example_input_cpu is not None:\n",
    "        onnx_path = save_path.replace('.pth', '.onnx')\n",
    "        try:\n",
    "            model_cpu = model.to('cpu')\n",
    "            torch.onnx.export(\n",
    "                model_cpu, example_input_cpu, onnx_path,\n",
    "                export_params=True, opset_version=13,\n",
    "                input_names=['input'], output_names=['output'],\n",
    "                dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "            )\n",
    "            print(f\"‚úÖ ONNX model saved to {onnx_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: ONNX export failed: {e}\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, example_input, criterion, device):\n",
    "    \"\"\"Evaluate model and return comprehensive metrics\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Calculate efficiency metrics\n",
    "    macs, params = calculate_macs_params(model, example_input.to(device))\n",
    "    model_size_mb = params * 4 / (1024 * 1024)  # Assuming float32\n",
    "\n",
    "    # Calculate accuracy and loss\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / total if total > 0 else float('nan')\n",
    "    accuracy = 100.0 * correct / total if total > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'loss': avg_loss,\n",
    "        'macs': macs,\n",
    "        'params': params,\n",
    "        'size_mb': model_size_mb\n",
    "    }\n",
    "\n",
    "\n",
    "def prune_model(model, strategy_config, sparsity_ratio, example_input, ignored_layers=None):\n",
    "    \"\"\"Apply structured pruning to model\"\"\"\n",
    "    if sparsity_ratio == 0.0:\n",
    "        print(\"No pruning needed (sparsity = 0.0)\")\n",
    "        return model\n",
    "\n",
    "    model.eval()\n",
    "    pruned_model = copy.deepcopy(model)\n",
    "    pruned_model.to(example_input.device)\n",
    "\n",
    "    # Calculate initial MACs\n",
    "    initial_macs, _ = calculate_macs_params(pruned_model, example_input)\n",
    "    print(f\"Initial MACs: {initial_macs / 1e6:.2f}M\")\n",
    "\n",
    "    ignored_layers = ignored_layers or []\n",
    "\n",
    "    # Create pruner based on strategy\n",
    "    pruner = strategy_config['pruner'](\n",
    "        pruned_model,\n",
    "        example_input,\n",
    "        importance=strategy_config['importance'],\n",
    "        iterative_steps=5,  # Use 5 iterative steps\n",
    "        ch_sparsity=sparsity_ratio,\n",
    "        root_module_types=[nn.Conv2d],\n",
    "        ignored_layers=ignored_layers\n",
    "    )\n",
    "\n",
    "    print(f\"Applying {strategy_config['importance'].__class__.__name__} pruning at {sparsity_ratio:.1%} sparsity...\")\n",
    "\n",
    "    # Apply pruning\n",
    "    pruner.step()\n",
    "\n",
    "    # Calculate final MACs\n",
    "    final_macs, _ = calculate_macs_params(pruned_model, example_input)\n",
    "    reduction = (initial_macs - final_macs) / initial_macs * 100 if initial_macs > 0 else 0\n",
    "    print(f\"Final MACs: {final_macs / 1e6:.2f}M (Reduction: {reduction:.1f}%)\")\n",
    "\n",
    "    return pruned_model\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs,\n",
    "                val_loader=None, patience=7, log_prefix=\"\"):\n",
    "    \"\"\"Train model with early stopping\"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total += target.size(0)\n",
    "            train_correct += (predicted == target).sum().item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_acc = 100.0 * train_correct / train_total\n",
    "\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "\n",
    "        log_msg = f\"Epoch {epoch + 1}/{num_epochs} ({log_prefix}): Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%\"\n",
    "\n",
    "        # Validation phase\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for data, target in val_loader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = torch.max(output.data, 1)\n",
    "                    val_total += target.size(0)\n",
    "                    val_correct += (predicted == target).sum().item()\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_acc = 100.0 * val_correct / val_total\n",
    "\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            history['val_acc'].append(val_acc)\n",
    "\n",
    "            log_msg += f\", Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%\"\n",
    "\n",
    "            # Early stopping check\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                epochs_no_improve = 0\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "                log_msg += \" (Best)\"\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"{log_msg}\")\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "                break\n",
    "        else:\n",
    "            history['val_loss'].append(None)\n",
    "            history['val_acc'].append(None)\n",
    "\n",
    "        print(log_msg)\n",
    "\n",
    "    # Load best model state if available\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"Loaded best model state\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def save_results_to_files(all_results, output_dir):\n",
    "    \"\"\"Save experimental results to JSON and CSV files\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save complete results as JSON\n",
    "    results_json_path = os.path.join(output_dir, 'complete_results.json')\n",
    "    with open(results_json_path, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2, default=str)\n",
    "    print(f\"‚úÖ Complete results saved to {results_json_path}\")\n",
    "\n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for strategy, strategy_results in all_results.items():\n",
    "        for sparsity, metrics in strategy_results.items():\n",
    "            row = {\n",
    "                'strategy': strategy,\n",
    "                'sparsity_ratio': sparsity,\n",
    "                'accuracy': metrics['accuracy'],\n",
    "                'loss': metrics['loss'],\n",
    "                'macs_millions': metrics['macs'] / 1e6,\n",
    "                'params_millions': metrics['params'] / 1e6,\n",
    "                'size_mb': metrics['size_mb']\n",
    "            }\n",
    "            summary_data.append(row)\n",
    "\n",
    "    # Save summary as CSV\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_csv_path = os.path.join(output_dir, 'summary_results.csv')\n",
    "    summary_df.to_csv(summary_csv_path, index=False)\n",
    "    print(f\"‚úÖ Summary results saved to {summary_csv_path}\")\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "def create_results_plots(summary_df, output_dir):\n",
    "    \"\"\"Create visualization plots\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    strategies = summary_df['strategy'].unique()\n",
    "    sparsity_levels = sorted(summary_df['sparsity_ratio'].unique())\n",
    "\n",
    "    # Plot 1: Accuracy vs Sparsity\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for strategy in strategies:\n",
    "        strategy_data = summary_df[summary_df['strategy'] == strategy].sort_values('sparsity_ratio')\n",
    "        plt.plot(strategy_data['sparsity_ratio'] * 100, strategy_data['accuracy'],\n",
    "                 'o-', linewidth=2, markersize=8, label=strategy)\n",
    "\n",
    "    plt.xlabel('Sparsity (%)', fontsize=12)\n",
    "    plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "    plt.title('ResNet-18: Accuracy vs Sparsity', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(output_dir, 'accuracy_vs_sparsity.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"‚úÖ Accuracy plot saved to {plot_path}\")\n",
    "\n",
    "    # Plot 2: Efficiency frontier (Accuracy vs MACs)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for strategy in strategies:\n",
    "        strategy_data = summary_df[summary_df['strategy'] == strategy].sort_values('sparsity_ratio')\n",
    "        plt.scatter(strategy_data['macs_millions'], strategy_data['accuracy'],\n",
    "                    s=100, label=strategy, alpha=0.8)\n",
    "        plt.plot(strategy_data['macs_millions'], strategy_data['accuracy'],\n",
    "                 '--', alpha=0.6)\n",
    "\n",
    "    plt.xlabel('MACs (Millions)', fontsize=12)\n",
    "    plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "    plt.title('ResNet-18: Efficiency Frontier (Accuracy vs MACs)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(output_dir, 'efficiency_frontier.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"‚úÖ Efficiency frontier plot saved to {plot_path}\")\n",
    "\n",
    "\n",
    "def print_results_table(summary_df):\n",
    "    \"\"\"Print formatted results table\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXPERIMENTAL RESULTS SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Baseline results\n",
    "    baseline_results = summary_df[summary_df['sparsity_ratio'] == 0.0].iloc[0]\n",
    "    print(f\"\\nBaseline Performance:\")\n",
    "    print(f\"  Accuracy: {baseline_results['accuracy']:.2f}%\")\n",
    "    print(f\"  MACs: {baseline_results['macs_millions']:.2f}M\")\n",
    "    print(f\"  Parameters: {baseline_results['params_millions']:.2f}M\")\n",
    "    print(f\"  Model Size: {baseline_results['size_mb']:.2f}MB\")\n",
    "\n",
    "    # Strategy comparison at 50% sparsity\n",
    "    print(f\"\\nStrategy Comparison at 50% Sparsity:\")\n",
    "    sparsity_50_data = summary_df[summary_df['sparsity_ratio'] == 0.5]\n",
    "    for _, row in sparsity_50_data.iterrows():\n",
    "        degradation = baseline_results['accuracy'] - row['accuracy']\n",
    "        retention = (row['accuracy'] / baseline_results['accuracy']) * 100\n",
    "        print(\n",
    "            f\"  {row['strategy']:>12}: {row['accuracy']:>6.2f}% accuracy ({degradation:>+5.2f}%, {retention:>5.1f}% retention)\")\n",
    "\n",
    "    # Complete results table\n",
    "    print(f\"\\nComplete Results Table:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Strategy':<12} {'Sparsity':<8} {'Accuracy':<8} {'MACs(M)':<8} {'Params(M)':<9} {'Size(MB)':<8}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for _, row in summary_df.sort_values(['strategy', 'sparsity_ratio']).iterrows():\n",
    "        print(f\"{row['strategy']:<12} {row['sparsity_ratio'] * 100:>6.0f}% \"\n",
    "              f\"{row['accuracy']:>7.2f}% {row['macs_millions']:>7.2f} \"\n",
    "              f\"{row['params_millions']:>8.2f} {row['size_mb']:>7.2f}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main experimental workflow\"\"\"\n",
    "    print(\"Starting ResNet-18 CIFAR-10 Pruning Experiments\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'strategies': {\n",
    "            'BNScale': {\n",
    "                'pruner': tp.pruner.BNScalePruner,\n",
    "                'importance': tp.importance.BNScaleImportance()\n",
    "            },\n",
    "            'MagnitudeL2': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.MagnitudeImportance(p=2)\n",
    "            },\n",
    "            'Random': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.RandomImportance()\n",
    "            },\n",
    "        },\n",
    "        'pruning_ratios': [0.0, 0.2, 0.5, 0.7],\n",
    "        'num_classes': 10,\n",
    "        'batch_size': 128,\n",
    "        'learning_rate': 0.0001,\n",
    "        'epochs': 1000,\n",
    "        'patience': 20,\n",
    "        'output_dir': './results_resnet18_cifar10',\n",
    "        'models_dir': './base',\n",
    "        'pretrained_path': './base/resnet18-f37072fd.pth'  # Path to pre-downloaded ResNet-18 weights\n",
    "    }\n",
    "\n",
    "    # Create output directories\n",
    "    os.makedirs(config['output_dir'], exist_ok=True)\n",
    "    os.makedirs(config['models_dir'], exist_ok=True)\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading CIFAR-10 dataset...\")\n",
    "    train_loader, val_loader, test_loader = get_data_loaders(\n",
    "        batch_size=config['batch_size']\n",
    "    )\n",
    "\n",
    "    # Prepare inputs and criterion\n",
    "    example_input_cpu = torch.randn(1, 3, 32, 32)\n",
    "    example_input_device = example_input_cpu.to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Get baseline model and train it\n",
    "    print(\"\\nCreating and training baseline model...\")\n",
    "    model = get_resnet18_model(\n",
    "        num_classes=config['num_classes'], \n",
    "        use_pretrained=True,\n",
    "        pretrained_path=config['pretrained_path']\n",
    "    )\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Train baseline model\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    trained_model, training_history = train_model(\n",
    "        model, train_loader, criterion, optimizer, DEVICE,\n",
    "        config['epochs'], val_loader, config['patience'], \"Baseline Training\"\n",
    "    )\n",
    "\n",
    "    # Save baseline model\n",
    "    baseline_model_path = os.path.join(config['models_dir'], 'resnet18_baseline_model.pth')\n",
    "    save_model(trained_model, baseline_model_path, example_input_cpu)\n",
    "\n",
    "    # Evaluate baseline\n",
    "    print(\"\\nEvaluating baseline model...\")\n",
    "    baseline_metrics = evaluate_model(trained_model, test_loader, example_input_device, criterion, DEVICE)\n",
    "    print(f\"Baseline Results: Accuracy={baseline_metrics['accuracy']:.2f}%, \"\n",
    "          f\"MACs={baseline_metrics['macs'] / 1e6:.2f}M, \"\n",
    "          f\"Params={baseline_metrics['params'] / 1e6:.2f}M\")\n",
    "\n",
    "    # Initialize results storage\n",
    "    all_results = {}\n",
    "    for strategy_name in config['strategies'].keys():\n",
    "        all_results[strategy_name] = {0.0: baseline_metrics}\n",
    "\n",
    "    # Get ignored layers\n",
    "    ignored_layers = get_ignored_layers(trained_model)\n",
    "\n",
    "    # Run pruning experiments\n",
    "    print(\"\\nStarting pruning experiments...\")\n",
    "    for strategy_name, strategy_config in config['strategies'].items():\n",
    "        print(f\"\\n--- Strategy: {strategy_name} ---\")\n",
    "\n",
    "        for sparsity_ratio in config['pruning_ratios']:\n",
    "            if sparsity_ratio == 0.0:\n",
    "                continue  # Skip baseline (already done)\n",
    "\n",
    "            print(f\"\\nProcessing {strategy_name} at {sparsity_ratio:.1%} sparsity...\")\n",
    "\n",
    "            # Load fresh copy of trained baseline\n",
    "            model_copy = get_resnet18_model(\n",
    "                num_classes=config['num_classes'], \n",
    "                use_pretrained=False\n",
    "            )\n",
    "            model_copy.load_state_dict(torch.load(baseline_model_path, map_location=DEVICE))\n",
    "            model_copy.to(DEVICE)\n",
    "\n",
    "            # Apply pruning\n",
    "            pruned_model = prune_model(\n",
    "                model_copy, strategy_config, sparsity_ratio,\n",
    "                example_input_device, ignored_layers\n",
    "            )\n",
    "\n",
    "            # Fine-tune pruned model\n",
    "            print(\"Fine-tuning pruned model...\")\n",
    "            optimizer_ft = optim.Adam(pruned_model.parameters(), lr=config['learning_rate'])\n",
    "            fine_tuned_model, ft_history = train_model(\n",
    "                pruned_model, train_loader, criterion, optimizer_ft, DEVICE,\n",
    "                config['epochs'], val_loader, config['patience'],\n",
    "                f\"{strategy_name}-{sparsity_ratio:.1%}\"\n",
    "            )\n",
    "\n",
    "            # Evaluate fine-tuned model\n",
    "            final_metrics = evaluate_model(fine_tuned_model, test_loader, example_input_device, criterion, DEVICE)\n",
    "            all_results[strategy_name][sparsity_ratio] = final_metrics\n",
    "\n",
    "            print(f\"Results: Accuracy={final_metrics['accuracy']:.2f}%, \"\n",
    "                  f\"MACs={final_metrics['macs'] / 1e6:.2f}M\")\n",
    "\n",
    "            # Save fine-tuned model\n",
    "            model_filename = f\"resnet18_{strategy_name.lower()}_sparsity_{sparsity_ratio:.1f}.pth\"\n",
    "            model_path = os.path.join(config['models_dir'], model_filename)\n",
    "            save_model(fine_tuned_model, model_path, example_input_cpu)\n",
    "\n",
    "    # Save and analyze results\n",
    "    print(\"\\nSaving results...\")\n",
    "    summary_df = save_results_to_files(all_results, config['output_dir'])\n",
    "\n",
    "    # Create plots\n",
    "    print(\"Creating plots...\")\n",
    "    create_results_plots(summary_df, config['output_dir'])\n",
    "\n",
    "    # Print summary\n",
    "    print_results_table(summary_df)\n",
    "\n",
    "    print(f\"\\nüéâ All experiments completed!\")\n",
    "    print(f\"üìÅ Results saved to: {os.path.abspath(config['output_dir'])}\")\n",
    "    print(f\"üìÅ Models saved to: {os.path.abspath(config['models_dir'])}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting ResNet-18 CIFAR-10 Pruning Experiments\n",
      "============================================================\n",
      "Loading CIFAR-10 dataset...\n",
      "Loading CIFAR-10 from: /home/muis/thesis/github-repo/master-thesis/cnn/resNet/data\n",
      "DataLoaders created - Train: 45000, Val: 5000, Test: 10000\n",
      "\n",
      "Creating and training baseline model...\n",
      "Loading pre-trained weights from: ./base/resnet18-f37072fd.pth\n",
      "‚úÖ Loaded ResNet-18 with pre-downloaded ImageNet weights\n",
      "‚úÖ Adapted classifier for 10 classes\n",
      "Epoch 1/1000 (Baseline Training): Train Loss: 1.0497, Train Acc: 63.49%, Val Loss: 0.7101, Val Acc: 75.10% (Best)\n",
      "Epoch 2/1000 (Baseline Training): Train Loss: 0.5670, Train Acc: 80.53%, Val Loss: 0.6352, Val Acc: 78.20% (Best)\n",
      "Epoch 3/1000 (Baseline Training): Train Loss: 0.3687, Train Acc: 87.21%, Val Loss: 0.6406, Val Acc: 78.54%\n",
      "Epoch 4/1000 (Baseline Training): Train Loss: 0.2301, Train Acc: 92.19%, Val Loss: 0.6873, Val Acc: 79.52%\n",
      "Epoch 5/1000 (Baseline Training): Train Loss: 0.1450, Train Acc: 95.12%, Val Loss: 0.7311, Val Acc: 79.70%\n",
      "Epoch 6/1000 (Baseline Training): Train Loss: 0.1053, Train Acc: 96.42%, Val Loss: 0.7692, Val Acc: 79.28%\n",
      "Epoch 7/1000 (Baseline Training): Train Loss: 0.0789, Train Acc: 97.39%, Val Loss: 0.8554, Val Acc: 79.72%\n",
      "Epoch 8/1000 (Baseline Training): Train Loss: 0.0721, Train Acc: 97.50%, Val Loss: 0.8679, Val Acc: 79.96%\n",
      "Epoch 9/1000 (Baseline Training): Train Loss: 0.0610, Train Acc: 97.97%, Val Loss: 0.8702, Val Acc: 80.36%\n",
      "Epoch 10/1000 (Baseline Training): Train Loss: 0.0499, Train Acc: 98.34%, Val Loss: 0.9089, Val Acc: 80.14%\n",
      "Epoch 11/1000 (Baseline Training): Train Loss: 0.0480, Train Acc: 98.38%, Val Loss: 0.9268, Val Acc: 79.82%\n",
      "Epoch 12/1000 (Baseline Training): Train Loss: 0.0512, Train Acc: 98.32%, Val Loss: 0.9284, Val Acc: 80.34%\n",
      "Epoch 13/1000 (Baseline Training): Train Loss: 0.0457, Train Acc: 98.44%, Val Loss: 0.9120, Val Acc: 80.42%\n",
      "Epoch 14/1000 (Baseline Training): Train Loss: 0.0363, Train Acc: 98.79%, Val Loss: 0.9713, Val Acc: 80.24%\n",
      "Epoch 15/1000 (Baseline Training): Train Loss: 0.0420, Train Acc: 98.56%, Val Loss: 0.9819, Val Acc: 80.30%\n",
      "Epoch 16/1000 (Baseline Training): Train Loss: 0.0391, Train Acc: 98.71%, Val Loss: 0.9426, Val Acc: 81.34%\n",
      "Epoch 17/1000 (Baseline Training): Train Loss: 0.0321, Train Acc: 98.92%, Val Loss: 0.9351, Val Acc: 81.06%\n",
      "Epoch 18/1000 (Baseline Training): Train Loss: 0.0327, Train Acc: 98.93%, Val Loss: 0.9342, Val Acc: 80.78%\n",
      "Epoch 19/1000 (Baseline Training): Train Loss: 0.0333, Train Acc: 98.87%, Val Loss: 0.9811, Val Acc: 81.24%\n",
      "Epoch 20/1000 (Baseline Training): Train Loss: 0.0319, Train Acc: 98.90%, Val Loss: 0.9718, Val Acc: 81.22%\n",
      "Epoch 21/1000 (Baseline Training): Train Loss: 0.0301, Train Acc: 98.99%, Val Loss: 0.9811, Val Acc: 81.08%\n",
      "Epoch 22/1000 (Baseline Training): Train Loss: 0.0350, Train Acc: 98.84%, Val Loss: 0.9777, Val Acc: 81.26%\n",
      "Early stopping triggered after 22 epochs\n",
      "Loaded best model state\n",
      "‚úÖ Model saved to ./base/resnet18_baseline_model.pth\n",
      "‚úÖ ONNX model saved to ./base/resnet18_baseline_model.onnx\n",
      "\n",
      "Evaluating baseline model...\n",
      "Baseline Results: Accuracy=78.16%, MACs=37.18M, Params=11.18M\n",
      "\n",
      "Starting pruning experiments...\n",
      "\n",
      "--- Strategy: BNScale ---\n",
      "\n",
      "Processing BNScale at 20.0% sparsity...\n",
      "‚úÖ Created ResNet-18 without pretrained weights\n",
      "‚úÖ Adapted classifier for 10 classes\n",
      "Initial MACs: 37.18M\n",
      "Applying BNScaleImportance pruning at 20.0% sparsity...\n",
      "Final MACs: 34.04M (Reduction: 8.4%)\n",
      "Fine-tuning pruned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch_pruning/pruner/algorithms/base_pruner.py:87: UserWarning: ch_sparsity is deprecated in v1.3.0. Please use pruning_ratio.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 (BNScale-20.0%): Train Loss: 0.5060, Train Acc: 82.39%, Val Loss: 0.6056, Val Acc: 79.40% (Best)\n",
      "Epoch 2/1000 (BNScale-20.0%): Train Loss: 0.3045, Train Acc: 89.63%, Val Loss: 0.6465, Val Acc: 79.12%\n",
      "Epoch 3/1000 (BNScale-20.0%): Train Loss: 0.1918, Train Acc: 93.35%, Val Loss: 0.7533, Val Acc: 78.72%\n",
      "Epoch 4/1000 (BNScale-20.0%): Train Loss: 0.1237, Train Acc: 95.85%, Val Loss: 0.7984, Val Acc: 79.78%\n",
      "Epoch 5/1000 (BNScale-20.0%): Train Loss: 0.0930, Train Acc: 96.80%, Val Loss: 0.8136, Val Acc: 80.58%\n",
      "Epoch 6/1000 (BNScale-20.0%): Train Loss: 0.0735, Train Acc: 97.55%, Val Loss: 0.8608, Val Acc: 80.12%\n",
      "Epoch 7/1000 (BNScale-20.0%): Train Loss: 0.0734, Train Acc: 97.46%, Val Loss: 0.8836, Val Acc: 80.20%\n",
      "Epoch 8/1000 (BNScale-20.0%): Train Loss: 0.0612, Train Acc: 97.96%, Val Loss: 0.9292, Val Acc: 80.34%\n",
      "Epoch 9/1000 (BNScale-20.0%): Train Loss: 0.0563, Train Acc: 98.04%, Val Loss: 0.9369, Val Acc: 80.08%\n",
      "Epoch 10/1000 (BNScale-20.0%): Train Loss: 0.0489, Train Acc: 98.36%, Val Loss: 0.9324, Val Acc: 80.08%\n",
      "Epoch 11/1000 (BNScale-20.0%): Train Loss: 0.0431, Train Acc: 98.57%, Val Loss: 0.9817, Val Acc: 78.96%\n",
      "Epoch 12/1000 (BNScale-20.0%): Train Loss: 0.0462, Train Acc: 98.46%, Val Loss: 0.9503, Val Acc: 80.38%\n",
      "Epoch 13/1000 (BNScale-20.0%): Train Loss: 0.0370, Train Acc: 98.70%, Val Loss: 1.0339, Val Acc: 80.04%\n",
      "Epoch 14/1000 (BNScale-20.0%): Train Loss: 0.0402, Train Acc: 98.62%, Val Loss: 0.9523, Val Acc: 81.08%\n",
      "Epoch 15/1000 (BNScale-20.0%): Train Loss: 0.0364, Train Acc: 98.76%, Val Loss: 0.9953, Val Acc: 80.76%\n",
      "Epoch 16/1000 (BNScale-20.0%): Train Loss: 0.0300, Train Acc: 99.00%, Val Loss: 1.0150, Val Acc: 80.30%\n",
      "Epoch 17/1000 (BNScale-20.0%): Train Loss: 0.0343, Train Acc: 98.79%, Val Loss: 1.0033, Val Acc: 80.76%\n",
      "Epoch 18/1000 (BNScale-20.0%): Train Loss: 0.0368, Train Acc: 98.72%, Val Loss: 1.0367, Val Acc: 80.38%\n",
      "Epoch 19/1000 (BNScale-20.0%): Train Loss: 0.0346, Train Acc: 98.84%, Val Loss: 1.0058, Val Acc: 81.10%\n",
      "Epoch 20/1000 (BNScale-20.0%): Train Loss: 0.0272, Train Acc: 99.07%, Val Loss: 1.0031, Val Acc: 80.94%\n",
      "Epoch 21/1000 (BNScale-20.0%): Train Loss: 0.0247, Train Acc: 99.16%, Val Loss: 1.0410, Val Acc: 80.42%\n",
      "Early stopping triggered after 21 epochs\n",
      "Loaded best model state\n",
      "Results: Accuracy=78.26%, MACs=34.04M\n",
      "‚úÖ Model saved to ./base/resnet18_bnscale_sparsity_0.2.pth\n",
      "‚úÖ ONNX model saved to ./base/resnet18_bnscale_sparsity_0.2.onnx\n",
      "\n",
      "Processing BNScale at 50.0% sparsity...\n",
      "‚úÖ Created ResNet-18 without pretrained weights\n",
      "‚úÖ Adapted classifier for 10 classes\n",
      "Initial MACs: 37.18M\n",
      "Applying BNScaleImportance pruning at 50.0% sparsity...\n",
      "Final MACs: 30.09M (Reduction: 19.1%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/1000 (BNScale-50.0%): Train Loss: 0.6394, Train Acc: 78.10%, Val Loss: 0.6551, Val Acc: 76.88% (Best)\n",
      "Epoch 2/1000 (BNScale-50.0%): Train Loss: 0.4059, Train Acc: 85.95%, Val Loss: 0.6385, Val Acc: 78.74% (Best)\n",
      "Epoch 3/1000 (BNScale-50.0%): Train Loss: 0.2605, Train Acc: 91.05%, Val Loss: 0.7167, Val Acc: 77.90%\n",
      "Epoch 4/1000 (BNScale-50.0%): Train Loss: 0.1738, Train Acc: 94.13%, Val Loss: 0.7532, Val Acc: 78.28%\n",
      "Epoch 5/1000 (BNScale-50.0%): Train Loss: 0.1183, Train Acc: 96.06%, Val Loss: 0.8053, Val Acc: 78.78%\n",
      "Epoch 6/1000 (BNScale-50.0%): Train Loss: 0.0878, Train Acc: 96.96%, Val Loss: 0.8689, Val Acc: 78.84%\n",
      "Epoch 7/1000 (BNScale-50.0%): Train Loss: 0.0813, Train Acc: 97.25%, Val Loss: 0.9169, Val Acc: 78.96%\n",
      "Epoch 8/1000 (BNScale-50.0%): Train Loss: 0.0693, Train Acc: 97.72%, Val Loss: 0.9585, Val Acc: 78.58%\n",
      "Epoch 9/1000 (BNScale-50.0%): Train Loss: 0.0637, Train Acc: 97.78%, Val Loss: 0.9423, Val Acc: 79.22%\n",
      "Epoch 10/1000 (BNScale-50.0%): Train Loss: 0.0524, Train Acc: 98.21%, Val Loss: 1.0022, Val Acc: 78.96%\n",
      "Epoch 11/1000 (BNScale-50.0%): Train Loss: 0.0553, Train Acc: 98.13%, Val Loss: 0.9831, Val Acc: 79.26%\n",
      "Epoch 12/1000 (BNScale-50.0%): Train Loss: 0.0470, Train Acc: 98.43%, Val Loss: 0.9819, Val Acc: 79.34%\n",
      "Epoch 13/1000 (BNScale-50.0%): Train Loss: 0.0417, Train Acc: 98.60%, Val Loss: 1.0259, Val Acc: 78.88%\n",
      "Epoch 14/1000 (BNScale-50.0%): Train Loss: 0.0442, Train Acc: 98.51%, Val Loss: 1.0216, Val Acc: 79.36%\n",
      "Epoch 15/1000 (BNScale-50.0%): Train Loss: 0.0409, Train Acc: 98.63%, Val Loss: 1.0086, Val Acc: 79.80%\n",
      "Epoch 16/1000 (BNScale-50.0%): Train Loss: 0.0334, Train Acc: 98.90%, Val Loss: 1.0318, Val Acc: 80.14%\n",
      "Epoch 17/1000 (BNScale-50.0%): Train Loss: 0.0400, Train Acc: 98.67%, Val Loss: 1.0371, Val Acc: 79.62%\n",
      "Epoch 18/1000 (BNScale-50.0%): Train Loss: 0.0341, Train Acc: 98.85%, Val Loss: 1.0293, Val Acc: 80.20%\n",
      "Epoch 19/1000 (BNScale-50.0%): Train Loss: 0.0352, Train Acc: 98.79%, Val Loss: 1.0210, Val Acc: 79.98%\n",
      "Epoch 20/1000 (BNScale-50.0%): Train Loss: 0.0302, Train Acc: 99.01%, Val Loss: 1.1134, Val Acc: 79.46%\n",
      "Epoch 21/1000 (BNScale-50.0%): Train Loss: 0.0349, Train Acc: 98.81%, Val Loss: 1.0449, Val Acc: 79.54%\n",
      "Epoch 22/1000 (BNScale-50.0%): Train Loss: 0.0309, Train Acc: 98.95%, Val Loss: 1.0706, Val Acc: 79.62%\n",
      "Early stopping triggered after 22 epochs\n",
      "Loaded best model state\n",
      "Results: Accuracy=78.37%, MACs=30.09M\n",
      "‚úÖ Model saved to ./base/resnet18_bnscale_sparsity_0.5.pth\n",
      "‚úÖ ONNX model saved to ./base/resnet18_bnscale_sparsity_0.5.onnx\n",
      "\n",
      "Processing BNScale at 70.0% sparsity...\n",
      "‚úÖ Created ResNet-18 without pretrained weights\n",
      "‚úÖ Adapted classifier for 10 classes\n",
      "Initial MACs: 37.18M\n",
      "Applying BNScaleImportance pruning at 70.0% sparsity...\n",
      "Final MACs: 27.77M (Reduction: 25.3%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/1000 (BNScale-70.0%): Train Loss: 0.7158, Train Acc: 75.55%, Val Loss: 0.6746, Val Acc: 76.16% (Best)\n",
      "Epoch 2/1000 (BNScale-70.0%): Train Loss: 0.4598, Train Acc: 84.33%, Val Loss: 0.6450, Val Acc: 78.16% (Best)\n",
      "Epoch 3/1000 (BNScale-70.0%): Train Loss: 0.3129, Train Acc: 89.33%, Val Loss: 0.7006, Val Acc: 78.02%\n",
      "Epoch 4/1000 (BNScale-70.0%): Train Loss: 0.2108, Train Acc: 92.72%, Val Loss: 0.7488, Val Acc: 78.50%\n",
      "Epoch 5/1000 (BNScale-70.0%): Train Loss: 0.1475, Train Acc: 95.01%, Val Loss: 0.8129, Val Acc: 78.64%\n",
      "Epoch 6/1000 (BNScale-70.0%): Train Loss: 0.1109, Train Acc: 96.33%, Val Loss: 0.8740, Val Acc: 77.78%\n",
      "Epoch 7/1000 (BNScale-70.0%): Train Loss: 0.0867, Train Acc: 97.07%, Val Loss: 0.9101, Val Acc: 78.30%\n",
      "Epoch 8/1000 (BNScale-70.0%): Train Loss: 0.0659, Train Acc: 97.82%, Val Loss: 0.9772, Val Acc: 78.22%\n",
      "Epoch 9/1000 (BNScale-70.0%): Train Loss: 0.0681, Train Acc: 97.69%, Val Loss: 0.9485, Val Acc: 78.82%\n",
      "Epoch 10/1000 (BNScale-70.0%): Train Loss: 0.0677, Train Acc: 97.73%, Val Loss: 0.9458, Val Acc: 79.64%\n",
      "Epoch 11/1000 (BNScale-70.0%): Train Loss: 0.0556, Train Acc: 98.15%, Val Loss: 0.9974, Val Acc: 78.42%\n",
      "Epoch 12/1000 (BNScale-70.0%): Train Loss: 0.0475, Train Acc: 98.47%, Val Loss: 1.0007, Val Acc: 78.86%\n",
      "Epoch 13/1000 (BNScale-70.0%): Train Loss: 0.0478, Train Acc: 98.39%, Val Loss: 1.0429, Val Acc: 78.86%\n",
      "Epoch 14/1000 (BNScale-70.0%): Train Loss: 0.0422, Train Acc: 98.58%, Val Loss: 1.0549, Val Acc: 78.58%\n",
      "Epoch 15/1000 (BNScale-70.0%): Train Loss: 0.0428, Train Acc: 98.56%, Val Loss: 1.0474, Val Acc: 78.68%\n",
      "Epoch 16/1000 (BNScale-70.0%): Train Loss: 0.0411, Train Acc: 98.58%, Val Loss: 1.0376, Val Acc: 79.04%\n",
      "Epoch 17/1000 (BNScale-70.0%): Train Loss: 0.0418, Train Acc: 98.59%, Val Loss: 1.0543, Val Acc: 78.54%\n",
      "Epoch 18/1000 (BNScale-70.0%): Train Loss: 0.0410, Train Acc: 98.59%, Val Loss: 1.0235, Val Acc: 79.38%\n",
      "Epoch 19/1000 (BNScale-70.0%): Train Loss: 0.0330, Train Acc: 98.88%, Val Loss: 1.0629, Val Acc: 79.60%\n",
      "Epoch 20/1000 (BNScale-70.0%): Train Loss: 0.0344, Train Acc: 98.88%, Val Loss: 1.0676, Val Acc: 79.20%\n",
      "Epoch 21/1000 (BNScale-70.0%): Train Loss: 0.0283, Train Acc: 99.06%, Val Loss: 1.0910, Val Acc: 78.68%\n",
      "Epoch 22/1000 (BNScale-70.0%): Train Loss: 0.0327, Train Acc: 98.88%, Val Loss: 1.0831, Val Acc: 78.90%\n",
      "Early stopping triggered after 22 epochs\n",
      "Loaded best model state\n",
      "Results: Accuracy=77.14%, MACs=27.77M\n",
      "‚úÖ Model saved to ./base/resnet18_bnscale_sparsity_0.7.pth\n",
      "‚úÖ ONNX model saved to ./base/resnet18_bnscale_sparsity_0.7.onnx\n",
      "\n",
      "--- Strategy: MagnitudeL2 ---\n",
      "\n",
      "Processing MagnitudeL2 at 20.0% sparsity...\n",
      "‚úÖ Created ResNet-18 without pretrained weights\n",
      "‚úÖ Adapted classifier for 10 classes\n",
      "Initial MACs: 37.18M\n",
      "Applying MagnitudeImportance pruning at 20.0% sparsity...\n",
      "Final MACs: 34.04M (Reduction: 8.4%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/1000 (MagnitudeL2-20.0%): Train Loss: 0.4537, Train Acc: 84.44%, Val Loss: 0.6066, Val Acc: 80.00% (Best)\n",
      "Epoch 2/1000 (MagnitudeL2-20.0%): Train Loss: 0.2732, Train Acc: 90.62%, Val Loss: 0.6526, Val Acc: 80.30%\n",
      "Epoch 3/1000 (MagnitudeL2-20.0%): Train Loss: 0.1693, Train Acc: 94.32%, Val Loss: 0.7230, Val Acc: 79.58%\n",
      "Epoch 4/1000 (MagnitudeL2-20.0%): Train Loss: 0.1104, Train Acc: 96.33%, Val Loss: 0.7615, Val Acc: 80.08%\n",
      "Epoch 5/1000 (MagnitudeL2-20.0%): Train Loss: 0.0887, Train Acc: 97.09%, Val Loss: 0.8169, Val Acc: 79.52%\n",
      "Epoch 6/1000 (MagnitudeL2-20.0%): Train Loss: 0.0745, Train Acc: 97.43%, Val Loss: 0.8545, Val Acc: 80.32%\n",
      "Epoch 7/1000 (MagnitudeL2-20.0%): Train Loss: 0.0632, Train Acc: 97.84%, Val Loss: 0.8692, Val Acc: 80.22%\n",
      "Epoch 8/1000 (MagnitudeL2-20.0%): Train Loss: 0.0541, Train Acc: 98.17%, Val Loss: 0.9230, Val Acc: 80.10%\n",
      "Epoch 9/1000 (MagnitudeL2-20.0%): Train Loss: 0.0566, Train Acc: 98.09%, Val Loss: 0.9416, Val Acc: 80.18%\n",
      "Epoch 10/1000 (MagnitudeL2-20.0%): Train Loss: 0.0482, Train Acc: 98.34%, Val Loss: 0.9139, Val Acc: 80.62%\n",
      "Epoch 11/1000 (MagnitudeL2-20.0%): Train Loss: 0.0427, Train Acc: 98.56%, Val Loss: 0.9562, Val Acc: 79.98%\n",
      "Epoch 12/1000 (MagnitudeL2-20.0%): Train Loss: 0.0399, Train Acc: 98.63%, Val Loss: 0.9747, Val Acc: 80.24%\n",
      "Epoch 13/1000 (MagnitudeL2-20.0%): Train Loss: 0.0405, Train Acc: 98.70%, Val Loss: 0.9573, Val Acc: 80.88%\n",
      "Epoch 14/1000 (MagnitudeL2-20.0%): Train Loss: 0.0392, Train Acc: 98.67%, Val Loss: 0.9405, Val Acc: 80.60%\n",
      "Epoch 15/1000 (MagnitudeL2-20.0%): Train Loss: 0.0373, Train Acc: 98.76%, Val Loss: 0.9752, Val Acc: 80.60%\n",
      "Epoch 16/1000 (MagnitudeL2-20.0%): Train Loss: 0.0370, Train Acc: 98.69%, Val Loss: 0.9824, Val Acc: 80.50%\n",
      "Epoch 17/1000 (MagnitudeL2-20.0%): Train Loss: 0.0325, Train Acc: 98.90%, Val Loss: 0.9677, Val Acc: 81.28%\n",
      "Epoch 18/1000 (MagnitudeL2-20.0%): Train Loss: 0.0304, Train Acc: 98.97%, Val Loss: 0.9958, Val Acc: 80.98%\n",
      "Epoch 19/1000 (MagnitudeL2-20.0%): Train Loss: 0.0335, Train Acc: 98.88%, Val Loss: 0.9736, Val Acc: 80.68%\n",
      "Epoch 20/1000 (MagnitudeL2-20.0%): Train Loss: 0.0318, Train Acc: 98.90%, Val Loss: 0.9913, Val Acc: 80.46%\n",
      "Epoch 21/1000 (MagnitudeL2-20.0%): Train Loss: 0.0239, Train Acc: 99.23%, Val Loss: 0.9828, Val Acc: 80.92%\n",
      "Early stopping triggered after 21 epochs\n",
      "Loaded best model state\n",
      "Results: Accuracy=79.13%, MACs=34.04M\n",
      "‚úÖ Model saved to ./base/resnet18_magnitudel2_sparsity_0.2.pth\n",
      "‚úÖ ONNX model saved to ./base/resnet18_magnitudel2_sparsity_0.2.onnx\n",
      "\n",
      "Processing MagnitudeL2 at 50.0% sparsity...\n",
      "‚úÖ Created ResNet-18 without pretrained weights\n",
      "‚úÖ Adapted classifier for 10 classes\n",
      "Initial MACs: 37.18M\n",
      "Applying MagnitudeImportance pruning at 50.0% sparsity...\n",
      "Final MACs: 30.09M (Reduction: 19.1%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/1000 (MagnitudeL2-50.0%): Train Loss: 0.5330, Train Acc: 81.70%, Val Loss: 0.6293, Val Acc: 78.34% (Best)\n",
      "Epoch 2/1000 (MagnitudeL2-50.0%): Train Loss: 0.3309, Train Acc: 88.68%, Val Loss: 0.6472, Val Acc: 78.88%\n",
      "Epoch 3/1000 (MagnitudeL2-50.0%): Train Loss: 0.2152, Train Acc: 92.70%, Val Loss: 0.6724, Val Acc: 79.10%\n",
      "Epoch 4/1000 (MagnitudeL2-50.0%): Train Loss: 0.1407, Train Acc: 95.39%, Val Loss: 0.7725, Val Acc: 79.62%\n",
      "Epoch 5/1000 (MagnitudeL2-50.0%): Train Loss: 0.0960, Train Acc: 96.81%, Val Loss: 0.8314, Val Acc: 79.12%\n",
      "Epoch 6/1000 (MagnitudeL2-50.0%): Train Loss: 0.0769, Train Acc: 97.42%, Val Loss: 0.8626, Val Acc: 78.88%\n",
      "Epoch 7/1000 (MagnitudeL2-50.0%): Train Loss: 0.0690, Train Acc: 97.72%, Val Loss: 0.8740, Val Acc: 79.32%\n",
      "Epoch 8/1000 (MagnitudeL2-50.0%): Train Loss: 0.0658, Train Acc: 97.74%, Val Loss: 0.9012, Val Acc: 79.42%\n",
      "Epoch 9/1000 (MagnitudeL2-50.0%): Train Loss: 0.0558, Train Acc: 98.10%, Val Loss: 0.9153, Val Acc: 80.18%\n",
      "Epoch 10/1000 (MagnitudeL2-50.0%): Train Loss: 0.0507, Train Acc: 98.29%, Val Loss: 0.9492, Val Acc: 79.28%\n",
      "Epoch 11/1000 (MagnitudeL2-50.0%): Train Loss: 0.0420, Train Acc: 98.58%, Val Loss: 0.9568, Val Acc: 80.32%\n",
      "Epoch 12/1000 (MagnitudeL2-50.0%): Train Loss: 0.0403, Train Acc: 98.68%, Val Loss: 0.9930, Val Acc: 80.16%\n",
      "Epoch 13/1000 (MagnitudeL2-50.0%): Train Loss: 0.0418, Train Acc: 98.64%, Val Loss: 1.0041, Val Acc: 79.90%\n",
      "Epoch 14/1000 (MagnitudeL2-50.0%): Train Loss: 0.0406, Train Acc: 98.61%, Val Loss: 1.0059, Val Acc: 80.40%\n",
      "Epoch 15/1000 (MagnitudeL2-50.0%): Train Loss: 0.0355, Train Acc: 98.82%, Val Loss: 0.9956, Val Acc: 80.46%\n",
      "Epoch 16/1000 (MagnitudeL2-50.0%): Train Loss: 0.0324, Train Acc: 98.95%, Val Loss: 1.0122, Val Acc: 80.56%\n",
      "Epoch 17/1000 (MagnitudeL2-50.0%): Train Loss: 0.0358, Train Acc: 98.80%, Val Loss: 0.9901, Val Acc: 80.38%\n",
      "Epoch 18/1000 (MagnitudeL2-50.0%): Train Loss: 0.0316, Train Acc: 98.92%, Val Loss: 0.9981, Val Acc: 80.20%\n",
      "Epoch 19/1000 (MagnitudeL2-50.0%): Train Loss: 0.0310, Train Acc: 98.96%, Val Loss: 1.0448, Val Acc: 80.00%\n",
      "Epoch 20/1000 (MagnitudeL2-50.0%): Train Loss: 0.0343, Train Acc: 98.82%, Val Loss: 0.9788, Val Acc: 80.52%\n",
      "Epoch 21/1000 (MagnitudeL2-50.0%): Train Loss: 0.0286, Train Acc: 98.97%, Val Loss: 1.0206, Val Acc: 80.82%\n",
      "Early stopping triggered after 21 epochs\n",
      "Loaded best model state\n",
      "Results: Accuracy=77.92%, MACs=30.09M\n",
      "‚úÖ Model saved to ./base/resnet18_magnitudel2_sparsity_0.5.pth\n",
      "‚úÖ ONNX model saved to ./base/resnet18_magnitudel2_sparsity_0.5.onnx\n",
      "\n",
      "Processing MagnitudeL2 at 70.0% sparsity...\n",
      "‚úÖ Created ResNet-18 without pretrained weights\n",
      "‚úÖ Adapted classifier for 10 classes\n",
      "Initial MACs: 37.18M\n",
      "Applying MagnitudeImportance pruning at 70.0% sparsity...\n",
      "Final MACs: 27.77M (Reduction: 25.3%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/1000 (MagnitudeL2-70.0%): Train Loss: 0.5888, Train Acc: 79.66%, Val Loss: 0.6449, Val Acc: 77.92% (Best)\n",
      "Epoch 2/1000 (MagnitudeL2-70.0%): Train Loss: 0.3692, Train Acc: 87.20%, Val Loss: 0.6436, Val Acc: 78.72% (Best)\n",
      "Epoch 3/1000 (MagnitudeL2-70.0%): Train Loss: 0.2425, Train Acc: 91.72%, Val Loss: 0.6831, Val Acc: 79.12%\n",
      "Epoch 4/1000 (MagnitudeL2-70.0%): Train Loss: 0.1583, Train Acc: 94.68%, Val Loss: 0.7488, Val Acc: 78.94%\n",
      "Epoch 5/1000 (MagnitudeL2-70.0%): Train Loss: 0.1117, Train Acc: 96.27%, Val Loss: 0.7952, Val Acc: 79.48%\n",
      "Epoch 6/1000 (MagnitudeL2-70.0%): Train Loss: 0.0843, Train Acc: 97.16%, Val Loss: 0.8491, Val Acc: 79.48%\n",
      "Epoch 7/1000 (MagnitudeL2-70.0%): Train Loss: 0.0768, Train Acc: 97.31%, Val Loss: 0.8691, Val Acc: 79.70%\n",
      "Epoch 8/1000 (MagnitudeL2-70.0%): Train Loss: 0.0681, Train Acc: 97.64%, Val Loss: 0.9154, Val Acc: 79.42%\n",
      "Epoch 9/1000 (MagnitudeL2-70.0%): Train Loss: 0.0576, Train Acc: 98.10%, Val Loss: 0.9357, Val Acc: 79.34%\n",
      "Epoch 10/1000 (MagnitudeL2-70.0%): Train Loss: 0.0498, Train Acc: 98.34%, Val Loss: 0.9606, Val Acc: 79.18%\n",
      "Epoch 11/1000 (MagnitudeL2-70.0%): Train Loss: 0.0488, Train Acc: 98.34%, Val Loss: 0.9350, Val Acc: 79.84%\n",
      "Epoch 12/1000 (MagnitudeL2-70.0%): Train Loss: 0.0448, Train Acc: 98.51%, Val Loss: 1.0028, Val Acc: 79.36%\n",
      "Epoch 13/1000 (MagnitudeL2-70.0%): Train Loss: 0.0334, Train Acc: 98.91%, Val Loss: 0.9822, Val Acc: 79.64%\n",
      "Epoch 14/1000 (MagnitudeL2-70.0%): Train Loss: 0.0411, Train Acc: 98.64%, Val Loss: 1.0292, Val Acc: 79.58%\n",
      "Epoch 15/1000 (MagnitudeL2-70.0%): Train Loss: 0.0401, Train Acc: 98.64%, Val Loss: 1.0123, Val Acc: 79.50%\n",
      "Epoch 16/1000 (MagnitudeL2-70.0%): Train Loss: 0.0442, Train Acc: 98.50%, Val Loss: 1.0285, Val Acc: 79.98%\n",
      "Epoch 17/1000 (MagnitudeL2-70.0%): Train Loss: 0.0324, Train Acc: 98.88%, Val Loss: 1.0354, Val Acc: 80.46%\n",
      "Epoch 18/1000 (MagnitudeL2-70.0%): Train Loss: 0.0332, Train Acc: 98.93%, Val Loss: 1.0476, Val Acc: 80.52%\n",
      "Epoch 19/1000 (MagnitudeL2-70.0%): Train Loss: 0.0313, Train Acc: 98.91%, Val Loss: 1.0365, Val Acc: 80.42%\n",
      "Epoch 20/1000 (MagnitudeL2-70.0%): Train Loss: 0.0314, Train Acc: 98.99%, Val Loss: 1.0174, Val Acc: 80.90%\n",
      "Epoch 21/1000 (MagnitudeL2-70.0%): Train Loss: 0.0278, Train Acc: 99.05%, Val Loss: 1.0757, Val Acc: 80.20%\n",
      "Epoch 22/1000 (MagnitudeL2-70.0%): Train Loss: 0.0328, Train Acc: 98.90%, Val Loss: 1.0348, Val Acc: 80.22%\n",
      "Early stopping triggered after 22 epochs\n",
      "Loaded best model state\n",
      "Results: Accuracy=78.07%, MACs=27.77M\n",
      "‚úÖ Model saved to ./base/resnet18_magnitudel2_sparsity_0.7.pth\n",
      "‚úÖ ONNX model saved to ./base/resnet18_magnitudel2_sparsity_0.7.onnx\n",
      "\n",
      "--- Strategy: Random ---\n",
      "\n",
      "Processing Random at 20.0% sparsity...\n",
      "‚úÖ Created ResNet-18 without pretrained weights\n",
      "‚úÖ Adapted classifier for 10 classes\n",
      "Initial MACs: 37.18M\n",
      "Applying RandomImportance pruning at 20.0% sparsity...\n",
      "Final MACs: 34.04M (Reduction: 8.4%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/1000 (Random-20.0%): Train Loss: 0.4824, Train Acc: 83.26%, Val Loss: 0.6225, Val Acc: 78.56% (Best)\n",
      "Epoch 2/1000 (Random-20.0%): Train Loss: 0.2902, Train Acc: 90.04%, Val Loss: 0.6443, Val Acc: 79.64%\n",
      "Epoch 3/1000 (Random-20.0%): Train Loss: 0.1811, Train Acc: 93.84%, Val Loss: 0.7044, Val Acc: 79.76%\n",
      "Epoch 4/1000 (Random-20.0%): Train Loss: 0.1219, Train Acc: 95.94%, Val Loss: 0.7467, Val Acc: 79.86%\n",
      "Epoch 5/1000 (Random-20.0%): Train Loss: 0.0876, Train Acc: 97.06%, Val Loss: 0.8215, Val Acc: 79.72%\n",
      "Epoch 6/1000 (Random-20.0%): Train Loss: 0.0752, Train Acc: 97.47%, Val Loss: 0.8581, Val Acc: 79.78%\n",
      "Epoch 7/1000 (Random-20.0%): Train Loss: 0.0678, Train Acc: 97.66%, Val Loss: 0.8776, Val Acc: 80.04%\n",
      "Epoch 8/1000 (Random-20.0%): Train Loss: 0.0529, Train Acc: 98.20%, Val Loss: 0.9129, Val Acc: 80.30%\n",
      "Epoch 9/1000 (Random-20.0%): Train Loss: 0.0493, Train Acc: 98.33%, Val Loss: 0.9136, Val Acc: 80.24%\n",
      "Epoch 10/1000 (Random-20.0%): Train Loss: 0.0505, Train Acc: 98.27%, Val Loss: 0.9737, Val Acc: 80.62%\n",
      "Epoch 11/1000 (Random-20.0%): Train Loss: 0.0468, Train Acc: 98.44%, Val Loss: 0.9065, Val Acc: 81.24%\n",
      "Epoch 12/1000 (Random-20.0%): Train Loss: 0.0407, Train Acc: 98.63%, Val Loss: 0.9544, Val Acc: 80.14%\n",
      "Epoch 13/1000 (Random-20.0%): Train Loss: 0.0407, Train Acc: 98.65%, Val Loss: 0.8980, Val Acc: 81.80%\n",
      "Epoch 14/1000 (Random-20.0%): Train Loss: 0.0361, Train Acc: 98.78%, Val Loss: 0.9004, Val Acc: 81.36%\n",
      "Epoch 15/1000 (Random-20.0%): Train Loss: 0.0396, Train Acc: 98.67%, Val Loss: 0.9773, Val Acc: 81.08%\n",
      "Epoch 16/1000 (Random-20.0%): Train Loss: 0.0347, Train Acc: 98.82%, Val Loss: 0.9185, Val Acc: 81.40%\n",
      "Epoch 17/1000 (Random-20.0%): Train Loss: 0.0297, Train Acc: 99.02%, Val Loss: 0.9372, Val Acc: 80.96%\n",
      "Epoch 18/1000 (Random-20.0%): Train Loss: 0.0323, Train Acc: 98.95%, Val Loss: 0.9829, Val Acc: 81.00%\n",
      "Epoch 19/1000 (Random-20.0%): Train Loss: 0.0282, Train Acc: 99.04%, Val Loss: 0.9994, Val Acc: 81.24%\n",
      "Epoch 20/1000 (Random-20.0%): Train Loss: 0.0317, Train Acc: 98.96%, Val Loss: 1.0323, Val Acc: 81.18%\n",
      "Epoch 21/1000 (Random-20.0%): Train Loss: 0.0245, Train Acc: 99.17%, Val Loss: 1.0441, Val Acc: 80.52%\n",
      "Early stopping triggered after 21 epochs\n",
      "Loaded best model state\n",
      "Results: Accuracy=78.39%, MACs=34.04M\n",
      "‚úÖ Model saved to ./base/resnet18_random_sparsity_0.2.pth\n",
      "‚úÖ ONNX model saved to ./base/resnet18_random_sparsity_0.2.onnx\n",
      "\n",
      "Processing Random at 50.0% sparsity...\n",
      "‚úÖ Created ResNet-18 without pretrained weights\n",
      "‚úÖ Adapted classifier for 10 classes\n",
      "Initial MACs: 37.18M\n",
      "Applying RandomImportance pruning at 50.0% sparsity...\n",
      "Final MACs: 30.09M (Reduction: 19.1%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/1000 (Random-50.0%): Train Loss: 0.5973, Train Acc: 79.14%, Val Loss: 0.6563, Val Acc: 77.54% (Best)\n",
      "Epoch 2/1000 (Random-50.0%): Train Loss: 0.3713, Train Acc: 87.12%, Val Loss: 0.6684, Val Acc: 78.56%\n",
      "Epoch 3/1000 (Random-50.0%): Train Loss: 0.2369, Train Acc: 91.84%, Val Loss: 0.7131, Val Acc: 79.30%\n",
      "Epoch 4/1000 (Random-50.0%): Train Loss: 0.1565, Train Acc: 94.72%, Val Loss: 0.8019, Val Acc: 78.02%\n",
      "Epoch 5/1000 (Random-50.0%): Train Loss: 0.1112, Train Acc: 96.26%, Val Loss: 0.8471, Val Acc: 78.68%\n",
      "Epoch 6/1000 (Random-50.0%): Train Loss: 0.0838, Train Acc: 97.27%, Val Loss: 0.8982, Val Acc: 78.56%\n",
      "Epoch 7/1000 (Random-50.0%): Train Loss: 0.0738, Train Acc: 97.55%, Val Loss: 0.9652, Val Acc: 77.76%\n",
      "Epoch 8/1000 (Random-50.0%): Train Loss: 0.0625, Train Acc: 97.89%, Val Loss: 0.9843, Val Acc: 78.54%\n",
      "Epoch 9/1000 (Random-50.0%): Train Loss: 0.0609, Train Acc: 97.91%, Val Loss: 0.9850, Val Acc: 79.26%\n",
      "Epoch 10/1000 (Random-50.0%): Train Loss: 0.0545, Train Acc: 98.14%, Val Loss: 0.9575, Val Acc: 79.58%\n",
      "Epoch 11/1000 (Random-50.0%): Train Loss: 0.0424, Train Acc: 98.53%, Val Loss: 1.0458, Val Acc: 78.96%\n",
      "Epoch 12/1000 (Random-50.0%): Train Loss: 0.0437, Train Acc: 98.53%, Val Loss: 1.0025, Val Acc: 79.92%\n",
      "Epoch 13/1000 (Random-50.0%): Train Loss: 0.0442, Train Acc: 98.50%, Val Loss: 1.0385, Val Acc: 79.46%\n",
      "Epoch 14/1000 (Random-50.0%): Train Loss: 0.0374, Train Acc: 98.70%, Val Loss: 1.0018, Val Acc: 80.38%\n",
      "Epoch 15/1000 (Random-50.0%): Train Loss: 0.0374, Train Acc: 98.72%, Val Loss: 1.0665, Val Acc: 80.12%\n",
      "Epoch 16/1000 (Random-50.0%): Train Loss: 0.0401, Train Acc: 98.66%, Val Loss: 1.0324, Val Acc: 79.88%\n",
      "Epoch 17/1000 (Random-50.0%): Train Loss: 0.0399, Train Acc: 98.66%, Val Loss: 1.0656, Val Acc: 79.76%\n",
      "Epoch 18/1000 (Random-50.0%): Train Loss: 0.0381, Train Acc: 98.74%, Val Loss: 1.0158, Val Acc: 80.18%\n",
      "Epoch 19/1000 (Random-50.0%): Train Loss: 0.0302, Train Acc: 98.99%, Val Loss: 1.0371, Val Acc: 80.00%\n",
      "Epoch 20/1000 (Random-50.0%): Train Loss: 0.0270, Train Acc: 99.11%, Val Loss: 1.0865, Val Acc: 79.66%\n",
      "Epoch 21/1000 (Random-50.0%): Train Loss: 0.0265, Train Acc: 99.08%, Val Loss: 1.1085, Val Acc: 80.28%\n",
      "Early stopping triggered after 21 epochs\n",
      "Loaded best model state\n",
      "Results: Accuracy=77.90%, MACs=30.09M\n",
      "‚úÖ Model saved to ./base/resnet18_random_sparsity_0.5.pth\n",
      "‚úÖ ONNX model saved to ./base/resnet18_random_sparsity_0.5.onnx\n",
      "\n",
      "Processing Random at 70.0% sparsity...\n",
      "‚úÖ Created ResNet-18 without pretrained weights\n",
      "‚úÖ Adapted classifier for 10 classes\n",
      "Initial MACs: 37.18M\n",
      "Applying RandomImportance pruning at 70.0% sparsity...\n",
      "Final MACs: 27.77M (Reduction: 25.3%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/1000 (Random-70.0%): Train Loss: 0.6854, Train Acc: 76.48%, Val Loss: 0.6525, Val Acc: 77.00% (Best)\n",
      "Epoch 2/1000 (Random-70.0%): Train Loss: 0.4419, Train Acc: 84.83%, Val Loss: 0.6425, Val Acc: 78.08% (Best)\n",
      "Epoch 3/1000 (Random-70.0%): Train Loss: 0.2909, Train Acc: 90.10%, Val Loss: 0.6632, Val Acc: 79.04%\n",
      "Epoch 4/1000 (Random-70.0%): Train Loss: 0.1971, Train Acc: 93.29%, Val Loss: 0.7498, Val Acc: 78.08%\n",
      "Epoch 5/1000 (Random-70.0%): Train Loss: 0.1299, Train Acc: 95.59%, Val Loss: 0.8390, Val Acc: 77.58%\n",
      "Epoch 6/1000 (Random-70.0%): Train Loss: 0.1000, Train Acc: 96.64%, Val Loss: 0.8780, Val Acc: 78.26%\n",
      "Epoch 7/1000 (Random-70.0%): Train Loss: 0.0885, Train Acc: 97.05%, Val Loss: 0.8962, Val Acc: 78.90%\n",
      "Epoch 8/1000 (Random-70.0%): Train Loss: 0.0672, Train Acc: 97.72%, Val Loss: 0.9272, Val Acc: 79.06%\n",
      "Epoch 9/1000 (Random-70.0%): Train Loss: 0.0652, Train Acc: 97.80%, Val Loss: 0.9647, Val Acc: 78.42%\n",
      "Epoch 10/1000 (Random-70.0%): Train Loss: 0.0544, Train Acc: 98.17%, Val Loss: 0.9524, Val Acc: 78.98%\n",
      "Epoch 11/1000 (Random-70.0%): Train Loss: 0.0565, Train Acc: 98.12%, Val Loss: 1.0152, Val Acc: 78.78%\n",
      "Epoch 12/1000 (Random-70.0%): Train Loss: 0.0507, Train Acc: 98.26%, Val Loss: 1.0290, Val Acc: 78.76%\n",
      "Epoch 13/1000 (Random-70.0%): Train Loss: 0.0436, Train Acc: 98.58%, Val Loss: 1.0283, Val Acc: 79.42%\n",
      "Epoch 14/1000 (Random-70.0%): Train Loss: 0.0449, Train Acc: 98.47%, Val Loss: 1.0324, Val Acc: 78.94%\n",
      "Epoch 15/1000 (Random-70.0%): Train Loss: 0.0422, Train Acc: 98.58%, Val Loss: 1.0572, Val Acc: 78.88%\n",
      "Epoch 16/1000 (Random-70.0%): Train Loss: 0.0362, Train Acc: 98.86%, Val Loss: 1.0613, Val Acc: 78.84%\n",
      "Epoch 17/1000 (Random-70.0%): Train Loss: 0.0333, Train Acc: 98.85%, Val Loss: 1.0371, Val Acc: 79.80%\n",
      "Epoch 18/1000 (Random-70.0%): Train Loss: 0.0408, Train Acc: 98.63%, Val Loss: 1.0431, Val Acc: 79.24%\n",
      "Epoch 19/1000 (Random-70.0%): Train Loss: 0.0413, Train Acc: 98.62%, Val Loss: 0.9866, Val Acc: 79.88%\n",
      "Epoch 20/1000 (Random-70.0%): Train Loss: 0.0299, Train Acc: 98.96%, Val Loss: 1.0966, Val Acc: 79.70%\n",
      "Epoch 21/1000 (Random-70.0%): Train Loss: 0.0315, Train Acc: 98.98%, Val Loss: 1.0188, Val Acc: 80.66%\n",
      "Epoch 22/1000 (Random-70.0%): Train Loss: 0.0256, Train Acc: 99.10%, Val Loss: 1.0868, Val Acc: 79.98%\n",
      "Early stopping triggered after 22 epochs\n",
      "Loaded best model state\n",
      "Results: Accuracy=77.76%, MACs=27.77M\n",
      "‚úÖ Model saved to ./base/resnet18_random_sparsity_0.7.pth\n",
      "‚úÖ ONNX model saved to ./base/resnet18_random_sparsity_0.7.onnx\n",
      "\n",
      "Saving results...\n",
      "‚úÖ Complete results saved to ./results_resnet18_cifar10/complete_results.json\n",
      "‚úÖ Summary results saved to ./results_resnet18_cifar10/summary_results.csv\n",
      "Creating plots...\n",
      "‚úÖ Accuracy plot saved to ./results_resnet18_cifar10/accuracy_vs_sparsity.png\n",
      "‚úÖ Efficiency frontier plot saved to ./results_resnet18_cifar10/efficiency_frontier.png\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENTAL RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Baseline Performance:\n",
      "  Accuracy: 78.16%\n",
      "  MACs: 37.18M\n",
      "  Parameters: 11.18M\n",
      "  Model Size: 42.65MB\n",
      "\n",
      "Strategy Comparison at 50% Sparsity:\n",
      "       BNScale:  78.37% accuracy (-0.21%, 100.3% retention)\n",
      "   MagnitudeL2:  77.92% accuracy (+0.24%,  99.7% retention)\n",
      "        Random:  77.90% accuracy (+0.26%,  99.7% retention)\n",
      "\n",
      "Complete Results Table:\n",
      "--------------------------------------------------------------------------------\n",
      "Strategy     Sparsity Accuracy MACs(M)  Params(M) Size(MB)\n",
      "--------------------------------------------------------------------------------\n",
      "BNScale           0%   78.16%   37.18    11.18   42.65\n",
      "BNScale          20%   78.26%   34.04    10.27   39.16\n",
      "BNScale          50%   78.37%   30.09     9.03   34.43\n",
      "BNScale          70%   77.14%   27.77     8.26   31.51\n",
      "MagnitudeL2       0%   78.16%   37.18    11.18   42.65\n",
      "MagnitudeL2      20%   79.13%   34.04    10.27   39.16\n",
      "MagnitudeL2      50%   77.92%   30.09     9.03   34.43\n",
      "MagnitudeL2      70%   78.07%   27.77     8.26   31.51\n",
      "Random            0%   78.16%   37.18    11.18   42.65\n",
      "Random           20%   78.39%   34.04    10.27   39.16\n",
      "Random           50%   77.90%   30.09     9.03   34.43\n",
      "Random           70%   77.76%   27.77     8.26   31.51\n",
      "\n",
      "üéâ All experiments completed!\n",
      "üìÅ Results saved to: /home/muis/thesis/github-repo/master-thesis/cnn/resNet/results_resnet18_cifar10\n",
      "üìÅ Models saved to: /home/muis/thesis/github-repo/master-thesis/cnn/resNet/base\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
