{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T23:02:10.347200Z",
     "start_time": "2025-06-02T21:20:01.617210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_pruning as tp\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_BASE_NAME = \"mobilenet_v2\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "def get_data_loaders(data_dir_path='./data', batch_size=128, val_split=0.1, seed=42, use_augmentation=True):\n",
    "    \"\"\"Load CIFAR-10 dataset with train/val/test splits and improved transforms\"\"\"\n",
    "    abs_data_dir = os.path.abspath(data_dir_path)\n",
    "    print(f\"Loading CIFAR-10 from: {abs_data_dir}\")\n",
    "\n",
    "    # Enhanced transforms with data augmentation for training\n",
    "    if use_augmentation:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.RandomRotation(degrees=10),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "            # Add random erasing for regularization\n",
    "            transforms.RandomErasing(p=0.1, scale=(0.02, 0.33), ratio=(0.3, 3.3))\n",
    "        ])\n",
    "    else:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "\n",
    "    # Load CIFAR-10 from local directory\n",
    "    full_train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=abs_data_dir, train=True, download=False, transform=train_transform\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=abs_data_dir, train=False, download=False, transform=test_transform\n",
    "    )\n",
    "\n",
    "    # Create train/validation split\n",
    "    val_size = int(len(full_train_dataset) * val_split)\n",
    "    train_size = len(full_train_dataset) - val_size\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_train_dataset, [train_size, val_size], generator=generator\n",
    "    )\n",
    "\n",
    "    # Apply test transform to validation set\n",
    "    val_dataset.dataset = copy.deepcopy(full_train_dataset)\n",
    "    val_dataset.dataset.transform = test_transform\n",
    "\n",
    "    # Create data loaders\n",
    "    num_workers = min(4, os.cpu_count() or 2)\n",
    "    pin_memory = True if DEVICE.type == 'cuda' else False\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=num_workers, pin_memory=pin_memory, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                            num_workers=num_workers, pin_memory=pin_memory)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                             num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    print(f\"DataLoaders created - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "def get_mobilenetv2_model(num_classes=10, use_pretrained=True, pretrained_path='./base/mobilenet_v2-b0353104.pth'):\n",
    "    \"\"\"Get MobileNetV2 model adapted for CIFAR-10 with improved classifier\"\"\"\n",
    "    # Always create model without weights first\n",
    "    model = models.mobilenet_v2(weights=None)\n",
    "\n",
    "    if use_pretrained and os.path.exists(pretrained_path):\n",
    "        # Load pre-downloaded weights from local file\n",
    "        print(f\"Loading pre-trained weights from: {pretrained_path}\")\n",
    "        pretrained_state_dict = torch.load(pretrained_path, map_location=DEVICE)\n",
    "\n",
    "        # Load the weights, ignoring the classifier layer if it doesn't match\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_state_dict.items()\n",
    "                           if k in model_dict and model_dict[k].shape == v.shape}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict, strict=False)\n",
    "        print(\"✅ Loaded MobileNetV2 with pre-downloaded ImageNet weights\")\n",
    "    else:\n",
    "        if use_pretrained:\n",
    "            print(f\"Warning: Pre-trained weights not found at {pretrained_path}\")\n",
    "        print(\"✅ Created MobileNetV2 without pretrained weights\")\n",
    "\n",
    "    # Enhanced classifier for better performance\n",
    "    in_features = model.classifier[1].in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(in_features, 512),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.BatchNorm1d(256),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(256, num_classes)\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Enhanced classifier created for {num_classes} classes\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_ignored_layers(model):\n",
    "    \"\"\"Get layers to ignore during pruning (typically final classifier)\"\"\"\n",
    "    ignored_layers = []\n",
    "    if hasattr(model, 'classifier'):\n",
    "        if isinstance(model.classifier, nn.Sequential):\n",
    "            for layer in model.classifier:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    ignored_layers.append(layer)\n",
    "        elif isinstance(model.classifier, nn.Linear):\n",
    "            ignored_layers.append(model.classifier)\n",
    "    return ignored_layers\n",
    "\n",
    "\n",
    "def calculate_macs_params(model, example_input):\n",
    "    \"\"\"Calculate MACs and parameters using torch_pruning\"\"\"\n",
    "    model.eval()\n",
    "    target_device = example_input.device\n",
    "    model_on_device = model.to(target_device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        macs, params = tp.utils.count_ops_and_params(model_on_device, example_input)\n",
    "\n",
    "    return macs, params\n",
    "\n",
    "\n",
    "def save_model(model, save_path, example_input_cpu=None):\n",
    "    \"\"\"Save model state dict and optionally ONNX\"\"\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"✅ Model saved to {save_path}\")\n",
    "\n",
    "    if example_input_cpu is not None:\n",
    "        onnx_path = save_path.replace('.pth', '.onnx')\n",
    "        try:\n",
    "            model_cpu = model.to('cpu')\n",
    "            torch.onnx.export(\n",
    "                model_cpu, example_input_cpu, onnx_path,\n",
    "                export_params=True, opset_version=13,\n",
    "                input_names=['input'], output_names=['output'],\n",
    "                dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "            )\n",
    "            print(f\"✅ ONNX model saved to {onnx_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: ONNX export failed: {e}\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, example_input, criterion, device):\n",
    "    \"\"\"Evaluate model and return comprehensive metrics\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Calculate efficiency metrics\n",
    "    macs, params = calculate_macs_params(model, example_input.to(device))\n",
    "    model_size_mb = params * 4 / (1024 * 1024)  # Assuming float32\n",
    "\n",
    "    # Calculate accuracy and loss\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / total if total > 0 else float('nan')\n",
    "    accuracy = 100.0 * correct / total if total > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'loss': avg_loss,\n",
    "        'macs': macs,\n",
    "        'params': params,\n",
    "        'size_mb': model_size_mb\n",
    "    }\n",
    "\n",
    "\n",
    "def prune_model(model, strategy_config, sparsity_ratio, example_input, ignored_layers=None):\n",
    "    \"\"\"Apply structured pruning to model\"\"\"\n",
    "    if sparsity_ratio == 0.0:\n",
    "        print(\"No pruning needed (sparsity = 0.0)\")\n",
    "        return model\n",
    "\n",
    "    model.eval()\n",
    "    pruned_model = copy.deepcopy(model)\n",
    "    pruned_model.to(example_input.device)\n",
    "\n",
    "    # Calculate initial MACs\n",
    "    initial_macs, _ = calculate_macs_params(pruned_model, example_input)\n",
    "    print(f\"Initial MACs: {initial_macs / 1e6:.2f}M\")\n",
    "\n",
    "    ignored_layers = ignored_layers or []\n",
    "\n",
    "    # Create pruner based on strategy\n",
    "    pruner = strategy_config['pruner'](\n",
    "        pruned_model,\n",
    "        example_input,\n",
    "        importance=strategy_config['importance'],\n",
    "        iterative_steps=5,  # Use 5 iterative steps\n",
    "        ch_sparsity=sparsity_ratio,\n",
    "        root_module_types=[nn.Conv2d],\n",
    "        ignored_layers=ignored_layers\n",
    "    )\n",
    "\n",
    "    print(f\"Applying {strategy_config['importance'].__class__.__name__} pruning at {sparsity_ratio:.1%} sparsity...\")\n",
    "\n",
    "    # Apply pruning\n",
    "    pruner.step()\n",
    "\n",
    "    # Calculate final MACs\n",
    "    final_macs, _ = calculate_macs_params(pruned_model, example_input)\n",
    "    reduction = (initial_macs - final_macs) / initial_macs * 100 if initial_macs > 0 else 0\n",
    "    print(f\"Final MACs: {final_macs / 1e6:.2f}M (Reduction: {reduction:.1f}%)\")\n",
    "\n",
    "    return pruned_model\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs,\n",
    "                val_loader=None, patience=10, log_prefix=\"\", scheduler=None):\n",
    "    \"\"\"Enhanced training function with better optimization and monitoring\"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_acc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'learning_rates': []\n",
    "    }\n",
    "\n",
    "    # Label smoothing for better generalization\n",
    "    smoothing_criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "\n",
    "            # Use label smoothing for training\n",
    "            loss = smoothing_criterion(output, target)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total += target.size(0)\n",
    "            train_correct += (predicted == target).sum().item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_acc = 100.0 * train_correct / train_total\n",
    "\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "\n",
    "        # Record learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        history['learning_rates'].append(current_lr)\n",
    "\n",
    "        log_msg = f\"Epoch {epoch + 1:3d}/{num_epochs} ({log_prefix}): Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%, LR: {current_lr:.6f}\"\n",
    "\n",
    "        # Validation phase\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for data, target in val_loader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)  # Use original criterion for validation\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = torch.max(output.data, 1)\n",
    "                    val_total += target.size(0)\n",
    "                    val_correct += (predicted == target).sum().item()\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_acc = 100.0 * val_correct / val_total\n",
    "\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            history['val_acc'].append(val_acc)\n",
    "\n",
    "            log_msg += f\", Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%\"\n",
    "\n",
    "            # Early stopping check - use validation accuracy as primary metric\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_val_loss = avg_val_loss\n",
    "                epochs_no_improve = 0\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "                log_msg += \" ✓\"\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            # Update scheduler\n",
    "            if scheduler is not None:\n",
    "                if isinstance(scheduler, ReduceLROnPlateau):\n",
    "                    scheduler.step(avg_val_loss)\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"{log_msg}\")\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs (Best Val Acc: {best_val_acc:.2f}%)\")\n",
    "                break\n",
    "        else:\n",
    "            history['val_loss'].append(None)\n",
    "            history['val_acc'].append(None)\n",
    "\n",
    "            # Update scheduler for training without validation\n",
    "            if scheduler is not None and not isinstance(scheduler, ReduceLROnPlateau):\n",
    "                scheduler.step()\n",
    "\n",
    "        print(log_msg)\n",
    "\n",
    "    # Load best model state if available\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"Loaded best model state (Val Acc: {best_val_acc:.2f}%)\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"Label smoothing cross entropy loss for better generalization\"\"\"\n",
    "\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        confidence = 1. - self.smoothing\n",
    "        logprobs = torch.nn.functional.log_softmax(x, dim=-1)\n",
    "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
    "        nll_loss = nll_loss.squeeze(1)\n",
    "        smooth_loss = -logprobs.mean(dim=-1)\n",
    "        loss = confidence * nll_loss + self.smoothing * smooth_loss\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "def get_optimizer_and_scheduler(model, config, total_steps=None):\n",
    "    \"\"\"Get optimized optimizer and learning rate scheduler\"\"\"\n",
    "\n",
    "    # Separate parameters for different learning rates\n",
    "    backbone_params = []\n",
    "    classifier_params = []\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'classifier' in name:\n",
    "            classifier_params.append(param)\n",
    "        else:\n",
    "            backbone_params.append(param)\n",
    "\n",
    "    # Use different learning rates for backbone and classifier\n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': backbone_params, 'lr': config['learning_rate'] * 0.1, 'weight_decay': config['weight_decay']},\n",
    "        {'params': classifier_params, 'lr': config['learning_rate'], 'weight_decay': config['weight_decay'] * 0.1}\n",
    "    ])\n",
    "\n",
    "    # Use cosine annealing with warm restarts\n",
    "    scheduler = CosineAnnealingWarmRestarts(\n",
    "        optimizer,\n",
    "        T_0=config['scheduler_restart_period'],\n",
    "        T_mult=2,\n",
    "        eta_min=config['learning_rate'] * 0.001\n",
    "    )\n",
    "\n",
    "    return optimizer, scheduler\n",
    "\n",
    "\n",
    "def save_results_to_files(all_results, output_dir):\n",
    "    \"\"\"Save experimental results to JSON and CSV files\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save complete results as JSON\n",
    "    results_json_path = os.path.join(output_dir, 'complete_results.json')\n",
    "    with open(results_json_path, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2, default=str)\n",
    "    print(f\"✅ Complete results saved to {results_json_path}\")\n",
    "\n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for strategy, strategy_results in all_results.items():\n",
    "        for sparsity, metrics in strategy_results.items():\n",
    "            row = {\n",
    "                'strategy': strategy,\n",
    "                'sparsity_ratio': sparsity,\n",
    "                'accuracy': metrics['accuracy'],\n",
    "                'loss': metrics['loss'],\n",
    "                'macs_millions': metrics['macs'] / 1e6,\n",
    "                'params_millions': metrics['params'] / 1e6,\n",
    "                'size_mb': metrics['size_mb']\n",
    "            }\n",
    "            summary_data.append(row)\n",
    "\n",
    "    # Save summary as CSV\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_csv_path = os.path.join(output_dir, 'summary_results.csv')\n",
    "    summary_df.to_csv(summary_csv_path, index=False)\n",
    "    print(f\"✅ Summary results saved to {summary_csv_path}\")\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "def create_results_plots(summary_df, output_dir):\n",
    "    \"\"\"Create visualization plots\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    strategies = summary_df['strategy'].unique()\n",
    "\n",
    "    # Plot 1: Accuracy vs Sparsity\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for strategy in strategies:\n",
    "        strategy_data = summary_df[summary_df['strategy'] == strategy].sort_values('sparsity_ratio')\n",
    "        plt.plot(strategy_data['sparsity_ratio'] * 100, strategy_data['accuracy'],\n",
    "                 'o-', linewidth=3, markersize=10, label=strategy)\n",
    "\n",
    "    plt.xlabel('Sparsity (%)', fontsize=14)\n",
    "    plt.ylabel('Accuracy (%)', fontsize=14)\n",
    "    plt.title('MobileNetV2: Accuracy vs Sparsity', fontsize=16, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(output_dir, 'accuracy_vs_sparsity.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✅ Accuracy plot saved to {plot_path}\")\n",
    "\n",
    "    # Plot 2: Efficiency frontier (Accuracy vs MACs)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "    for i, strategy in enumerate(strategies):\n",
    "        strategy_data = summary_df[summary_df['strategy'] == strategy].sort_values('sparsity_ratio')\n",
    "        plt.scatter(strategy_data['macs_millions'], strategy_data['accuracy'],\n",
    "                    s=150, label=strategy, alpha=0.8, color=colors[i % len(colors)])\n",
    "        plt.plot(strategy_data['macs_millions'], strategy_data['accuracy'],\n",
    "                 '--', alpha=0.7, linewidth=2, color=colors[i % len(colors)])\n",
    "\n",
    "    plt.xlabel('MACs (Millions)', fontsize=14)\n",
    "    plt.ylabel('Accuracy (%)', fontsize=14)\n",
    "    plt.title('MobileNetV2: Efficiency Frontier (Accuracy vs MACs)', fontsize=16, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(output_dir, 'efficiency_frontier.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✅ Efficiency frontier plot saved to {plot_path}\")\n",
    "\n",
    "\n",
    "def print_results_table(summary_df):\n",
    "    \"\"\"Print formatted results table\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"EXPERIMENTAL RESULTS SUMMARY\")\n",
    "    print(\"=\" * 90)\n",
    "\n",
    "    # Baseline results\n",
    "    baseline_results = summary_df[summary_df['sparsity_ratio'] == 0.0].iloc[0]\n",
    "    print(f\"\\nBaseline Performance:\")\n",
    "    print(f\"  Accuracy: {baseline_results['accuracy']:.2f}%\")\n",
    "    print(f\"  MACs: {baseline_results['macs_millions']:.2f}M\")\n",
    "    print(f\"  Parameters: {baseline_results['params_millions']:.2f}M\")\n",
    "    print(f\"  Model Size: {baseline_results['size_mb']:.2f}MB\")\n",
    "\n",
    "    # Strategy comparison at 50% sparsity\n",
    "    print(f\"\\nStrategy Comparison at 50% Sparsity:\")\n",
    "    sparsity_50_data = summary_df[summary_df['sparsity_ratio'] == 0.5]\n",
    "    for _, row in sparsity_50_data.iterrows():\n",
    "        degradation = baseline_results['accuracy'] - row['accuracy']\n",
    "        retention = (row['accuracy'] / baseline_results['accuracy']) * 100\n",
    "        print(\n",
    "            f\"  {row['strategy']:>12}: {row['accuracy']:>6.2f}% accuracy ({degradation:>+5.2f}%, {retention:>5.1f}% retention)\")\n",
    "\n",
    "    # Complete results table\n",
    "    print(f\"\\nComplete Results Table:\")\n",
    "    print(\"-\" * 90)\n",
    "    print(f\"{'Strategy':<12} {'Sparsity':<8} {'Accuracy':<8} {'MACs(M)':<8} {'Params(M)':<9} {'Size(MB)':<8}\")\n",
    "    print(\"-\" * 90)\n",
    "\n",
    "    for _, row in summary_df.sort_values(['strategy', 'sparsity_ratio']).iterrows():\n",
    "        print(f\"{row['strategy']:<12} {row['sparsity_ratio'] * 100:>6.0f}% \"\n",
    "              f\"{row['accuracy']:>7.2f}% {row['macs_millions']:>7.2f} \"\n",
    "              f\"{row['params_millions']:>8.2f} {row['size_mb']:>7.2f}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main experimental workflow with improved training\"\"\"\n",
    "    print(\"Starting Enhanced MobileNetV2 CIFAR-10 Pruning Experiments\")\n",
    "    print(\"=\" * 65)\n",
    "\n",
    "    # Enhanced Configuration\n",
    "    config = {\n",
    "        'strategies': {\n",
    "            'BNScale': {\n",
    "                'pruner': tp.pruner.BNScalePruner,\n",
    "                'importance': tp.importance.BNScaleImportance()\n",
    "            },\n",
    "            'MagnitudeL2': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.MagnitudeImportance(p=2)\n",
    "            },\n",
    "            'Random': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.RandomImportance()\n",
    "            },\n",
    "        },\n",
    "        'pruning_ratios': [0.0, 0.2, 0.5, 0.7],\n",
    "        'num_classes': 10,\n",
    "        'batch_size': 128,\n",
    "        'learning_rate': 0.001,  # Increased from 0.0001\n",
    "        'weight_decay': 1e-4,  # Added L2 regularization\n",
    "        'epochs': 1000,  # Reduced from 1000 for faster iteration\n",
    "        'patience': 20,  # Increased patience\n",
    "        'scheduler_restart_period': 10,  # For cosine annealing\n",
    "        'use_augmentation': True,  # Enable data augmentation\n",
    "        'output_dir': './results_mobilenetv2_cifar10_enhanced',\n",
    "        'models_dir': './base',\n",
    "        'pretrained_path': './base/mobilenet_v2-b0353104.pth'\n",
    "    }\n",
    "\n",
    "    # Create output directories\n",
    "    os.makedirs(config['output_dir'], exist_ok=True)\n",
    "    os.makedirs(config['models_dir'], exist_ok=True)\n",
    "\n",
    "    # Load data with augmentation\n",
    "    print(\"Loading CIFAR-10 dataset with enhanced preprocessing...\")\n",
    "    train_loader, val_loader, test_loader = get_data_loaders(\n",
    "        batch_size=config['batch_size'],\n",
    "        use_augmentation=config['use_augmentation']\n",
    "    )\n",
    "\n",
    "    # Prepare inputs and criterion\n",
    "    example_input_cpu = torch.randn(1, 3, 32, 32)\n",
    "    example_input_device = example_input_cpu.to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Get baseline model and train it\n",
    "    print(\"\\nCreating enhanced baseline model...\")\n",
    "    model = get_mobilenetv2_model(\n",
    "        num_classes=config['num_classes'],\n",
    "        use_pretrained=True,\n",
    "        pretrained_path=config['pretrained_path']\n",
    "    )\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Get enhanced optimizer and scheduler\n",
    "    optimizer, scheduler = get_optimizer_and_scheduler(model, config)\n",
    "\n",
    "    print(\"Training enhanced baseline model...\")\n",
    "    trained_model, training_history = train_model(\n",
    "        model, train_loader, criterion, optimizer, DEVICE,\n",
    "        config['epochs'], val_loader, config['patience'],\n",
    "        \"Enhanced Baseline\", scheduler\n",
    "    )\n",
    "\n",
    "    # Save baseline model\n",
    "    baseline_model_path = os.path.join(config['models_dir'], 'enhanced_baseline_model.pth')\n",
    "    save_model(trained_model, baseline_model_path, example_input_cpu)\n",
    "\n",
    "    # Evaluate baseline\n",
    "    print(\"\\nEvaluating enhanced baseline model...\")\n",
    "    baseline_metrics = evaluate_model(trained_model, test_loader, example_input_device, criterion, DEVICE)\n",
    "    print(f\"Enhanced Baseline Results: Accuracy={baseline_metrics['accuracy']:.2f}%, \"\n",
    "          f\"MACs={baseline_metrics['macs'] / 1e6:.2f}M, \"\n",
    "          f\"Params={baseline_metrics['params'] / 1e6:.2f}M\")\n",
    "\n",
    "    # Initialize results storage\n",
    "    all_results = {}\n",
    "    for strategy_name in config['strategies'].keys():\n",
    "        all_results[strategy_name] = {0.0: baseline_metrics}\n",
    "\n",
    "    # Get ignored layers\n",
    "    ignored_layers = get_ignored_layers(trained_model)\n",
    "\n",
    "    # Run pruning experiments\n",
    "    print(\"\\nStarting enhanced pruning experiments...\")\n",
    "    for strategy_name, strategy_config in config['strategies'].items():\n",
    "        print(f\"\\n--- Strategy: {strategy_name} ---\")\n",
    "\n",
    "        for sparsity_ratio in config['pruning_ratios']:\n",
    "            if sparsity_ratio == 0.0:\n",
    "                continue  # Skip baseline (already done)\n",
    "\n",
    "            print(f\"\\nProcessing {strategy_name} at {sparsity_ratio:.1%} sparsity...\")\n",
    "\n",
    "            # Load fresh copy of trained baseline\n",
    "            model_copy = get_mobilenetv2_model(\n",
    "                num_classes=config['num_classes'],\n",
    "                use_pretrained=False\n",
    "            )\n",
    "            model_copy.load_state_dict(torch.load(baseline_model_path, map_location=DEVICE))\n",
    "            model_copy.to(DEVICE)\n",
    "\n",
    "            # Apply pruning\n",
    "            pruned_model = prune_model(\n",
    "                model_copy, strategy_config, sparsity_ratio,\n",
    "                example_input_device, ignored_layers\n",
    "            )\n",
    "\n",
    "            # Enhanced fine-tuning with reduced learning rate\n",
    "            print(\"Fine-tuning pruned model with enhanced settings...\")\n",
    "            ft_config = config.copy()\n",
    "            ft_config['learning_rate'] = config['learning_rate'] * 0.1  # Reduce LR for fine-tuning\n",
    "            ft_config['epochs'] = config['epochs']  # Fewer epochs for fine-tuning\n",
    "\n",
    "            optimizer_ft, scheduler_ft = get_optimizer_and_scheduler(pruned_model, ft_config)\n",
    "\n",
    "            fine_tuned_model, ft_history = train_model(\n",
    "                pruned_model, train_loader, criterion, optimizer_ft, DEVICE,\n",
    "                ft_config['epochs'], val_loader, config['patience'],\n",
    "                f\"{strategy_name}-{sparsity_ratio:.1%}\", scheduler_ft\n",
    "            )\n",
    "\n",
    "            # Evaluate fine-tuned model\n",
    "            final_metrics = evaluate_model(fine_tuned_model, test_loader, example_input_device, criterion, DEVICE)\n",
    "            all_results[strategy_name][sparsity_ratio] = final_metrics\n",
    "\n",
    "            print(f\"Results: Accuracy={final_metrics['accuracy']:.2f}%, \"\n",
    "                  f\"MACs={final_metrics['macs'] / 1e6:.2f}M\")\n",
    "\n",
    "            # Save fine-tuned model\n",
    "            model_filename = f\"enhanced_{strategy_name.lower()}_sparsity_{sparsity_ratio:.1f}.pth\"\n",
    "            model_path = os.path.join(config['models_dir'], model_filename)\n",
    "            save_model(fine_tuned_model, model_path, example_input_cpu)\n",
    "\n",
    "    # Save and analyze results\n",
    "    print(\"\\nSaving enhanced results...\")\n",
    "    summary_df = save_results_to_files(all_results, config['output_dir'])\n",
    "\n",
    "    # Create enhanced plots\n",
    "    print(\"Creating enhanced plots...\")\n",
    "    create_results_plots(summary_df, config['output_dir'])\n",
    "\n",
    "    # Print comprehensive summary\n",
    "    print_results_table(summary_df)\n",
    "\n",
    "    print(f\"\\n🎉 All enhanced experiments completed!\")\n",
    "    print(f\"📁 Results saved to: {os.path.abspath(config['output_dir'])}\")\n",
    "    print(f\"📁 Models saved to: {os.path.abspath(config['models_dir'])}\")\n",
    "\n",
    "    # Performance expectations\n",
    "    print(f\"\\n📊 Expected Performance Improvements:\")\n",
    "    print(f\"   • Baseline accuracy should reach 85-92% (vs previous ~70%)\")\n",
    "    print(f\"   • Better accuracy retention after pruning\")\n",
    "    print(f\"   • More stable training with enhanced optimizations\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "10ba42bbb036a1fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting Enhanced MobileNetV2 CIFAR-10 Pruning Experiments\n",
      "=================================================================\n",
      "Loading CIFAR-10 dataset with enhanced preprocessing...\n",
      "Loading CIFAR-10 from: /home/muis/thesis/github-repo/master-thesis/cnn/mobile_net_v2/data\n",
      "DataLoaders created - Train: 45000, Val: 5000, Test: 10000\n",
      "\n",
      "Creating enhanced baseline model...\n",
      "Loading pre-trained weights from: ./base/mobilenet_v2-b0353104.pth\n",
      "✅ Loaded MobileNetV2 with pre-downloaded ImageNet weights\n",
      "✅ Enhanced classifier created for 10 classes\n",
      "Training enhanced baseline model...\n",
      "Epoch   1/1000 (Enhanced Baseline): Train Loss: 1.6498, Train Acc: 47.37%, LR: 0.000100, Val Loss: 1.1409, Val Acc: 60.92% ✓\n",
      "Epoch   2/1000 (Enhanced Baseline): Train Loss: 1.3706, Train Acc: 61.01%, LR: 0.000098, Val Loss: 0.9460, Val Acc: 68.32% ✓\n",
      "Epoch   3/1000 (Enhanced Baseline): Train Loss: 1.2741, Train Acc: 65.82%, LR: 0.000091, Val Loss: 0.8256, Val Acc: 73.04% ✓\n",
      "Epoch   4/1000 (Enhanced Baseline): Train Loss: 1.2158, Train Acc: 68.45%, LR: 0.000080, Val Loss: 0.7828, Val Acc: 74.40% ✓\n",
      "Epoch   5/1000 (Enhanced Baseline): Train Loss: 1.1734, Train Acc: 70.26%, LR: 0.000066, Val Loss: 0.7325, Val Acc: 76.08% ✓\n",
      "Epoch   6/1000 (Enhanced Baseline): Train Loss: 1.1420, Train Acc: 71.56%, LR: 0.000051, Val Loss: 0.7036, Val Acc: 77.36% ✓\n",
      "Epoch   7/1000 (Enhanced Baseline): Train Loss: 1.1175, Train Acc: 72.52%, LR: 0.000035, Val Loss: 0.7005, Val Acc: 77.22%\n",
      "Epoch   8/1000 (Enhanced Baseline): Train Loss: 1.1024, Train Acc: 73.58%, LR: 0.000021, Val Loss: 0.6807, Val Acc: 77.78% ✓\n",
      "Epoch   9/1000 (Enhanced Baseline): Train Loss: 1.0899, Train Acc: 73.89%, LR: 0.000010, Val Loss: 0.6779, Val Acc: 77.90% ✓\n",
      "Epoch  10/1000 (Enhanced Baseline): Train Loss: 1.0866, Train Acc: 73.96%, LR: 0.000003, Val Loss: 0.6761, Val Acc: 77.70%\n",
      "Epoch  11/1000 (Enhanced Baseline): Train Loss: 1.1159, Train Acc: 72.86%, LR: 0.000100, Val Loss: 0.6887, Val Acc: 78.32% ✓\n",
      "Epoch  12/1000 (Enhanced Baseline): Train Loss: 1.1043, Train Acc: 73.42%, LR: 0.000099, Val Loss: 0.6636, Val Acc: 78.96% ✓\n",
      "Epoch  13/1000 (Enhanced Baseline): Train Loss: 1.0846, Train Acc: 74.33%, LR: 0.000098, Val Loss: 0.6433, Val Acc: 79.30% ✓\n",
      "Epoch  14/1000 (Enhanced Baseline): Train Loss: 1.0593, Train Acc: 75.28%, LR: 0.000095, Val Loss: 0.6358, Val Acc: 79.80% ✓\n",
      "Epoch  15/1000 (Enhanced Baseline): Train Loss: 1.0448, Train Acc: 76.08%, LR: 0.000091, Val Loss: 0.6294, Val Acc: 79.82% ✓\n",
      "Epoch  16/1000 (Enhanced Baseline): Train Loss: 1.0330, Train Acc: 76.72%, LR: 0.000086, Val Loss: 0.6062, Val Acc: 80.92% ✓\n",
      "Epoch  17/1000 (Enhanced Baseline): Train Loss: 1.0172, Train Acc: 77.29%, LR: 0.000080, Val Loss: 0.6096, Val Acc: 80.64%\n",
      "Epoch  18/1000 (Enhanced Baseline): Train Loss: 1.0073, Train Acc: 77.62%, LR: 0.000073, Val Loss: 0.5734, Val Acc: 82.46% ✓\n",
      "Epoch  19/1000 (Enhanced Baseline): Train Loss: 0.9910, Train Acc: 78.43%, LR: 0.000066, Val Loss: 0.5766, Val Acc: 81.98%\n",
      "Epoch  20/1000 (Enhanced Baseline): Train Loss: 0.9812, Train Acc: 78.81%, LR: 0.000058, Val Loss: 0.5612, Val Acc: 82.62% ✓\n",
      "Epoch  21/1000 (Enhanced Baseline): Train Loss: 0.9681, Train Acc: 79.38%, LR: 0.000051, Val Loss: 0.5627, Val Acc: 82.84% ✓\n",
      "Epoch  22/1000 (Enhanced Baseline): Train Loss: 0.9596, Train Acc: 79.83%, LR: 0.000043, Val Loss: 0.5503, Val Acc: 83.16% ✓\n",
      "Epoch  23/1000 (Enhanced Baseline): Train Loss: 0.9514, Train Acc: 80.13%, LR: 0.000035, Val Loss: 0.5585, Val Acc: 83.12%\n",
      "Epoch  24/1000 (Enhanced Baseline): Train Loss: 0.9458, Train Acc: 80.18%, LR: 0.000028, Val Loss: 0.5463, Val Acc: 83.62% ✓\n",
      "Epoch  25/1000 (Enhanced Baseline): Train Loss: 0.9370, Train Acc: 80.79%, LR: 0.000021, Val Loss: 0.5480, Val Acc: 83.60%\n",
      "Epoch  26/1000 (Enhanced Baseline): Train Loss: 0.9335, Train Acc: 80.86%, LR: 0.000015, Val Loss: 0.5459, Val Acc: 83.40%\n",
      "Epoch  27/1000 (Enhanced Baseline): Train Loss: 0.9248, Train Acc: 81.19%, LR: 0.000010, Val Loss: 0.5390, Val Acc: 83.46%\n",
      "Epoch  28/1000 (Enhanced Baseline): Train Loss: 0.9216, Train Acc: 81.50%, LR: 0.000006, Val Loss: 0.5433, Val Acc: 83.08%\n",
      "Epoch  29/1000 (Enhanced Baseline): Train Loss: 0.9187, Train Acc: 81.57%, LR: 0.000003, Val Loss: 0.5403, Val Acc: 83.72% ✓\n",
      "Epoch  30/1000 (Enhanced Baseline): Train Loss: 0.9205, Train Acc: 81.40%, LR: 0.000002, Val Loss: 0.5374, Val Acc: 83.44%\n",
      "Epoch  31/1000 (Enhanced Baseline): Train Loss: 0.9656, Train Acc: 79.77%, LR: 0.000100, Val Loss: 0.5689, Val Acc: 82.24%\n",
      "Epoch  32/1000 (Enhanced Baseline): Train Loss: 0.9630, Train Acc: 79.66%, LR: 0.000100, Val Loss: 0.5645, Val Acc: 82.76%\n",
      "Epoch  33/1000 (Enhanced Baseline): Train Loss: 0.9599, Train Acc: 79.85%, LR: 0.000099, Val Loss: 0.5815, Val Acc: 82.18%\n",
      "Epoch  34/1000 (Enhanced Baseline): Train Loss: 0.9479, Train Acc: 80.32%, LR: 0.000099, Val Loss: 0.5581, Val Acc: 82.50%\n",
      "Epoch  35/1000 (Enhanced Baseline): Train Loss: 0.9423, Train Acc: 80.74%, LR: 0.000098, Val Loss: 0.5643, Val Acc: 82.76%\n",
      "Epoch  36/1000 (Enhanced Baseline): Train Loss: 0.9340, Train Acc: 81.13%, LR: 0.000096, Val Loss: 0.5438, Val Acc: 83.18%\n",
      "Epoch  37/1000 (Enhanced Baseline): Train Loss: 0.9283, Train Acc: 81.23%, LR: 0.000095, Val Loss: 0.5502, Val Acc: 82.88%\n",
      "Epoch  38/1000 (Enhanced Baseline): Train Loss: 0.9206, Train Acc: 81.58%, LR: 0.000093, Val Loss: 0.5551, Val Acc: 82.90%\n",
      "Epoch  39/1000 (Enhanced Baseline): Train Loss: 0.9108, Train Acc: 81.98%, LR: 0.000091, Val Loss: 0.5469, Val Acc: 83.26%\n",
      "Epoch  40/1000 (Enhanced Baseline): Train Loss: 0.9035, Train Acc: 82.18%, LR: 0.000088, Val Loss: 0.5382, Val Acc: 83.16%\n",
      "Epoch  41/1000 (Enhanced Baseline): Train Loss: 0.8996, Train Acc: 82.53%, LR: 0.000086, Val Loss: 0.5484, Val Acc: 83.14%\n",
      "Epoch  42/1000 (Enhanced Baseline): Train Loss: 0.8973, Train Acc: 82.57%, LR: 0.000083, Val Loss: 0.5311, Val Acc: 83.88% ✓\n",
      "Epoch  43/1000 (Enhanced Baseline): Train Loss: 0.8876, Train Acc: 83.01%, LR: 0.000080, Val Loss: 0.5270, Val Acc: 83.60%\n",
      "Epoch  44/1000 (Enhanced Baseline): Train Loss: 0.8772, Train Acc: 83.34%, LR: 0.000076, Val Loss: 0.5331, Val Acc: 83.28%\n",
      "Epoch  45/1000 (Enhanced Baseline): Train Loss: 0.8767, Train Acc: 83.50%, LR: 0.000073, Val Loss: 0.5306, Val Acc: 84.04% ✓\n",
      "Epoch  46/1000 (Enhanced Baseline): Train Loss: 0.8706, Train Acc: 83.85%, LR: 0.000069, Val Loss: 0.5355, Val Acc: 83.62%\n",
      "Epoch  47/1000 (Enhanced Baseline): Train Loss: 0.8640, Train Acc: 84.20%, LR: 0.000066, Val Loss: 0.5364, Val Acc: 83.26%\n",
      "Epoch  48/1000 (Enhanced Baseline): Train Loss: 0.8605, Train Acc: 84.07%, LR: 0.000062, Val Loss: 0.5337, Val Acc: 83.54%\n",
      "Epoch  49/1000 (Enhanced Baseline): Train Loss: 0.8506, Train Acc: 84.68%, LR: 0.000058, Val Loss: 0.5385, Val Acc: 83.98%\n",
      "Epoch  50/1000 (Enhanced Baseline): Train Loss: 0.8478, Train Acc: 84.84%, LR: 0.000054, Val Loss: 0.5156, Val Acc: 84.40% ✓\n",
      "Epoch  51/1000 (Enhanced Baseline): Train Loss: 0.8418, Train Acc: 85.14%, LR: 0.000051, Val Loss: 0.5276, Val Acc: 83.80%\n",
      "Epoch  52/1000 (Enhanced Baseline): Train Loss: 0.8368, Train Acc: 85.28%, LR: 0.000047, Val Loss: 0.5247, Val Acc: 84.22%\n",
      "Epoch  53/1000 (Enhanced Baseline): Train Loss: 0.8346, Train Acc: 85.49%, LR: 0.000043, Val Loss: 0.5251, Val Acc: 84.02%\n",
      "Epoch  54/1000 (Enhanced Baseline): Train Loss: 0.8247, Train Acc: 85.97%, LR: 0.000039, Val Loss: 0.5163, Val Acc: 84.24%\n",
      "Epoch  55/1000 (Enhanced Baseline): Train Loss: 0.8237, Train Acc: 85.79%, LR: 0.000035, Val Loss: 0.5252, Val Acc: 84.12%\n",
      "Epoch  56/1000 (Enhanced Baseline): Train Loss: 0.8157, Train Acc: 86.33%, LR: 0.000032, Val Loss: 0.5265, Val Acc: 84.46% ✓\n",
      "Epoch  57/1000 (Enhanced Baseline): Train Loss: 0.8142, Train Acc: 86.32%, LR: 0.000028, Val Loss: 0.5286, Val Acc: 84.32%\n",
      "Epoch  58/1000 (Enhanced Baseline): Train Loss: 0.8089, Train Acc: 86.58%, LR: 0.000025, Val Loss: 0.5216, Val Acc: 84.42%\n",
      "Epoch  59/1000 (Enhanced Baseline): Train Loss: 0.8048, Train Acc: 86.64%, LR: 0.000021, Val Loss: 0.5261, Val Acc: 84.14%\n",
      "Epoch  60/1000 (Enhanced Baseline): Train Loss: 0.8049, Train Acc: 86.75%, LR: 0.000018, Val Loss: 0.5181, Val Acc: 84.72% ✓\n",
      "Epoch  61/1000 (Enhanced Baseline): Train Loss: 0.8010, Train Acc: 86.81%, LR: 0.000015, Val Loss: 0.5239, Val Acc: 84.54%\n",
      "Epoch  62/1000 (Enhanced Baseline): Train Loss: 0.8015, Train Acc: 86.75%, LR: 0.000013, Val Loss: 0.5172, Val Acc: 84.62%\n",
      "Epoch  63/1000 (Enhanced Baseline): Train Loss: 0.7908, Train Acc: 87.40%, LR: 0.000010, Val Loss: 0.5200, Val Acc: 84.92% ✓\n",
      "Epoch  64/1000 (Enhanced Baseline): Train Loss: 0.7945, Train Acc: 87.16%, LR: 0.000008, Val Loss: 0.5213, Val Acc: 84.72%\n",
      "Epoch  65/1000 (Enhanced Baseline): Train Loss: 0.7904, Train Acc: 87.53%, LR: 0.000006, Val Loss: 0.5212, Val Acc: 84.96% ✓\n",
      "Epoch  66/1000 (Enhanced Baseline): Train Loss: 0.7894, Train Acc: 87.55%, LR: 0.000005, Val Loss: 0.5227, Val Acc: 84.68%\n",
      "Epoch  67/1000 (Enhanced Baseline): Train Loss: 0.7823, Train Acc: 87.76%, LR: 0.000003, Val Loss: 0.5202, Val Acc: 84.92%\n",
      "Epoch  68/1000 (Enhanced Baseline): Train Loss: 0.7878, Train Acc: 87.49%, LR: 0.000002, Val Loss: 0.5194, Val Acc: 84.74%\n",
      "Epoch  69/1000 (Enhanced Baseline): Train Loss: 0.7896, Train Acc: 87.42%, LR: 0.000002, Val Loss: 0.5189, Val Acc: 84.94%\n",
      "Epoch  70/1000 (Enhanced Baseline): Train Loss: 0.7889, Train Acc: 87.36%, LR: 0.000001, Val Loss: 0.5200, Val Acc: 84.96%\n",
      "Epoch  71/1000 (Enhanced Baseline): Train Loss: 0.8311, Train Acc: 85.80%, LR: 0.000100, Val Loss: 0.5379, Val Acc: 84.36%\n",
      "Epoch  72/1000 (Enhanced Baseline): Train Loss: 0.8396, Train Acc: 85.14%, LR: 0.000100, Val Loss: 0.5268, Val Acc: 84.28%\n",
      "Epoch  73/1000 (Enhanced Baseline): Train Loss: 0.8396, Train Acc: 85.24%, LR: 0.000100, Val Loss: 0.5162, Val Acc: 84.70%\n",
      "Epoch  74/1000 (Enhanced Baseline): Train Loss: 0.8390, Train Acc: 85.36%, LR: 0.000100, Val Loss: 0.5211, Val Acc: 84.54%\n",
      "Epoch  75/1000 (Enhanced Baseline): Train Loss: 0.8324, Train Acc: 85.43%, LR: 0.000099, Val Loss: 0.5442, Val Acc: 83.90%\n",
      "Epoch  76/1000 (Enhanced Baseline): Train Loss: 0.8318, Train Acc: 85.44%, LR: 0.000099, Val Loss: 0.5346, Val Acc: 84.04%\n",
      "Epoch  77/1000 (Enhanced Baseline): Train Loss: 0.8333, Train Acc: 85.44%, LR: 0.000099, Val Loss: 0.5211, Val Acc: 84.46%\n",
      "Epoch  78/1000 (Enhanced Baseline): Train Loss: 0.8245, Train Acc: 85.94%, LR: 0.000098, Val Loss: 0.5187, Val Acc: 84.56%\n",
      "Epoch  79/1000 (Enhanced Baseline): Train Loss: 0.8193, Train Acc: 86.17%, LR: 0.000098, Val Loss: 0.5352, Val Acc: 84.08%\n",
      "Epoch  80/1000 (Enhanced Baseline): Train Loss: 0.8153, Train Acc: 86.32%, LR: 0.000097, Val Loss: 0.5463, Val Acc: 84.40%\n",
      "Epoch  81/1000 (Enhanced Baseline): Train Loss: 0.8133, Train Acc: 86.48%, LR: 0.000096, Val Loss: 0.5353, Val Acc: 84.36%\n",
      "Epoch  82/1000 (Enhanced Baseline): Train Loss: 0.8123, Train Acc: 86.59%, LR: 0.000095, Val Loss: 0.5342, Val Acc: 84.60%\n",
      "Epoch  83/1000 (Enhanced Baseline): Train Loss: 0.8057, Train Acc: 86.57%, LR: 0.000095, Val Loss: 0.5379, Val Acc: 83.60%\n",
      "Epoch  84/1000 (Enhanced Baseline): Train Loss: 0.8040, Train Acc: 86.73%, LR: 0.000094, Val Loss: 0.5259, Val Acc: 84.36%\n",
      "Epoch  85/1000 (Enhanced Baseline): Train Loss: 0.7985, Train Acc: 87.06%, LR: 0.000093, Val Loss: 0.5336, Val Acc: 84.66%\n",
      "Early stopping triggered after 85 epochs (Best Val Acc: 84.96%)\n",
      "Loaded best model state (Val Acc: 84.96%)\n",
      "✅ Model saved to ./base/enhanced_baseline_model.pth\n",
      "✅ ONNX model saved to ./base/enhanced_baseline_model.onnx\n",
      "\n",
      "Evaluating enhanced baseline model...\n",
      "Enhanced Baseline Results: Accuracy=84.62%, MACs=7.30M, Params=3.02M\n",
      "\n",
      "Starting enhanced pruning experiments...\n",
      "\n",
      "--- Strategy: BNScale ---\n",
      "\n",
      "Processing BNScale at 20.0% sparsity...\n",
      "✅ Created MobileNetV2 without pretrained weights\n",
      "✅ Enhanced classifier created for 10 classes\n",
      "Initial MACs: 7.30M\n",
      "Applying BNScaleImportance pruning at 20.0% sparsity...\n",
      "Final MACs: 6.75M (Reduction: 7.5%)\n",
      "Fine-tuning pruned model with enhanced settings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch_pruning/pruner/algorithms/base_pruner.py:87: UserWarning: ch_sparsity is deprecated in v1.3.0. Please use pruning_ratio.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/1000 (BNScale-20.0%): Train Loss: 1.0763, Train Acc: 74.90%, LR: 0.000010, Val Loss: 0.6810, Val Acc: 78.86% ✓\n",
      "Epoch   2/1000 (BNScale-20.0%): Train Loss: 1.0153, Train Acc: 77.29%, LR: 0.000010, Val Loss: 0.6488, Val Acc: 79.54% ✓\n",
      "Epoch   3/1000 (BNScale-20.0%): Train Loss: 0.9855, Train Acc: 78.73%, LR: 0.000009, Val Loss: 0.6247, Val Acc: 80.56% ✓\n",
      "Epoch   4/1000 (BNScale-20.0%): Train Loss: 0.9684, Train Acc: 79.29%, LR: 0.000008, Val Loss: 0.6142, Val Acc: 80.56%\n",
      "Epoch   5/1000 (BNScale-20.0%): Train Loss: 0.9674, Train Acc: 79.36%, LR: 0.000007, Val Loss: 0.6074, Val Acc: 80.66% ✓\n",
      "Epoch   6/1000 (BNScale-20.0%): Train Loss: 0.9539, Train Acc: 79.83%, LR: 0.000005, Val Loss: 0.6095, Val Acc: 81.14% ✓\n",
      "Epoch   7/1000 (BNScale-20.0%): Train Loss: 0.9464, Train Acc: 80.23%, LR: 0.000004, Val Loss: 0.6050, Val Acc: 81.00%\n",
      "Epoch   8/1000 (BNScale-20.0%): Train Loss: 0.9419, Train Acc: 80.30%, LR: 0.000002, Val Loss: 0.5979, Val Acc: 81.14%\n",
      "Epoch   9/1000 (BNScale-20.0%): Train Loss: 0.9456, Train Acc: 80.36%, LR: 0.000001, Val Loss: 0.6001, Val Acc: 81.24% ✓\n",
      "Epoch  10/1000 (BNScale-20.0%): Train Loss: 0.9434, Train Acc: 80.33%, LR: 0.000000, Val Loss: 0.5985, Val Acc: 81.14%\n",
      "Epoch  11/1000 (BNScale-20.0%): Train Loss: 0.9419, Train Acc: 80.39%, LR: 0.000010, Val Loss: 0.5899, Val Acc: 81.32% ✓\n",
      "Epoch  12/1000 (BNScale-20.0%): Train Loss: 0.9330, Train Acc: 80.81%, LR: 0.000010, Val Loss: 0.5837, Val Acc: 81.62% ✓\n",
      "Epoch  13/1000 (BNScale-20.0%): Train Loss: 0.9253, Train Acc: 81.26%, LR: 0.000010, Val Loss: 0.5810, Val Acc: 81.92% ✓\n",
      "Epoch  14/1000 (BNScale-20.0%): Train Loss: 0.9200, Train Acc: 81.43%, LR: 0.000009, Val Loss: 0.5837, Val Acc: 82.14% ✓\n",
      "Epoch  15/1000 (BNScale-20.0%): Train Loss: 0.9112, Train Acc: 81.79%, LR: 0.000009, Val Loss: 0.5729, Val Acc: 81.96%\n",
      "Epoch  16/1000 (BNScale-20.0%): Train Loss: 0.9122, Train Acc: 81.74%, LR: 0.000009, Val Loss: 0.5748, Val Acc: 82.44% ✓\n",
      "Epoch  17/1000 (BNScale-20.0%): Train Loss: 0.9084, Train Acc: 81.96%, LR: 0.000008, Val Loss: 0.5729, Val Acc: 82.46% ✓\n",
      "Epoch  18/1000 (BNScale-20.0%): Train Loss: 0.8990, Train Acc: 82.39%, LR: 0.000007, Val Loss: 0.5670, Val Acc: 82.40%\n",
      "Epoch  19/1000 (BNScale-20.0%): Train Loss: 0.8979, Train Acc: 82.69%, LR: 0.000007, Val Loss: 0.5687, Val Acc: 82.46%\n",
      "Epoch  20/1000 (BNScale-20.0%): Train Loss: 0.8933, Train Acc: 82.65%, LR: 0.000006, Val Loss: 0.5682, Val Acc: 82.48% ✓\n",
      "Epoch  21/1000 (BNScale-20.0%): Train Loss: 0.8953, Train Acc: 82.58%, LR: 0.000005, Val Loss: 0.5666, Val Acc: 82.40%\n",
      "Epoch  22/1000 (BNScale-20.0%): Train Loss: 0.8939, Train Acc: 82.67%, LR: 0.000004, Val Loss: 0.5635, Val Acc: 82.62% ✓\n",
      "Epoch  23/1000 (BNScale-20.0%): Train Loss: 0.8898, Train Acc: 82.66%, LR: 0.000004, Val Loss: 0.5690, Val Acc: 82.34%\n",
      "Epoch  24/1000 (BNScale-20.0%): Train Loss: 0.8904, Train Acc: 82.65%, LR: 0.000003, Val Loss: 0.5631, Val Acc: 82.68% ✓\n",
      "Epoch  25/1000 (BNScale-20.0%): Train Loss: 0.8876, Train Acc: 82.79%, LR: 0.000002, Val Loss: 0.5651, Val Acc: 82.72% ✓\n",
      "Epoch  26/1000 (BNScale-20.0%): Train Loss: 0.8859, Train Acc: 82.83%, LR: 0.000002, Val Loss: 0.5638, Val Acc: 82.88% ✓\n",
      "Epoch  27/1000 (BNScale-20.0%): Train Loss: 0.8820, Train Acc: 83.10%, LR: 0.000001, Val Loss: 0.5657, Val Acc: 82.64%\n",
      "Epoch  28/1000 (BNScale-20.0%): Train Loss: 0.8821, Train Acc: 83.13%, LR: 0.000001, Val Loss: 0.5637, Val Acc: 82.68%\n",
      "Epoch  29/1000 (BNScale-20.0%): Train Loss: 0.8865, Train Acc: 83.02%, LR: 0.000000, Val Loss: 0.5646, Val Acc: 82.72%\n",
      "Epoch  30/1000 (BNScale-20.0%): Train Loss: 0.8897, Train Acc: 82.89%, LR: 0.000000, Val Loss: 0.5656, Val Acc: 82.92% ✓\n",
      "Epoch  31/1000 (BNScale-20.0%): Train Loss: 0.8864, Train Acc: 83.03%, LR: 0.000010, Val Loss: 0.5651, Val Acc: 82.66%\n",
      "Epoch  32/1000 (BNScale-20.0%): Train Loss: 0.8793, Train Acc: 83.26%, LR: 0.000010, Val Loss: 0.5637, Val Acc: 82.78%\n",
      "Epoch  33/1000 (BNScale-20.0%): Train Loss: 0.8767, Train Acc: 83.36%, LR: 0.000010, Val Loss: 0.5641, Val Acc: 82.34%\n",
      "Epoch  34/1000 (BNScale-20.0%): Train Loss: 0.8770, Train Acc: 83.64%, LR: 0.000010, Val Loss: 0.5645, Val Acc: 82.84%\n",
      "Epoch  35/1000 (BNScale-20.0%): Train Loss: 0.8769, Train Acc: 83.48%, LR: 0.000010, Val Loss: 0.5614, Val Acc: 82.60%\n",
      "Epoch  36/1000 (BNScale-20.0%): Train Loss: 0.8738, Train Acc: 83.61%, LR: 0.000010, Val Loss: 0.5609, Val Acc: 82.48%\n",
      "Epoch  37/1000 (BNScale-20.0%): Train Loss: 0.8738, Train Acc: 83.58%, LR: 0.000009, Val Loss: 0.5587, Val Acc: 83.00% ✓\n",
      "Epoch  38/1000 (BNScale-20.0%): Train Loss: 0.8688, Train Acc: 83.72%, LR: 0.000009, Val Loss: 0.5636, Val Acc: 82.74%\n",
      "Epoch  39/1000 (BNScale-20.0%): Train Loss: 0.8718, Train Acc: 83.78%, LR: 0.000009, Val Loss: 0.5608, Val Acc: 83.06% ✓\n",
      "Epoch  40/1000 (BNScale-20.0%): Train Loss: 0.8672, Train Acc: 83.85%, LR: 0.000009, Val Loss: 0.5568, Val Acc: 82.82%\n",
      "Epoch  41/1000 (BNScale-20.0%): Train Loss: 0.8622, Train Acc: 84.00%, LR: 0.000009, Val Loss: 0.5575, Val Acc: 82.92%\n",
      "Epoch  42/1000 (BNScale-20.0%): Train Loss: 0.8625, Train Acc: 84.00%, LR: 0.000008, Val Loss: 0.5529, Val Acc: 83.00%\n",
      "Epoch  43/1000 (BNScale-20.0%): Train Loss: 0.8645, Train Acc: 84.08%, LR: 0.000008, Val Loss: 0.5539, Val Acc: 83.14% ✓\n",
      "Epoch  44/1000 (BNScale-20.0%): Train Loss: 0.8572, Train Acc: 84.24%, LR: 0.000008, Val Loss: 0.5537, Val Acc: 83.12%\n",
      "Epoch  45/1000 (BNScale-20.0%): Train Loss: 0.8607, Train Acc: 84.14%, LR: 0.000007, Val Loss: 0.5502, Val Acc: 83.16% ✓\n",
      "Epoch  46/1000 (BNScale-20.0%): Train Loss: 0.8527, Train Acc: 84.52%, LR: 0.000007, Val Loss: 0.5521, Val Acc: 83.12%\n",
      "Epoch  47/1000 (BNScale-20.0%): Train Loss: 0.8544, Train Acc: 84.41%, LR: 0.000007, Val Loss: 0.5529, Val Acc: 83.00%\n",
      "Epoch  48/1000 (BNScale-20.0%): Train Loss: 0.8604, Train Acc: 84.22%, LR: 0.000006, Val Loss: 0.5498, Val Acc: 83.38% ✓\n",
      "Epoch  49/1000 (BNScale-20.0%): Train Loss: 0.8492, Train Acc: 84.51%, LR: 0.000006, Val Loss: 0.5544, Val Acc: 83.30%\n",
      "Epoch  50/1000 (BNScale-20.0%): Train Loss: 0.8505, Train Acc: 84.62%, LR: 0.000005, Val Loss: 0.5512, Val Acc: 83.32%\n",
      "Epoch  51/1000 (BNScale-20.0%): Train Loss: 0.8473, Train Acc: 84.72%, LR: 0.000005, Val Loss: 0.5480, Val Acc: 83.62% ✓\n",
      "Epoch  52/1000 (BNScale-20.0%): Train Loss: 0.8483, Train Acc: 84.55%, LR: 0.000005, Val Loss: 0.5518, Val Acc: 83.68% ✓\n",
      "Epoch  53/1000 (BNScale-20.0%): Train Loss: 0.8525, Train Acc: 84.46%, LR: 0.000004, Val Loss: 0.5448, Val Acc: 83.44%\n",
      "Epoch  54/1000 (BNScale-20.0%): Train Loss: 0.8477, Train Acc: 84.60%, LR: 0.000004, Val Loss: 0.5468, Val Acc: 83.42%\n",
      "Epoch  55/1000 (BNScale-20.0%): Train Loss: 0.8446, Train Acc: 84.85%, LR: 0.000004, Val Loss: 0.5469, Val Acc: 83.36%\n",
      "Epoch  56/1000 (BNScale-20.0%): Train Loss: 0.8447, Train Acc: 84.83%, LR: 0.000003, Val Loss: 0.5524, Val Acc: 83.16%\n",
      "Epoch  57/1000 (BNScale-20.0%): Train Loss: 0.8419, Train Acc: 84.92%, LR: 0.000003, Val Loss: 0.5460, Val Acc: 83.30%\n",
      "Epoch  58/1000 (BNScale-20.0%): Train Loss: 0.8436, Train Acc: 84.99%, LR: 0.000002, Val Loss: 0.5498, Val Acc: 83.16%\n",
      "Epoch  59/1000 (BNScale-20.0%): Train Loss: 0.8444, Train Acc: 85.00%, LR: 0.000002, Val Loss: 0.5512, Val Acc: 83.14%\n",
      "Epoch  60/1000 (BNScale-20.0%): Train Loss: 0.8435, Train Acc: 84.80%, LR: 0.000002, Val Loss: 0.5516, Val Acc: 83.24%\n",
      "Epoch  61/1000 (BNScale-20.0%): Train Loss: 0.8463, Train Acc: 84.80%, LR: 0.000002, Val Loss: 0.5521, Val Acc: 83.08%\n",
      "Epoch  62/1000 (BNScale-20.0%): Train Loss: 0.8447, Train Acc: 84.87%, LR: 0.000001, Val Loss: 0.5473, Val Acc: 83.48%\n",
      "Epoch  63/1000 (BNScale-20.0%): Train Loss: 0.8422, Train Acc: 84.88%, LR: 0.000001, Val Loss: 0.5466, Val Acc: 83.30%\n",
      "Epoch  64/1000 (BNScale-20.0%): Train Loss: 0.8445, Train Acc: 84.90%, LR: 0.000001, Val Loss: 0.5485, Val Acc: 83.30%\n",
      "Epoch  65/1000 (BNScale-20.0%): Train Loss: 0.8419, Train Acc: 84.98%, LR: 0.000001, Val Loss: 0.5485, Val Acc: 83.58%\n",
      "Epoch  66/1000 (BNScale-20.0%): Train Loss: 0.8412, Train Acc: 85.04%, LR: 0.000000, Val Loss: 0.5477, Val Acc: 83.26%\n",
      "Epoch  67/1000 (BNScale-20.0%): Train Loss: 0.8427, Train Acc: 85.07%, LR: 0.000000, Val Loss: 0.5446, Val Acc: 83.64%\n",
      "Epoch  68/1000 (BNScale-20.0%): Train Loss: 0.8437, Train Acc: 84.94%, LR: 0.000000, Val Loss: 0.5446, Val Acc: 83.48%\n",
      "Epoch  69/1000 (BNScale-20.0%): Train Loss: 0.8407, Train Acc: 84.98%, LR: 0.000000, Val Loss: 0.5490, Val Acc: 83.28%\n",
      "Epoch  70/1000 (BNScale-20.0%): Train Loss: 0.8398, Train Acc: 85.16%, LR: 0.000000, Val Loss: 0.5510, Val Acc: 83.30%\n",
      "Epoch  71/1000 (BNScale-20.0%): Train Loss: 0.8434, Train Acc: 85.00%, LR: 0.000010, Val Loss: 0.5512, Val Acc: 83.34%\n",
      "Epoch  72/1000 (BNScale-20.0%): Train Loss: 0.8417, Train Acc: 84.96%, LR: 0.000010, Val Loss: 0.5491, Val Acc: 83.66%\n",
      "Early stopping triggered after 72 epochs (Best Val Acc: 83.68%)\n",
      "Loaded best model state (Val Acc: 83.68%)\n",
      "Results: Accuracy=83.35%, MACs=6.75M\n",
      "✅ Model saved to ./base/enhanced_bnscale_sparsity_0.2.pth\n",
      "✅ ONNX model saved to ./base/enhanced_bnscale_sparsity_0.2.onnx\n",
      "\n",
      "Processing BNScale at 50.0% sparsity...\n",
      "✅ Created MobileNetV2 without pretrained weights\n",
      "✅ Enhanced classifier created for 10 classes\n",
      "Initial MACs: 7.30M\n",
      "Applying BNScaleImportance pruning at 50.0% sparsity...\n",
      "Final MACs: 6.02M (Reduction: 17.6%)\n",
      "Fine-tuning pruned model with enhanced settings...\n",
      "Epoch   1/1000 (BNScale-50.0%): Train Loss: 1.4216, Train Acc: 61.27%, LR: 0.000010, Val Loss: 0.9276, Val Acc: 70.26% ✓\n",
      "Epoch   2/1000 (BNScale-50.0%): Train Loss: 1.2739, Train Acc: 66.13%, LR: 0.000010, Val Loss: 0.8598, Val Acc: 72.74% ✓\n",
      "Epoch   3/1000 (BNScale-50.0%): Train Loss: 1.2243, Train Acc: 68.04%, LR: 0.000009, Val Loss: 0.8165, Val Acc: 73.54% ✓\n",
      "Epoch   4/1000 (BNScale-50.0%): Train Loss: 1.1886, Train Acc: 69.83%, LR: 0.000008, Val Loss: 0.7915, Val Acc: 74.20% ✓\n",
      "Epoch   5/1000 (BNScale-50.0%): Train Loss: 1.1649, Train Acc: 70.63%, LR: 0.000007, Val Loss: 0.7702, Val Acc: 75.10% ✓\n",
      "Epoch   6/1000 (BNScale-50.0%): Train Loss: 1.1563, Train Acc: 70.90%, LR: 0.000005, Val Loss: 0.7623, Val Acc: 75.30% ✓\n",
      "Epoch   7/1000 (BNScale-50.0%): Train Loss: 1.1461, Train Acc: 71.49%, LR: 0.000004, Val Loss: 0.7606, Val Acc: 75.40% ✓\n",
      "Epoch   8/1000 (BNScale-50.0%): Train Loss: 1.1445, Train Acc: 71.41%, LR: 0.000002, Val Loss: 0.7526, Val Acc: 75.76% ✓\n",
      "Epoch   9/1000 (BNScale-50.0%): Train Loss: 1.1401, Train Acc: 71.66%, LR: 0.000001, Val Loss: 0.7493, Val Acc: 75.70%\n",
      "Epoch  10/1000 (BNScale-50.0%): Train Loss: 1.1279, Train Acc: 72.18%, LR: 0.000000, Val Loss: 0.7513, Val Acc: 75.86% ✓\n",
      "Epoch  11/1000 (BNScale-50.0%): Train Loss: 1.1307, Train Acc: 72.06%, LR: 0.000010, Val Loss: 0.7353, Val Acc: 76.04% ✓\n",
      "Epoch  12/1000 (BNScale-50.0%): Train Loss: 1.1103, Train Acc: 73.03%, LR: 0.000010, Val Loss: 0.7184, Val Acc: 76.56% ✓\n",
      "Epoch  13/1000 (BNScale-50.0%): Train Loss: 1.1058, Train Acc: 73.38%, LR: 0.000010, Val Loss: 0.7098, Val Acc: 76.90% ✓\n",
      "Epoch  14/1000 (BNScale-50.0%): Train Loss: 1.0892, Train Acc: 73.81%, LR: 0.000009, Val Loss: 0.6986, Val Acc: 77.40% ✓\n",
      "Epoch  15/1000 (BNScale-50.0%): Train Loss: 1.0854, Train Acc: 74.01%, LR: 0.000009, Val Loss: 0.6946, Val Acc: 77.66% ✓\n",
      "Epoch  16/1000 (BNScale-50.0%): Train Loss: 1.0716, Train Acc: 74.72%, LR: 0.000009, Val Loss: 0.6827, Val Acc: 77.92% ✓\n",
      "Epoch  17/1000 (BNScale-50.0%): Train Loss: 1.0629, Train Acc: 74.95%, LR: 0.000008, Val Loss: 0.6795, Val Acc: 78.02% ✓\n",
      "Epoch  18/1000 (BNScale-50.0%): Train Loss: 1.0597, Train Acc: 75.03%, LR: 0.000007, Val Loss: 0.6692, Val Acc: 78.42% ✓\n",
      "Epoch  19/1000 (BNScale-50.0%): Train Loss: 1.0566, Train Acc: 75.30%, LR: 0.000007, Val Loss: 0.6690, Val Acc: 78.54% ✓\n",
      "Epoch  20/1000 (BNScale-50.0%): Train Loss: 1.0526, Train Acc: 75.55%, LR: 0.000006, Val Loss: 0.6659, Val Acc: 78.68% ✓\n",
      "Epoch  21/1000 (BNScale-50.0%): Train Loss: 1.0485, Train Acc: 75.66%, LR: 0.000005, Val Loss: 0.6630, Val Acc: 78.50%\n",
      "Epoch  22/1000 (BNScale-50.0%): Train Loss: 1.0415, Train Acc: 75.93%, LR: 0.000004, Val Loss: 0.6614, Val Acc: 78.58%\n",
      "Epoch  23/1000 (BNScale-50.0%): Train Loss: 1.0442, Train Acc: 75.80%, LR: 0.000004, Val Loss: 0.6626, Val Acc: 78.80% ✓\n",
      "Epoch  24/1000 (BNScale-50.0%): Train Loss: 1.0422, Train Acc: 75.98%, LR: 0.000003, Val Loss: 0.6590, Val Acc: 78.46%\n",
      "Epoch  25/1000 (BNScale-50.0%): Train Loss: 1.0325, Train Acc: 76.40%, LR: 0.000002, Val Loss: 0.6577, Val Acc: 78.74%\n",
      "Epoch  26/1000 (BNScale-50.0%): Train Loss: 1.0335, Train Acc: 76.46%, LR: 0.000002, Val Loss: 0.6605, Val Acc: 78.48%\n",
      "Epoch  27/1000 (BNScale-50.0%): Train Loss: 1.0357, Train Acc: 76.33%, LR: 0.000001, Val Loss: 0.6581, Val Acc: 78.80%\n",
      "Epoch  28/1000 (BNScale-50.0%): Train Loss: 1.0380, Train Acc: 76.07%, LR: 0.000001, Val Loss: 0.6553, Val Acc: 78.94% ✓\n",
      "Epoch  29/1000 (BNScale-50.0%): Train Loss: 1.0330, Train Acc: 76.24%, LR: 0.000000, Val Loss: 0.6604, Val Acc: 78.66%\n",
      "Epoch  30/1000 (BNScale-50.0%): Train Loss: 1.0307, Train Acc: 76.68%, LR: 0.000000, Val Loss: 0.6545, Val Acc: 78.84%\n",
      "Epoch  31/1000 (BNScale-50.0%): Train Loss: 1.0348, Train Acc: 76.10%, LR: 0.000010, Val Loss: 0.6518, Val Acc: 78.62%\n",
      "Epoch  32/1000 (BNScale-50.0%): Train Loss: 1.0293, Train Acc: 76.43%, LR: 0.000010, Val Loss: 0.6473, Val Acc: 79.04% ✓\n",
      "Epoch  33/1000 (BNScale-50.0%): Train Loss: 1.0217, Train Acc: 76.85%, LR: 0.000010, Val Loss: 0.6442, Val Acc: 79.18% ✓\n",
      "Epoch  34/1000 (BNScale-50.0%): Train Loss: 1.0209, Train Acc: 76.76%, LR: 0.000010, Val Loss: 0.6430, Val Acc: 79.36% ✓\n",
      "Epoch  35/1000 (BNScale-50.0%): Train Loss: 1.0167, Train Acc: 77.13%, LR: 0.000010, Val Loss: 0.6448, Val Acc: 78.74%\n",
      "Epoch  36/1000 (BNScale-50.0%): Train Loss: 1.0162, Train Acc: 77.17%, LR: 0.000010, Val Loss: 0.6395, Val Acc: 79.34%\n",
      "Epoch  37/1000 (BNScale-50.0%): Train Loss: 1.0087, Train Acc: 77.46%, LR: 0.000009, Val Loss: 0.6375, Val Acc: 79.16%\n",
      "Epoch  38/1000 (BNScale-50.0%): Train Loss: 1.0052, Train Acc: 77.66%, LR: 0.000009, Val Loss: 0.6298, Val Acc: 79.60% ✓\n",
      "Epoch  39/1000 (BNScale-50.0%): Train Loss: 0.9979, Train Acc: 77.83%, LR: 0.000009, Val Loss: 0.6274, Val Acc: 79.42%\n",
      "Epoch  40/1000 (BNScale-50.0%): Train Loss: 1.0019, Train Acc: 77.73%, LR: 0.000009, Val Loss: 0.6276, Val Acc: 79.66% ✓\n",
      "Epoch  41/1000 (BNScale-50.0%): Train Loss: 0.9947, Train Acc: 78.06%, LR: 0.000009, Val Loss: 0.6282, Val Acc: 79.72% ✓\n",
      "Epoch  42/1000 (BNScale-50.0%): Train Loss: 0.9959, Train Acc: 77.96%, LR: 0.000008, Val Loss: 0.6249, Val Acc: 79.90% ✓\n",
      "Epoch  43/1000 (BNScale-50.0%): Train Loss: 0.9846, Train Acc: 78.42%, LR: 0.000008, Val Loss: 0.6192, Val Acc: 79.56%\n",
      "Epoch  44/1000 (BNScale-50.0%): Train Loss: 0.9871, Train Acc: 78.49%, LR: 0.000008, Val Loss: 0.6168, Val Acc: 79.92% ✓\n",
      "Epoch  45/1000 (BNScale-50.0%): Train Loss: 0.9780, Train Acc: 78.78%, LR: 0.000007, Val Loss: 0.6216, Val Acc: 79.74%\n",
      "Epoch  46/1000 (BNScale-50.0%): Train Loss: 0.9823, Train Acc: 78.63%, LR: 0.000007, Val Loss: 0.6205, Val Acc: 79.98% ✓\n",
      "Epoch  47/1000 (BNScale-50.0%): Train Loss: 0.9825, Train Acc: 78.32%, LR: 0.000007, Val Loss: 0.6185, Val Acc: 79.86%\n",
      "Epoch  48/1000 (BNScale-50.0%): Train Loss: 0.9808, Train Acc: 78.81%, LR: 0.000006, Val Loss: 0.6184, Val Acc: 79.72%\n",
      "Epoch  49/1000 (BNScale-50.0%): Train Loss: 0.9760, Train Acc: 78.83%, LR: 0.000006, Val Loss: 0.6187, Val Acc: 79.62%\n",
      "Epoch  50/1000 (BNScale-50.0%): Train Loss: 0.9782, Train Acc: 78.86%, LR: 0.000005, Val Loss: 0.6169, Val Acc: 79.82%\n",
      "Epoch  51/1000 (BNScale-50.0%): Train Loss: 0.9733, Train Acc: 79.07%, LR: 0.000005, Val Loss: 0.6089, Val Acc: 80.06% ✓\n",
      "Epoch  52/1000 (BNScale-50.0%): Train Loss: 0.9758, Train Acc: 78.78%, LR: 0.000005, Val Loss: 0.6134, Val Acc: 79.96%\n",
      "Epoch  53/1000 (BNScale-50.0%): Train Loss: 0.9697, Train Acc: 79.15%, LR: 0.000004, Val Loss: 0.6120, Val Acc: 79.94%\n",
      "Epoch  54/1000 (BNScale-50.0%): Train Loss: 0.9701, Train Acc: 78.90%, LR: 0.000004, Val Loss: 0.6086, Val Acc: 80.18% ✓\n",
      "Epoch  55/1000 (BNScale-50.0%): Train Loss: 0.9712, Train Acc: 79.01%, LR: 0.000004, Val Loss: 0.6110, Val Acc: 80.20% ✓\n",
      "Epoch  56/1000 (BNScale-50.0%): Train Loss: 0.9686, Train Acc: 79.34%, LR: 0.000003, Val Loss: 0.6073, Val Acc: 80.32% ✓\n",
      "Epoch  57/1000 (BNScale-50.0%): Train Loss: 0.9726, Train Acc: 79.04%, LR: 0.000003, Val Loss: 0.6146, Val Acc: 80.02%\n",
      "Epoch  58/1000 (BNScale-50.0%): Train Loss: 0.9668, Train Acc: 79.07%, LR: 0.000002, Val Loss: 0.6096, Val Acc: 79.96%\n",
      "Epoch  59/1000 (BNScale-50.0%): Train Loss: 0.9637, Train Acc: 79.37%, LR: 0.000002, Val Loss: 0.6072, Val Acc: 80.30%\n",
      "Epoch  60/1000 (BNScale-50.0%): Train Loss: 0.9662, Train Acc: 79.14%, LR: 0.000002, Val Loss: 0.6083, Val Acc: 80.20%\n",
      "Epoch  61/1000 (BNScale-50.0%): Train Loss: 0.9625, Train Acc: 79.47%, LR: 0.000002, Val Loss: 0.6079, Val Acc: 80.30%\n",
      "Epoch  62/1000 (BNScale-50.0%): Train Loss: 0.9625, Train Acc: 79.47%, LR: 0.000001, Val Loss: 0.6089, Val Acc: 80.12%\n",
      "Epoch  63/1000 (BNScale-50.0%): Train Loss: 0.9633, Train Acc: 79.27%, LR: 0.000001, Val Loss: 0.6056, Val Acc: 80.36% ✓\n",
      "Epoch  64/1000 (BNScale-50.0%): Train Loss: 0.9653, Train Acc: 79.29%, LR: 0.000001, Val Loss: 0.6081, Val Acc: 80.46% ✓\n",
      "Epoch  65/1000 (BNScale-50.0%): Train Loss: 0.9637, Train Acc: 79.55%, LR: 0.000001, Val Loss: 0.6069, Val Acc: 80.44%\n",
      "Epoch  66/1000 (BNScale-50.0%): Train Loss: 0.9651, Train Acc: 79.32%, LR: 0.000000, Val Loss: 0.6069, Val Acc: 80.34%\n",
      "Epoch  67/1000 (BNScale-50.0%): Train Loss: 0.9618, Train Acc: 79.50%, LR: 0.000000, Val Loss: 0.6057, Val Acc: 80.42%\n",
      "Epoch  68/1000 (BNScale-50.0%): Train Loss: 0.9584, Train Acc: 79.77%, LR: 0.000000, Val Loss: 0.6085, Val Acc: 80.26%\n",
      "Epoch  69/1000 (BNScale-50.0%): Train Loss: 0.9601, Train Acc: 79.59%, LR: 0.000000, Val Loss: 0.6083, Val Acc: 80.48% ✓\n",
      "Epoch  70/1000 (BNScale-50.0%): Train Loss: 0.9595, Train Acc: 79.56%, LR: 0.000000, Val Loss: 0.6054, Val Acc: 80.38%\n",
      "Epoch  71/1000 (BNScale-50.0%): Train Loss: 0.9634, Train Acc: 79.55%, LR: 0.000010, Val Loss: 0.6063, Val Acc: 80.50% ✓\n",
      "Epoch  72/1000 (BNScale-50.0%): Train Loss: 0.9606, Train Acc: 79.61%, LR: 0.000010, Val Loss: 0.6097, Val Acc: 80.46%\n",
      "Epoch  73/1000 (BNScale-50.0%): Train Loss: 0.9559, Train Acc: 79.63%, LR: 0.000010, Val Loss: 0.6037, Val Acc: 80.78% ✓\n",
      "Epoch  74/1000 (BNScale-50.0%): Train Loss: 0.9609, Train Acc: 79.39%, LR: 0.000010, Val Loss: 0.6045, Val Acc: 80.54%\n",
      "Epoch  75/1000 (BNScale-50.0%): Train Loss: 0.9606, Train Acc: 79.63%, LR: 0.000010, Val Loss: 0.6050, Val Acc: 80.68%\n",
      "Epoch  76/1000 (BNScale-50.0%): Train Loss: 0.9553, Train Acc: 79.85%, LR: 0.000010, Val Loss: 0.6077, Val Acc: 80.28%\n",
      "Epoch  77/1000 (BNScale-50.0%): Train Loss: 0.9486, Train Acc: 80.13%, LR: 0.000010, Val Loss: 0.6083, Val Acc: 80.34%\n",
      "Epoch  78/1000 (BNScale-50.0%): Train Loss: 0.9513, Train Acc: 79.99%, LR: 0.000010, Val Loss: 0.6097, Val Acc: 80.24%\n",
      "Epoch  79/1000 (BNScale-50.0%): Train Loss: 0.9471, Train Acc: 80.20%, LR: 0.000010, Val Loss: 0.6059, Val Acc: 80.68%\n",
      "Epoch  80/1000 (BNScale-50.0%): Train Loss: 0.9416, Train Acc: 80.38%, LR: 0.000010, Val Loss: 0.6103, Val Acc: 80.60%\n",
      "Epoch  81/1000 (BNScale-50.0%): Train Loss: 0.9461, Train Acc: 80.30%, LR: 0.000010, Val Loss: 0.6036, Val Acc: 80.58%\n",
      "Epoch  82/1000 (BNScale-50.0%): Train Loss: 0.9438, Train Acc: 80.40%, LR: 0.000010, Val Loss: 0.6057, Val Acc: 80.86% ✓\n",
      "Epoch  83/1000 (BNScale-50.0%): Train Loss: 0.9393, Train Acc: 80.63%, LR: 0.000009, Val Loss: 0.6027, Val Acc: 80.64%\n",
      "Epoch  84/1000 (BNScale-50.0%): Train Loss: 0.9356, Train Acc: 80.63%, LR: 0.000009, Val Loss: 0.5996, Val Acc: 81.02% ✓\n",
      "Epoch  85/1000 (BNScale-50.0%): Train Loss: 0.9305, Train Acc: 81.03%, LR: 0.000009, Val Loss: 0.5995, Val Acc: 80.58%\n",
      "Epoch  86/1000 (BNScale-50.0%): Train Loss: 0.9391, Train Acc: 80.49%, LR: 0.000009, Val Loss: 0.5989, Val Acc: 80.70%\n",
      "Epoch  87/1000 (BNScale-50.0%): Train Loss: 0.9294, Train Acc: 80.97%, LR: 0.000009, Val Loss: 0.6020, Val Acc: 80.26%\n",
      "Epoch  88/1000 (BNScale-50.0%): Train Loss: 0.9318, Train Acc: 80.93%, LR: 0.000009, Val Loss: 0.6011, Val Acc: 80.60%\n",
      "Epoch  89/1000 (BNScale-50.0%): Train Loss: 0.9285, Train Acc: 80.99%, LR: 0.000009, Val Loss: 0.5972, Val Acc: 81.14% ✓\n",
      "Epoch  90/1000 (BNScale-50.0%): Train Loss: 0.9309, Train Acc: 80.92%, LR: 0.000009, Val Loss: 0.5987, Val Acc: 80.62%\n",
      "Epoch  91/1000 (BNScale-50.0%): Train Loss: 0.9262, Train Acc: 81.09%, LR: 0.000009, Val Loss: 0.5980, Val Acc: 80.98%\n",
      "Epoch  92/1000 (BNScale-50.0%): Train Loss: 0.9299, Train Acc: 80.81%, LR: 0.000008, Val Loss: 0.5961, Val Acc: 80.80%\n",
      "Epoch  93/1000 (BNScale-50.0%): Train Loss: 0.9212, Train Acc: 81.27%, LR: 0.000008, Val Loss: 0.5978, Val Acc: 80.92%\n",
      "Epoch  94/1000 (BNScale-50.0%): Train Loss: 0.9251, Train Acc: 81.25%, LR: 0.000008, Val Loss: 0.5961, Val Acc: 80.98%\n",
      "Epoch  95/1000 (BNScale-50.0%): Train Loss: 0.9250, Train Acc: 81.11%, LR: 0.000008, Val Loss: 0.5934, Val Acc: 81.22% ✓\n",
      "Epoch  96/1000 (BNScale-50.0%): Train Loss: 0.9198, Train Acc: 81.54%, LR: 0.000008, Val Loss: 0.5928, Val Acc: 80.98%\n",
      "Epoch  97/1000 (BNScale-50.0%): Train Loss: 0.9213, Train Acc: 81.46%, LR: 0.000008, Val Loss: 0.5907, Val Acc: 80.96%\n",
      "Epoch  98/1000 (BNScale-50.0%): Train Loss: 0.9171, Train Acc: 81.61%, LR: 0.000007, Val Loss: 0.5925, Val Acc: 81.22%\n",
      "Epoch  99/1000 (BNScale-50.0%): Train Loss: 0.9172, Train Acc: 81.53%, LR: 0.000007, Val Loss: 0.5937, Val Acc: 80.98%\n",
      "Epoch 100/1000 (BNScale-50.0%): Train Loss: 0.9169, Train Acc: 81.49%, LR: 0.000007, Val Loss: 0.5934, Val Acc: 81.08%\n",
      "Epoch 101/1000 (BNScale-50.0%): Train Loss: 0.9187, Train Acc: 81.30%, LR: 0.000007, Val Loss: 0.5925, Val Acc: 81.24% ✓\n",
      "Epoch 102/1000 (BNScale-50.0%): Train Loss: 0.9133, Train Acc: 81.76%, LR: 0.000007, Val Loss: 0.5892, Val Acc: 81.34% ✓\n",
      "Epoch 103/1000 (BNScale-50.0%): Train Loss: 0.9081, Train Acc: 81.98%, LR: 0.000007, Val Loss: 0.5946, Val Acc: 81.22%\n",
      "Epoch 104/1000 (BNScale-50.0%): Train Loss: 0.9113, Train Acc: 81.66%, LR: 0.000006, Val Loss: 0.5902, Val Acc: 81.18%\n",
      "Epoch 105/1000 (BNScale-50.0%): Train Loss: 0.9168, Train Acc: 81.35%, LR: 0.000006, Val Loss: 0.5949, Val Acc: 81.28%\n",
      "Epoch 106/1000 (BNScale-50.0%): Train Loss: 0.9064, Train Acc: 82.02%, LR: 0.000006, Val Loss: 0.5890, Val Acc: 81.44% ✓\n",
      "Epoch 107/1000 (BNScale-50.0%): Train Loss: 0.9086, Train Acc: 81.92%, LR: 0.000006, Val Loss: 0.5914, Val Acc: 81.14%\n",
      "Epoch 108/1000 (BNScale-50.0%): Train Loss: 0.9074, Train Acc: 81.90%, LR: 0.000006, Val Loss: 0.5862, Val Acc: 81.50% ✓\n",
      "Epoch 109/1000 (BNScale-50.0%): Train Loss: 0.9101, Train Acc: 81.68%, LR: 0.000005, Val Loss: 0.5879, Val Acc: 81.20%\n",
      "Epoch 110/1000 (BNScale-50.0%): Train Loss: 0.9097, Train Acc: 81.87%, LR: 0.000005, Val Loss: 0.5884, Val Acc: 81.12%\n",
      "Epoch 111/1000 (BNScale-50.0%): Train Loss: 0.9060, Train Acc: 81.97%, LR: 0.000005, Val Loss: 0.5890, Val Acc: 80.90%\n",
      "Epoch 112/1000 (BNScale-50.0%): Train Loss: 0.9076, Train Acc: 82.15%, LR: 0.000005, Val Loss: 0.5849, Val Acc: 81.40%\n",
      "Epoch 113/1000 (BNScale-50.0%): Train Loss: 0.9077, Train Acc: 82.12%, LR: 0.000005, Val Loss: 0.5906, Val Acc: 81.30%\n",
      "Epoch 114/1000 (BNScale-50.0%): Train Loss: 0.9055, Train Acc: 82.04%, LR: 0.000004, Val Loss: 0.5868, Val Acc: 81.34%\n",
      "Epoch 115/1000 (BNScale-50.0%): Train Loss: 0.8994, Train Acc: 82.18%, LR: 0.000004, Val Loss: 0.5872, Val Acc: 81.40%\n",
      "Epoch 116/1000 (BNScale-50.0%): Train Loss: 0.9017, Train Acc: 82.13%, LR: 0.000004, Val Loss: 0.5838, Val Acc: 81.26%\n",
      "Epoch 117/1000 (BNScale-50.0%): Train Loss: 0.9095, Train Acc: 82.03%, LR: 0.000004, Val Loss: 0.5844, Val Acc: 81.66% ✓\n",
      "Epoch 118/1000 (BNScale-50.0%): Train Loss: 0.8989, Train Acc: 82.37%, LR: 0.000004, Val Loss: 0.5835, Val Acc: 81.36%\n",
      "Epoch 119/1000 (BNScale-50.0%): Train Loss: 0.9019, Train Acc: 82.19%, LR: 0.000004, Val Loss: 0.5822, Val Acc: 81.38%\n",
      "Epoch 120/1000 (BNScale-50.0%): Train Loss: 0.8996, Train Acc: 82.28%, LR: 0.000003, Val Loss: 0.5877, Val Acc: 81.22%\n",
      "Epoch 121/1000 (BNScale-50.0%): Train Loss: 0.8986, Train Acc: 82.45%, LR: 0.000003, Val Loss: 0.5848, Val Acc: 81.46%\n",
      "Epoch 122/1000 (BNScale-50.0%): Train Loss: 0.8979, Train Acc: 82.48%, LR: 0.000003, Val Loss: 0.5856, Val Acc: 81.46%\n",
      "Epoch 123/1000 (BNScale-50.0%): Train Loss: 0.8969, Train Acc: 82.32%, LR: 0.000003, Val Loss: 0.5828, Val Acc: 81.46%\n",
      "Epoch 124/1000 (BNScale-50.0%): Train Loss: 0.8991, Train Acc: 82.32%, LR: 0.000003, Val Loss: 0.5894, Val Acc: 81.32%\n",
      "Epoch 125/1000 (BNScale-50.0%): Train Loss: 0.8937, Train Acc: 82.71%, LR: 0.000002, Val Loss: 0.5826, Val Acc: 81.42%\n",
      "Epoch 126/1000 (BNScale-50.0%): Train Loss: 0.8954, Train Acc: 82.28%, LR: 0.000002, Val Loss: 0.5837, Val Acc: 81.58%\n",
      "Epoch 127/1000 (BNScale-50.0%): Train Loss: 0.8971, Train Acc: 82.37%, LR: 0.000002, Val Loss: 0.5814, Val Acc: 81.32%\n",
      "Epoch 128/1000 (BNScale-50.0%): Train Loss: 0.8988, Train Acc: 82.37%, LR: 0.000002, Val Loss: 0.5867, Val Acc: 81.36%\n",
      "Epoch 129/1000 (BNScale-50.0%): Train Loss: 0.8975, Train Acc: 82.56%, LR: 0.000002, Val Loss: 0.5859, Val Acc: 81.14%\n",
      "Epoch 130/1000 (BNScale-50.0%): Train Loss: 0.8921, Train Acc: 82.69%, LR: 0.000002, Val Loss: 0.5823, Val Acc: 81.68% ✓\n",
      "Epoch 131/1000 (BNScale-50.0%): Train Loss: 0.8919, Train Acc: 82.70%, LR: 0.000002, Val Loss: 0.5845, Val Acc: 81.54%\n",
      "Epoch 132/1000 (BNScale-50.0%): Train Loss: 0.9006, Train Acc: 82.22%, LR: 0.000001, Val Loss: 0.5857, Val Acc: 81.52%\n",
      "Epoch 133/1000 (BNScale-50.0%): Train Loss: 0.8974, Train Acc: 82.49%, LR: 0.000001, Val Loss: 0.5855, Val Acc: 81.26%\n",
      "Epoch 134/1000 (BNScale-50.0%): Train Loss: 0.8935, Train Acc: 82.66%, LR: 0.000001, Val Loss: 0.5818, Val Acc: 81.50%\n",
      "Epoch 135/1000 (BNScale-50.0%): Train Loss: 0.9017, Train Acc: 82.28%, LR: 0.000001, Val Loss: 0.5849, Val Acc: 81.48%\n",
      "Epoch 136/1000 (BNScale-50.0%): Train Loss: 0.8964, Train Acc: 82.64%, LR: 0.000001, Val Loss: 0.5820, Val Acc: 81.54%\n",
      "Epoch 137/1000 (BNScale-50.0%): Train Loss: 0.8906, Train Acc: 82.73%, LR: 0.000001, Val Loss: 0.5882, Val Acc: 81.22%\n",
      "Epoch 138/1000 (BNScale-50.0%): Train Loss: 0.8906, Train Acc: 82.65%, LR: 0.000001, Val Loss: 0.5857, Val Acc: 81.46%\n",
      "Epoch 139/1000 (BNScale-50.0%): Train Loss: 0.8946, Train Acc: 82.56%, LR: 0.000001, Val Loss: 0.5818, Val Acc: 81.58%\n",
      "Epoch 140/1000 (BNScale-50.0%): Train Loss: 0.8941, Train Acc: 82.53%, LR: 0.000001, Val Loss: 0.5831, Val Acc: 81.32%\n",
      "Epoch 141/1000 (BNScale-50.0%): Train Loss: 0.8939, Train Acc: 82.67%, LR: 0.000000, Val Loss: 0.5828, Val Acc: 81.52%\n",
      "Epoch 142/1000 (BNScale-50.0%): Train Loss: 0.8903, Train Acc: 82.68%, LR: 0.000000, Val Loss: 0.5838, Val Acc: 81.36%\n",
      "Epoch 143/1000 (BNScale-50.0%): Train Loss: 0.8914, Train Acc: 82.73%, LR: 0.000000, Val Loss: 0.5848, Val Acc: 81.54%\n",
      "Epoch 144/1000 (BNScale-50.0%): Train Loss: 0.8874, Train Acc: 82.84%, LR: 0.000000, Val Loss: 0.5834, Val Acc: 81.32%\n",
      "Epoch 145/1000 (BNScale-50.0%): Train Loss: 0.8890, Train Acc: 82.97%, LR: 0.000000, Val Loss: 0.5812, Val Acc: 81.50%\n",
      "Epoch 146/1000 (BNScale-50.0%): Train Loss: 0.8953, Train Acc: 82.34%, LR: 0.000000, Val Loss: 0.5812, Val Acc: 81.54%\n",
      "Epoch 147/1000 (BNScale-50.0%): Train Loss: 0.8922, Train Acc: 82.72%, LR: 0.000000, Val Loss: 0.5842, Val Acc: 81.62%\n",
      "Epoch 148/1000 (BNScale-50.0%): Train Loss: 0.8890, Train Acc: 82.76%, LR: 0.000000, Val Loss: 0.5841, Val Acc: 81.56%\n",
      "Epoch 149/1000 (BNScale-50.0%): Train Loss: 0.8884, Train Acc: 82.94%, LR: 0.000000, Val Loss: 0.5812, Val Acc: 81.70% ✓\n",
      "Epoch 150/1000 (BNScale-50.0%): Train Loss: 0.8914, Train Acc: 82.58%, LR: 0.000000, Val Loss: 0.5828, Val Acc: 81.42%\n",
      "Epoch 151/1000 (BNScale-50.0%): Train Loss: 0.8979, Train Acc: 82.50%, LR: 0.000010, Val Loss: 0.5854, Val Acc: 81.44%\n",
      "Epoch 152/1000 (BNScale-50.0%): Train Loss: 0.8891, Train Acc: 82.86%, LR: 0.000010, Val Loss: 0.5828, Val Acc: 81.34%\n",
      "Epoch 153/1000 (BNScale-50.0%): Train Loss: 0.8955, Train Acc: 82.58%, LR: 0.000010, Val Loss: 0.5836, Val Acc: 81.56%\n",
      "Epoch 154/1000 (BNScale-50.0%): Train Loss: 0.8942, Train Acc: 82.59%, LR: 0.000010, Val Loss: 0.5828, Val Acc: 81.66%\n",
      "Epoch 155/1000 (BNScale-50.0%): Train Loss: 0.8885, Train Acc: 82.66%, LR: 0.000010, Val Loss: 0.5836, Val Acc: 81.70%\n",
      "Epoch 156/1000 (BNScale-50.0%): Train Loss: 0.8921, Train Acc: 82.74%, LR: 0.000010, Val Loss: 0.5817, Val Acc: 81.62%\n",
      "Epoch 157/1000 (BNScale-50.0%): Train Loss: 0.8935, Train Acc: 82.52%, LR: 0.000010, Val Loss: 0.5817, Val Acc: 81.76% ✓\n",
      "Epoch 158/1000 (BNScale-50.0%): Train Loss: 0.8930, Train Acc: 82.59%, LR: 0.000010, Val Loss: 0.5792, Val Acc: 81.84% ✓\n",
      "Epoch 159/1000 (BNScale-50.0%): Train Loss: 0.8873, Train Acc: 82.97%, LR: 0.000010, Val Loss: 0.5820, Val Acc: 81.70%\n",
      "Epoch 160/1000 (BNScale-50.0%): Train Loss: 0.8912, Train Acc: 82.79%, LR: 0.000010, Val Loss: 0.5810, Val Acc: 81.56%\n",
      "Epoch 161/1000 (BNScale-50.0%): Train Loss: 0.8895, Train Acc: 82.89%, LR: 0.000010, Val Loss: 0.5852, Val Acc: 81.40%\n",
      "Epoch 162/1000 (BNScale-50.0%): Train Loss: 0.8899, Train Acc: 82.85%, LR: 0.000010, Val Loss: 0.5803, Val Acc: 81.80%\n",
      "Epoch 163/1000 (BNScale-50.0%): Train Loss: 0.8851, Train Acc: 82.91%, LR: 0.000010, Val Loss: 0.5802, Val Acc: 81.50%\n",
      "Epoch 164/1000 (BNScale-50.0%): Train Loss: 0.8826, Train Acc: 83.08%, LR: 0.000010, Val Loss: 0.5861, Val Acc: 81.66%\n",
      "Epoch 165/1000 (BNScale-50.0%): Train Loss: 0.8877, Train Acc: 82.81%, LR: 0.000010, Val Loss: 0.5872, Val Acc: 81.58%\n",
      "Epoch 166/1000 (BNScale-50.0%): Train Loss: 0.8791, Train Acc: 83.20%, LR: 0.000010, Val Loss: 0.5835, Val Acc: 81.96% ✓\n",
      "Epoch 167/1000 (BNScale-50.0%): Train Loss: 0.8851, Train Acc: 83.03%, LR: 0.000010, Val Loss: 0.5832, Val Acc: 81.68%\n",
      "Epoch 168/1000 (BNScale-50.0%): Train Loss: 0.8805, Train Acc: 83.30%, LR: 0.000010, Val Loss: 0.5834, Val Acc: 81.62%\n",
      "Epoch 169/1000 (BNScale-50.0%): Train Loss: 0.8805, Train Acc: 82.94%, LR: 0.000010, Val Loss: 0.5797, Val Acc: 81.94%\n",
      "Epoch 170/1000 (BNScale-50.0%): Train Loss: 0.8784, Train Acc: 83.23%, LR: 0.000010, Val Loss: 0.5779, Val Acc: 81.82%\n",
      "Epoch 171/1000 (BNScale-50.0%): Train Loss: 0.8801, Train Acc: 83.17%, LR: 0.000010, Val Loss: 0.5744, Val Acc: 81.84%\n",
      "Epoch 172/1000 (BNScale-50.0%): Train Loss: 0.8702, Train Acc: 83.65%, LR: 0.000010, Val Loss: 0.5788, Val Acc: 81.84%\n",
      "Epoch 173/1000 (BNScale-50.0%): Train Loss: 0.8738, Train Acc: 83.58%, LR: 0.000010, Val Loss: 0.5793, Val Acc: 81.80%\n",
      "Epoch 174/1000 (BNScale-50.0%): Train Loss: 0.8740, Train Acc: 83.46%, LR: 0.000010, Val Loss: 0.5854, Val Acc: 81.50%\n",
      "Epoch 175/1000 (BNScale-50.0%): Train Loss: 0.8719, Train Acc: 83.58%, LR: 0.000009, Val Loss: 0.5823, Val Acc: 81.70%\n",
      "Epoch 176/1000 (BNScale-50.0%): Train Loss: 0.8725, Train Acc: 83.60%, LR: 0.000009, Val Loss: 0.5798, Val Acc: 81.80%\n",
      "Epoch 177/1000 (BNScale-50.0%): Train Loss: 0.8743, Train Acc: 83.62%, LR: 0.000009, Val Loss: 0.5817, Val Acc: 82.06% ✓\n",
      "Epoch 178/1000 (BNScale-50.0%): Train Loss: 0.8742, Train Acc: 83.48%, LR: 0.000009, Val Loss: 0.5849, Val Acc: 81.98%\n",
      "Epoch 179/1000 (BNScale-50.0%): Train Loss: 0.8676, Train Acc: 83.67%, LR: 0.000009, Val Loss: 0.5846, Val Acc: 81.74%\n",
      "Epoch 180/1000 (BNScale-50.0%): Train Loss: 0.8692, Train Acc: 83.68%, LR: 0.000009, Val Loss: 0.5800, Val Acc: 81.90%\n",
      "Epoch 181/1000 (BNScale-50.0%): Train Loss: 0.8678, Train Acc: 83.82%, LR: 0.000009, Val Loss: 0.5784, Val Acc: 81.94%\n",
      "Epoch 182/1000 (BNScale-50.0%): Train Loss: 0.8670, Train Acc: 83.86%, LR: 0.000009, Val Loss: 0.5841, Val Acc: 81.92%\n",
      "Epoch 183/1000 (BNScale-50.0%): Train Loss: 0.8635, Train Acc: 84.03%, LR: 0.000009, Val Loss: 0.5782, Val Acc: 81.88%\n",
      "Epoch 184/1000 (BNScale-50.0%): Train Loss: 0.8669, Train Acc: 83.94%, LR: 0.000009, Val Loss: 0.5763, Val Acc: 82.08% ✓\n",
      "Epoch 185/1000 (BNScale-50.0%): Train Loss: 0.8658, Train Acc: 84.02%, LR: 0.000009, Val Loss: 0.5852, Val Acc: 81.90%\n",
      "Epoch 186/1000 (BNScale-50.0%): Train Loss: 0.8608, Train Acc: 84.28%, LR: 0.000009, Val Loss: 0.5792, Val Acc: 81.88%\n",
      "Epoch 187/1000 (BNScale-50.0%): Train Loss: 0.8622, Train Acc: 84.00%, LR: 0.000009, Val Loss: 0.5821, Val Acc: 81.70%\n",
      "Epoch 188/1000 (BNScale-50.0%): Train Loss: 0.8607, Train Acc: 84.09%, LR: 0.000009, Val Loss: 0.5745, Val Acc: 82.24% ✓\n",
      "Epoch 189/1000 (BNScale-50.0%): Train Loss: 0.8665, Train Acc: 83.97%, LR: 0.000009, Val Loss: 0.5752, Val Acc: 82.14%\n",
      "Epoch 190/1000 (BNScale-50.0%): Train Loss: 0.8599, Train Acc: 84.14%, LR: 0.000009, Val Loss: 0.5748, Val Acc: 82.08%\n",
      "Epoch 191/1000 (BNScale-50.0%): Train Loss: 0.8552, Train Acc: 84.37%, LR: 0.000009, Val Loss: 0.5788, Val Acc: 82.02%\n",
      "Epoch 192/1000 (BNScale-50.0%): Train Loss: 0.8583, Train Acc: 84.18%, LR: 0.000008, Val Loss: 0.5798, Val Acc: 81.86%\n",
      "Epoch 193/1000 (BNScale-50.0%): Train Loss: 0.8600, Train Acc: 84.08%, LR: 0.000008, Val Loss: 0.5783, Val Acc: 81.94%\n",
      "Epoch 194/1000 (BNScale-50.0%): Train Loss: 0.8606, Train Acc: 84.27%, LR: 0.000008, Val Loss: 0.5799, Val Acc: 82.08%\n",
      "Epoch 195/1000 (BNScale-50.0%): Train Loss: 0.8580, Train Acc: 84.21%, LR: 0.000008, Val Loss: 0.5810, Val Acc: 82.02%\n",
      "Epoch 196/1000 (BNScale-50.0%): Train Loss: 0.8527, Train Acc: 84.33%, LR: 0.000008, Val Loss: 0.5757, Val Acc: 82.10%\n",
      "Epoch 197/1000 (BNScale-50.0%): Train Loss: 0.8533, Train Acc: 84.31%, LR: 0.000008, Val Loss: 0.5718, Val Acc: 82.42% ✓\n",
      "Epoch 198/1000 (BNScale-50.0%): Train Loss: 0.8563, Train Acc: 84.33%, LR: 0.000008, Val Loss: 0.5769, Val Acc: 82.26%\n",
      "Epoch 199/1000 (BNScale-50.0%): Train Loss: 0.8551, Train Acc: 84.42%, LR: 0.000008, Val Loss: 0.5726, Val Acc: 82.14%\n",
      "Epoch 200/1000 (BNScale-50.0%): Train Loss: 0.8500, Train Acc: 84.80%, LR: 0.000008, Val Loss: 0.5732, Val Acc: 82.38%\n",
      "Epoch 201/1000 (BNScale-50.0%): Train Loss: 0.8507, Train Acc: 84.68%, LR: 0.000008, Val Loss: 0.5697, Val Acc: 82.50% ✓\n",
      "Epoch 202/1000 (BNScale-50.0%): Train Loss: 0.8513, Train Acc: 84.50%, LR: 0.000008, Val Loss: 0.5728, Val Acc: 82.26%\n",
      "Epoch 203/1000 (BNScale-50.0%): Train Loss: 0.8509, Train Acc: 84.66%, LR: 0.000008, Val Loss: 0.5762, Val Acc: 82.20%\n",
      "Epoch 204/1000 (BNScale-50.0%): Train Loss: 0.8525, Train Acc: 84.50%, LR: 0.000008, Val Loss: 0.5781, Val Acc: 82.26%\n",
      "Epoch 205/1000 (BNScale-50.0%): Train Loss: 0.8493, Train Acc: 84.54%, LR: 0.000007, Val Loss: 0.5803, Val Acc: 82.42%\n",
      "Epoch 206/1000 (BNScale-50.0%): Train Loss: 0.8474, Train Acc: 84.65%, LR: 0.000007, Val Loss: 0.5719, Val Acc: 82.44%\n",
      "Epoch 207/1000 (BNScale-50.0%): Train Loss: 0.8470, Train Acc: 84.69%, LR: 0.000007, Val Loss: 0.5754, Val Acc: 82.28%\n",
      "Epoch 208/1000 (BNScale-50.0%): Train Loss: 0.8447, Train Acc: 84.81%, LR: 0.000007, Val Loss: 0.5749, Val Acc: 82.16%\n",
      "Epoch 209/1000 (BNScale-50.0%): Train Loss: 0.8479, Train Acc: 84.63%, LR: 0.000007, Val Loss: 0.5754, Val Acc: 82.32%\n",
      "Epoch 210/1000 (BNScale-50.0%): Train Loss: 0.8449, Train Acc: 84.85%, LR: 0.000007, Val Loss: 0.5698, Val Acc: 82.58% ✓\n",
      "Epoch 211/1000 (BNScale-50.0%): Train Loss: 0.8475, Train Acc: 84.81%, LR: 0.000007, Val Loss: 0.5680, Val Acc: 82.52%\n",
      "Epoch 212/1000 (BNScale-50.0%): Train Loss: 0.8442, Train Acc: 84.85%, LR: 0.000007, Val Loss: 0.5692, Val Acc: 82.74% ✓\n",
      "Epoch 213/1000 (BNScale-50.0%): Train Loss: 0.8449, Train Acc: 85.00%, LR: 0.000007, Val Loss: 0.5687, Val Acc: 82.64%\n",
      "Epoch 214/1000 (BNScale-50.0%): Train Loss: 0.8445, Train Acc: 84.76%, LR: 0.000007, Val Loss: 0.5761, Val Acc: 82.36%\n",
      "Epoch 215/1000 (BNScale-50.0%): Train Loss: 0.8456, Train Acc: 84.81%, LR: 0.000007, Val Loss: 0.5725, Val Acc: 82.50%\n",
      "Epoch 216/1000 (BNScale-50.0%): Train Loss: 0.8444, Train Acc: 84.77%, LR: 0.000006, Val Loss: 0.5711, Val Acc: 82.44%\n",
      "Epoch 217/1000 (BNScale-50.0%): Train Loss: 0.8399, Train Acc: 85.05%, LR: 0.000006, Val Loss: 0.5702, Val Acc: 82.52%\n",
      "Epoch 218/1000 (BNScale-50.0%): Train Loss: 0.8432, Train Acc: 84.92%, LR: 0.000006, Val Loss: 0.5712, Val Acc: 82.58%\n",
      "Epoch 219/1000 (BNScale-50.0%): Train Loss: 0.8364, Train Acc: 85.23%, LR: 0.000006, Val Loss: 0.5716, Val Acc: 82.72%\n",
      "Epoch 220/1000 (BNScale-50.0%): Train Loss: 0.8446, Train Acc: 84.83%, LR: 0.000006, Val Loss: 0.5700, Val Acc: 82.50%\n",
      "Epoch 221/1000 (BNScale-50.0%): Train Loss: 0.8349, Train Acc: 85.27%, LR: 0.000006, Val Loss: 0.5749, Val Acc: 82.10%\n",
      "Epoch 222/1000 (BNScale-50.0%): Train Loss: 0.8374, Train Acc: 85.18%, LR: 0.000006, Val Loss: 0.5731, Val Acc: 82.62%\n",
      "Epoch 223/1000 (BNScale-50.0%): Train Loss: 0.8388, Train Acc: 85.11%, LR: 0.000006, Val Loss: 0.5706, Val Acc: 82.60%\n",
      "Epoch 224/1000 (BNScale-50.0%): Train Loss: 0.8425, Train Acc: 85.04%, LR: 0.000006, Val Loss: 0.5727, Val Acc: 82.02%\n",
      "Epoch 225/1000 (BNScale-50.0%): Train Loss: 0.8421, Train Acc: 85.10%, LR: 0.000006, Val Loss: 0.5700, Val Acc: 82.40%\n",
      "Epoch 226/1000 (BNScale-50.0%): Train Loss: 0.8337, Train Acc: 85.28%, LR: 0.000006, Val Loss: 0.5808, Val Acc: 82.28%\n",
      "Epoch 227/1000 (BNScale-50.0%): Train Loss: 0.8360, Train Acc: 85.31%, LR: 0.000005, Val Loss: 0.5750, Val Acc: 82.48%\n",
      "Epoch 228/1000 (BNScale-50.0%): Train Loss: 0.8400, Train Acc: 85.00%, LR: 0.000005, Val Loss: 0.5693, Val Acc: 82.52%\n",
      "Epoch 229/1000 (BNScale-50.0%): Train Loss: 0.8376, Train Acc: 85.08%, LR: 0.000005, Val Loss: 0.5724, Val Acc: 82.30%\n",
      "Epoch 230/1000 (BNScale-50.0%): Train Loss: 0.8395, Train Acc: 85.10%, LR: 0.000005, Val Loss: 0.5740, Val Acc: 82.42%\n",
      "Epoch 231/1000 (BNScale-50.0%): Train Loss: 0.8350, Train Acc: 85.24%, LR: 0.000005, Val Loss: 0.5697, Val Acc: 82.48%\n",
      "Epoch 232/1000 (BNScale-50.0%): Train Loss: 0.8345, Train Acc: 85.31%, LR: 0.000005, Val Loss: 0.5743, Val Acc: 82.54%\n",
      "Early stopping triggered after 232 epochs (Best Val Acc: 82.74%)\n",
      "Loaded best model state (Val Acc: 82.74%)\n",
      "Results: Accuracy=83.09%, MACs=6.02M\n",
      "✅ Model saved to ./base/enhanced_bnscale_sparsity_0.5.pth\n",
      "✅ ONNX model saved to ./base/enhanced_bnscale_sparsity_0.5.onnx\n",
      "\n",
      "Processing BNScale at 70.0% sparsity...\n",
      "✅ Created MobileNetV2 without pretrained weights\n",
      "✅ Enhanced classifier created for 10 classes\n",
      "Initial MACs: 7.30M\n",
      "Applying BNScaleImportance pruning at 70.0% sparsity...\n",
      "Final MACs: 5.56M (Reduction: 23.8%)\n",
      "Fine-tuning pruned model with enhanced settings...\n",
      "Epoch   1/1000 (BNScale-70.0%): Train Loss: 1.6892, Train Acc: 51.00%, LR: 0.000010, Val Loss: 1.1407, Val Acc: 62.74% ✓\n",
      "Epoch   2/1000 (BNScale-70.0%): Train Loss: 1.4608, Train Acc: 58.15%, LR: 0.000010, Val Loss: 1.0461, Val Acc: 66.44% ✓\n",
      "Epoch   3/1000 (BNScale-70.0%): Train Loss: 1.3952, Train Acc: 60.84%, LR: 0.000009, Val Loss: 0.9855, Val Acc: 68.22% ✓\n",
      "Epoch   4/1000 (BNScale-70.0%): Train Loss: 1.3581, Train Acc: 62.38%, LR: 0.000008, Val Loss: 0.9549, Val Acc: 69.12% ✓\n",
      "Epoch   5/1000 (BNScale-70.0%): Train Loss: 1.3269, Train Acc: 63.47%, LR: 0.000007, Val Loss: 0.9228, Val Acc: 69.86% ✓\n",
      "Epoch   6/1000 (BNScale-70.0%): Train Loss: 1.3023, Train Acc: 64.78%, LR: 0.000005, Val Loss: 0.9019, Val Acc: 70.36% ✓\n",
      "Epoch   7/1000 (BNScale-70.0%): Train Loss: 1.2973, Train Acc: 64.95%, LR: 0.000004, Val Loss: 0.8996, Val Acc: 70.70% ✓\n",
      "Epoch   8/1000 (BNScale-70.0%): Train Loss: 1.2911, Train Acc: 65.30%, LR: 0.000002, Val Loss: 0.8893, Val Acc: 71.04% ✓\n",
      "Epoch   9/1000 (BNScale-70.0%): Train Loss: 1.2918, Train Acc: 65.30%, LR: 0.000001, Val Loss: 0.8872, Val Acc: 71.14% ✓\n",
      "Epoch  10/1000 (BNScale-70.0%): Train Loss: 1.2807, Train Acc: 65.44%, LR: 0.000000, Val Loss: 0.8847, Val Acc: 71.26% ✓\n",
      "Epoch  11/1000 (BNScale-70.0%): Train Loss: 1.2809, Train Acc: 65.58%, LR: 0.000010, Val Loss: 0.8626, Val Acc: 72.10% ✓\n",
      "Epoch  12/1000 (BNScale-70.0%): Train Loss: 1.2525, Train Acc: 66.92%, LR: 0.000010, Val Loss: 0.8436, Val Acc: 72.64% ✓\n",
      "Epoch  13/1000 (BNScale-70.0%): Train Loss: 1.2393, Train Acc: 67.38%, LR: 0.000010, Val Loss: 0.8274, Val Acc: 73.28% ✓\n",
      "Epoch  14/1000 (BNScale-70.0%): Train Loss: 1.2285, Train Acc: 67.57%, LR: 0.000009, Val Loss: 0.8118, Val Acc: 74.16% ✓\n",
      "Epoch  15/1000 (BNScale-70.0%): Train Loss: 1.2180, Train Acc: 68.21%, LR: 0.000009, Val Loss: 0.8030, Val Acc: 74.50% ✓\n",
      "Epoch  16/1000 (BNScale-70.0%): Train Loss: 1.2064, Train Acc: 68.44%, LR: 0.000009, Val Loss: 0.7904, Val Acc: 74.56% ✓\n",
      "Epoch  17/1000 (BNScale-70.0%): Train Loss: 1.1953, Train Acc: 68.93%, LR: 0.000008, Val Loss: 0.7839, Val Acc: 74.54%\n",
      "Epoch  18/1000 (BNScale-70.0%): Train Loss: 1.1936, Train Acc: 69.21%, LR: 0.000007, Val Loss: 0.7783, Val Acc: 75.22% ✓\n",
      "Epoch  19/1000 (BNScale-70.0%): Train Loss: 1.1832, Train Acc: 69.65%, LR: 0.000007, Val Loss: 0.7681, Val Acc: 75.14%\n",
      "Epoch  20/1000 (BNScale-70.0%): Train Loss: 1.1789, Train Acc: 69.78%, LR: 0.000006, Val Loss: 0.7641, Val Acc: 75.26% ✓\n",
      "Epoch  21/1000 (BNScale-70.0%): Train Loss: 1.1671, Train Acc: 70.40%, LR: 0.000005, Val Loss: 0.7551, Val Acc: 75.38% ✓\n",
      "Epoch  22/1000 (BNScale-70.0%): Train Loss: 1.1632, Train Acc: 70.48%, LR: 0.000004, Val Loss: 0.7577, Val Acc: 75.46% ✓\n",
      "Epoch  23/1000 (BNScale-70.0%): Train Loss: 1.1568, Train Acc: 70.76%, LR: 0.000004, Val Loss: 0.7498, Val Acc: 75.58% ✓\n",
      "Epoch  24/1000 (BNScale-70.0%): Train Loss: 1.1569, Train Acc: 70.60%, LR: 0.000003, Val Loss: 0.7494, Val Acc: 75.58%\n",
      "Epoch  25/1000 (BNScale-70.0%): Train Loss: 1.1593, Train Acc: 70.74%, LR: 0.000002, Val Loss: 0.7474, Val Acc: 75.88% ✓\n",
      "Epoch  26/1000 (BNScale-70.0%): Train Loss: 1.1558, Train Acc: 70.76%, LR: 0.000002, Val Loss: 0.7455, Val Acc: 76.16% ✓\n",
      "Epoch  27/1000 (BNScale-70.0%): Train Loss: 1.1473, Train Acc: 71.40%, LR: 0.000001, Val Loss: 0.7432, Val Acc: 76.04%\n",
      "Epoch  28/1000 (BNScale-70.0%): Train Loss: 1.1544, Train Acc: 70.96%, LR: 0.000001, Val Loss: 0.7450, Val Acc: 75.82%\n",
      "Epoch  29/1000 (BNScale-70.0%): Train Loss: 1.1505, Train Acc: 71.06%, LR: 0.000000, Val Loss: 0.7423, Val Acc: 75.88%\n",
      "Epoch  30/1000 (BNScale-70.0%): Train Loss: 1.1542, Train Acc: 70.92%, LR: 0.000000, Val Loss: 0.7421, Val Acc: 76.16%\n",
      "Epoch  31/1000 (BNScale-70.0%): Train Loss: 1.1533, Train Acc: 70.94%, LR: 0.000010, Val Loss: 0.7354, Val Acc: 76.16%\n",
      "Epoch  32/1000 (BNScale-70.0%): Train Loss: 1.1470, Train Acc: 71.44%, LR: 0.000010, Val Loss: 0.7308, Val Acc: 76.14%\n",
      "Epoch  33/1000 (BNScale-70.0%): Train Loss: 1.1375, Train Acc: 71.82%, LR: 0.000010, Val Loss: 0.7264, Val Acc: 76.62% ✓\n",
      "Epoch  34/1000 (BNScale-70.0%): Train Loss: 1.1322, Train Acc: 71.98%, LR: 0.000010, Val Loss: 0.7242, Val Acc: 76.18%\n",
      "Epoch  35/1000 (BNScale-70.0%): Train Loss: 1.1282, Train Acc: 72.22%, LR: 0.000010, Val Loss: 0.7200, Val Acc: 76.42%\n",
      "Epoch  36/1000 (BNScale-70.0%): Train Loss: 1.1209, Train Acc: 72.25%, LR: 0.000010, Val Loss: 0.7157, Val Acc: 76.90% ✓\n",
      "Epoch  37/1000 (BNScale-70.0%): Train Loss: 1.1153, Train Acc: 72.79%, LR: 0.000009, Val Loss: 0.7051, Val Acc: 77.04% ✓\n",
      "Epoch  38/1000 (BNScale-70.0%): Train Loss: 1.1072, Train Acc: 73.23%, LR: 0.000009, Val Loss: 0.7024, Val Acc: 76.92%\n",
      "Epoch  39/1000 (BNScale-70.0%): Train Loss: 1.1076, Train Acc: 73.18%, LR: 0.000009, Val Loss: 0.7016, Val Acc: 77.20% ✓\n",
      "Epoch  40/1000 (BNScale-70.0%): Train Loss: 1.1006, Train Acc: 73.21%, LR: 0.000009, Val Loss: 0.6988, Val Acc: 77.04%\n",
      "Epoch  41/1000 (BNScale-70.0%): Train Loss: 1.0986, Train Acc: 73.36%, LR: 0.000009, Val Loss: 0.6874, Val Acc: 77.52% ✓\n",
      "Epoch  42/1000 (BNScale-70.0%): Train Loss: 1.0919, Train Acc: 73.62%, LR: 0.000008, Val Loss: 0.6847, Val Acc: 77.44%\n",
      "Epoch  43/1000 (BNScale-70.0%): Train Loss: 1.0883, Train Acc: 73.83%, LR: 0.000008, Val Loss: 0.6820, Val Acc: 77.98% ✓\n",
      "Epoch  44/1000 (BNScale-70.0%): Train Loss: 1.0857, Train Acc: 73.99%, LR: 0.000008, Val Loss: 0.6786, Val Acc: 77.62%\n",
      "Epoch  45/1000 (BNScale-70.0%): Train Loss: 1.0814, Train Acc: 74.25%, LR: 0.000007, Val Loss: 0.6779, Val Acc: 77.70%\n",
      "Epoch  46/1000 (BNScale-70.0%): Train Loss: 1.0780, Train Acc: 74.25%, LR: 0.000007, Val Loss: 0.6755, Val Acc: 77.90%\n",
      "Epoch  47/1000 (BNScale-70.0%): Train Loss: 1.0752, Train Acc: 74.36%, LR: 0.000007, Val Loss: 0.6748, Val Acc: 77.90%\n",
      "Epoch  48/1000 (BNScale-70.0%): Train Loss: 1.0723, Train Acc: 74.51%, LR: 0.000006, Val Loss: 0.6705, Val Acc: 77.84%\n",
      "Epoch  49/1000 (BNScale-70.0%): Train Loss: 1.0663, Train Acc: 75.00%, LR: 0.000006, Val Loss: 0.6700, Val Acc: 78.32% ✓\n",
      "Epoch  50/1000 (BNScale-70.0%): Train Loss: 1.0714, Train Acc: 74.72%, LR: 0.000005, Val Loss: 0.6652, Val Acc: 78.40% ✓\n",
      "Epoch  51/1000 (BNScale-70.0%): Train Loss: 1.0674, Train Acc: 74.86%, LR: 0.000005, Val Loss: 0.6699, Val Acc: 78.16%\n",
      "Epoch  52/1000 (BNScale-70.0%): Train Loss: 1.0636, Train Acc: 74.87%, LR: 0.000005, Val Loss: 0.6659, Val Acc: 78.32%\n",
      "Epoch  53/1000 (BNScale-70.0%): Train Loss: 1.0651, Train Acc: 74.84%, LR: 0.000004, Val Loss: 0.6667, Val Acc: 78.42% ✓\n",
      "Epoch  54/1000 (BNScale-70.0%): Train Loss: 1.0613, Train Acc: 74.96%, LR: 0.000004, Val Loss: 0.6605, Val Acc: 78.24%\n",
      "Epoch  55/1000 (BNScale-70.0%): Train Loss: 1.0593, Train Acc: 75.06%, LR: 0.000004, Val Loss: 0.6614, Val Acc: 78.60% ✓\n",
      "Epoch  56/1000 (BNScale-70.0%): Train Loss: 1.0565, Train Acc: 75.33%, LR: 0.000003, Val Loss: 0.6610, Val Acc: 78.66% ✓\n",
      "Epoch  57/1000 (BNScale-70.0%): Train Loss: 1.0584, Train Acc: 75.15%, LR: 0.000003, Val Loss: 0.6609, Val Acc: 78.68% ✓\n",
      "Epoch  58/1000 (BNScale-70.0%): Train Loss: 1.0579, Train Acc: 75.17%, LR: 0.000002, Val Loss: 0.6596, Val Acc: 78.26%\n",
      "Epoch  59/1000 (BNScale-70.0%): Train Loss: 1.0541, Train Acc: 75.41%, LR: 0.000002, Val Loss: 0.6606, Val Acc: 78.58%\n",
      "Epoch  60/1000 (BNScale-70.0%): Train Loss: 1.0478, Train Acc: 75.60%, LR: 0.000002, Val Loss: 0.6565, Val Acc: 78.92% ✓\n",
      "Epoch  61/1000 (BNScale-70.0%): Train Loss: 1.0511, Train Acc: 75.45%, LR: 0.000002, Val Loss: 0.6594, Val Acc: 78.54%\n",
      "Epoch  62/1000 (BNScale-70.0%): Train Loss: 1.0529, Train Acc: 75.63%, LR: 0.000001, Val Loss: 0.6578, Val Acc: 78.78%\n",
      "Epoch  63/1000 (BNScale-70.0%): Train Loss: 1.0513, Train Acc: 75.40%, LR: 0.000001, Val Loss: 0.6631, Val Acc: 78.50%\n",
      "Epoch  64/1000 (BNScale-70.0%): Train Loss: 1.0524, Train Acc: 75.33%, LR: 0.000001, Val Loss: 0.6596, Val Acc: 78.54%\n",
      "Epoch  65/1000 (BNScale-70.0%): Train Loss: 1.0529, Train Acc: 75.48%, LR: 0.000001, Val Loss: 0.6583, Val Acc: 78.92%\n",
      "Epoch  66/1000 (BNScale-70.0%): Train Loss: 1.0541, Train Acc: 75.34%, LR: 0.000000, Val Loss: 0.6575, Val Acc: 78.46%\n",
      "Epoch  67/1000 (BNScale-70.0%): Train Loss: 1.0499, Train Acc: 75.60%, LR: 0.000000, Val Loss: 0.6562, Val Acc: 78.88%\n",
      "Epoch  68/1000 (BNScale-70.0%): Train Loss: 1.0508, Train Acc: 75.49%, LR: 0.000000, Val Loss: 0.6583, Val Acc: 78.84%\n",
      "Epoch  69/1000 (BNScale-70.0%): Train Loss: 1.0488, Train Acc: 75.50%, LR: 0.000000, Val Loss: 0.6557, Val Acc: 78.84%\n",
      "Epoch  70/1000 (BNScale-70.0%): Train Loss: 1.0485, Train Acc: 75.50%, LR: 0.000000, Val Loss: 0.6566, Val Acc: 78.76%\n",
      "Epoch  71/1000 (BNScale-70.0%): Train Loss: 1.0525, Train Acc: 75.44%, LR: 0.000010, Val Loss: 0.6545, Val Acc: 79.00% ✓\n",
      "Epoch  72/1000 (BNScale-70.0%): Train Loss: 1.0511, Train Acc: 75.34%, LR: 0.000010, Val Loss: 0.6540, Val Acc: 79.16% ✓\n",
      "Epoch  73/1000 (BNScale-70.0%): Train Loss: 1.0509, Train Acc: 75.63%, LR: 0.000010, Val Loss: 0.6486, Val Acc: 78.96%\n",
      "Epoch  74/1000 (BNScale-70.0%): Train Loss: 1.0398, Train Acc: 75.84%, LR: 0.000010, Val Loss: 0.6529, Val Acc: 79.08%\n",
      "Epoch  75/1000 (BNScale-70.0%): Train Loss: 1.0392, Train Acc: 76.02%, LR: 0.000010, Val Loss: 0.6423, Val Acc: 79.26% ✓\n",
      "Epoch  76/1000 (BNScale-70.0%): Train Loss: 1.0392, Train Acc: 76.16%, LR: 0.000010, Val Loss: 0.6462, Val Acc: 79.06%\n",
      "Epoch  77/1000 (BNScale-70.0%): Train Loss: 1.0354, Train Acc: 76.26%, LR: 0.000010, Val Loss: 0.6362, Val Acc: 79.50% ✓\n",
      "Epoch  78/1000 (BNScale-70.0%): Train Loss: 1.0325, Train Acc: 76.33%, LR: 0.000010, Val Loss: 0.6377, Val Acc: 79.14%\n",
      "Epoch  79/1000 (BNScale-70.0%): Train Loss: 1.0307, Train Acc: 76.40%, LR: 0.000010, Val Loss: 0.6354, Val Acc: 79.58% ✓\n",
      "Epoch  80/1000 (BNScale-70.0%): Train Loss: 1.0250, Train Acc: 76.69%, LR: 0.000010, Val Loss: 0.6335, Val Acc: 79.70% ✓\n",
      "Epoch  81/1000 (BNScale-70.0%): Train Loss: 1.0232, Train Acc: 76.86%, LR: 0.000010, Val Loss: 0.6314, Val Acc: 79.56%\n",
      "Epoch  82/1000 (BNScale-70.0%): Train Loss: 1.0253, Train Acc: 76.51%, LR: 0.000010, Val Loss: 0.6338, Val Acc: 79.62%\n",
      "Epoch  83/1000 (BNScale-70.0%): Train Loss: 1.0222, Train Acc: 76.70%, LR: 0.000009, Val Loss: 0.6343, Val Acc: 79.36%\n",
      "Epoch  84/1000 (BNScale-70.0%): Train Loss: 1.0150, Train Acc: 77.09%, LR: 0.000009, Val Loss: 0.6302, Val Acc: 79.72% ✓\n",
      "Epoch  85/1000 (BNScale-70.0%): Train Loss: 1.0169, Train Acc: 77.10%, LR: 0.000009, Val Loss: 0.6294, Val Acc: 79.80% ✓\n",
      "Epoch  86/1000 (BNScale-70.0%): Train Loss: 1.0157, Train Acc: 77.12%, LR: 0.000009, Val Loss: 0.6309, Val Acc: 79.52%\n",
      "Epoch  87/1000 (BNScale-70.0%): Train Loss: 1.0112, Train Acc: 77.29%, LR: 0.000009, Val Loss: 0.6257, Val Acc: 79.88% ✓\n",
      "Epoch  88/1000 (BNScale-70.0%): Train Loss: 1.0090, Train Acc: 77.33%, LR: 0.000009, Val Loss: 0.6251, Val Acc: 79.96% ✓\n",
      "Epoch  89/1000 (BNScale-70.0%): Train Loss: 1.0122, Train Acc: 77.35%, LR: 0.000009, Val Loss: 0.6214, Val Acc: 80.34% ✓\n",
      "Epoch  90/1000 (BNScale-70.0%): Train Loss: 1.0060, Train Acc: 77.48%, LR: 0.000009, Val Loss: 0.6208, Val Acc: 80.14%\n",
      "Epoch  91/1000 (BNScale-70.0%): Train Loss: 1.0045, Train Acc: 77.60%, LR: 0.000009, Val Loss: 0.6225, Val Acc: 80.06%\n",
      "Epoch  92/1000 (BNScale-70.0%): Train Loss: 1.0044, Train Acc: 77.64%, LR: 0.000008, Val Loss: 0.6165, Val Acc: 80.12%\n",
      "Epoch  93/1000 (BNScale-70.0%): Train Loss: 0.9990, Train Acc: 77.71%, LR: 0.000008, Val Loss: 0.6197, Val Acc: 80.12%\n",
      "Epoch  94/1000 (BNScale-70.0%): Train Loss: 0.9997, Train Acc: 77.57%, LR: 0.000008, Val Loss: 0.6172, Val Acc: 80.00%\n",
      "Epoch  95/1000 (BNScale-70.0%): Train Loss: 0.9969, Train Acc: 78.06%, LR: 0.000008, Val Loss: 0.6137, Val Acc: 80.30%\n",
      "Epoch  96/1000 (BNScale-70.0%): Train Loss: 0.9934, Train Acc: 77.89%, LR: 0.000008, Val Loss: 0.6142, Val Acc: 80.34%\n",
      "Epoch  97/1000 (BNScale-70.0%): Train Loss: 0.9965, Train Acc: 77.93%, LR: 0.000008, Val Loss: 0.6148, Val Acc: 80.36% ✓\n",
      "Epoch  98/1000 (BNScale-70.0%): Train Loss: 0.9897, Train Acc: 78.11%, LR: 0.000007, Val Loss: 0.6167, Val Acc: 80.32%\n",
      "Epoch  99/1000 (BNScale-70.0%): Train Loss: 0.9905, Train Acc: 78.39%, LR: 0.000007, Val Loss: 0.6127, Val Acc: 80.48% ✓\n",
      "Epoch 100/1000 (BNScale-70.0%): Train Loss: 0.9894, Train Acc: 78.23%, LR: 0.000007, Val Loss: 0.6111, Val Acc: 80.42%\n",
      "Epoch 101/1000 (BNScale-70.0%): Train Loss: 0.9925, Train Acc: 78.10%, LR: 0.000007, Val Loss: 0.6115, Val Acc: 80.30%\n",
      "Epoch 102/1000 (BNScale-70.0%): Train Loss: 0.9873, Train Acc: 78.22%, LR: 0.000007, Val Loss: 0.6132, Val Acc: 80.28%\n",
      "Epoch 103/1000 (BNScale-70.0%): Train Loss: 0.9915, Train Acc: 78.26%, LR: 0.000007, Val Loss: 0.6095, Val Acc: 80.60% ✓\n",
      "Epoch 104/1000 (BNScale-70.0%): Train Loss: 0.9849, Train Acc: 78.51%, LR: 0.000006, Val Loss: 0.6118, Val Acc: 80.50%\n",
      "Epoch 105/1000 (BNScale-70.0%): Train Loss: 0.9867, Train Acc: 78.41%, LR: 0.000006, Val Loss: 0.6078, Val Acc: 80.64% ✓\n",
      "Epoch 106/1000 (BNScale-70.0%): Train Loss: 0.9812, Train Acc: 78.61%, LR: 0.000006, Val Loss: 0.6054, Val Acc: 80.76% ✓\n",
      "Epoch 107/1000 (BNScale-70.0%): Train Loss: 0.9789, Train Acc: 78.81%, LR: 0.000006, Val Loss: 0.6059, Val Acc: 80.70%\n",
      "Epoch 108/1000 (BNScale-70.0%): Train Loss: 0.9833, Train Acc: 78.51%, LR: 0.000006, Val Loss: 0.6094, Val Acc: 80.50%\n",
      "Epoch 109/1000 (BNScale-70.0%): Train Loss: 0.9795, Train Acc: 78.79%, LR: 0.000005, Val Loss: 0.6102, Val Acc: 80.36%\n",
      "Epoch 110/1000 (BNScale-70.0%): Train Loss: 0.9747, Train Acc: 79.02%, LR: 0.000005, Val Loss: 0.6035, Val Acc: 80.54%\n",
      "Epoch 111/1000 (BNScale-70.0%): Train Loss: 0.9715, Train Acc: 79.16%, LR: 0.000005, Val Loss: 0.6061, Val Acc: 80.54%\n",
      "Epoch 112/1000 (BNScale-70.0%): Train Loss: 0.9766, Train Acc: 78.86%, LR: 0.000005, Val Loss: 0.6024, Val Acc: 80.74%\n",
      "Epoch 113/1000 (BNScale-70.0%): Train Loss: 0.9690, Train Acc: 79.25%, LR: 0.000005, Val Loss: 0.6072, Val Acc: 80.50%\n",
      "Epoch 114/1000 (BNScale-70.0%): Train Loss: 0.9761, Train Acc: 79.01%, LR: 0.000004, Val Loss: 0.6075, Val Acc: 80.50%\n",
      "Epoch 115/1000 (BNScale-70.0%): Train Loss: 0.9704, Train Acc: 79.06%, LR: 0.000004, Val Loss: 0.6053, Val Acc: 80.74%\n",
      "Epoch 116/1000 (BNScale-70.0%): Train Loss: 0.9772, Train Acc: 78.83%, LR: 0.000004, Val Loss: 0.6047, Val Acc: 80.94% ✓\n",
      "Epoch 117/1000 (BNScale-70.0%): Train Loss: 0.9713, Train Acc: 78.94%, LR: 0.000004, Val Loss: 0.6025, Val Acc: 80.70%\n",
      "Epoch 118/1000 (BNScale-70.0%): Train Loss: 0.9694, Train Acc: 79.09%, LR: 0.000004, Val Loss: 0.6000, Val Acc: 80.80%\n",
      "Epoch 119/1000 (BNScale-70.0%): Train Loss: 0.9730, Train Acc: 79.07%, LR: 0.000004, Val Loss: 0.6016, Val Acc: 80.90%\n",
      "Epoch 120/1000 (BNScale-70.0%): Train Loss: 0.9637, Train Acc: 79.30%, LR: 0.000003, Val Loss: 0.6057, Val Acc: 80.66%\n",
      "Epoch 121/1000 (BNScale-70.0%): Train Loss: 0.9699, Train Acc: 79.22%, LR: 0.000003, Val Loss: 0.6032, Val Acc: 80.76%\n",
      "Epoch 122/1000 (BNScale-70.0%): Train Loss: 0.9702, Train Acc: 79.01%, LR: 0.000003, Val Loss: 0.6016, Val Acc: 80.94%\n",
      "Epoch 123/1000 (BNScale-70.0%): Train Loss: 0.9711, Train Acc: 79.12%, LR: 0.000003, Val Loss: 0.5982, Val Acc: 80.94%\n",
      "Epoch 124/1000 (BNScale-70.0%): Train Loss: 0.9643, Train Acc: 79.38%, LR: 0.000003, Val Loss: 0.5989, Val Acc: 81.24% ✓\n",
      "Epoch 125/1000 (BNScale-70.0%): Train Loss: 0.9714, Train Acc: 79.23%, LR: 0.000002, Val Loss: 0.6014, Val Acc: 81.14%\n",
      "Epoch 126/1000 (BNScale-70.0%): Train Loss: 0.9707, Train Acc: 79.15%, LR: 0.000002, Val Loss: 0.6014, Val Acc: 80.66%\n",
      "Epoch 127/1000 (BNScale-70.0%): Train Loss: 0.9665, Train Acc: 79.48%, LR: 0.000002, Val Loss: 0.5976, Val Acc: 81.06%\n",
      "Epoch 128/1000 (BNScale-70.0%): Train Loss: 0.9647, Train Acc: 79.32%, LR: 0.000002, Val Loss: 0.5991, Val Acc: 81.04%\n",
      "Epoch 129/1000 (BNScale-70.0%): Train Loss: 0.9676, Train Acc: 79.11%, LR: 0.000002, Val Loss: 0.5945, Val Acc: 81.16%\n",
      "Epoch 130/1000 (BNScale-70.0%): Train Loss: 0.9668, Train Acc: 79.22%, LR: 0.000002, Val Loss: 0.5952, Val Acc: 80.96%\n",
      "Epoch 131/1000 (BNScale-70.0%): Train Loss: 0.9682, Train Acc: 79.34%, LR: 0.000002, Val Loss: 0.5964, Val Acc: 80.92%\n",
      "Epoch 132/1000 (BNScale-70.0%): Train Loss: 0.9658, Train Acc: 79.32%, LR: 0.000001, Val Loss: 0.6020, Val Acc: 80.84%\n",
      "Epoch 133/1000 (BNScale-70.0%): Train Loss: 0.9669, Train Acc: 79.36%, LR: 0.000001, Val Loss: 0.6014, Val Acc: 80.94%\n",
      "Epoch 134/1000 (BNScale-70.0%): Train Loss: 0.9624, Train Acc: 79.39%, LR: 0.000001, Val Loss: 0.5975, Val Acc: 80.92%\n",
      "Epoch 135/1000 (BNScale-70.0%): Train Loss: 0.9589, Train Acc: 79.73%, LR: 0.000001, Val Loss: 0.5984, Val Acc: 80.94%\n",
      "Epoch 136/1000 (BNScale-70.0%): Train Loss: 0.9657, Train Acc: 79.32%, LR: 0.000001, Val Loss: 0.5978, Val Acc: 81.00%\n",
      "Epoch 137/1000 (BNScale-70.0%): Train Loss: 0.9619, Train Acc: 79.62%, LR: 0.000001, Val Loss: 0.5994, Val Acc: 80.82%\n",
      "Epoch 138/1000 (BNScale-70.0%): Train Loss: 0.9646, Train Acc: 79.29%, LR: 0.000001, Val Loss: 0.5955, Val Acc: 80.98%\n",
      "Epoch 139/1000 (BNScale-70.0%): Train Loss: 0.9667, Train Acc: 79.25%, LR: 0.000001, Val Loss: 0.5999, Val Acc: 80.98%\n",
      "Epoch 140/1000 (BNScale-70.0%): Train Loss: 0.9605, Train Acc: 79.76%, LR: 0.000001, Val Loss: 0.5965, Val Acc: 80.98%\n",
      "Epoch 141/1000 (BNScale-70.0%): Train Loss: 0.9639, Train Acc: 79.51%, LR: 0.000000, Val Loss: 0.6005, Val Acc: 80.72%\n",
      "Epoch 142/1000 (BNScale-70.0%): Train Loss: 0.9631, Train Acc: 79.34%, LR: 0.000000, Val Loss: 0.5986, Val Acc: 80.96%\n",
      "Epoch 143/1000 (BNScale-70.0%): Train Loss: 0.9627, Train Acc: 79.41%, LR: 0.000000, Val Loss: 0.5987, Val Acc: 80.90%\n",
      "Epoch 144/1000 (BNScale-70.0%): Train Loss: 0.9644, Train Acc: 79.39%, LR: 0.000000, Val Loss: 0.6010, Val Acc: 80.80%\n",
      "Early stopping triggered after 144 epochs (Best Val Acc: 81.24%)\n",
      "Loaded best model state (Val Acc: 81.24%)\n",
      "Results: Accuracy=80.82%, MACs=5.56M\n",
      "✅ Model saved to ./base/enhanced_bnscale_sparsity_0.7.pth\n",
      "✅ ONNX model saved to ./base/enhanced_bnscale_sparsity_0.7.onnx\n",
      "\n",
      "--- Strategy: MagnitudeL2 ---\n",
      "\n",
      "Processing MagnitudeL2 at 20.0% sparsity...\n",
      "✅ Created MobileNetV2 without pretrained weights\n",
      "✅ Enhanced classifier created for 10 classes\n",
      "Initial MACs: 7.30M\n",
      "Applying MagnitudeImportance pruning at 20.0% sparsity...\n",
      "Final MACs: 6.75M (Reduction: 7.5%)\n",
      "Fine-tuning pruned model with enhanced settings...\n",
      "Epoch   1/1000 (MagnitudeL2-20.0%): Train Loss: 0.9280, Train Acc: 81.35%, LR: 0.000010, Val Loss: 0.6155, Val Acc: 81.78% ✓\n",
      "Epoch   2/1000 (MagnitudeL2-20.0%): Train Loss: 0.9074, Train Acc: 82.14%, LR: 0.000010, Val Loss: 0.6013, Val Acc: 81.84% ✓\n",
      "Epoch   3/1000 (MagnitudeL2-20.0%): Train Loss: 0.8947, Train Acc: 82.69%, LR: 0.000009, Val Loss: 0.5935, Val Acc: 81.78%\n",
      "Epoch   4/1000 (MagnitudeL2-20.0%): Train Loss: 0.8858, Train Acc: 83.10%, LR: 0.000008, Val Loss: 0.5874, Val Acc: 82.32% ✓\n",
      "Epoch   5/1000 (MagnitudeL2-20.0%): Train Loss: 0.8870, Train Acc: 82.85%, LR: 0.000007, Val Loss: 0.5916, Val Acc: 81.78%\n",
      "Epoch   6/1000 (MagnitudeL2-20.0%): Train Loss: 0.8748, Train Acc: 83.46%, LR: 0.000005, Val Loss: 0.5842, Val Acc: 82.26%\n",
      "Epoch   7/1000 (MagnitudeL2-20.0%): Train Loss: 0.8751, Train Acc: 83.49%, LR: 0.000004, Val Loss: 0.5814, Val Acc: 82.48% ✓\n",
      "Epoch   8/1000 (MagnitudeL2-20.0%): Train Loss: 0.8728, Train Acc: 83.60%, LR: 0.000002, Val Loss: 0.5808, Val Acc: 82.54% ✓\n",
      "Epoch   9/1000 (MagnitudeL2-20.0%): Train Loss: 0.8689, Train Acc: 83.98%, LR: 0.000001, Val Loss: 0.5792, Val Acc: 82.40%\n",
      "Epoch  10/1000 (MagnitudeL2-20.0%): Train Loss: 0.8747, Train Acc: 83.39%, LR: 0.000000, Val Loss: 0.5810, Val Acc: 82.48%\n",
      "Epoch  11/1000 (MagnitudeL2-20.0%): Train Loss: 0.8708, Train Acc: 83.72%, LR: 0.000010, Val Loss: 0.5774, Val Acc: 82.78% ✓\n",
      "Epoch  12/1000 (MagnitudeL2-20.0%): Train Loss: 0.8713, Train Acc: 83.59%, LR: 0.000010, Val Loss: 0.5746, Val Acc: 82.78%\n",
      "Epoch  13/1000 (MagnitudeL2-20.0%): Train Loss: 0.8645, Train Acc: 84.02%, LR: 0.000010, Val Loss: 0.5749, Val Acc: 82.60%\n",
      "Epoch  14/1000 (MagnitudeL2-20.0%): Train Loss: 0.8645, Train Acc: 83.77%, LR: 0.000009, Val Loss: 0.5719, Val Acc: 82.58%\n",
      "Epoch  15/1000 (MagnitudeL2-20.0%): Train Loss: 0.8634, Train Acc: 84.00%, LR: 0.000009, Val Loss: 0.5741, Val Acc: 82.96% ✓\n",
      "Epoch  16/1000 (MagnitudeL2-20.0%): Train Loss: 0.8569, Train Acc: 84.41%, LR: 0.000009, Val Loss: 0.5710, Val Acc: 82.90%\n",
      "Epoch  17/1000 (MagnitudeL2-20.0%): Train Loss: 0.8559, Train Acc: 84.31%, LR: 0.000008, Val Loss: 0.5714, Val Acc: 82.66%\n",
      "Epoch  18/1000 (MagnitudeL2-20.0%): Train Loss: 0.8524, Train Acc: 84.61%, LR: 0.000007, Val Loss: 0.5661, Val Acc: 83.04% ✓\n",
      "Epoch  19/1000 (MagnitudeL2-20.0%): Train Loss: 0.8488, Train Acc: 84.58%, LR: 0.000007, Val Loss: 0.5706, Val Acc: 82.80%\n",
      "Epoch  20/1000 (MagnitudeL2-20.0%): Train Loss: 0.8509, Train Acc: 84.52%, LR: 0.000006, Val Loss: 0.5713, Val Acc: 82.82%\n",
      "Epoch  21/1000 (MagnitudeL2-20.0%): Train Loss: 0.8508, Train Acc: 84.54%, LR: 0.000005, Val Loss: 0.5721, Val Acc: 83.36% ✓\n",
      "Epoch  22/1000 (MagnitudeL2-20.0%): Train Loss: 0.8468, Train Acc: 84.89%, LR: 0.000004, Val Loss: 0.5654, Val Acc: 83.28%\n",
      "Epoch  23/1000 (MagnitudeL2-20.0%): Train Loss: 0.8484, Train Acc: 84.70%, LR: 0.000004, Val Loss: 0.5658, Val Acc: 83.18%\n",
      "Epoch  24/1000 (MagnitudeL2-20.0%): Train Loss: 0.8502, Train Acc: 84.68%, LR: 0.000003, Val Loss: 0.5687, Val Acc: 82.82%\n",
      "Epoch  25/1000 (MagnitudeL2-20.0%): Train Loss: 0.8482, Train Acc: 84.74%, LR: 0.000002, Val Loss: 0.5617, Val Acc: 83.08%\n",
      "Epoch  26/1000 (MagnitudeL2-20.0%): Train Loss: 0.8427, Train Acc: 84.91%, LR: 0.000002, Val Loss: 0.5665, Val Acc: 83.00%\n",
      "Epoch  27/1000 (MagnitudeL2-20.0%): Train Loss: 0.8391, Train Acc: 85.26%, LR: 0.000001, Val Loss: 0.5633, Val Acc: 83.20%\n",
      "Epoch  28/1000 (MagnitudeL2-20.0%): Train Loss: 0.8477, Train Acc: 84.76%, LR: 0.000001, Val Loss: 0.5678, Val Acc: 83.36%\n",
      "Epoch  29/1000 (MagnitudeL2-20.0%): Train Loss: 0.8459, Train Acc: 84.78%, LR: 0.000000, Val Loss: 0.5638, Val Acc: 83.30%\n",
      "Epoch  30/1000 (MagnitudeL2-20.0%): Train Loss: 0.8430, Train Acc: 84.90%, LR: 0.000000, Val Loss: 0.5650, Val Acc: 83.28%\n",
      "Epoch  31/1000 (MagnitudeL2-20.0%): Train Loss: 0.8494, Train Acc: 84.47%, LR: 0.000010, Val Loss: 0.5626, Val Acc: 83.18%\n",
      "Epoch  32/1000 (MagnitudeL2-20.0%): Train Loss: 0.8432, Train Acc: 85.08%, LR: 0.000010, Val Loss: 0.5603, Val Acc: 83.60% ✓\n",
      "Epoch  33/1000 (MagnitudeL2-20.0%): Train Loss: 0.8418, Train Acc: 84.98%, LR: 0.000010, Val Loss: 0.5600, Val Acc: 83.10%\n",
      "Epoch  34/1000 (MagnitudeL2-20.0%): Train Loss: 0.8407, Train Acc: 85.04%, LR: 0.000010, Val Loss: 0.5618, Val Acc: 83.70% ✓\n",
      "Epoch  35/1000 (MagnitudeL2-20.0%): Train Loss: 0.8373, Train Acc: 85.39%, LR: 0.000010, Val Loss: 0.5648, Val Acc: 83.06%\n",
      "Epoch  36/1000 (MagnitudeL2-20.0%): Train Loss: 0.8392, Train Acc: 85.00%, LR: 0.000010, Val Loss: 0.5644, Val Acc: 83.40%\n",
      "Epoch  37/1000 (MagnitudeL2-20.0%): Train Loss: 0.8402, Train Acc: 85.12%, LR: 0.000009, Val Loss: 0.5650, Val Acc: 83.36%\n",
      "Epoch  38/1000 (MagnitudeL2-20.0%): Train Loss: 0.8330, Train Acc: 85.23%, LR: 0.000009, Val Loss: 0.5579, Val Acc: 83.24%\n",
      "Epoch  39/1000 (MagnitudeL2-20.0%): Train Loss: 0.8400, Train Acc: 85.04%, LR: 0.000009, Val Loss: 0.5625, Val Acc: 83.08%\n",
      "Epoch  40/1000 (MagnitudeL2-20.0%): Train Loss: 0.8331, Train Acc: 85.41%, LR: 0.000009, Val Loss: 0.5557, Val Acc: 83.66%\n",
      "Epoch  41/1000 (MagnitudeL2-20.0%): Train Loss: 0.8365, Train Acc: 85.22%, LR: 0.000009, Val Loss: 0.5523, Val Acc: 83.28%\n",
      "Epoch  42/1000 (MagnitudeL2-20.0%): Train Loss: 0.8280, Train Acc: 85.46%, LR: 0.000008, Val Loss: 0.5596, Val Acc: 83.60%\n",
      "Epoch  43/1000 (MagnitudeL2-20.0%): Train Loss: 0.8280, Train Acc: 85.63%, LR: 0.000008, Val Loss: 0.5564, Val Acc: 83.48%\n",
      "Epoch  44/1000 (MagnitudeL2-20.0%): Train Loss: 0.8303, Train Acc: 85.35%, LR: 0.000008, Val Loss: 0.5633, Val Acc: 83.38%\n",
      "Epoch  45/1000 (MagnitudeL2-20.0%): Train Loss: 0.8234, Train Acc: 85.74%, LR: 0.000007, Val Loss: 0.5608, Val Acc: 83.54%\n",
      "Epoch  46/1000 (MagnitudeL2-20.0%): Train Loss: 0.8248, Train Acc: 85.85%, LR: 0.000007, Val Loss: 0.5616, Val Acc: 83.44%\n",
      "Epoch  47/1000 (MagnitudeL2-20.0%): Train Loss: 0.8244, Train Acc: 85.56%, LR: 0.000007, Val Loss: 0.5571, Val Acc: 83.36%\n",
      "Epoch  48/1000 (MagnitudeL2-20.0%): Train Loss: 0.8225, Train Acc: 85.98%, LR: 0.000006, Val Loss: 0.5572, Val Acc: 83.48%\n",
      "Epoch  49/1000 (MagnitudeL2-20.0%): Train Loss: 0.8253, Train Acc: 85.80%, LR: 0.000006, Val Loss: 0.5576, Val Acc: 83.42%\n",
      "Epoch  50/1000 (MagnitudeL2-20.0%): Train Loss: 0.8231, Train Acc: 85.94%, LR: 0.000005, Val Loss: 0.5579, Val Acc: 83.38%\n",
      "Epoch  51/1000 (MagnitudeL2-20.0%): Train Loss: 0.8234, Train Acc: 86.04%, LR: 0.000005, Val Loss: 0.5579, Val Acc: 83.44%\n",
      "Epoch  52/1000 (MagnitudeL2-20.0%): Train Loss: 0.8182, Train Acc: 85.98%, LR: 0.000005, Val Loss: 0.5544, Val Acc: 83.54%\n",
      "Epoch  53/1000 (MagnitudeL2-20.0%): Train Loss: 0.8208, Train Acc: 85.95%, LR: 0.000004, Val Loss: 0.5585, Val Acc: 83.46%\n",
      "Epoch  54/1000 (MagnitudeL2-20.0%): Train Loss: 0.8170, Train Acc: 86.21%, LR: 0.000004, Val Loss: 0.5540, Val Acc: 83.60%\n",
      "Early stopping triggered after 54 epochs (Best Val Acc: 83.70%)\n",
      "Loaded best model state (Val Acc: 83.70%)\n",
      "Results: Accuracy=83.40%, MACs=6.75M\n",
      "✅ Model saved to ./base/enhanced_magnitudel2_sparsity_0.2.pth\n",
      "✅ ONNX model saved to ./base/enhanced_magnitudel2_sparsity_0.2.onnx\n",
      "\n",
      "Processing MagnitudeL2 at 50.0% sparsity...\n",
      "✅ Created MobileNetV2 without pretrained weights\n",
      "✅ Enhanced classifier created for 10 classes\n",
      "Initial MACs: 7.30M\n",
      "Applying MagnitudeImportance pruning at 50.0% sparsity...\n",
      "Final MACs: 6.02M (Reduction: 17.6%)\n",
      "Fine-tuning pruned model with enhanced settings...\n",
      "Epoch   1/1000 (MagnitudeL2-50.0%): Train Loss: 1.1835, Train Acc: 70.35%, LR: 0.000010, Val Loss: 0.7533, Val Acc: 76.06% ✓\n",
      "Epoch   2/1000 (MagnitudeL2-50.0%): Train Loss: 1.1055, Train Acc: 73.31%, LR: 0.000010, Val Loss: 0.7182, Val Acc: 77.56% ✓\n",
      "Epoch   3/1000 (MagnitudeL2-50.0%): Train Loss: 1.0676, Train Acc: 74.83%, LR: 0.000009, Val Loss: 0.6888, Val Acc: 78.40% ✓\n",
      "Epoch   4/1000 (MagnitudeL2-50.0%): Train Loss: 1.0498, Train Acc: 75.71%, LR: 0.000008, Val Loss: 0.6721, Val Acc: 79.12% ✓\n",
      "Epoch   5/1000 (MagnitudeL2-50.0%): Train Loss: 1.0332, Train Acc: 76.37%, LR: 0.000007, Val Loss: 0.6691, Val Acc: 79.30% ✓\n",
      "Epoch   6/1000 (MagnitudeL2-50.0%): Train Loss: 1.0266, Train Acc: 76.62%, LR: 0.000005, Val Loss: 0.6585, Val Acc: 79.38% ✓\n",
      "Epoch   7/1000 (MagnitudeL2-50.0%): Train Loss: 1.0254, Train Acc: 76.75%, LR: 0.000004, Val Loss: 0.6538, Val Acc: 79.78% ✓\n",
      "Epoch   8/1000 (MagnitudeL2-50.0%): Train Loss: 1.0205, Train Acc: 76.96%, LR: 0.000002, Val Loss: 0.6536, Val Acc: 79.78%\n",
      "Epoch   9/1000 (MagnitudeL2-50.0%): Train Loss: 1.0165, Train Acc: 77.27%, LR: 0.000001, Val Loss: 0.6517, Val Acc: 79.56%\n",
      "Epoch  10/1000 (MagnitudeL2-50.0%): Train Loss: 1.0130, Train Acc: 77.21%, LR: 0.000000, Val Loss: 0.6512, Val Acc: 79.54%\n",
      "Epoch  11/1000 (MagnitudeL2-50.0%): Train Loss: 1.0180, Train Acc: 77.11%, LR: 0.000010, Val Loss: 0.6417, Val Acc: 79.98% ✓\n",
      "Epoch  12/1000 (MagnitudeL2-50.0%): Train Loss: 1.0105, Train Acc: 77.39%, LR: 0.000010, Val Loss: 0.6409, Val Acc: 80.20% ✓\n",
      "Epoch  13/1000 (MagnitudeL2-50.0%): Train Loss: 1.0002, Train Acc: 77.90%, LR: 0.000010, Val Loss: 0.6306, Val Acc: 80.68% ✓\n",
      "Epoch  14/1000 (MagnitudeL2-50.0%): Train Loss: 0.9887, Train Acc: 78.25%, LR: 0.000009, Val Loss: 0.6292, Val Acc: 80.52%\n",
      "Epoch  15/1000 (MagnitudeL2-50.0%): Train Loss: 0.9900, Train Acc: 78.33%, LR: 0.000009, Val Loss: 0.6205, Val Acc: 80.96% ✓\n",
      "Epoch  16/1000 (MagnitudeL2-50.0%): Train Loss: 0.9795, Train Acc: 78.74%, LR: 0.000009, Val Loss: 0.6208, Val Acc: 80.90%\n",
      "Epoch  17/1000 (MagnitudeL2-50.0%): Train Loss: 0.9745, Train Acc: 78.98%, LR: 0.000008, Val Loss: 0.6256, Val Acc: 80.60%\n",
      "Epoch  18/1000 (MagnitudeL2-50.0%): Train Loss: 0.9739, Train Acc: 78.88%, LR: 0.000007, Val Loss: 0.6116, Val Acc: 81.10% ✓\n",
      "Epoch  19/1000 (MagnitudeL2-50.0%): Train Loss: 0.9664, Train Acc: 79.23%, LR: 0.000007, Val Loss: 0.6093, Val Acc: 81.40% ✓\n",
      "Epoch  20/1000 (MagnitudeL2-50.0%): Train Loss: 0.9673, Train Acc: 79.38%, LR: 0.000006, Val Loss: 0.6066, Val Acc: 81.80% ✓\n",
      "Epoch  21/1000 (MagnitudeL2-50.0%): Train Loss: 0.9654, Train Acc: 79.44%, LR: 0.000005, Val Loss: 0.6060, Val Acc: 81.56%\n",
      "Epoch  22/1000 (MagnitudeL2-50.0%): Train Loss: 0.9593, Train Acc: 79.35%, LR: 0.000004, Val Loss: 0.6066, Val Acc: 81.76%\n",
      "Epoch  23/1000 (MagnitudeL2-50.0%): Train Loss: 0.9646, Train Acc: 79.49%, LR: 0.000004, Val Loss: 0.6044, Val Acc: 81.70%\n",
      "Epoch  24/1000 (MagnitudeL2-50.0%): Train Loss: 0.9590, Train Acc: 79.57%, LR: 0.000003, Val Loss: 0.5985, Val Acc: 81.74%\n",
      "Epoch  25/1000 (MagnitudeL2-50.0%): Train Loss: 0.9574, Train Acc: 79.76%, LR: 0.000002, Val Loss: 0.6010, Val Acc: 81.56%\n",
      "Epoch  26/1000 (MagnitudeL2-50.0%): Train Loss: 0.9580, Train Acc: 79.64%, LR: 0.000002, Val Loss: 0.6010, Val Acc: 81.30%\n",
      "Epoch  27/1000 (MagnitudeL2-50.0%): Train Loss: 0.9502, Train Acc: 80.08%, LR: 0.000001, Val Loss: 0.6083, Val Acc: 81.40%\n",
      "Epoch  28/1000 (MagnitudeL2-50.0%): Train Loss: 0.9577, Train Acc: 79.77%, LR: 0.000001, Val Loss: 0.5986, Val Acc: 81.78%\n",
      "Epoch  29/1000 (MagnitudeL2-50.0%): Train Loss: 0.9496, Train Acc: 79.99%, LR: 0.000000, Val Loss: 0.5998, Val Acc: 81.66%\n",
      "Epoch  30/1000 (MagnitudeL2-50.0%): Train Loss: 0.9528, Train Acc: 79.96%, LR: 0.000000, Val Loss: 0.5981, Val Acc: 81.82% ✓\n",
      "Epoch  31/1000 (MagnitudeL2-50.0%): Train Loss: 0.9521, Train Acc: 80.00%, LR: 0.000010, Val Loss: 0.5963, Val Acc: 81.90% ✓\n",
      "Epoch  32/1000 (MagnitudeL2-50.0%): Train Loss: 0.9551, Train Acc: 79.86%, LR: 0.000010, Val Loss: 0.6004, Val Acc: 81.74%\n",
      "Epoch  33/1000 (MagnitudeL2-50.0%): Train Loss: 0.9450, Train Acc: 80.38%, LR: 0.000010, Val Loss: 0.5974, Val Acc: 81.72%\n",
      "Epoch  34/1000 (MagnitudeL2-50.0%): Train Loss: 0.9466, Train Acc: 80.19%, LR: 0.000010, Val Loss: 0.5939, Val Acc: 81.70%\n",
      "Epoch  35/1000 (MagnitudeL2-50.0%): Train Loss: 0.9468, Train Acc: 80.10%, LR: 0.000010, Val Loss: 0.5907, Val Acc: 81.88%\n",
      "Epoch  36/1000 (MagnitudeL2-50.0%): Train Loss: 0.9380, Train Acc: 80.49%, LR: 0.000010, Val Loss: 0.5884, Val Acc: 81.76%\n",
      "Epoch  37/1000 (MagnitudeL2-50.0%): Train Loss: 0.9393, Train Acc: 80.67%, LR: 0.000009, Val Loss: 0.5900, Val Acc: 81.94% ✓\n",
      "Epoch  38/1000 (MagnitudeL2-50.0%): Train Loss: 0.9356, Train Acc: 80.75%, LR: 0.000009, Val Loss: 0.5958, Val Acc: 81.66%\n",
      "Epoch  39/1000 (MagnitudeL2-50.0%): Train Loss: 0.9336, Train Acc: 80.76%, LR: 0.000009, Val Loss: 0.5924, Val Acc: 81.82%\n",
      "Epoch  40/1000 (MagnitudeL2-50.0%): Train Loss: 0.9287, Train Acc: 81.13%, LR: 0.000009, Val Loss: 0.5852, Val Acc: 81.76%\n",
      "Epoch  41/1000 (MagnitudeL2-50.0%): Train Loss: 0.9308, Train Acc: 80.93%, LR: 0.000009, Val Loss: 0.5894, Val Acc: 81.78%\n",
      "Epoch  42/1000 (MagnitudeL2-50.0%): Train Loss: 0.9258, Train Acc: 81.05%, LR: 0.000008, Val Loss: 0.5855, Val Acc: 82.06% ✓\n",
      "Epoch  43/1000 (MagnitudeL2-50.0%): Train Loss: 0.9220, Train Acc: 81.23%, LR: 0.000008, Val Loss: 0.5789, Val Acc: 82.12% ✓\n",
      "Epoch  44/1000 (MagnitudeL2-50.0%): Train Loss: 0.9197, Train Acc: 81.48%, LR: 0.000008, Val Loss: 0.5797, Val Acc: 81.86%\n",
      "Epoch  45/1000 (MagnitudeL2-50.0%): Train Loss: 0.9217, Train Acc: 81.36%, LR: 0.000007, Val Loss: 0.5810, Val Acc: 82.32% ✓\n",
      "Epoch  46/1000 (MagnitudeL2-50.0%): Train Loss: 0.9231, Train Acc: 81.25%, LR: 0.000007, Val Loss: 0.5794, Val Acc: 82.02%\n",
      "Epoch  47/1000 (MagnitudeL2-50.0%): Train Loss: 0.9181, Train Acc: 81.49%, LR: 0.000007, Val Loss: 0.5778, Val Acc: 82.18%\n",
      "Epoch  48/1000 (MagnitudeL2-50.0%): Train Loss: 0.9193, Train Acc: 81.57%, LR: 0.000006, Val Loss: 0.5794, Val Acc: 81.96%\n",
      "Epoch  49/1000 (MagnitudeL2-50.0%): Train Loss: 0.9138, Train Acc: 81.67%, LR: 0.000006, Val Loss: 0.5793, Val Acc: 82.20%\n",
      "Epoch  50/1000 (MagnitudeL2-50.0%): Train Loss: 0.9072, Train Acc: 81.91%, LR: 0.000005, Val Loss: 0.5743, Val Acc: 82.10%\n",
      "Epoch  51/1000 (MagnitudeL2-50.0%): Train Loss: 0.9084, Train Acc: 81.86%, LR: 0.000005, Val Loss: 0.5802, Val Acc: 82.04%\n",
      "Epoch  52/1000 (MagnitudeL2-50.0%): Train Loss: 0.9098, Train Acc: 82.00%, LR: 0.000005, Val Loss: 0.5831, Val Acc: 82.24%\n",
      "Epoch  53/1000 (MagnitudeL2-50.0%): Train Loss: 0.9092, Train Acc: 81.95%, LR: 0.000004, Val Loss: 0.5752, Val Acc: 82.36% ✓\n",
      "Epoch  54/1000 (MagnitudeL2-50.0%): Train Loss: 0.9040, Train Acc: 82.20%, LR: 0.000004, Val Loss: 0.5780, Val Acc: 82.50% ✓\n",
      "Epoch  55/1000 (MagnitudeL2-50.0%): Train Loss: 0.9054, Train Acc: 81.90%, LR: 0.000004, Val Loss: 0.5787, Val Acc: 82.26%\n",
      "Epoch  56/1000 (MagnitudeL2-50.0%): Train Loss: 0.9094, Train Acc: 81.74%, LR: 0.000003, Val Loss: 0.5746, Val Acc: 82.46%\n",
      "Epoch  57/1000 (MagnitudeL2-50.0%): Train Loss: 0.9088, Train Acc: 81.92%, LR: 0.000003, Val Loss: 0.5801, Val Acc: 82.26%\n",
      "Epoch  58/1000 (MagnitudeL2-50.0%): Train Loss: 0.9052, Train Acc: 81.86%, LR: 0.000002, Val Loss: 0.5761, Val Acc: 82.46%\n",
      "Epoch  59/1000 (MagnitudeL2-50.0%): Train Loss: 0.9055, Train Acc: 81.84%, LR: 0.000002, Val Loss: 0.5764, Val Acc: 82.24%\n",
      "Epoch  60/1000 (MagnitudeL2-50.0%): Train Loss: 0.9007, Train Acc: 82.25%, LR: 0.000002, Val Loss: 0.5733, Val Acc: 82.52% ✓\n",
      "Epoch  61/1000 (MagnitudeL2-50.0%): Train Loss: 0.9092, Train Acc: 81.86%, LR: 0.000002, Val Loss: 0.5762, Val Acc: 82.24%\n",
      "Epoch  62/1000 (MagnitudeL2-50.0%): Train Loss: 0.9028, Train Acc: 82.11%, LR: 0.000001, Val Loss: 0.5790, Val Acc: 82.24%\n",
      "Epoch  63/1000 (MagnitudeL2-50.0%): Train Loss: 0.9000, Train Acc: 82.37%, LR: 0.000001, Val Loss: 0.5767, Val Acc: 82.16%\n",
      "Epoch  64/1000 (MagnitudeL2-50.0%): Train Loss: 0.9044, Train Acc: 82.18%, LR: 0.000001, Val Loss: 0.5747, Val Acc: 82.38%\n",
      "Epoch  65/1000 (MagnitudeL2-50.0%): Train Loss: 0.9030, Train Acc: 82.18%, LR: 0.000001, Val Loss: 0.5799, Val Acc: 82.18%\n",
      "Epoch  66/1000 (MagnitudeL2-50.0%): Train Loss: 0.9055, Train Acc: 81.90%, LR: 0.000000, Val Loss: 0.5760, Val Acc: 82.58% ✓\n",
      "Epoch  67/1000 (MagnitudeL2-50.0%): Train Loss: 0.9029, Train Acc: 81.96%, LR: 0.000000, Val Loss: 0.5778, Val Acc: 82.32%\n",
      "Epoch  68/1000 (MagnitudeL2-50.0%): Train Loss: 0.9041, Train Acc: 82.10%, LR: 0.000000, Val Loss: 0.5743, Val Acc: 82.22%\n",
      "Epoch  69/1000 (MagnitudeL2-50.0%): Train Loss: 0.8994, Train Acc: 82.47%, LR: 0.000000, Val Loss: 0.5740, Val Acc: 82.36%\n",
      "Epoch  70/1000 (MagnitudeL2-50.0%): Train Loss: 0.9026, Train Acc: 82.18%, LR: 0.000000, Val Loss: 0.5770, Val Acc: 82.52%\n",
      "Epoch  71/1000 (MagnitudeL2-50.0%): Train Loss: 0.9037, Train Acc: 82.12%, LR: 0.000010, Val Loss: 0.5766, Val Acc: 82.44%\n",
      "Epoch  72/1000 (MagnitudeL2-50.0%): Train Loss: 0.9042, Train Acc: 82.04%, LR: 0.000010, Val Loss: 0.5741, Val Acc: 82.28%\n",
      "Epoch  73/1000 (MagnitudeL2-50.0%): Train Loss: 0.9019, Train Acc: 82.32%, LR: 0.000010, Val Loss: 0.5734, Val Acc: 82.30%\n",
      "Epoch  74/1000 (MagnitudeL2-50.0%): Train Loss: 0.8991, Train Acc: 82.34%, LR: 0.000010, Val Loss: 0.5726, Val Acc: 82.50%\n",
      "Epoch  75/1000 (MagnitudeL2-50.0%): Train Loss: 0.8995, Train Acc: 82.25%, LR: 0.000010, Val Loss: 0.5718, Val Acc: 82.54%\n",
      "Epoch  76/1000 (MagnitudeL2-50.0%): Train Loss: 0.8993, Train Acc: 82.30%, LR: 0.000010, Val Loss: 0.5758, Val Acc: 82.12%\n",
      "Epoch  77/1000 (MagnitudeL2-50.0%): Train Loss: 0.8899, Train Acc: 82.69%, LR: 0.000010, Val Loss: 0.5687, Val Acc: 82.66% ✓\n",
      "Epoch  78/1000 (MagnitudeL2-50.0%): Train Loss: 0.8941, Train Acc: 82.52%, LR: 0.000010, Val Loss: 0.5736, Val Acc: 82.58%\n",
      "Epoch  79/1000 (MagnitudeL2-50.0%): Train Loss: 0.8932, Train Acc: 82.51%, LR: 0.000010, Val Loss: 0.5705, Val Acc: 82.40%\n",
      "Epoch  80/1000 (MagnitudeL2-50.0%): Train Loss: 0.8915, Train Acc: 82.83%, LR: 0.000010, Val Loss: 0.5753, Val Acc: 82.42%\n",
      "Epoch  81/1000 (MagnitudeL2-50.0%): Train Loss: 0.8940, Train Acc: 82.70%, LR: 0.000010, Val Loss: 0.5737, Val Acc: 82.38%\n",
      "Epoch  82/1000 (MagnitudeL2-50.0%): Train Loss: 0.8907, Train Acc: 82.68%, LR: 0.000010, Val Loss: 0.5728, Val Acc: 82.44%\n",
      "Epoch  83/1000 (MagnitudeL2-50.0%): Train Loss: 0.8869, Train Acc: 82.82%, LR: 0.000009, Val Loss: 0.5690, Val Acc: 82.50%\n",
      "Epoch  84/1000 (MagnitudeL2-50.0%): Train Loss: 0.8884, Train Acc: 82.72%, LR: 0.000009, Val Loss: 0.5690, Val Acc: 82.80% ✓\n",
      "Epoch  85/1000 (MagnitudeL2-50.0%): Train Loss: 0.8827, Train Acc: 83.14%, LR: 0.000009, Val Loss: 0.5705, Val Acc: 82.52%\n",
      "Epoch  86/1000 (MagnitudeL2-50.0%): Train Loss: 0.8828, Train Acc: 83.02%, LR: 0.000009, Val Loss: 0.5663, Val Acc: 82.84% ✓\n",
      "Epoch  87/1000 (MagnitudeL2-50.0%): Train Loss: 0.8824, Train Acc: 82.99%, LR: 0.000009, Val Loss: 0.5692, Val Acc: 82.74%\n",
      "Epoch  88/1000 (MagnitudeL2-50.0%): Train Loss: 0.8795, Train Acc: 83.30%, LR: 0.000009, Val Loss: 0.5677, Val Acc: 82.86% ✓\n",
      "Epoch  89/1000 (MagnitudeL2-50.0%): Train Loss: 0.8792, Train Acc: 83.19%, LR: 0.000009, Val Loss: 0.5684, Val Acc: 82.62%\n",
      "Epoch  90/1000 (MagnitudeL2-50.0%): Train Loss: 0.8780, Train Acc: 83.15%, LR: 0.000009, Val Loss: 0.5671, Val Acc: 83.02% ✓\n",
      "Epoch  91/1000 (MagnitudeL2-50.0%): Train Loss: 0.8788, Train Acc: 83.26%, LR: 0.000009, Val Loss: 0.5660, Val Acc: 82.94%\n",
      "Epoch  92/1000 (MagnitudeL2-50.0%): Train Loss: 0.8745, Train Acc: 83.47%, LR: 0.000008, Val Loss: 0.5637, Val Acc: 82.86%\n",
      "Epoch  93/1000 (MagnitudeL2-50.0%): Train Loss: 0.8780, Train Acc: 83.15%, LR: 0.000008, Val Loss: 0.5644, Val Acc: 82.98%\n",
      "Epoch  94/1000 (MagnitudeL2-50.0%): Train Loss: 0.8814, Train Acc: 83.23%, LR: 0.000008, Val Loss: 0.5719, Val Acc: 82.74%\n",
      "Epoch  95/1000 (MagnitudeL2-50.0%): Train Loss: 0.8710, Train Acc: 83.36%, LR: 0.000008, Val Loss: 0.5701, Val Acc: 82.52%\n",
      "Epoch  96/1000 (MagnitudeL2-50.0%): Train Loss: 0.8698, Train Acc: 83.53%, LR: 0.000008, Val Loss: 0.5728, Val Acc: 82.50%\n",
      "Epoch  97/1000 (MagnitudeL2-50.0%): Train Loss: 0.8732, Train Acc: 83.42%, LR: 0.000008, Val Loss: 0.5668, Val Acc: 82.56%\n",
      "Epoch  98/1000 (MagnitudeL2-50.0%): Train Loss: 0.8719, Train Acc: 83.63%, LR: 0.000007, Val Loss: 0.5698, Val Acc: 82.46%\n",
      "Epoch  99/1000 (MagnitudeL2-50.0%): Train Loss: 0.8709, Train Acc: 83.64%, LR: 0.000007, Val Loss: 0.5664, Val Acc: 82.70%\n",
      "Epoch 100/1000 (MagnitudeL2-50.0%): Train Loss: 0.8662, Train Acc: 83.73%, LR: 0.000007, Val Loss: 0.5680, Val Acc: 82.60%\n",
      "Epoch 101/1000 (MagnitudeL2-50.0%): Train Loss: 0.8658, Train Acc: 83.88%, LR: 0.000007, Val Loss: 0.5673, Val Acc: 82.74%\n",
      "Epoch 102/1000 (MagnitudeL2-50.0%): Train Loss: 0.8644, Train Acc: 84.03%, LR: 0.000007, Val Loss: 0.5681, Val Acc: 82.90%\n",
      "Epoch 103/1000 (MagnitudeL2-50.0%): Train Loss: 0.8645, Train Acc: 83.92%, LR: 0.000007, Val Loss: 0.5683, Val Acc: 82.58%\n",
      "Epoch 104/1000 (MagnitudeL2-50.0%): Train Loss: 0.8621, Train Acc: 84.13%, LR: 0.000006, Val Loss: 0.5669, Val Acc: 82.78%\n",
      "Epoch 105/1000 (MagnitudeL2-50.0%): Train Loss: 0.8693, Train Acc: 83.74%, LR: 0.000006, Val Loss: 0.5672, Val Acc: 82.82%\n",
      "Epoch 106/1000 (MagnitudeL2-50.0%): Train Loss: 0.8663, Train Acc: 83.85%, LR: 0.000006, Val Loss: 0.5652, Val Acc: 82.98%\n",
      "Epoch 107/1000 (MagnitudeL2-50.0%): Train Loss: 0.8657, Train Acc: 83.89%, LR: 0.000006, Val Loss: 0.5729, Val Acc: 82.82%\n",
      "Epoch 108/1000 (MagnitudeL2-50.0%): Train Loss: 0.8608, Train Acc: 83.99%, LR: 0.000006, Val Loss: 0.5677, Val Acc: 82.80%\n",
      "Epoch 109/1000 (MagnitudeL2-50.0%): Train Loss: 0.8623, Train Acc: 83.96%, LR: 0.000005, Val Loss: 0.5661, Val Acc: 82.82%\n",
      "Epoch 110/1000 (MagnitudeL2-50.0%): Train Loss: 0.8588, Train Acc: 84.21%, LR: 0.000005, Val Loss: 0.5705, Val Acc: 82.90%\n",
      "Early stopping triggered after 110 epochs (Best Val Acc: 83.02%)\n",
      "Loaded best model state (Val Acc: 83.02%)\n",
      "Results: Accuracy=82.49%, MACs=6.02M\n",
      "✅ Model saved to ./base/enhanced_magnitudel2_sparsity_0.5.pth\n",
      "✅ ONNX model saved to ./base/enhanced_magnitudel2_sparsity_0.5.onnx\n",
      "\n",
      "Processing MagnitudeL2 at 70.0% sparsity...\n",
      "✅ Created MobileNetV2 without pretrained weights\n",
      "✅ Enhanced classifier created for 10 classes\n",
      "Initial MACs: 7.30M\n",
      "Applying MagnitudeImportance pruning at 70.0% sparsity...\n",
      "Final MACs: 5.56M (Reduction: 23.8%)\n",
      "Fine-tuning pruned model with enhanced settings...\n",
      "Epoch   1/1000 (MagnitudeL2-70.0%): Train Loss: 1.3826, Train Acc: 62.24%, LR: 0.000010, Val Loss: 0.9031, Val Acc: 71.30% ✓\n",
      "Epoch   2/1000 (MagnitudeL2-70.0%): Train Loss: 1.2428, Train Acc: 67.25%, LR: 0.000010, Val Loss: 0.8403, Val Acc: 73.34% ✓\n",
      "Epoch   3/1000 (MagnitudeL2-70.0%): Train Loss: 1.1919, Train Acc: 69.59%, LR: 0.000009, Val Loss: 0.8076, Val Acc: 74.12% ✓\n",
      "Epoch   4/1000 (MagnitudeL2-70.0%): Train Loss: 1.1640, Train Acc: 70.62%, LR: 0.000008, Val Loss: 0.7790, Val Acc: 75.22% ✓\n",
      "Epoch   5/1000 (MagnitudeL2-70.0%): Train Loss: 1.1500, Train Acc: 71.29%, LR: 0.000007, Val Loss: 0.7673, Val Acc: 75.86% ✓\n",
      "Epoch   6/1000 (MagnitudeL2-70.0%): Train Loss: 1.1327, Train Acc: 72.06%, LR: 0.000005, Val Loss: 0.7583, Val Acc: 75.96% ✓\n",
      "Epoch   7/1000 (MagnitudeL2-70.0%): Train Loss: 1.1281, Train Acc: 72.16%, LR: 0.000004, Val Loss: 0.7467, Val Acc: 76.14% ✓\n",
      "Epoch   8/1000 (MagnitudeL2-70.0%): Train Loss: 1.1181, Train Acc: 72.56%, LR: 0.000002, Val Loss: 0.7449, Val Acc: 76.12%\n",
      "Epoch   9/1000 (MagnitudeL2-70.0%): Train Loss: 1.1171, Train Acc: 72.52%, LR: 0.000001, Val Loss: 0.7443, Val Acc: 76.26% ✓\n",
      "Epoch  10/1000 (MagnitudeL2-70.0%): Train Loss: 1.1139, Train Acc: 72.78%, LR: 0.000000, Val Loss: 0.7418, Val Acc: 76.50% ✓\n",
      "Epoch  11/1000 (MagnitudeL2-70.0%): Train Loss: 1.1173, Train Acc: 72.64%, LR: 0.000010, Val Loss: 0.7289, Val Acc: 76.74% ✓\n",
      "Epoch  12/1000 (MagnitudeL2-70.0%): Train Loss: 1.1055, Train Acc: 73.23%, LR: 0.000010, Val Loss: 0.7214, Val Acc: 76.74%\n",
      "Epoch  13/1000 (MagnitudeL2-70.0%): Train Loss: 1.0935, Train Acc: 73.62%, LR: 0.000010, Val Loss: 0.7038, Val Acc: 77.58% ✓\n",
      "Epoch  14/1000 (MagnitudeL2-70.0%): Train Loss: 1.0826, Train Acc: 74.26%, LR: 0.000009, Val Loss: 0.6976, Val Acc: 77.78% ✓\n",
      "Epoch  15/1000 (MagnitudeL2-70.0%): Train Loss: 1.0754, Train Acc: 74.30%, LR: 0.000009, Val Loss: 0.6857, Val Acc: 78.48% ✓\n",
      "Epoch  16/1000 (MagnitudeL2-70.0%): Train Loss: 1.0632, Train Acc: 74.87%, LR: 0.000009, Val Loss: 0.6825, Val Acc: 78.24%\n",
      "Epoch  17/1000 (MagnitudeL2-70.0%): Train Loss: 1.0571, Train Acc: 75.16%, LR: 0.000008, Val Loss: 0.6739, Val Acc: 78.78% ✓\n",
      "Epoch  18/1000 (MagnitudeL2-70.0%): Train Loss: 1.0491, Train Acc: 75.46%, LR: 0.000007, Val Loss: 0.6704, Val Acc: 78.82% ✓\n",
      "Epoch  19/1000 (MagnitudeL2-70.0%): Train Loss: 1.0459, Train Acc: 75.85%, LR: 0.000007, Val Loss: 0.6674, Val Acc: 78.76%\n",
      "Epoch  20/1000 (MagnitudeL2-70.0%): Train Loss: 1.0409, Train Acc: 76.03%, LR: 0.000006, Val Loss: 0.6649, Val Acc: 78.98% ✓\n",
      "Epoch  21/1000 (MagnitudeL2-70.0%): Train Loss: 1.0399, Train Acc: 75.82%, LR: 0.000005, Val Loss: 0.6637, Val Acc: 79.28% ✓\n",
      "Epoch  22/1000 (MagnitudeL2-70.0%): Train Loss: 1.0349, Train Acc: 76.26%, LR: 0.000004, Val Loss: 0.6616, Val Acc: 78.98%\n",
      "Epoch  23/1000 (MagnitudeL2-70.0%): Train Loss: 1.0393, Train Acc: 75.95%, LR: 0.000004, Val Loss: 0.6625, Val Acc: 79.28%\n",
      "Epoch  24/1000 (MagnitudeL2-70.0%): Train Loss: 1.0351, Train Acc: 76.09%, LR: 0.000003, Val Loss: 0.6569, Val Acc: 79.24%\n",
      "Epoch  25/1000 (MagnitudeL2-70.0%): Train Loss: 1.0301, Train Acc: 76.49%, LR: 0.000002, Val Loss: 0.6588, Val Acc: 79.34% ✓\n",
      "Epoch  26/1000 (MagnitudeL2-70.0%): Train Loss: 1.0323, Train Acc: 76.26%, LR: 0.000002, Val Loss: 0.6546, Val Acc: 79.26%\n",
      "Epoch  27/1000 (MagnitudeL2-70.0%): Train Loss: 1.0285, Train Acc: 76.30%, LR: 0.000001, Val Loss: 0.6546, Val Acc: 79.30%\n",
      "Epoch  28/1000 (MagnitudeL2-70.0%): Train Loss: 1.0271, Train Acc: 76.64%, LR: 0.000001, Val Loss: 0.6546, Val Acc: 78.98%\n",
      "Epoch  29/1000 (MagnitudeL2-70.0%): Train Loss: 1.0296, Train Acc: 76.38%, LR: 0.000000, Val Loss: 0.6530, Val Acc: 79.38% ✓\n",
      "Epoch  30/1000 (MagnitudeL2-70.0%): Train Loss: 1.0294, Train Acc: 76.59%, LR: 0.000000, Val Loss: 0.6530, Val Acc: 79.30%\n",
      "Epoch  31/1000 (MagnitudeL2-70.0%): Train Loss: 1.0332, Train Acc: 76.27%, LR: 0.000010, Val Loss: 0.6490, Val Acc: 79.30%\n",
      "Epoch  32/1000 (MagnitudeL2-70.0%): Train Loss: 1.0298, Train Acc: 76.49%, LR: 0.000010, Val Loss: 0.6493, Val Acc: 79.38%\n",
      "Epoch  33/1000 (MagnitudeL2-70.0%): Train Loss: 1.0251, Train Acc: 76.82%, LR: 0.000010, Val Loss: 0.6424, Val Acc: 79.74% ✓\n",
      "Epoch  34/1000 (MagnitudeL2-70.0%): Train Loss: 1.0152, Train Acc: 77.03%, LR: 0.000010, Val Loss: 0.6424, Val Acc: 79.30%\n",
      "Epoch  35/1000 (MagnitudeL2-70.0%): Train Loss: 1.0092, Train Acc: 77.34%, LR: 0.000010, Val Loss: 0.6337, Val Acc: 79.86% ✓\n",
      "Epoch  36/1000 (MagnitudeL2-70.0%): Train Loss: 1.0079, Train Acc: 77.31%, LR: 0.000010, Val Loss: 0.6352, Val Acc: 79.96% ✓\n",
      "Epoch  37/1000 (MagnitudeL2-70.0%): Train Loss: 1.0083, Train Acc: 77.34%, LR: 0.000009, Val Loss: 0.6303, Val Acc: 80.10% ✓\n",
      "Epoch  38/1000 (MagnitudeL2-70.0%): Train Loss: 1.0018, Train Acc: 77.67%, LR: 0.000009, Val Loss: 0.6317, Val Acc: 80.06%\n",
      "Epoch  39/1000 (MagnitudeL2-70.0%): Train Loss: 0.9941, Train Acc: 78.00%, LR: 0.000009, Val Loss: 0.6257, Val Acc: 80.40% ✓\n",
      "Epoch  40/1000 (MagnitudeL2-70.0%): Train Loss: 0.9988, Train Acc: 77.82%, LR: 0.000009, Val Loss: 0.6231, Val Acc: 80.48% ✓\n",
      "Epoch  41/1000 (MagnitudeL2-70.0%): Train Loss: 0.9934, Train Acc: 78.12%, LR: 0.000009, Val Loss: 0.6218, Val Acc: 80.32%\n",
      "Epoch  42/1000 (MagnitudeL2-70.0%): Train Loss: 0.9914, Train Acc: 78.21%, LR: 0.000008, Val Loss: 0.6170, Val Acc: 80.96% ✓\n",
      "Epoch  43/1000 (MagnitudeL2-70.0%): Train Loss: 0.9805, Train Acc: 78.67%, LR: 0.000008, Val Loss: 0.6165, Val Acc: 80.84%\n",
      "Epoch  44/1000 (MagnitudeL2-70.0%): Train Loss: 0.9826, Train Acc: 78.72%, LR: 0.000008, Val Loss: 0.6164, Val Acc: 81.18% ✓\n",
      "Epoch  45/1000 (MagnitudeL2-70.0%): Train Loss: 0.9872, Train Acc: 78.55%, LR: 0.000007, Val Loss: 0.6179, Val Acc: 81.04%\n",
      "Epoch  46/1000 (MagnitudeL2-70.0%): Train Loss: 0.9767, Train Acc: 78.90%, LR: 0.000007, Val Loss: 0.6156, Val Acc: 80.76%\n",
      "Epoch  47/1000 (MagnitudeL2-70.0%): Train Loss: 0.9799, Train Acc: 78.86%, LR: 0.000007, Val Loss: 0.6165, Val Acc: 81.02%\n",
      "Epoch  48/1000 (MagnitudeL2-70.0%): Train Loss: 0.9753, Train Acc: 78.88%, LR: 0.000006, Val Loss: 0.6133, Val Acc: 81.20% ✓\n",
      "Epoch  49/1000 (MagnitudeL2-70.0%): Train Loss: 0.9727, Train Acc: 78.96%, LR: 0.000006, Val Loss: 0.6122, Val Acc: 81.40% ✓\n",
      "Epoch  50/1000 (MagnitudeL2-70.0%): Train Loss: 0.9774, Train Acc: 78.86%, LR: 0.000005, Val Loss: 0.6142, Val Acc: 81.24%\n",
      "Epoch  51/1000 (MagnitudeL2-70.0%): Train Loss: 0.9735, Train Acc: 78.97%, LR: 0.000005, Val Loss: 0.6162, Val Acc: 81.12%\n",
      "Epoch  52/1000 (MagnitudeL2-70.0%): Train Loss: 0.9726, Train Acc: 79.09%, LR: 0.000005, Val Loss: 0.6089, Val Acc: 81.38%\n",
      "Epoch  53/1000 (MagnitudeL2-70.0%): Train Loss: 0.9769, Train Acc: 78.77%, LR: 0.000004, Val Loss: 0.6100, Val Acc: 81.24%\n",
      "Epoch  54/1000 (MagnitudeL2-70.0%): Train Loss: 0.9701, Train Acc: 79.29%, LR: 0.000004, Val Loss: 0.6087, Val Acc: 81.36%\n",
      "Epoch  55/1000 (MagnitudeL2-70.0%): Train Loss: 0.9673, Train Acc: 79.24%, LR: 0.000004, Val Loss: 0.6118, Val Acc: 81.24%\n",
      "Epoch  56/1000 (MagnitudeL2-70.0%): Train Loss: 0.9671, Train Acc: 79.28%, LR: 0.000003, Val Loss: 0.6074, Val Acc: 81.60% ✓\n",
      "Epoch  57/1000 (MagnitudeL2-70.0%): Train Loss: 0.9657, Train Acc: 79.29%, LR: 0.000003, Val Loss: 0.6122, Val Acc: 81.28%\n",
      "Epoch  58/1000 (MagnitudeL2-70.0%): Train Loss: 0.9617, Train Acc: 79.59%, LR: 0.000002, Val Loss: 0.6060, Val Acc: 81.48%\n",
      "Epoch  59/1000 (MagnitudeL2-70.0%): Train Loss: 0.9614, Train Acc: 79.57%, LR: 0.000002, Val Loss: 0.6079, Val Acc: 81.66% ✓\n",
      "Epoch  60/1000 (MagnitudeL2-70.0%): Train Loss: 0.9633, Train Acc: 79.41%, LR: 0.000002, Val Loss: 0.6056, Val Acc: 81.46%\n",
      "Epoch  61/1000 (MagnitudeL2-70.0%): Train Loss: 0.9650, Train Acc: 79.48%, LR: 0.000002, Val Loss: 0.6037, Val Acc: 81.62%\n",
      "Epoch  62/1000 (MagnitudeL2-70.0%): Train Loss: 0.9620, Train Acc: 79.42%, LR: 0.000001, Val Loss: 0.6077, Val Acc: 81.24%\n",
      "Epoch  63/1000 (MagnitudeL2-70.0%): Train Loss: 0.9633, Train Acc: 79.42%, LR: 0.000001, Val Loss: 0.6083, Val Acc: 81.26%\n",
      "Epoch  64/1000 (MagnitudeL2-70.0%): Train Loss: 0.9631, Train Acc: 79.40%, LR: 0.000001, Val Loss: 0.6025, Val Acc: 81.76% ✓\n",
      "Epoch  65/1000 (MagnitudeL2-70.0%): Train Loss: 0.9679, Train Acc: 79.16%, LR: 0.000001, Val Loss: 0.6033, Val Acc: 81.66%\n",
      "Epoch  66/1000 (MagnitudeL2-70.0%): Train Loss: 0.9607, Train Acc: 79.62%, LR: 0.000000, Val Loss: 0.6058, Val Acc: 81.74%\n",
      "Epoch  67/1000 (MagnitudeL2-70.0%): Train Loss: 0.9610, Train Acc: 79.60%, LR: 0.000000, Val Loss: 0.6056, Val Acc: 81.50%\n",
      "Epoch  68/1000 (MagnitudeL2-70.0%): Train Loss: 0.9592, Train Acc: 79.71%, LR: 0.000000, Val Loss: 0.6074, Val Acc: 81.38%\n",
      "Epoch  69/1000 (MagnitudeL2-70.0%): Train Loss: 0.9616, Train Acc: 79.67%, LR: 0.000000, Val Loss: 0.6085, Val Acc: 81.28%\n",
      "Epoch  70/1000 (MagnitudeL2-70.0%): Train Loss: 0.9609, Train Acc: 79.48%, LR: 0.000000, Val Loss: 0.6064, Val Acc: 81.62%\n",
      "Epoch  71/1000 (MagnitudeL2-70.0%): Train Loss: 0.9603, Train Acc: 79.59%, LR: 0.000010, Val Loss: 0.6108, Val Acc: 81.40%\n",
      "Epoch  72/1000 (MagnitudeL2-70.0%): Train Loss: 0.9593, Train Acc: 79.81%, LR: 0.000010, Val Loss: 0.6124, Val Acc: 81.00%\n",
      "Epoch  73/1000 (MagnitudeL2-70.0%): Train Loss: 0.9593, Train Acc: 79.49%, LR: 0.000010, Val Loss: 0.6066, Val Acc: 81.44%\n",
      "Epoch  74/1000 (MagnitudeL2-70.0%): Train Loss: 0.9534, Train Acc: 79.81%, LR: 0.000010, Val Loss: 0.6043, Val Acc: 81.60%\n",
      "Epoch  75/1000 (MagnitudeL2-70.0%): Train Loss: 0.9508, Train Acc: 79.96%, LR: 0.000010, Val Loss: 0.5999, Val Acc: 81.54%\n",
      "Epoch  76/1000 (MagnitudeL2-70.0%): Train Loss: 0.9553, Train Acc: 79.87%, LR: 0.000010, Val Loss: 0.6039, Val Acc: 81.48%\n",
      "Epoch  77/1000 (MagnitudeL2-70.0%): Train Loss: 0.9501, Train Acc: 80.09%, LR: 0.000010, Val Loss: 0.6019, Val Acc: 81.54%\n",
      "Epoch  78/1000 (MagnitudeL2-70.0%): Train Loss: 0.9498, Train Acc: 79.98%, LR: 0.000010, Val Loss: 0.6000, Val Acc: 81.62%\n",
      "Epoch  79/1000 (MagnitudeL2-70.0%): Train Loss: 0.9492, Train Acc: 80.03%, LR: 0.000010, Val Loss: 0.6021, Val Acc: 81.46%\n",
      "Epoch  80/1000 (MagnitudeL2-70.0%): Train Loss: 0.9485, Train Acc: 80.11%, LR: 0.000010, Val Loss: 0.6024, Val Acc: 81.48%\n",
      "Epoch  81/1000 (MagnitudeL2-70.0%): Train Loss: 0.9478, Train Acc: 79.90%, LR: 0.000010, Val Loss: 0.6009, Val Acc: 81.48%\n",
      "Epoch  82/1000 (MagnitudeL2-70.0%): Train Loss: 0.9480, Train Acc: 80.19%, LR: 0.000010, Val Loss: 0.5984, Val Acc: 81.70%\n",
      "Epoch  83/1000 (MagnitudeL2-70.0%): Train Loss: 0.9437, Train Acc: 80.41%, LR: 0.000009, Val Loss: 0.6003, Val Acc: 81.56%\n",
      "Epoch  84/1000 (MagnitudeL2-70.0%): Train Loss: 0.9379, Train Acc: 80.54%, LR: 0.000009, Val Loss: 0.5997, Val Acc: 81.60%\n",
      "Early stopping triggered after 84 epochs (Best Val Acc: 81.76%)\n",
      "Loaded best model state (Val Acc: 81.76%)\n",
      "Results: Accuracy=81.01%, MACs=5.56M\n",
      "✅ Model saved to ./base/enhanced_magnitudel2_sparsity_0.7.pth\n",
      "✅ ONNX model saved to ./base/enhanced_magnitudel2_sparsity_0.7.onnx\n",
      "\n",
      "--- Strategy: Random ---\n",
      "\n",
      "Processing Random at 20.0% sparsity...\n",
      "✅ Created MobileNetV2 without pretrained weights\n",
      "✅ Enhanced classifier created for 10 classes\n",
      "Initial MACs: 7.30M\n",
      "Applying RandomImportance pruning at 20.0% sparsity...\n",
      "Final MACs: 6.75M (Reduction: 7.5%)\n",
      "Fine-tuning pruned model with enhanced settings...\n",
      "Epoch   1/1000 (Random-20.0%): Train Loss: 1.1165, Train Acc: 73.32%, LR: 0.000010, Val Loss: 0.7195, Val Acc: 76.82% ✓\n",
      "Epoch   2/1000 (Random-20.0%): Train Loss: 1.0385, Train Acc: 76.34%, LR: 0.000010, Val Loss: 0.6769, Val Acc: 78.34% ✓\n",
      "Epoch   3/1000 (Random-20.0%): Train Loss: 1.0115, Train Acc: 77.29%, LR: 0.000009, Val Loss: 0.6553, Val Acc: 79.10% ✓\n",
      "Epoch   4/1000 (Random-20.0%): Train Loss: 0.9968, Train Acc: 78.04%, LR: 0.000008, Val Loss: 0.6384, Val Acc: 79.96% ✓\n",
      "Epoch   5/1000 (Random-20.0%): Train Loss: 0.9824, Train Acc: 78.62%, LR: 0.000007, Val Loss: 0.6333, Val Acc: 80.20% ✓\n",
      "Epoch   6/1000 (Random-20.0%): Train Loss: 0.9722, Train Acc: 79.10%, LR: 0.000005, Val Loss: 0.6217, Val Acc: 80.44% ✓\n",
      "Epoch   7/1000 (Random-20.0%): Train Loss: 0.9733, Train Acc: 78.80%, LR: 0.000004, Val Loss: 0.6184, Val Acc: 80.68% ✓\n",
      "Epoch   8/1000 (Random-20.0%): Train Loss: 0.9714, Train Acc: 79.29%, LR: 0.000002, Val Loss: 0.6209, Val Acc: 80.20%\n",
      "Epoch   9/1000 (Random-20.0%): Train Loss: 0.9642, Train Acc: 79.36%, LR: 0.000001, Val Loss: 0.6140, Val Acc: 80.68%\n",
      "Epoch  10/1000 (Random-20.0%): Train Loss: 0.9631, Train Acc: 79.27%, LR: 0.000000, Val Loss: 0.6142, Val Acc: 80.60%\n",
      "Epoch  11/1000 (Random-20.0%): Train Loss: 0.9666, Train Acc: 79.32%, LR: 0.000010, Val Loss: 0.6070, Val Acc: 81.16% ✓\n",
      "Epoch  12/1000 (Random-20.0%): Train Loss: 0.9510, Train Acc: 80.09%, LR: 0.000010, Val Loss: 0.5995, Val Acc: 81.54% ✓\n",
      "Epoch  13/1000 (Random-20.0%): Train Loss: 0.9473, Train Acc: 80.24%, LR: 0.000010, Val Loss: 0.5975, Val Acc: 81.42%\n",
      "Epoch  14/1000 (Random-20.0%): Train Loss: 0.9429, Train Acc: 80.34%, LR: 0.000009, Val Loss: 0.5918, Val Acc: 82.00% ✓\n",
      "Epoch  15/1000 (Random-20.0%): Train Loss: 0.9370, Train Acc: 80.60%, LR: 0.000009, Val Loss: 0.5920, Val Acc: 81.86%\n",
      "Epoch  16/1000 (Random-20.0%): Train Loss: 0.9298, Train Acc: 80.93%, LR: 0.000009, Val Loss: 0.5857, Val Acc: 81.88%\n",
      "Epoch  17/1000 (Random-20.0%): Train Loss: 0.9256, Train Acc: 81.07%, LR: 0.000008, Val Loss: 0.5818, Val Acc: 81.78%\n",
      "Epoch  18/1000 (Random-20.0%): Train Loss: 0.9223, Train Acc: 81.29%, LR: 0.000007, Val Loss: 0.5800, Val Acc: 82.00%\n",
      "Epoch  19/1000 (Random-20.0%): Train Loss: 0.9218, Train Acc: 81.42%, LR: 0.000007, Val Loss: 0.5796, Val Acc: 82.08% ✓\n",
      "Epoch  20/1000 (Random-20.0%): Train Loss: 0.9104, Train Acc: 81.83%, LR: 0.000006, Val Loss: 0.5780, Val Acc: 82.34% ✓\n",
      "Epoch  21/1000 (Random-20.0%): Train Loss: 0.9091, Train Acc: 81.81%, LR: 0.000005, Val Loss: 0.5756, Val Acc: 82.46% ✓\n",
      "Epoch  22/1000 (Random-20.0%): Train Loss: 0.9063, Train Acc: 82.05%, LR: 0.000004, Val Loss: 0.5741, Val Acc: 82.36%\n",
      "Epoch  23/1000 (Random-20.0%): Train Loss: 0.9039, Train Acc: 82.12%, LR: 0.000004, Val Loss: 0.5760, Val Acc: 82.06%\n",
      "Epoch  24/1000 (Random-20.0%): Train Loss: 0.9048, Train Acc: 82.06%, LR: 0.000003, Val Loss: 0.5749, Val Acc: 82.32%\n",
      "Epoch  25/1000 (Random-20.0%): Train Loss: 0.9028, Train Acc: 82.03%, LR: 0.000002, Val Loss: 0.5732, Val Acc: 82.26%\n",
      "Epoch  26/1000 (Random-20.0%): Train Loss: 0.9037, Train Acc: 82.13%, LR: 0.000002, Val Loss: 0.5720, Val Acc: 82.40%\n",
      "Epoch  27/1000 (Random-20.0%): Train Loss: 0.9024, Train Acc: 82.16%, LR: 0.000001, Val Loss: 0.5715, Val Acc: 82.36%\n",
      "Epoch  28/1000 (Random-20.0%): Train Loss: 0.8982, Train Acc: 82.44%, LR: 0.000001, Val Loss: 0.5765, Val Acc: 82.28%\n",
      "Epoch  29/1000 (Random-20.0%): Train Loss: 0.9044, Train Acc: 81.94%, LR: 0.000000, Val Loss: 0.5706, Val Acc: 82.40%\n",
      "Epoch  30/1000 (Random-20.0%): Train Loss: 0.9041, Train Acc: 82.08%, LR: 0.000000, Val Loss: 0.5756, Val Acc: 82.22%\n",
      "Epoch  31/1000 (Random-20.0%): Train Loss: 0.9018, Train Acc: 82.28%, LR: 0.000010, Val Loss: 0.5741, Val Acc: 82.16%\n",
      "Epoch  32/1000 (Random-20.0%): Train Loss: 0.9019, Train Acc: 82.28%, LR: 0.000010, Val Loss: 0.5698, Val Acc: 82.34%\n",
      "Epoch  33/1000 (Random-20.0%): Train Loss: 0.8969, Train Acc: 82.29%, LR: 0.000010, Val Loss: 0.5699, Val Acc: 82.56% ✓\n",
      "Epoch  34/1000 (Random-20.0%): Train Loss: 0.8970, Train Acc: 82.29%, LR: 0.000010, Val Loss: 0.5613, Val Acc: 82.90% ✓\n",
      "Epoch  35/1000 (Random-20.0%): Train Loss: 0.8914, Train Acc: 82.87%, LR: 0.000010, Val Loss: 0.5634, Val Acc: 82.62%\n",
      "Epoch  36/1000 (Random-20.0%): Train Loss: 0.8849, Train Acc: 82.91%, LR: 0.000010, Val Loss: 0.5639, Val Acc: 82.72%\n",
      "Epoch  37/1000 (Random-20.0%): Train Loss: 0.8832, Train Acc: 82.91%, LR: 0.000009, Val Loss: 0.5614, Val Acc: 82.80%\n",
      "Epoch  38/1000 (Random-20.0%): Train Loss: 0.8878, Train Acc: 82.81%, LR: 0.000009, Val Loss: 0.5629, Val Acc: 82.88%\n",
      "Epoch  39/1000 (Random-20.0%): Train Loss: 0.8807, Train Acc: 83.20%, LR: 0.000009, Val Loss: 0.5649, Val Acc: 82.58%\n",
      "Epoch  40/1000 (Random-20.0%): Train Loss: 0.8787, Train Acc: 83.23%, LR: 0.000009, Val Loss: 0.5603, Val Acc: 83.22% ✓\n",
      "Epoch  41/1000 (Random-20.0%): Train Loss: 0.8778, Train Acc: 83.26%, LR: 0.000009, Val Loss: 0.5640, Val Acc: 82.80%\n",
      "Epoch  42/1000 (Random-20.0%): Train Loss: 0.8779, Train Acc: 83.26%, LR: 0.000008, Val Loss: 0.5623, Val Acc: 83.00%\n",
      "Epoch  43/1000 (Random-20.0%): Train Loss: 0.8748, Train Acc: 83.30%, LR: 0.000008, Val Loss: 0.5658, Val Acc: 83.12%\n",
      "Epoch  44/1000 (Random-20.0%): Train Loss: 0.8703, Train Acc: 83.77%, LR: 0.000008, Val Loss: 0.5665, Val Acc: 82.98%\n",
      "Epoch  45/1000 (Random-20.0%): Train Loss: 0.8716, Train Acc: 83.62%, LR: 0.000007, Val Loss: 0.5623, Val Acc: 82.80%\n",
      "Epoch  46/1000 (Random-20.0%): Train Loss: 0.8670, Train Acc: 83.75%, LR: 0.000007, Val Loss: 0.5619, Val Acc: 82.86%\n",
      "Epoch  47/1000 (Random-20.0%): Train Loss: 0.8702, Train Acc: 83.69%, LR: 0.000007, Val Loss: 0.5589, Val Acc: 83.00%\n",
      "Epoch  48/1000 (Random-20.0%): Train Loss: 0.8679, Train Acc: 83.83%, LR: 0.000006, Val Loss: 0.5568, Val Acc: 83.20%\n",
      "Epoch  49/1000 (Random-20.0%): Train Loss: 0.8663, Train Acc: 83.80%, LR: 0.000006, Val Loss: 0.5603, Val Acc: 82.94%\n",
      "Epoch  50/1000 (Random-20.0%): Train Loss: 0.8667, Train Acc: 83.90%, LR: 0.000005, Val Loss: 0.5593, Val Acc: 82.94%\n",
      "Epoch  51/1000 (Random-20.0%): Train Loss: 0.8595, Train Acc: 84.14%, LR: 0.000005, Val Loss: 0.5536, Val Acc: 82.96%\n",
      "Epoch  52/1000 (Random-20.0%): Train Loss: 0.8623, Train Acc: 84.03%, LR: 0.000005, Val Loss: 0.5602, Val Acc: 83.22%\n",
      "Epoch  53/1000 (Random-20.0%): Train Loss: 0.8606, Train Acc: 84.03%, LR: 0.000004, Val Loss: 0.5578, Val Acc: 83.14%\n",
      "Epoch  54/1000 (Random-20.0%): Train Loss: 0.8632, Train Acc: 83.95%, LR: 0.000004, Val Loss: 0.5561, Val Acc: 83.10%\n",
      "Epoch  55/1000 (Random-20.0%): Train Loss: 0.8615, Train Acc: 83.89%, LR: 0.000004, Val Loss: 0.5580, Val Acc: 83.18%\n",
      "Epoch  56/1000 (Random-20.0%): Train Loss: 0.8576, Train Acc: 84.28%, LR: 0.000003, Val Loss: 0.5577, Val Acc: 83.12%\n",
      "Epoch  57/1000 (Random-20.0%): Train Loss: 0.8565, Train Acc: 84.26%, LR: 0.000003, Val Loss: 0.5561, Val Acc: 83.18%\n",
      "Epoch  58/1000 (Random-20.0%): Train Loss: 0.8596, Train Acc: 84.13%, LR: 0.000002, Val Loss: 0.5572, Val Acc: 83.14%\n",
      "Epoch  59/1000 (Random-20.0%): Train Loss: 0.8573, Train Acc: 84.10%, LR: 0.000002, Val Loss: 0.5602, Val Acc: 82.92%\n",
      "Epoch  60/1000 (Random-20.0%): Train Loss: 0.8516, Train Acc: 84.50%, LR: 0.000002, Val Loss: 0.5574, Val Acc: 83.28% ✓\n",
      "Epoch  61/1000 (Random-20.0%): Train Loss: 0.8496, Train Acc: 84.59%, LR: 0.000002, Val Loss: 0.5599, Val Acc: 83.28%\n",
      "Epoch  62/1000 (Random-20.0%): Train Loss: 0.8546, Train Acc: 84.29%, LR: 0.000001, Val Loss: 0.5544, Val Acc: 83.38% ✓\n",
      "Epoch  63/1000 (Random-20.0%): Train Loss: 0.8535, Train Acc: 84.48%, LR: 0.000001, Val Loss: 0.5573, Val Acc: 83.24%\n",
      "Epoch  64/1000 (Random-20.0%): Train Loss: 0.8543, Train Acc: 84.22%, LR: 0.000001, Val Loss: 0.5591, Val Acc: 83.12%\n",
      "Epoch  65/1000 (Random-20.0%): Train Loss: 0.8533, Train Acc: 84.33%, LR: 0.000001, Val Loss: 0.5580, Val Acc: 83.14%\n",
      "Epoch  66/1000 (Random-20.0%): Train Loss: 0.8519, Train Acc: 84.43%, LR: 0.000000, Val Loss: 0.5569, Val Acc: 82.98%\n",
      "Epoch  67/1000 (Random-20.0%): Train Loss: 0.8546, Train Acc: 84.35%, LR: 0.000000, Val Loss: 0.5577, Val Acc: 83.36%\n",
      "Epoch  68/1000 (Random-20.0%): Train Loss: 0.8535, Train Acc: 84.35%, LR: 0.000000, Val Loss: 0.5586, Val Acc: 83.20%\n",
      "Epoch  69/1000 (Random-20.0%): Train Loss: 0.8501, Train Acc: 84.67%, LR: 0.000000, Val Loss: 0.5569, Val Acc: 83.24%\n",
      "Epoch  70/1000 (Random-20.0%): Train Loss: 0.8529, Train Acc: 84.41%, LR: 0.000000, Val Loss: 0.5582, Val Acc: 82.96%\n",
      "Epoch  71/1000 (Random-20.0%): Train Loss: 0.8542, Train Acc: 84.35%, LR: 0.000010, Val Loss: 0.5590, Val Acc: 83.24%\n",
      "Epoch  72/1000 (Random-20.0%): Train Loss: 0.8578, Train Acc: 84.31%, LR: 0.000010, Val Loss: 0.5561, Val Acc: 83.50% ✓\n",
      "Epoch  73/1000 (Random-20.0%): Train Loss: 0.8567, Train Acc: 84.24%, LR: 0.000010, Val Loss: 0.5602, Val Acc: 83.34%\n",
      "Epoch  74/1000 (Random-20.0%): Train Loss: 0.8536, Train Acc: 84.40%, LR: 0.000010, Val Loss: 0.5594, Val Acc: 83.48%\n",
      "Epoch  75/1000 (Random-20.0%): Train Loss: 0.8517, Train Acc: 84.72%, LR: 0.000010, Val Loss: 0.5523, Val Acc: 83.46%\n",
      "Epoch  76/1000 (Random-20.0%): Train Loss: 0.8513, Train Acc: 84.60%, LR: 0.000010, Val Loss: 0.5592, Val Acc: 83.40%\n",
      "Epoch  77/1000 (Random-20.0%): Train Loss: 0.8520, Train Acc: 84.48%, LR: 0.000010, Val Loss: 0.5560, Val Acc: 83.44%\n",
      "Epoch  78/1000 (Random-20.0%): Train Loss: 0.8509, Train Acc: 84.49%, LR: 0.000010, Val Loss: 0.5586, Val Acc: 83.70% ✓\n",
      "Epoch  79/1000 (Random-20.0%): Train Loss: 0.8508, Train Acc: 84.61%, LR: 0.000010, Val Loss: 0.5538, Val Acc: 83.60%\n",
      "Epoch  80/1000 (Random-20.0%): Train Loss: 0.8471, Train Acc: 84.71%, LR: 0.000010, Val Loss: 0.5616, Val Acc: 83.40%\n",
      "Epoch  81/1000 (Random-20.0%): Train Loss: 0.8446, Train Acc: 84.76%, LR: 0.000010, Val Loss: 0.5552, Val Acc: 83.38%\n",
      "Epoch  82/1000 (Random-20.0%): Train Loss: 0.8437, Train Acc: 84.90%, LR: 0.000010, Val Loss: 0.5560, Val Acc: 83.38%\n",
      "Epoch  83/1000 (Random-20.0%): Train Loss: 0.8412, Train Acc: 84.95%, LR: 0.000009, Val Loss: 0.5550, Val Acc: 83.52%\n",
      "Epoch  84/1000 (Random-20.0%): Train Loss: 0.8430, Train Acc: 84.85%, LR: 0.000009, Val Loss: 0.5530, Val Acc: 83.34%\n",
      "Epoch  85/1000 (Random-20.0%): Train Loss: 0.8385, Train Acc: 85.07%, LR: 0.000009, Val Loss: 0.5571, Val Acc: 83.18%\n",
      "Epoch  86/1000 (Random-20.0%): Train Loss: 0.8432, Train Acc: 84.92%, LR: 0.000009, Val Loss: 0.5578, Val Acc: 82.90%\n",
      "Epoch  87/1000 (Random-20.0%): Train Loss: 0.8358, Train Acc: 85.16%, LR: 0.000009, Val Loss: 0.5621, Val Acc: 83.30%\n",
      "Epoch  88/1000 (Random-20.0%): Train Loss: 0.8367, Train Acc: 85.02%, LR: 0.000009, Val Loss: 0.5590, Val Acc: 83.32%\n",
      "Epoch  89/1000 (Random-20.0%): Train Loss: 0.8349, Train Acc: 85.26%, LR: 0.000009, Val Loss: 0.5520, Val Acc: 83.66%\n",
      "Epoch  90/1000 (Random-20.0%): Train Loss: 0.8359, Train Acc: 85.16%, LR: 0.000009, Val Loss: 0.5613, Val Acc: 83.26%\n",
      "Epoch  91/1000 (Random-20.0%): Train Loss: 0.8364, Train Acc: 85.18%, LR: 0.000009, Val Loss: 0.5525, Val Acc: 83.28%\n",
      "Epoch  92/1000 (Random-20.0%): Train Loss: 0.8321, Train Acc: 85.38%, LR: 0.000008, Val Loss: 0.5604, Val Acc: 83.44%\n",
      "Epoch  93/1000 (Random-20.0%): Train Loss: 0.8293, Train Acc: 85.38%, LR: 0.000008, Val Loss: 0.5678, Val Acc: 83.46%\n",
      "Epoch  94/1000 (Random-20.0%): Train Loss: 0.8341, Train Acc: 85.32%, LR: 0.000008, Val Loss: 0.5629, Val Acc: 82.98%\n",
      "Epoch  95/1000 (Random-20.0%): Train Loss: 0.8341, Train Acc: 85.24%, LR: 0.000008, Val Loss: 0.5618, Val Acc: 83.30%\n",
      "Epoch  96/1000 (Random-20.0%): Train Loss: 0.8326, Train Acc: 85.36%, LR: 0.000008, Val Loss: 0.5670, Val Acc: 83.02%\n",
      "Epoch  97/1000 (Random-20.0%): Train Loss: 0.8305, Train Acc: 85.50%, LR: 0.000008, Val Loss: 0.5641, Val Acc: 83.40%\n",
      "Epoch  98/1000 (Random-20.0%): Train Loss: 0.8296, Train Acc: 85.32%, LR: 0.000007, Val Loss: 0.5632, Val Acc: 83.26%\n",
      "Early stopping triggered after 98 epochs (Best Val Acc: 83.70%)\n",
      "Loaded best model state (Val Acc: 83.70%)\n",
      "Results: Accuracy=82.92%, MACs=6.75M\n",
      "✅ Model saved to ./base/enhanced_random_sparsity_0.2.pth\n",
      "✅ ONNX model saved to ./base/enhanced_random_sparsity_0.2.onnx\n",
      "\n",
      "Processing Random at 50.0% sparsity...\n",
      "✅ Created MobileNetV2 without pretrained weights\n",
      "✅ Enhanced classifier created for 10 classes\n",
      "Initial MACs: 7.30M\n",
      "Applying RandomImportance pruning at 50.0% sparsity...\n",
      "Final MACs: 6.02M (Reduction: 17.6%)\n",
      "Fine-tuning pruned model with enhanced settings...\n",
      "Epoch   1/1000 (Random-50.0%): Train Loss: 1.5812, Train Acc: 54.62%, LR: 0.000010, Val Loss: 1.0694, Val Acc: 64.96% ✓\n",
      "Epoch   2/1000 (Random-50.0%): Train Loss: 1.3722, Train Acc: 61.72%, LR: 0.000010, Val Loss: 0.9699, Val Acc: 68.54% ✓\n",
      "Epoch   3/1000 (Random-50.0%): Train Loss: 1.3146, Train Acc: 63.98%, LR: 0.000009, Val Loss: 0.9056, Val Acc: 70.56% ✓\n",
      "Epoch   4/1000 (Random-50.0%): Train Loss: 1.2696, Train Acc: 65.83%, LR: 0.000008, Val Loss: 0.8660, Val Acc: 72.14% ✓\n",
      "Epoch   5/1000 (Random-50.0%): Train Loss: 1.2458, Train Acc: 66.81%, LR: 0.000007, Val Loss: 0.8477, Val Acc: 72.84% ✓\n",
      "Epoch   6/1000 (Random-50.0%): Train Loss: 1.2322, Train Acc: 67.55%, LR: 0.000005, Val Loss: 0.8344, Val Acc: 73.18% ✓\n",
      "Epoch   7/1000 (Random-50.0%): Train Loss: 1.2161, Train Acc: 68.08%, LR: 0.000004, Val Loss: 0.8226, Val Acc: 73.32% ✓\n",
      "Epoch   8/1000 (Random-50.0%): Train Loss: 1.2070, Train Acc: 68.66%, LR: 0.000002, Val Loss: 0.8197, Val Acc: 73.34% ✓\n",
      "Epoch   9/1000 (Random-50.0%): Train Loss: 1.2011, Train Acc: 68.77%, LR: 0.000001, Val Loss: 0.8162, Val Acc: 73.64% ✓\n",
      "Epoch  10/1000 (Random-50.0%): Train Loss: 1.2041, Train Acc: 68.72%, LR: 0.000000, Val Loss: 0.8137, Val Acc: 73.80% ✓\n",
      "Epoch  11/1000 (Random-50.0%): Train Loss: 1.1961, Train Acc: 69.18%, LR: 0.000010, Val Loss: 0.7929, Val Acc: 74.64% ✓\n",
      "Epoch  12/1000 (Random-50.0%): Train Loss: 1.1779, Train Acc: 69.86%, LR: 0.000010, Val Loss: 0.7790, Val Acc: 74.92% ✓\n",
      "Epoch  13/1000 (Random-50.0%): Train Loss: 1.1633, Train Acc: 70.70%, LR: 0.000010, Val Loss: 0.7618, Val Acc: 75.20% ✓\n",
      "Epoch  14/1000 (Random-50.0%): Train Loss: 1.1541, Train Acc: 70.85%, LR: 0.000009, Val Loss: 0.7499, Val Acc: 75.32% ✓\n",
      "Epoch  15/1000 (Random-50.0%): Train Loss: 1.1446, Train Acc: 71.49%, LR: 0.000009, Val Loss: 0.7386, Val Acc: 76.04% ✓\n",
      "Epoch  16/1000 (Random-50.0%): Train Loss: 1.1281, Train Acc: 71.89%, LR: 0.000009, Val Loss: 0.7309, Val Acc: 76.68% ✓\n",
      "Epoch  17/1000 (Random-50.0%): Train Loss: 1.1179, Train Acc: 72.50%, LR: 0.000008, Val Loss: 0.7229, Val Acc: 76.78% ✓\n",
      "Epoch  18/1000 (Random-50.0%): Train Loss: 1.1185, Train Acc: 72.49%, LR: 0.000007, Val Loss: 0.7168, Val Acc: 76.64%\n",
      "Epoch  19/1000 (Random-50.0%): Train Loss: 1.1126, Train Acc: 72.92%, LR: 0.000007, Val Loss: 0.7088, Val Acc: 77.44% ✓\n",
      "Epoch  20/1000 (Random-50.0%): Train Loss: 1.0991, Train Acc: 73.38%, LR: 0.000006, Val Loss: 0.7075, Val Acc: 77.00%\n",
      "Epoch  21/1000 (Random-50.0%): Train Loss: 1.0938, Train Acc: 73.37%, LR: 0.000005, Val Loss: 0.7002, Val Acc: 77.60% ✓\n",
      "Epoch  22/1000 (Random-50.0%): Train Loss: 1.0954, Train Acc: 73.34%, LR: 0.000004, Val Loss: 0.7028, Val Acc: 77.48%\n",
      "Epoch  23/1000 (Random-50.0%): Train Loss: 1.0937, Train Acc: 73.70%, LR: 0.000004, Val Loss: 0.6982, Val Acc: 77.34%\n",
      "Epoch  24/1000 (Random-50.0%): Train Loss: 1.0874, Train Acc: 73.76%, LR: 0.000003, Val Loss: 0.6980, Val Acc: 77.62% ✓\n",
      "Epoch  25/1000 (Random-50.0%): Train Loss: 1.0862, Train Acc: 74.05%, LR: 0.000002, Val Loss: 0.7001, Val Acc: 77.60%\n",
      "Epoch  26/1000 (Random-50.0%): Train Loss: 1.0827, Train Acc: 73.99%, LR: 0.000002, Val Loss: 0.6946, Val Acc: 77.82% ✓\n",
      "Epoch  27/1000 (Random-50.0%): Train Loss: 1.0870, Train Acc: 73.90%, LR: 0.000001, Val Loss: 0.6961, Val Acc: 77.46%\n",
      "Epoch  28/1000 (Random-50.0%): Train Loss: 1.0836, Train Acc: 73.94%, LR: 0.000001, Val Loss: 0.6945, Val Acc: 77.78%\n",
      "Epoch  29/1000 (Random-50.0%): Train Loss: 1.0839, Train Acc: 74.09%, LR: 0.000000, Val Loss: 0.6922, Val Acc: 77.68%\n",
      "Epoch  30/1000 (Random-50.0%): Train Loss: 1.0801, Train Acc: 74.35%, LR: 0.000000, Val Loss: 0.6958, Val Acc: 77.52%\n",
      "Epoch  31/1000 (Random-50.0%): Train Loss: 1.0801, Train Acc: 74.05%, LR: 0.000010, Val Loss: 0.6882, Val Acc: 78.10% ✓\n",
      "Epoch  32/1000 (Random-50.0%): Train Loss: 1.0785, Train Acc: 74.33%, LR: 0.000010, Val Loss: 0.6850, Val Acc: 78.26% ✓\n",
      "Epoch  33/1000 (Random-50.0%): Train Loss: 1.0760, Train Acc: 74.37%, LR: 0.000010, Val Loss: 0.6848, Val Acc: 78.04%\n",
      "Epoch  34/1000 (Random-50.0%): Train Loss: 1.0618, Train Acc: 75.03%, LR: 0.000010, Val Loss: 0.6726, Val Acc: 78.48% ✓\n",
      "Epoch  35/1000 (Random-50.0%): Train Loss: 1.0608, Train Acc: 75.01%, LR: 0.000010, Val Loss: 0.6708, Val Acc: 78.68% ✓\n",
      "Epoch  36/1000 (Random-50.0%): Train Loss: 1.0562, Train Acc: 75.30%, LR: 0.000010, Val Loss: 0.6674, Val Acc: 78.46%\n",
      "Epoch  37/1000 (Random-50.0%): Train Loss: 1.0450, Train Acc: 75.88%, LR: 0.000009, Val Loss: 0.6639, Val Acc: 78.66%\n",
      "Epoch  38/1000 (Random-50.0%): Train Loss: 1.0473, Train Acc: 75.62%, LR: 0.000009, Val Loss: 0.6634, Val Acc: 78.62%\n",
      "Epoch  39/1000 (Random-50.0%): Train Loss: 1.0455, Train Acc: 75.89%, LR: 0.000009, Val Loss: 0.6592, Val Acc: 78.98% ✓\n",
      "Epoch  40/1000 (Random-50.0%): Train Loss: 1.0450, Train Acc: 75.92%, LR: 0.000009, Val Loss: 0.6561, Val Acc: 79.08% ✓\n",
      "Epoch  41/1000 (Random-50.0%): Train Loss: 1.0391, Train Acc: 75.80%, LR: 0.000009, Val Loss: 0.6543, Val Acc: 78.96%\n",
      "Epoch  42/1000 (Random-50.0%): Train Loss: 1.0327, Train Acc: 76.29%, LR: 0.000008, Val Loss: 0.6508, Val Acc: 79.28% ✓\n",
      "Epoch  43/1000 (Random-50.0%): Train Loss: 1.0277, Train Acc: 76.35%, LR: 0.000008, Val Loss: 0.6505, Val Acc: 79.08%\n",
      "Epoch  44/1000 (Random-50.0%): Train Loss: 1.0306, Train Acc: 76.48%, LR: 0.000008, Val Loss: 0.6490, Val Acc: 79.68% ✓\n",
      "Epoch  45/1000 (Random-50.0%): Train Loss: 1.0278, Train Acc: 76.39%, LR: 0.000007, Val Loss: 0.6443, Val Acc: 79.34%\n",
      "Epoch  46/1000 (Random-50.0%): Train Loss: 1.0236, Train Acc: 76.57%, LR: 0.000007, Val Loss: 0.6417, Val Acc: 79.40%\n",
      "Epoch  47/1000 (Random-50.0%): Train Loss: 1.0206, Train Acc: 76.84%, LR: 0.000007, Val Loss: 0.6380, Val Acc: 79.42%\n",
      "Epoch  48/1000 (Random-50.0%): Train Loss: 1.0194, Train Acc: 76.83%, LR: 0.000006, Val Loss: 0.6416, Val Acc: 79.62%\n",
      "Epoch  49/1000 (Random-50.0%): Train Loss: 1.0186, Train Acc: 76.85%, LR: 0.000006, Val Loss: 0.6376, Val Acc: 79.72% ✓\n",
      "Epoch  50/1000 (Random-50.0%): Train Loss: 1.0138, Train Acc: 77.24%, LR: 0.000005, Val Loss: 0.6372, Val Acc: 79.88% ✓\n",
      "Epoch  51/1000 (Random-50.0%): Train Loss: 1.0084, Train Acc: 77.49%, LR: 0.000005, Val Loss: 0.6384, Val Acc: 79.84%\n",
      "Epoch  52/1000 (Random-50.0%): Train Loss: 1.0084, Train Acc: 77.26%, LR: 0.000005, Val Loss: 0.6346, Val Acc: 80.00% ✓\n",
      "Epoch  53/1000 (Random-50.0%): Train Loss: 1.0103, Train Acc: 77.27%, LR: 0.000004, Val Loss: 0.6355, Val Acc: 79.80%\n",
      "Epoch  54/1000 (Random-50.0%): Train Loss: 1.0095, Train Acc: 77.38%, LR: 0.000004, Val Loss: 0.6341, Val Acc: 80.04% ✓\n",
      "Epoch  55/1000 (Random-50.0%): Train Loss: 1.0080, Train Acc: 77.25%, LR: 0.000004, Val Loss: 0.6348, Val Acc: 80.10% ✓\n",
      "Epoch  56/1000 (Random-50.0%): Train Loss: 1.0058, Train Acc: 77.43%, LR: 0.000003, Val Loss: 0.6347, Val Acc: 79.82%\n",
      "Epoch  57/1000 (Random-50.0%): Train Loss: 1.0027, Train Acc: 77.50%, LR: 0.000003, Val Loss: 0.6353, Val Acc: 79.68%\n",
      "Epoch  58/1000 (Random-50.0%): Train Loss: 1.0052, Train Acc: 77.33%, LR: 0.000002, Val Loss: 0.6365, Val Acc: 79.60%\n",
      "Epoch  59/1000 (Random-50.0%): Train Loss: 1.0059, Train Acc: 77.64%, LR: 0.000002, Val Loss: 0.6338, Val Acc: 80.04%\n",
      "Epoch  60/1000 (Random-50.0%): Train Loss: 1.0008, Train Acc: 77.53%, LR: 0.000002, Val Loss: 0.6328, Val Acc: 80.10%\n",
      "Epoch  61/1000 (Random-50.0%): Train Loss: 1.0046, Train Acc: 77.47%, LR: 0.000002, Val Loss: 0.6300, Val Acc: 79.66%\n",
      "Epoch  62/1000 (Random-50.0%): Train Loss: 1.0047, Train Acc: 77.49%, LR: 0.000001, Val Loss: 0.6342, Val Acc: 80.18% ✓\n",
      "Epoch  63/1000 (Random-50.0%): Train Loss: 1.0026, Train Acc: 77.71%, LR: 0.000001, Val Loss: 0.6326, Val Acc: 80.06%\n",
      "Epoch  64/1000 (Random-50.0%): Train Loss: 1.0039, Train Acc: 77.61%, LR: 0.000001, Val Loss: 0.6329, Val Acc: 79.92%\n",
      "Epoch  65/1000 (Random-50.0%): Train Loss: 1.0040, Train Acc: 77.51%, LR: 0.000001, Val Loss: 0.6326, Val Acc: 79.98%\n",
      "Epoch  66/1000 (Random-50.0%): Train Loss: 0.9921, Train Acc: 78.13%, LR: 0.000000, Val Loss: 0.6342, Val Acc: 80.02%\n",
      "Epoch  67/1000 (Random-50.0%): Train Loss: 1.0025, Train Acc: 77.46%, LR: 0.000000, Val Loss: 0.6337, Val Acc: 79.72%\n",
      "Epoch  68/1000 (Random-50.0%): Train Loss: 0.9994, Train Acc: 77.62%, LR: 0.000000, Val Loss: 0.6333, Val Acc: 80.16%\n",
      "Epoch  69/1000 (Random-50.0%): Train Loss: 0.9989, Train Acc: 77.59%, LR: 0.000000, Val Loss: 0.6339, Val Acc: 80.18%\n",
      "Epoch  70/1000 (Random-50.0%): Train Loss: 0.9960, Train Acc: 78.08%, LR: 0.000000, Val Loss: 0.6340, Val Acc: 79.86%\n",
      "Epoch  71/1000 (Random-50.0%): Train Loss: 1.0020, Train Acc: 77.62%, LR: 0.000010, Val Loss: 0.6298, Val Acc: 79.74%\n",
      "Epoch  72/1000 (Random-50.0%): Train Loss: 1.0032, Train Acc: 77.54%, LR: 0.000010, Val Loss: 0.6350, Val Acc: 80.02%\n",
      "Epoch  73/1000 (Random-50.0%): Train Loss: 0.9997, Train Acc: 77.74%, LR: 0.000010, Val Loss: 0.6244, Val Acc: 80.16%\n",
      "Epoch  74/1000 (Random-50.0%): Train Loss: 0.9978, Train Acc: 77.64%, LR: 0.000010, Val Loss: 0.6267, Val Acc: 79.94%\n",
      "Epoch  75/1000 (Random-50.0%): Train Loss: 0.9980, Train Acc: 77.84%, LR: 0.000010, Val Loss: 0.6268, Val Acc: 79.78%\n",
      "Epoch  76/1000 (Random-50.0%): Train Loss: 0.9919, Train Acc: 78.09%, LR: 0.000010, Val Loss: 0.6263, Val Acc: 79.92%\n",
      "Epoch  77/1000 (Random-50.0%): Train Loss: 0.9899, Train Acc: 78.31%, LR: 0.000010, Val Loss: 0.6231, Val Acc: 80.18%\n",
      "Epoch  78/1000 (Random-50.0%): Train Loss: 0.9815, Train Acc: 78.66%, LR: 0.000010, Val Loss: 0.6188, Val Acc: 80.30% ✓\n",
      "Epoch  79/1000 (Random-50.0%): Train Loss: 0.9884, Train Acc: 78.29%, LR: 0.000010, Val Loss: 0.6209, Val Acc: 80.48% ✓\n",
      "Epoch  80/1000 (Random-50.0%): Train Loss: 0.9856, Train Acc: 78.41%, LR: 0.000010, Val Loss: 0.6210, Val Acc: 80.56% ✓\n",
      "Epoch  81/1000 (Random-50.0%): Train Loss: 0.9837, Train Acc: 78.36%, LR: 0.000010, Val Loss: 0.6195, Val Acc: 80.76% ✓\n",
      "Epoch  82/1000 (Random-50.0%): Train Loss: 0.9820, Train Acc: 78.60%, LR: 0.000010, Val Loss: 0.6166, Val Acc: 80.68%\n",
      "Epoch  83/1000 (Random-50.0%): Train Loss: 0.9796, Train Acc: 78.79%, LR: 0.000009, Val Loss: 0.6165, Val Acc: 80.72%\n",
      "Epoch  84/1000 (Random-50.0%): Train Loss: 0.9739, Train Acc: 78.78%, LR: 0.000009, Val Loss: 0.6152, Val Acc: 80.80% ✓\n",
      "Epoch  85/1000 (Random-50.0%): Train Loss: 0.9711, Train Acc: 79.14%, LR: 0.000009, Val Loss: 0.6145, Val Acc: 80.58%\n",
      "Epoch  86/1000 (Random-50.0%): Train Loss: 0.9729, Train Acc: 78.96%, LR: 0.000009, Val Loss: 0.6139, Val Acc: 80.56%\n",
      "Epoch  87/1000 (Random-50.0%): Train Loss: 0.9712, Train Acc: 79.09%, LR: 0.000009, Val Loss: 0.6104, Val Acc: 80.64%\n",
      "Epoch  88/1000 (Random-50.0%): Train Loss: 0.9721, Train Acc: 79.20%, LR: 0.000009, Val Loss: 0.6160, Val Acc: 80.80%\n",
      "Epoch  89/1000 (Random-50.0%): Train Loss: 0.9596, Train Acc: 79.40%, LR: 0.000009, Val Loss: 0.6118, Val Acc: 80.68%\n",
      "Epoch  90/1000 (Random-50.0%): Train Loss: 0.9687, Train Acc: 79.12%, LR: 0.000009, Val Loss: 0.6078, Val Acc: 80.84% ✓\n",
      "Epoch  91/1000 (Random-50.0%): Train Loss: 0.9617, Train Acc: 79.53%, LR: 0.000009, Val Loss: 0.6095, Val Acc: 81.04% ✓\n",
      "Epoch  92/1000 (Random-50.0%): Train Loss: 0.9648, Train Acc: 79.43%, LR: 0.000008, Val Loss: 0.6111, Val Acc: 80.98%\n",
      "Epoch  93/1000 (Random-50.0%): Train Loss: 0.9625, Train Acc: 79.35%, LR: 0.000008, Val Loss: 0.6086, Val Acc: 81.02%\n",
      "Epoch  94/1000 (Random-50.0%): Train Loss: 0.9654, Train Acc: 79.41%, LR: 0.000008, Val Loss: 0.6086, Val Acc: 80.94%\n",
      "Epoch  95/1000 (Random-50.0%): Train Loss: 0.9599, Train Acc: 79.54%, LR: 0.000008, Val Loss: 0.6083, Val Acc: 80.74%\n",
      "Epoch  96/1000 (Random-50.0%): Train Loss: 0.9578, Train Acc: 79.53%, LR: 0.000008, Val Loss: 0.6073, Val Acc: 80.78%\n",
      "Epoch  97/1000 (Random-50.0%): Train Loss: 0.9509, Train Acc: 79.96%, LR: 0.000008, Val Loss: 0.6042, Val Acc: 80.76%\n",
      "Epoch  98/1000 (Random-50.0%): Train Loss: 0.9555, Train Acc: 79.60%, LR: 0.000007, Val Loss: 0.6045, Val Acc: 81.12% ✓\n",
      "Epoch  99/1000 (Random-50.0%): Train Loss: 0.9537, Train Acc: 79.77%, LR: 0.000007, Val Loss: 0.6061, Val Acc: 81.30% ✓\n",
      "Epoch 100/1000 (Random-50.0%): Train Loss: 0.9513, Train Acc: 80.04%, LR: 0.000007, Val Loss: 0.6023, Val Acc: 81.00%\n",
      "Epoch 101/1000 (Random-50.0%): Train Loss: 0.9503, Train Acc: 79.97%, LR: 0.000007, Val Loss: 0.6045, Val Acc: 80.96%\n",
      "Epoch 102/1000 (Random-50.0%): Train Loss: 0.9494, Train Acc: 80.00%, LR: 0.000007, Val Loss: 0.6045, Val Acc: 80.90%\n",
      "Epoch 103/1000 (Random-50.0%): Train Loss: 0.9505, Train Acc: 79.93%, LR: 0.000007, Val Loss: 0.6052, Val Acc: 81.02%\n",
      "Epoch 104/1000 (Random-50.0%): Train Loss: 0.9477, Train Acc: 80.16%, LR: 0.000006, Val Loss: 0.6045, Val Acc: 81.08%\n",
      "Epoch 105/1000 (Random-50.0%): Train Loss: 0.9411, Train Acc: 80.26%, LR: 0.000006, Val Loss: 0.6082, Val Acc: 80.98%\n",
      "Epoch 106/1000 (Random-50.0%): Train Loss: 0.9482, Train Acc: 80.09%, LR: 0.000006, Val Loss: 0.6012, Val Acc: 81.32% ✓\n",
      "Epoch 107/1000 (Random-50.0%): Train Loss: 0.9439, Train Acc: 80.34%, LR: 0.000006, Val Loss: 0.6056, Val Acc: 80.74%\n",
      "Epoch 108/1000 (Random-50.0%): Train Loss: 0.9416, Train Acc: 80.30%, LR: 0.000006, Val Loss: 0.6001, Val Acc: 81.00%\n",
      "Epoch 109/1000 (Random-50.0%): Train Loss: 0.9421, Train Acc: 80.29%, LR: 0.000005, Val Loss: 0.6006, Val Acc: 81.00%\n",
      "Epoch 110/1000 (Random-50.0%): Train Loss: 0.9381, Train Acc: 80.47%, LR: 0.000005, Val Loss: 0.6044, Val Acc: 80.90%\n",
      "Epoch 111/1000 (Random-50.0%): Train Loss: 0.9358, Train Acc: 80.56%, LR: 0.000005, Val Loss: 0.6023, Val Acc: 81.06%\n",
      "Epoch 112/1000 (Random-50.0%): Train Loss: 0.9383, Train Acc: 80.53%, LR: 0.000005, Val Loss: 0.6013, Val Acc: 81.14%\n",
      "Epoch 113/1000 (Random-50.0%): Train Loss: 0.9385, Train Acc: 80.86%, LR: 0.000005, Val Loss: 0.6046, Val Acc: 81.08%\n",
      "Epoch 114/1000 (Random-50.0%): Train Loss: 0.9379, Train Acc: 80.53%, LR: 0.000004, Val Loss: 0.6044, Val Acc: 80.98%\n",
      "Epoch 115/1000 (Random-50.0%): Train Loss: 0.9346, Train Acc: 80.73%, LR: 0.000004, Val Loss: 0.6023, Val Acc: 80.98%\n",
      "Epoch 116/1000 (Random-50.0%): Train Loss: 0.9383, Train Acc: 80.50%, LR: 0.000004, Val Loss: 0.6018, Val Acc: 81.04%\n",
      "Epoch 117/1000 (Random-50.0%): Train Loss: 0.9374, Train Acc: 80.64%, LR: 0.000004, Val Loss: 0.5994, Val Acc: 81.14%\n",
      "Epoch 118/1000 (Random-50.0%): Train Loss: 0.9316, Train Acc: 80.74%, LR: 0.000004, Val Loss: 0.6014, Val Acc: 81.10%\n",
      "Epoch 119/1000 (Random-50.0%): Train Loss: 0.9347, Train Acc: 80.76%, LR: 0.000004, Val Loss: 0.6021, Val Acc: 81.02%\n",
      "Epoch 120/1000 (Random-50.0%): Train Loss: 0.9384, Train Acc: 80.55%, LR: 0.000003, Val Loss: 0.5976, Val Acc: 81.04%\n",
      "Epoch 121/1000 (Random-50.0%): Train Loss: 0.9288, Train Acc: 81.07%, LR: 0.000003, Val Loss: 0.6025, Val Acc: 80.92%\n",
      "Epoch 122/1000 (Random-50.0%): Train Loss: 0.9329, Train Acc: 80.58%, LR: 0.000003, Val Loss: 0.5979, Val Acc: 81.18%\n",
      "Epoch 123/1000 (Random-50.0%): Train Loss: 0.9346, Train Acc: 80.82%, LR: 0.000003, Val Loss: 0.6012, Val Acc: 81.36% ✓\n",
      "Epoch 124/1000 (Random-50.0%): Train Loss: 0.9298, Train Acc: 80.99%, LR: 0.000003, Val Loss: 0.5995, Val Acc: 81.12%\n",
      "Epoch 125/1000 (Random-50.0%): Train Loss: 0.9291, Train Acc: 81.18%, LR: 0.000002, Val Loss: 0.6039, Val Acc: 81.30%\n",
      "Epoch 126/1000 (Random-50.0%): Train Loss: 0.9290, Train Acc: 81.17%, LR: 0.000002, Val Loss: 0.6017, Val Acc: 81.10%\n",
      "Epoch 127/1000 (Random-50.0%): Train Loss: 0.9256, Train Acc: 81.14%, LR: 0.000002, Val Loss: 0.6019, Val Acc: 81.26%\n",
      "Epoch 128/1000 (Random-50.0%): Train Loss: 0.9269, Train Acc: 80.97%, LR: 0.000002, Val Loss: 0.6024, Val Acc: 81.28%\n",
      "Epoch 129/1000 (Random-50.0%): Train Loss: 0.9310, Train Acc: 80.83%, LR: 0.000002, Val Loss: 0.5991, Val Acc: 81.10%\n",
      "Epoch 130/1000 (Random-50.0%): Train Loss: 0.9257, Train Acc: 81.21%, LR: 0.000002, Val Loss: 0.6016, Val Acc: 81.20%\n",
      "Epoch 131/1000 (Random-50.0%): Train Loss: 0.9298, Train Acc: 81.21%, LR: 0.000002, Val Loss: 0.6036, Val Acc: 81.12%\n",
      "Epoch 132/1000 (Random-50.0%): Train Loss: 0.9269, Train Acc: 80.98%, LR: 0.000001, Val Loss: 0.6008, Val Acc: 81.20%\n",
      "Epoch 133/1000 (Random-50.0%): Train Loss: 0.9272, Train Acc: 81.00%, LR: 0.000001, Val Loss: 0.5972, Val Acc: 81.44% ✓\n",
      "Epoch 134/1000 (Random-50.0%): Train Loss: 0.9257, Train Acc: 81.29%, LR: 0.000001, Val Loss: 0.5983, Val Acc: 81.44%\n",
      "Epoch 135/1000 (Random-50.0%): Train Loss: 0.9213, Train Acc: 81.37%, LR: 0.000001, Val Loss: 0.5993, Val Acc: 81.48% ✓\n",
      "Epoch 136/1000 (Random-50.0%): Train Loss: 0.9324, Train Acc: 80.82%, LR: 0.000001, Val Loss: 0.6028, Val Acc: 81.16%\n",
      "Epoch 137/1000 (Random-50.0%): Train Loss: 0.9227, Train Acc: 81.30%, LR: 0.000001, Val Loss: 0.6032, Val Acc: 80.92%\n",
      "Epoch 138/1000 (Random-50.0%): Train Loss: 0.9300, Train Acc: 81.05%, LR: 0.000001, Val Loss: 0.6017, Val Acc: 81.00%\n",
      "Epoch 139/1000 (Random-50.0%): Train Loss: 0.9272, Train Acc: 81.03%, LR: 0.000001, Val Loss: 0.6038, Val Acc: 80.96%\n",
      "Epoch 140/1000 (Random-50.0%): Train Loss: 0.9252, Train Acc: 81.20%, LR: 0.000001, Val Loss: 0.5992, Val Acc: 81.36%\n",
      "Epoch 141/1000 (Random-50.0%): Train Loss: 0.9280, Train Acc: 80.97%, LR: 0.000000, Val Loss: 0.5991, Val Acc: 81.02%\n",
      "Epoch 142/1000 (Random-50.0%): Train Loss: 0.9245, Train Acc: 81.28%, LR: 0.000000, Val Loss: 0.6013, Val Acc: 81.12%\n",
      "Epoch 143/1000 (Random-50.0%): Train Loss: 0.9286, Train Acc: 80.89%, LR: 0.000000, Val Loss: 0.6012, Val Acc: 81.30%\n",
      "Epoch 144/1000 (Random-50.0%): Train Loss: 0.9272, Train Acc: 81.23%, LR: 0.000000, Val Loss: 0.5995, Val Acc: 81.32%\n",
      "Epoch 145/1000 (Random-50.0%): Train Loss: 0.9224, Train Acc: 81.29%, LR: 0.000000, Val Loss: 0.6005, Val Acc: 81.26%\n",
      "Epoch 146/1000 (Random-50.0%): Train Loss: 0.9241, Train Acc: 81.06%, LR: 0.000000, Val Loss: 0.6002, Val Acc: 81.14%\n",
      "Epoch 147/1000 (Random-50.0%): Train Loss: 0.9276, Train Acc: 81.09%, LR: 0.000000, Val Loss: 0.6051, Val Acc: 81.12%\n",
      "Epoch 148/1000 (Random-50.0%): Train Loss: 0.9295, Train Acc: 80.92%, LR: 0.000000, Val Loss: 0.5978, Val Acc: 81.40%\n",
      "Epoch 149/1000 (Random-50.0%): Train Loss: 0.9273, Train Acc: 81.06%, LR: 0.000000, Val Loss: 0.5980, Val Acc: 81.38%\n",
      "Epoch 150/1000 (Random-50.0%): Train Loss: 0.9243, Train Acc: 81.29%, LR: 0.000000, Val Loss: 0.5970, Val Acc: 81.26%\n",
      "Epoch 151/1000 (Random-50.0%): Train Loss: 0.9293, Train Acc: 80.79%, LR: 0.000010, Val Loss: 0.6029, Val Acc: 81.46%\n",
      "Epoch 152/1000 (Random-50.0%): Train Loss: 0.9276, Train Acc: 81.05%, LR: 0.000010, Val Loss: 0.6029, Val Acc: 81.36%\n",
      "Epoch 153/1000 (Random-50.0%): Train Loss: 0.9308, Train Acc: 80.91%, LR: 0.000010, Val Loss: 0.6032, Val Acc: 81.36%\n",
      "Epoch 154/1000 (Random-50.0%): Train Loss: 0.9230, Train Acc: 81.22%, LR: 0.000010, Val Loss: 0.6002, Val Acc: 81.36%\n",
      "Epoch 155/1000 (Random-50.0%): Train Loss: 0.9284, Train Acc: 80.78%, LR: 0.000010, Val Loss: 0.5952, Val Acc: 81.22%\n",
      "Early stopping triggered after 155 epochs (Best Val Acc: 81.48%)\n",
      "Loaded best model state (Val Acc: 81.48%)\n",
      "Results: Accuracy=81.73%, MACs=6.02M\n",
      "✅ Model saved to ./base/enhanced_random_sparsity_0.5.pth\n",
      "✅ ONNX model saved to ./base/enhanced_random_sparsity_0.5.onnx\n",
      "\n",
      "Processing Random at 70.0% sparsity...\n",
      "✅ Created MobileNetV2 without pretrained weights\n",
      "✅ Enhanced classifier created for 10 classes\n",
      "Initial MACs: 7.30M\n",
      "Applying RandomImportance pruning at 70.0% sparsity...\n",
      "Final MACs: 5.56M (Reduction: 23.8%)\n",
      "Fine-tuning pruned model with enhanced settings...\n",
      "Epoch   1/1000 (Random-70.0%): Train Loss: 1.8457, Train Acc: 44.80%, LR: 0.000010, Val Loss: 1.2705, Val Acc: 57.98% ✓\n",
      "Epoch   2/1000 (Random-70.0%): Train Loss: 1.5735, Train Acc: 52.50%, LR: 0.000010, Val Loss: 1.1623, Val Acc: 62.08% ✓\n",
      "Epoch   3/1000 (Random-70.0%): Train Loss: 1.5006, Train Acc: 55.63%, LR: 0.000009, Val Loss: 1.0913, Val Acc: 64.34% ✓\n",
      "Epoch   4/1000 (Random-70.0%): Train Loss: 1.4603, Train Acc: 57.57%, LR: 0.000008, Val Loss: 1.0484, Val Acc: 65.30% ✓\n",
      "Epoch   5/1000 (Random-70.0%): Train Loss: 1.4189, Train Acc: 59.47%, LR: 0.000007, Val Loss: 1.0065, Val Acc: 66.68% ✓\n",
      "Epoch   6/1000 (Random-70.0%): Train Loss: 1.3944, Train Acc: 60.42%, LR: 0.000005, Val Loss: 0.9940, Val Acc: 67.56% ✓\n",
      "Epoch   7/1000 (Random-70.0%): Train Loss: 1.3864, Train Acc: 60.55%, LR: 0.000004, Val Loss: 0.9846, Val Acc: 67.76% ✓\n",
      "Epoch   8/1000 (Random-70.0%): Train Loss: 1.3728, Train Acc: 61.24%, LR: 0.000002, Val Loss: 0.9775, Val Acc: 67.88% ✓\n",
      "Epoch   9/1000 (Random-70.0%): Train Loss: 1.3727, Train Acc: 61.37%, LR: 0.000001, Val Loss: 0.9708, Val Acc: 68.22% ✓\n",
      "Epoch  10/1000 (Random-70.0%): Train Loss: 1.3665, Train Acc: 61.35%, LR: 0.000000, Val Loss: 0.9693, Val Acc: 68.64% ✓\n",
      "Epoch  11/1000 (Random-70.0%): Train Loss: 1.3598, Train Acc: 61.70%, LR: 0.000010, Val Loss: 0.9459, Val Acc: 69.02% ✓\n",
      "Epoch  12/1000 (Random-70.0%): Train Loss: 1.3287, Train Acc: 63.12%, LR: 0.000010, Val Loss: 0.9276, Val Acc: 69.76% ✓\n",
      "Epoch  13/1000 (Random-70.0%): Train Loss: 1.3145, Train Acc: 63.81%, LR: 0.000010, Val Loss: 0.9068, Val Acc: 70.60% ✓\n",
      "Epoch  14/1000 (Random-70.0%): Train Loss: 1.2942, Train Acc: 64.54%, LR: 0.000009, Val Loss: 0.8810, Val Acc: 71.22% ✓\n",
      "Epoch  15/1000 (Random-70.0%): Train Loss: 1.2807, Train Acc: 65.39%, LR: 0.000009, Val Loss: 0.8689, Val Acc: 71.74% ✓\n",
      "Epoch  16/1000 (Random-70.0%): Train Loss: 1.2676, Train Acc: 65.86%, LR: 0.000009, Val Loss: 0.8547, Val Acc: 72.52% ✓\n",
      "Epoch  17/1000 (Random-70.0%): Train Loss: 1.2556, Train Acc: 66.32%, LR: 0.000008, Val Loss: 0.8448, Val Acc: 72.46%\n",
      "Epoch  18/1000 (Random-70.0%): Train Loss: 1.2436, Train Acc: 66.87%, LR: 0.000007, Val Loss: 0.8339, Val Acc: 72.94% ✓\n",
      "Epoch  19/1000 (Random-70.0%): Train Loss: 1.2332, Train Acc: 67.53%, LR: 0.000007, Val Loss: 0.8190, Val Acc: 73.56% ✓\n",
      "Epoch  20/1000 (Random-70.0%): Train Loss: 1.2344, Train Acc: 67.10%, LR: 0.000006, Val Loss: 0.8213, Val Acc: 73.44%\n",
      "Epoch  21/1000 (Random-70.0%): Train Loss: 1.2231, Train Acc: 67.80%, LR: 0.000005, Val Loss: 0.8164, Val Acc: 73.58% ✓\n",
      "Epoch  22/1000 (Random-70.0%): Train Loss: 1.2173, Train Acc: 68.01%, LR: 0.000004, Val Loss: 0.8070, Val Acc: 73.72% ✓\n",
      "Epoch  23/1000 (Random-70.0%): Train Loss: 1.2128, Train Acc: 68.13%, LR: 0.000004, Val Loss: 0.8070, Val Acc: 74.20% ✓\n",
      "Epoch  24/1000 (Random-70.0%): Train Loss: 1.2058, Train Acc: 68.65%, LR: 0.000003, Val Loss: 0.8043, Val Acc: 74.10%\n",
      "Epoch  25/1000 (Random-70.0%): Train Loss: 1.2043, Train Acc: 68.51%, LR: 0.000002, Val Loss: 0.8038, Val Acc: 74.10%\n",
      "Epoch  26/1000 (Random-70.0%): Train Loss: 1.2027, Train Acc: 68.45%, LR: 0.000002, Val Loss: 0.7965, Val Acc: 74.32% ✓\n",
      "Epoch  27/1000 (Random-70.0%): Train Loss: 1.2041, Train Acc: 68.62%, LR: 0.000001, Val Loss: 0.8038, Val Acc: 73.80%\n",
      "Epoch  28/1000 (Random-70.0%): Train Loss: 1.2018, Train Acc: 68.77%, LR: 0.000001, Val Loss: 0.7962, Val Acc: 74.32%\n",
      "Epoch  29/1000 (Random-70.0%): Train Loss: 1.2060, Train Acc: 68.60%, LR: 0.000000, Val Loss: 0.7989, Val Acc: 74.12%\n",
      "Epoch  30/1000 (Random-70.0%): Train Loss: 1.2029, Train Acc: 68.68%, LR: 0.000000, Val Loss: 0.7948, Val Acc: 74.36% ✓\n",
      "Epoch  31/1000 (Random-70.0%): Train Loss: 1.2018, Train Acc: 68.69%, LR: 0.000010, Val Loss: 0.7929, Val Acc: 74.28%\n",
      "Epoch  32/1000 (Random-70.0%): Train Loss: 1.1856, Train Acc: 69.30%, LR: 0.000010, Val Loss: 0.7821, Val Acc: 74.52% ✓\n",
      "Epoch  33/1000 (Random-70.0%): Train Loss: 1.1817, Train Acc: 69.87%, LR: 0.000010, Val Loss: 0.7828, Val Acc: 74.64% ✓\n",
      "Epoch  34/1000 (Random-70.0%): Train Loss: 1.1740, Train Acc: 69.99%, LR: 0.000010, Val Loss: 0.7711, Val Acc: 75.34% ✓\n",
      "Epoch  35/1000 (Random-70.0%): Train Loss: 1.1726, Train Acc: 69.99%, LR: 0.000010, Val Loss: 0.7658, Val Acc: 75.46% ✓\n",
      "Epoch  36/1000 (Random-70.0%): Train Loss: 1.1651, Train Acc: 70.32%, LR: 0.000010, Val Loss: 0.7573, Val Acc: 75.80% ✓\n",
      "Epoch  37/1000 (Random-70.0%): Train Loss: 1.1553, Train Acc: 70.67%, LR: 0.000009, Val Loss: 0.7529, Val Acc: 76.06% ✓\n",
      "Epoch  38/1000 (Random-70.0%): Train Loss: 1.1530, Train Acc: 70.94%, LR: 0.000009, Val Loss: 0.7487, Val Acc: 76.36% ✓\n",
      "Epoch  39/1000 (Random-70.0%): Train Loss: 1.1495, Train Acc: 70.95%, LR: 0.000009, Val Loss: 0.7442, Val Acc: 76.66% ✓\n",
      "Epoch  40/1000 (Random-70.0%): Train Loss: 1.1418, Train Acc: 71.15%, LR: 0.000009, Val Loss: 0.7372, Val Acc: 76.64%\n",
      "Epoch  41/1000 (Random-70.0%): Train Loss: 1.1397, Train Acc: 71.51%, LR: 0.000009, Val Loss: 0.7326, Val Acc: 77.18% ✓\n",
      "Epoch  42/1000 (Random-70.0%): Train Loss: 1.1281, Train Acc: 72.02%, LR: 0.000008, Val Loss: 0.7277, Val Acc: 76.92%\n",
      "Epoch  43/1000 (Random-70.0%): Train Loss: 1.1359, Train Acc: 71.44%, LR: 0.000008, Val Loss: 0.7284, Val Acc: 77.20% ✓\n",
      "Epoch  44/1000 (Random-70.0%): Train Loss: 1.1264, Train Acc: 72.10%, LR: 0.000008, Val Loss: 0.7210, Val Acc: 77.12%\n",
      "Epoch  45/1000 (Random-70.0%): Train Loss: 1.1222, Train Acc: 72.18%, LR: 0.000007, Val Loss: 0.7262, Val Acc: 77.12%\n",
      "Epoch  46/1000 (Random-70.0%): Train Loss: 1.1196, Train Acc: 72.36%, LR: 0.000007, Val Loss: 0.7183, Val Acc: 77.24% ✓\n",
      "Epoch  47/1000 (Random-70.0%): Train Loss: 1.1120, Train Acc: 72.77%, LR: 0.000007, Val Loss: 0.7151, Val Acc: 77.32% ✓\n",
      "Epoch  48/1000 (Random-70.0%): Train Loss: 1.1088, Train Acc: 72.88%, LR: 0.000006, Val Loss: 0.7134, Val Acc: 77.36% ✓\n",
      "Epoch  49/1000 (Random-70.0%): Train Loss: 1.1074, Train Acc: 72.92%, LR: 0.000006, Val Loss: 0.7108, Val Acc: 77.24%\n",
      "Epoch  50/1000 (Random-70.0%): Train Loss: 1.1051, Train Acc: 73.13%, LR: 0.000005, Val Loss: 0.7107, Val Acc: 77.44% ✓\n",
      "Epoch  51/1000 (Random-70.0%): Train Loss: 1.1027, Train Acc: 73.16%, LR: 0.000005, Val Loss: 0.7044, Val Acc: 77.98% ✓\n",
      "Epoch  52/1000 (Random-70.0%): Train Loss: 1.1038, Train Acc: 73.26%, LR: 0.000005, Val Loss: 0.7061, Val Acc: 77.86%\n",
      "Epoch  53/1000 (Random-70.0%): Train Loss: 1.1013, Train Acc: 73.18%, LR: 0.000004, Val Loss: 0.7020, Val Acc: 77.94%\n",
      "Epoch  54/1000 (Random-70.0%): Train Loss: 1.0976, Train Acc: 73.49%, LR: 0.000004, Val Loss: 0.7013, Val Acc: 77.76%\n",
      "Epoch  55/1000 (Random-70.0%): Train Loss: 1.0954, Train Acc: 73.40%, LR: 0.000004, Val Loss: 0.6998, Val Acc: 77.74%\n",
      "Epoch  56/1000 (Random-70.0%): Train Loss: 1.0945, Train Acc: 73.44%, LR: 0.000003, Val Loss: 0.6991, Val Acc: 77.70%\n",
      "Epoch  57/1000 (Random-70.0%): Train Loss: 1.0921, Train Acc: 73.72%, LR: 0.000003, Val Loss: 0.7019, Val Acc: 77.86%\n",
      "Epoch  58/1000 (Random-70.0%): Train Loss: 1.0917, Train Acc: 73.97%, LR: 0.000002, Val Loss: 0.7002, Val Acc: 77.90%\n",
      "Epoch  59/1000 (Random-70.0%): Train Loss: 1.0976, Train Acc: 73.38%, LR: 0.000002, Val Loss: 0.6963, Val Acc: 78.04% ✓\n",
      "Epoch  60/1000 (Random-70.0%): Train Loss: 1.0887, Train Acc: 73.74%, LR: 0.000002, Val Loss: 0.6933, Val Acc: 78.00%\n",
      "Epoch  61/1000 (Random-70.0%): Train Loss: 1.0911, Train Acc: 73.69%, LR: 0.000002, Val Loss: 0.6950, Val Acc: 78.02%\n",
      "Epoch  62/1000 (Random-70.0%): Train Loss: 1.0923, Train Acc: 73.56%, LR: 0.000001, Val Loss: 0.6970, Val Acc: 78.08% ✓\n",
      "Epoch  63/1000 (Random-70.0%): Train Loss: 1.0921, Train Acc: 73.71%, LR: 0.000001, Val Loss: 0.6968, Val Acc: 78.00%\n",
      "Epoch  64/1000 (Random-70.0%): Train Loss: 1.0894, Train Acc: 73.73%, LR: 0.000001, Val Loss: 0.6965, Val Acc: 78.02%\n",
      "Epoch  65/1000 (Random-70.0%): Train Loss: 1.0884, Train Acc: 73.82%, LR: 0.000001, Val Loss: 0.7035, Val Acc: 77.66%\n",
      "Epoch  66/1000 (Random-70.0%): Train Loss: 1.0878, Train Acc: 73.81%, LR: 0.000000, Val Loss: 0.6952, Val Acc: 78.08%\n",
      "Epoch  67/1000 (Random-70.0%): Train Loss: 1.0914, Train Acc: 73.64%, LR: 0.000000, Val Loss: 0.6961, Val Acc: 78.16% ✓\n",
      "Epoch  68/1000 (Random-70.0%): Train Loss: 1.0898, Train Acc: 73.77%, LR: 0.000000, Val Loss: 0.6994, Val Acc: 77.76%\n",
      "Epoch  69/1000 (Random-70.0%): Train Loss: 1.0892, Train Acc: 73.79%, LR: 0.000000, Val Loss: 0.6946, Val Acc: 77.92%\n",
      "Epoch  70/1000 (Random-70.0%): Train Loss: 1.0893, Train Acc: 73.84%, LR: 0.000000, Val Loss: 0.6948, Val Acc: 78.08%\n",
      "Epoch  71/1000 (Random-70.0%): Train Loss: 1.0885, Train Acc: 73.78%, LR: 0.000010, Val Loss: 0.6967, Val Acc: 78.16%\n",
      "Epoch  72/1000 (Random-70.0%): Train Loss: 1.0847, Train Acc: 74.09%, LR: 0.000010, Val Loss: 0.6897, Val Acc: 78.54% ✓\n",
      "Epoch  73/1000 (Random-70.0%): Train Loss: 1.0791, Train Acc: 74.09%, LR: 0.000010, Val Loss: 0.6917, Val Acc: 78.16%\n",
      "Epoch  74/1000 (Random-70.0%): Train Loss: 1.0855, Train Acc: 73.86%, LR: 0.000010, Val Loss: 0.6875, Val Acc: 78.62% ✓\n",
      "Epoch  75/1000 (Random-70.0%): Train Loss: 1.0749, Train Acc: 74.41%, LR: 0.000010, Val Loss: 0.6850, Val Acc: 78.54%\n",
      "Epoch  76/1000 (Random-70.0%): Train Loss: 1.0741, Train Acc: 74.61%, LR: 0.000010, Val Loss: 0.6813, Val Acc: 78.74% ✓\n",
      "Epoch  77/1000 (Random-70.0%): Train Loss: 1.0762, Train Acc: 74.34%, LR: 0.000010, Val Loss: 0.6807, Val Acc: 78.74%\n",
      "Epoch  78/1000 (Random-70.0%): Train Loss: 1.0695, Train Acc: 74.54%, LR: 0.000010, Val Loss: 0.6777, Val Acc: 78.96% ✓\n",
      "Epoch  79/1000 (Random-70.0%): Train Loss: 1.0621, Train Acc: 74.89%, LR: 0.000010, Val Loss: 0.6782, Val Acc: 79.24% ✓\n",
      "Epoch  80/1000 (Random-70.0%): Train Loss: 1.0663, Train Acc: 74.77%, LR: 0.000010, Val Loss: 0.6772, Val Acc: 78.66%\n",
      "Epoch  81/1000 (Random-70.0%): Train Loss: 1.0640, Train Acc: 74.87%, LR: 0.000010, Val Loss: 0.6767, Val Acc: 79.00%\n",
      "Epoch  82/1000 (Random-70.0%): Train Loss: 1.0618, Train Acc: 74.98%, LR: 0.000010, Val Loss: 0.6720, Val Acc: 78.96%\n",
      "Epoch  83/1000 (Random-70.0%): Train Loss: 1.0561, Train Acc: 75.11%, LR: 0.000009, Val Loss: 0.6704, Val Acc: 79.32% ✓\n",
      "Epoch  84/1000 (Random-70.0%): Train Loss: 1.0553, Train Acc: 75.44%, LR: 0.000009, Val Loss: 0.6713, Val Acc: 79.04%\n",
      "Epoch  85/1000 (Random-70.0%): Train Loss: 1.0483, Train Acc: 75.75%, LR: 0.000009, Val Loss: 0.6693, Val Acc: 79.04%\n",
      "Epoch  86/1000 (Random-70.0%): Train Loss: 1.0461, Train Acc: 75.82%, LR: 0.000009, Val Loss: 0.6670, Val Acc: 79.24%\n",
      "Epoch  87/1000 (Random-70.0%): Train Loss: 1.0406, Train Acc: 75.96%, LR: 0.000009, Val Loss: 0.6641, Val Acc: 79.12%\n",
      "Epoch  88/1000 (Random-70.0%): Train Loss: 1.0480, Train Acc: 75.70%, LR: 0.000009, Val Loss: 0.6623, Val Acc: 79.44% ✓\n",
      "Epoch  89/1000 (Random-70.0%): Train Loss: 1.0416, Train Acc: 75.71%, LR: 0.000009, Val Loss: 0.6604, Val Acc: 79.16%\n",
      "Epoch  90/1000 (Random-70.0%): Train Loss: 1.0418, Train Acc: 75.95%, LR: 0.000009, Val Loss: 0.6603, Val Acc: 79.28%\n",
      "Epoch  91/1000 (Random-70.0%): Train Loss: 1.0420, Train Acc: 75.94%, LR: 0.000009, Val Loss: 0.6594, Val Acc: 79.72% ✓\n",
      "Epoch  92/1000 (Random-70.0%): Train Loss: 1.0348, Train Acc: 76.40%, LR: 0.000008, Val Loss: 0.6586, Val Acc: 79.62%\n",
      "Epoch  93/1000 (Random-70.0%): Train Loss: 1.0356, Train Acc: 75.94%, LR: 0.000008, Val Loss: 0.6580, Val Acc: 79.60%\n",
      "Epoch  94/1000 (Random-70.0%): Train Loss: 1.0327, Train Acc: 76.40%, LR: 0.000008, Val Loss: 0.6606, Val Acc: 79.24%\n",
      "Epoch  95/1000 (Random-70.0%): Train Loss: 1.0331, Train Acc: 76.32%, LR: 0.000008, Val Loss: 0.6535, Val Acc: 79.42%\n",
      "Epoch  96/1000 (Random-70.0%): Train Loss: 1.0289, Train Acc: 76.60%, LR: 0.000008, Val Loss: 0.6521, Val Acc: 79.86% ✓\n",
      "Epoch  97/1000 (Random-70.0%): Train Loss: 1.0298, Train Acc: 76.37%, LR: 0.000008, Val Loss: 0.6532, Val Acc: 79.68%\n",
      "Epoch  98/1000 (Random-70.0%): Train Loss: 1.0229, Train Acc: 76.62%, LR: 0.000007, Val Loss: 0.6493, Val Acc: 79.66%\n",
      "Epoch  99/1000 (Random-70.0%): Train Loss: 1.0238, Train Acc: 76.63%, LR: 0.000007, Val Loss: 0.6480, Val Acc: 80.02% ✓\n",
      "Epoch 100/1000 (Random-70.0%): Train Loss: 1.0215, Train Acc: 76.66%, LR: 0.000007, Val Loss: 0.6502, Val Acc: 79.90%\n",
      "Epoch 101/1000 (Random-70.0%): Train Loss: 1.0239, Train Acc: 76.83%, LR: 0.000007, Val Loss: 0.6490, Val Acc: 79.82%\n",
      "Epoch 102/1000 (Random-70.0%): Train Loss: 1.0189, Train Acc: 77.00%, LR: 0.000007, Val Loss: 0.6465, Val Acc: 79.86%\n",
      "Epoch 103/1000 (Random-70.0%): Train Loss: 1.0196, Train Acc: 76.75%, LR: 0.000007, Val Loss: 0.6491, Val Acc: 80.02%\n",
      "Epoch 104/1000 (Random-70.0%): Train Loss: 1.0182, Train Acc: 76.89%, LR: 0.000006, Val Loss: 0.6450, Val Acc: 79.70%\n",
      "Epoch 105/1000 (Random-70.0%): Train Loss: 1.0156, Train Acc: 77.14%, LR: 0.000006, Val Loss: 0.6475, Val Acc: 79.92%\n",
      "Epoch 106/1000 (Random-70.0%): Train Loss: 1.0156, Train Acc: 77.13%, LR: 0.000006, Val Loss: 0.6459, Val Acc: 80.00%\n",
      "Epoch 107/1000 (Random-70.0%): Train Loss: 1.0156, Train Acc: 77.03%, LR: 0.000006, Val Loss: 0.6423, Val Acc: 79.98%\n",
      "Epoch 108/1000 (Random-70.0%): Train Loss: 1.0156, Train Acc: 76.87%, LR: 0.000006, Val Loss: 0.6460, Val Acc: 80.06% ✓\n",
      "Epoch 109/1000 (Random-70.0%): Train Loss: 1.0113, Train Acc: 77.19%, LR: 0.000005, Val Loss: 0.6426, Val Acc: 79.92%\n",
      "Epoch 110/1000 (Random-70.0%): Train Loss: 1.0083, Train Acc: 77.48%, LR: 0.000005, Val Loss: 0.6441, Val Acc: 80.08% ✓\n",
      "Epoch 111/1000 (Random-70.0%): Train Loss: 1.0140, Train Acc: 76.98%, LR: 0.000005, Val Loss: 0.6447, Val Acc: 80.22% ✓\n",
      "Epoch 112/1000 (Random-70.0%): Train Loss: 1.0122, Train Acc: 77.36%, LR: 0.000005, Val Loss: 0.6446, Val Acc: 80.10%\n",
      "Epoch 113/1000 (Random-70.0%): Train Loss: 1.0089, Train Acc: 77.36%, LR: 0.000005, Val Loss: 0.6426, Val Acc: 79.80%\n",
      "Epoch 114/1000 (Random-70.0%): Train Loss: 1.0060, Train Acc: 77.40%, LR: 0.000004, Val Loss: 0.6408, Val Acc: 79.94%\n",
      "Epoch 115/1000 (Random-70.0%): Train Loss: 1.0024, Train Acc: 77.51%, LR: 0.000004, Val Loss: 0.6427, Val Acc: 79.94%\n",
      "Epoch 116/1000 (Random-70.0%): Train Loss: 1.0057, Train Acc: 77.44%, LR: 0.000004, Val Loss: 0.6455, Val Acc: 80.06%\n",
      "Epoch 117/1000 (Random-70.0%): Train Loss: 1.0028, Train Acc: 77.63%, LR: 0.000004, Val Loss: 0.6391, Val Acc: 79.92%\n",
      "Epoch 118/1000 (Random-70.0%): Train Loss: 1.0026, Train Acc: 77.64%, LR: 0.000004, Val Loss: 0.6388, Val Acc: 80.20%\n",
      "Epoch 119/1000 (Random-70.0%): Train Loss: 1.0031, Train Acc: 77.48%, LR: 0.000004, Val Loss: 0.6360, Val Acc: 80.28% ✓\n",
      "Epoch 120/1000 (Random-70.0%): Train Loss: 0.9983, Train Acc: 77.78%, LR: 0.000003, Val Loss: 0.6369, Val Acc: 80.20%\n",
      "Epoch 121/1000 (Random-70.0%): Train Loss: 1.0012, Train Acc: 77.64%, LR: 0.000003, Val Loss: 0.6407, Val Acc: 79.98%\n",
      "Epoch 122/1000 (Random-70.0%): Train Loss: 1.0004, Train Acc: 77.60%, LR: 0.000003, Val Loss: 0.6345, Val Acc: 80.42% ✓\n",
      "Epoch 123/1000 (Random-70.0%): Train Loss: 0.9986, Train Acc: 77.91%, LR: 0.000003, Val Loss: 0.6370, Val Acc: 80.08%\n",
      "Epoch 124/1000 (Random-70.0%): Train Loss: 1.0016, Train Acc: 77.82%, LR: 0.000003, Val Loss: 0.6364, Val Acc: 80.16%\n",
      "Epoch 125/1000 (Random-70.0%): Train Loss: 0.9932, Train Acc: 78.14%, LR: 0.000002, Val Loss: 0.6364, Val Acc: 79.96%\n",
      "Epoch 126/1000 (Random-70.0%): Train Loss: 0.9990, Train Acc: 77.85%, LR: 0.000002, Val Loss: 0.6386, Val Acc: 80.10%\n",
      "Epoch 127/1000 (Random-70.0%): Train Loss: 0.9943, Train Acc: 78.06%, LR: 0.000002, Val Loss: 0.6345, Val Acc: 80.30%\n",
      "Epoch 128/1000 (Random-70.0%): Train Loss: 0.9920, Train Acc: 77.78%, LR: 0.000002, Val Loss: 0.6389, Val Acc: 79.90%\n",
      "Epoch 129/1000 (Random-70.0%): Train Loss: 0.9948, Train Acc: 77.96%, LR: 0.000002, Val Loss: 0.6367, Val Acc: 80.16%\n",
      "Epoch 130/1000 (Random-70.0%): Train Loss: 0.9971, Train Acc: 77.76%, LR: 0.000002, Val Loss: 0.6361, Val Acc: 80.04%\n",
      "Epoch 131/1000 (Random-70.0%): Train Loss: 0.9948, Train Acc: 78.11%, LR: 0.000002, Val Loss: 0.6367, Val Acc: 80.18%\n",
      "Epoch 132/1000 (Random-70.0%): Train Loss: 0.9974, Train Acc: 78.03%, LR: 0.000001, Val Loss: 0.6352, Val Acc: 80.14%\n",
      "Epoch 133/1000 (Random-70.0%): Train Loss: 0.9941, Train Acc: 78.20%, LR: 0.000001, Val Loss: 0.6358, Val Acc: 80.22%\n",
      "Epoch 134/1000 (Random-70.0%): Train Loss: 0.9981, Train Acc: 77.81%, LR: 0.000001, Val Loss: 0.6367, Val Acc: 80.12%\n",
      "Epoch 135/1000 (Random-70.0%): Train Loss: 0.9974, Train Acc: 77.97%, LR: 0.000001, Val Loss: 0.6346, Val Acc: 80.28%\n",
      "Epoch 136/1000 (Random-70.0%): Train Loss: 1.0000, Train Acc: 77.78%, LR: 0.000001, Val Loss: 0.6336, Val Acc: 80.46% ✓\n",
      "Epoch 137/1000 (Random-70.0%): Train Loss: 0.9926, Train Acc: 78.15%, LR: 0.000001, Val Loss: 0.6351, Val Acc: 80.42%\n",
      "Epoch 138/1000 (Random-70.0%): Train Loss: 0.9936, Train Acc: 77.95%, LR: 0.000001, Val Loss: 0.6327, Val Acc: 80.32%\n",
      "Epoch 139/1000 (Random-70.0%): Train Loss: 0.9959, Train Acc: 77.88%, LR: 0.000001, Val Loss: 0.6366, Val Acc: 80.22%\n",
      "Epoch 140/1000 (Random-70.0%): Train Loss: 0.9994, Train Acc: 77.79%, LR: 0.000001, Val Loss: 0.6323, Val Acc: 80.38%\n",
      "Epoch 141/1000 (Random-70.0%): Train Loss: 0.9969, Train Acc: 77.94%, LR: 0.000000, Val Loss: 0.6324, Val Acc: 80.42%\n",
      "Epoch 142/1000 (Random-70.0%): Train Loss: 0.9939, Train Acc: 78.04%, LR: 0.000000, Val Loss: 0.6333, Val Acc: 80.48% ✓\n",
      "Epoch 143/1000 (Random-70.0%): Train Loss: 0.9928, Train Acc: 77.98%, LR: 0.000000, Val Loss: 0.6360, Val Acc: 80.28%\n",
      "Epoch 144/1000 (Random-70.0%): Train Loss: 0.9928, Train Acc: 78.17%, LR: 0.000000, Val Loss: 0.6340, Val Acc: 80.34%\n",
      "Epoch 145/1000 (Random-70.0%): Train Loss: 0.9977, Train Acc: 77.68%, LR: 0.000000, Val Loss: 0.6353, Val Acc: 80.32%\n",
      "Epoch 146/1000 (Random-70.0%): Train Loss: 0.9948, Train Acc: 78.14%, LR: 0.000000, Val Loss: 0.6346, Val Acc: 80.20%\n",
      "Epoch 147/1000 (Random-70.0%): Train Loss: 0.9977, Train Acc: 77.84%, LR: 0.000000, Val Loss: 0.6338, Val Acc: 80.24%\n",
      "Epoch 148/1000 (Random-70.0%): Train Loss: 0.9944, Train Acc: 77.66%, LR: 0.000000, Val Loss: 0.6383, Val Acc: 80.08%\n",
      "Epoch 149/1000 (Random-70.0%): Train Loss: 0.9945, Train Acc: 77.88%, LR: 0.000000, Val Loss: 0.6333, Val Acc: 80.46%\n",
      "Epoch 150/1000 (Random-70.0%): Train Loss: 0.9935, Train Acc: 77.91%, LR: 0.000000, Val Loss: 0.6343, Val Acc: 80.30%\n",
      "Epoch 151/1000 (Random-70.0%): Train Loss: 1.0015, Train Acc: 77.73%, LR: 0.000010, Val Loss: 0.6372, Val Acc: 80.10%\n",
      "Epoch 152/1000 (Random-70.0%): Train Loss: 0.9943, Train Acc: 77.88%, LR: 0.000010, Val Loss: 0.6327, Val Acc: 80.56% ✓\n",
      "Epoch 153/1000 (Random-70.0%): Train Loss: 0.9939, Train Acc: 77.96%, LR: 0.000010, Val Loss: 0.6329, Val Acc: 79.98%\n",
      "Epoch 154/1000 (Random-70.0%): Train Loss: 0.9945, Train Acc: 77.84%, LR: 0.000010, Val Loss: 0.6399, Val Acc: 79.90%\n",
      "Epoch 155/1000 (Random-70.0%): Train Loss: 0.9943, Train Acc: 77.89%, LR: 0.000010, Val Loss: 0.6284, Val Acc: 80.20%\n",
      "Epoch 156/1000 (Random-70.0%): Train Loss: 0.9941, Train Acc: 77.97%, LR: 0.000010, Val Loss: 0.6369, Val Acc: 79.98%\n",
      "Epoch 157/1000 (Random-70.0%): Train Loss: 0.9899, Train Acc: 77.95%, LR: 0.000010, Val Loss: 0.6272, Val Acc: 80.38%\n",
      "Epoch 158/1000 (Random-70.0%): Train Loss: 0.9836, Train Acc: 78.49%, LR: 0.000010, Val Loss: 0.6283, Val Acc: 80.18%\n",
      "Epoch 159/1000 (Random-70.0%): Train Loss: 0.9876, Train Acc: 78.17%, LR: 0.000010, Val Loss: 0.6293, Val Acc: 80.38%\n",
      "Epoch 160/1000 (Random-70.0%): Train Loss: 0.9846, Train Acc: 78.40%, LR: 0.000010, Val Loss: 0.6260, Val Acc: 80.44%\n",
      "Epoch 161/1000 (Random-70.0%): Train Loss: 0.9796, Train Acc: 78.53%, LR: 0.000010, Val Loss: 0.6269, Val Acc: 80.46%\n",
      "Epoch 162/1000 (Random-70.0%): Train Loss: 0.9876, Train Acc: 78.32%, LR: 0.000010, Val Loss: 0.6287, Val Acc: 80.00%\n",
      "Epoch 163/1000 (Random-70.0%): Train Loss: 0.9754, Train Acc: 78.77%, LR: 0.000010, Val Loss: 0.6242, Val Acc: 80.44%\n",
      "Epoch 164/1000 (Random-70.0%): Train Loss: 0.9780, Train Acc: 78.72%, LR: 0.000010, Val Loss: 0.6301, Val Acc: 80.18%\n",
      "Epoch 165/1000 (Random-70.0%): Train Loss: 0.9785, Train Acc: 78.88%, LR: 0.000010, Val Loss: 0.6285, Val Acc: 80.02%\n",
      "Epoch 166/1000 (Random-70.0%): Train Loss: 0.9790, Train Acc: 78.77%, LR: 0.000010, Val Loss: 0.6269, Val Acc: 80.42%\n",
      "Epoch 167/1000 (Random-70.0%): Train Loss: 0.9736, Train Acc: 78.94%, LR: 0.000010, Val Loss: 0.6244, Val Acc: 80.62% ✓\n",
      "Epoch 168/1000 (Random-70.0%): Train Loss: 0.9773, Train Acc: 78.66%, LR: 0.000010, Val Loss: 0.6261, Val Acc: 80.42%\n",
      "Epoch 169/1000 (Random-70.0%): Train Loss: 0.9696, Train Acc: 79.04%, LR: 0.000010, Val Loss: 0.6217, Val Acc: 80.52%\n",
      "Epoch 170/1000 (Random-70.0%): Train Loss: 0.9737, Train Acc: 78.82%, LR: 0.000010, Val Loss: 0.6218, Val Acc: 80.68% ✓\n",
      "Epoch 171/1000 (Random-70.0%): Train Loss: 0.9725, Train Acc: 79.06%, LR: 0.000010, Val Loss: 0.6203, Val Acc: 80.54%\n",
      "Epoch 172/1000 (Random-70.0%): Train Loss: 0.9698, Train Acc: 79.15%, LR: 0.000010, Val Loss: 0.6239, Val Acc: 80.70% ✓\n",
      "Epoch 173/1000 (Random-70.0%): Train Loss: 0.9703, Train Acc: 79.29%, LR: 0.000010, Val Loss: 0.6204, Val Acc: 80.80% ✓\n",
      "Epoch 174/1000 (Random-70.0%): Train Loss: 0.9692, Train Acc: 79.16%, LR: 0.000010, Val Loss: 0.6227, Val Acc: 80.14%\n",
      "Epoch 175/1000 (Random-70.0%): Train Loss: 0.9681, Train Acc: 79.29%, LR: 0.000009, Val Loss: 0.6179, Val Acc: 80.78%\n",
      "Epoch 176/1000 (Random-70.0%): Train Loss: 0.9615, Train Acc: 79.53%, LR: 0.000009, Val Loss: 0.6201, Val Acc: 80.66%\n",
      "Epoch 177/1000 (Random-70.0%): Train Loss: 0.9627, Train Acc: 79.44%, LR: 0.000009, Val Loss: 0.6187, Val Acc: 80.66%\n",
      "Epoch 178/1000 (Random-70.0%): Train Loss: 0.9635, Train Acc: 79.44%, LR: 0.000009, Val Loss: 0.6186, Val Acc: 80.32%\n",
      "Epoch 179/1000 (Random-70.0%): Train Loss: 0.9608, Train Acc: 79.70%, LR: 0.000009, Val Loss: 0.6175, Val Acc: 80.78%\n",
      "Epoch 180/1000 (Random-70.0%): Train Loss: 0.9629, Train Acc: 79.41%, LR: 0.000009, Val Loss: 0.6143, Val Acc: 80.88% ✓\n",
      "Epoch 181/1000 (Random-70.0%): Train Loss: 0.9619, Train Acc: 79.54%, LR: 0.000009, Val Loss: 0.6149, Val Acc: 80.96% ✓\n",
      "Epoch 182/1000 (Random-70.0%): Train Loss: 0.9608, Train Acc: 79.66%, LR: 0.000009, Val Loss: 0.6177, Val Acc: 80.66%\n",
      "Epoch 183/1000 (Random-70.0%): Train Loss: 0.9593, Train Acc: 79.59%, LR: 0.000009, Val Loss: 0.6185, Val Acc: 80.64%\n",
      "Epoch 184/1000 (Random-70.0%): Train Loss: 0.9552, Train Acc: 79.73%, LR: 0.000009, Val Loss: 0.6149, Val Acc: 80.82%\n",
      "Epoch 185/1000 (Random-70.0%): Train Loss: 0.9533, Train Acc: 79.86%, LR: 0.000009, Val Loss: 0.6130, Val Acc: 80.62%\n",
      "Epoch 186/1000 (Random-70.0%): Train Loss: 0.9544, Train Acc: 79.82%, LR: 0.000009, Val Loss: 0.6124, Val Acc: 81.02% ✓\n",
      "Epoch 187/1000 (Random-70.0%): Train Loss: 0.9468, Train Acc: 80.02%, LR: 0.000009, Val Loss: 0.6132, Val Acc: 81.20% ✓\n",
      "Epoch 188/1000 (Random-70.0%): Train Loss: 0.9541, Train Acc: 79.95%, LR: 0.000009, Val Loss: 0.6147, Val Acc: 80.82%\n",
      "Epoch 189/1000 (Random-70.0%): Train Loss: 0.9516, Train Acc: 80.10%, LR: 0.000009, Val Loss: 0.6100, Val Acc: 81.18%\n",
      "Epoch 190/1000 (Random-70.0%): Train Loss: 0.9522, Train Acc: 80.01%, LR: 0.000009, Val Loss: 0.6150, Val Acc: 80.76%\n",
      "Epoch 191/1000 (Random-70.0%): Train Loss: 0.9514, Train Acc: 79.97%, LR: 0.000009, Val Loss: 0.6111, Val Acc: 81.08%\n",
      "Epoch 192/1000 (Random-70.0%): Train Loss: 0.9454, Train Acc: 80.23%, LR: 0.000008, Val Loss: 0.6123, Val Acc: 81.06%\n",
      "Epoch 193/1000 (Random-70.0%): Train Loss: 0.9484, Train Acc: 80.05%, LR: 0.000008, Val Loss: 0.6134, Val Acc: 81.20%\n",
      "Epoch 194/1000 (Random-70.0%): Train Loss: 0.9455, Train Acc: 80.19%, LR: 0.000008, Val Loss: 0.6099, Val Acc: 81.00%\n",
      "Epoch 195/1000 (Random-70.0%): Train Loss: 0.9461, Train Acc: 79.99%, LR: 0.000008, Val Loss: 0.6097, Val Acc: 81.18%\n",
      "Epoch 196/1000 (Random-70.0%): Train Loss: 0.9408, Train Acc: 80.39%, LR: 0.000008, Val Loss: 0.6086, Val Acc: 81.38% ✓\n",
      "Epoch 197/1000 (Random-70.0%): Train Loss: 0.9450, Train Acc: 80.29%, LR: 0.000008, Val Loss: 0.6119, Val Acc: 80.92%\n",
      "Epoch 198/1000 (Random-70.0%): Train Loss: 0.9443, Train Acc: 80.30%, LR: 0.000008, Val Loss: 0.6062, Val Acc: 81.34%\n",
      "Epoch 199/1000 (Random-70.0%): Train Loss: 0.9474, Train Acc: 80.13%, LR: 0.000008, Val Loss: 0.6080, Val Acc: 81.34%\n",
      "Epoch 200/1000 (Random-70.0%): Train Loss: 0.9409, Train Acc: 80.26%, LR: 0.000008, Val Loss: 0.6087, Val Acc: 81.24%\n",
      "Epoch 201/1000 (Random-70.0%): Train Loss: 0.9381, Train Acc: 80.49%, LR: 0.000008, Val Loss: 0.6122, Val Acc: 81.02%\n",
      "Epoch 202/1000 (Random-70.0%): Train Loss: 0.9312, Train Acc: 80.91%, LR: 0.000008, Val Loss: 0.6005, Val Acc: 81.56% ✓\n",
      "Epoch 203/1000 (Random-70.0%): Train Loss: 0.9387, Train Acc: 80.69%, LR: 0.000008, Val Loss: 0.6027, Val Acc: 81.34%\n",
      "Epoch 204/1000 (Random-70.0%): Train Loss: 0.9451, Train Acc: 80.42%, LR: 0.000008, Val Loss: 0.6036, Val Acc: 81.48%\n",
      "Epoch 205/1000 (Random-70.0%): Train Loss: 0.9358, Train Acc: 80.50%, LR: 0.000007, Val Loss: 0.6056, Val Acc: 81.24%\n",
      "Epoch 206/1000 (Random-70.0%): Train Loss: 0.9390, Train Acc: 80.50%, LR: 0.000007, Val Loss: 0.6057, Val Acc: 81.26%\n",
      "Epoch 207/1000 (Random-70.0%): Train Loss: 0.9358, Train Acc: 80.43%, LR: 0.000007, Val Loss: 0.6105, Val Acc: 81.18%\n",
      "Epoch 208/1000 (Random-70.0%): Train Loss: 0.9308, Train Acc: 81.10%, LR: 0.000007, Val Loss: 0.6094, Val Acc: 81.10%\n",
      "Epoch 209/1000 (Random-70.0%): Train Loss: 0.9304, Train Acc: 80.95%, LR: 0.000007, Val Loss: 0.6048, Val Acc: 81.16%\n",
      "Epoch 210/1000 (Random-70.0%): Train Loss: 0.9369, Train Acc: 80.59%, LR: 0.000007, Val Loss: 0.6103, Val Acc: 81.30%\n",
      "Epoch 211/1000 (Random-70.0%): Train Loss: 0.9342, Train Acc: 80.72%, LR: 0.000007, Val Loss: 0.6049, Val Acc: 81.40%\n",
      "Epoch 212/1000 (Random-70.0%): Train Loss: 0.9339, Train Acc: 80.64%, LR: 0.000007, Val Loss: 0.6097, Val Acc: 81.06%\n",
      "Epoch 213/1000 (Random-70.0%): Train Loss: 0.9360, Train Acc: 80.67%, LR: 0.000007, Val Loss: 0.6037, Val Acc: 81.40%\n",
      "Epoch 214/1000 (Random-70.0%): Train Loss: 0.9290, Train Acc: 81.00%, LR: 0.000007, Val Loss: 0.6058, Val Acc: 81.26%\n",
      "Epoch 215/1000 (Random-70.0%): Train Loss: 0.9293, Train Acc: 80.85%, LR: 0.000007, Val Loss: 0.6020, Val Acc: 81.30%\n",
      "Epoch 216/1000 (Random-70.0%): Train Loss: 0.9263, Train Acc: 81.03%, LR: 0.000006, Val Loss: 0.6049, Val Acc: 81.40%\n",
      "Epoch 217/1000 (Random-70.0%): Train Loss: 0.9333, Train Acc: 81.00%, LR: 0.000006, Val Loss: 0.6033, Val Acc: 81.46%\n",
      "Epoch 218/1000 (Random-70.0%): Train Loss: 0.9247, Train Acc: 81.02%, LR: 0.000006, Val Loss: 0.6071, Val Acc: 81.08%\n",
      "Epoch 219/1000 (Random-70.0%): Train Loss: 0.9222, Train Acc: 81.17%, LR: 0.000006, Val Loss: 0.6015, Val Acc: 81.32%\n",
      "Epoch 220/1000 (Random-70.0%): Train Loss: 0.9225, Train Acc: 81.17%, LR: 0.000006, Val Loss: 0.6066, Val Acc: 81.04%\n",
      "Epoch 221/1000 (Random-70.0%): Train Loss: 0.9290, Train Acc: 81.00%, LR: 0.000006, Val Loss: 0.6059, Val Acc: 81.40%\n",
      "Epoch 222/1000 (Random-70.0%): Train Loss: 0.9238, Train Acc: 81.06%, LR: 0.000006, Val Loss: 0.6033, Val Acc: 81.34%\n",
      "Early stopping triggered after 222 epochs (Best Val Acc: 81.56%)\n",
      "Loaded best model state (Val Acc: 81.56%)\n",
      "Results: Accuracy=81.02%, MACs=5.56M\n",
      "✅ Model saved to ./base/enhanced_random_sparsity_0.7.pth\n",
      "✅ ONNX model saved to ./base/enhanced_random_sparsity_0.7.onnx\n",
      "\n",
      "Saving enhanced results...\n",
      "✅ Complete results saved to ./results_mobilenetv2_cifar10_enhanced/complete_results.json\n",
      "✅ Summary results saved to ./results_mobilenetv2_cifar10_enhanced/summary_results.csv\n",
      "Creating enhanced plots...\n",
      "✅ Accuracy plot saved to ./results_mobilenetv2_cifar10_enhanced/accuracy_vs_sparsity.png\n",
      "✅ Efficiency frontier plot saved to ./results_mobilenetv2_cifar10_enhanced/efficiency_frontier.png\n",
      "\n",
      "==========================================================================================\n",
      "EXPERIMENTAL RESULTS SUMMARY\n",
      "==========================================================================================\n",
      "\n",
      "Baseline Performance:\n",
      "  Accuracy: 84.62%\n",
      "  MACs: 7.30M\n",
      "  Parameters: 3.02M\n",
      "  Model Size: 11.50MB\n",
      "\n",
      "Strategy Comparison at 50% Sparsity:\n",
      "       BNScale:  83.09% accuracy (+1.53%,  98.2% retention)\n",
      "   MagnitudeL2:  82.49% accuracy (+2.13%,  97.5% retention)\n",
      "        Random:  81.73% accuracy (+2.89%,  96.6% retention)\n",
      "\n",
      "Complete Results Table:\n",
      "------------------------------------------------------------------------------------------\n",
      "Strategy     Sparsity Accuracy MACs(M)  Params(M) Size(MB)\n",
      "------------------------------------------------------------------------------------------\n",
      "BNScale           0%   84.62%    7.30     3.02   11.50\n",
      "BNScale          20%   83.35%    6.75     2.81   10.72\n",
      "BNScale          50%   83.09%    6.02     2.53    9.66\n",
      "BNScale          70%   80.82%    5.56     2.35    8.96\n",
      "MagnitudeL2       0%   84.62%    7.30     3.02   11.50\n",
      "MagnitudeL2      20%   83.40%    6.75     2.81   10.72\n",
      "MagnitudeL2      50%   82.49%    6.02     2.53    9.66\n",
      "MagnitudeL2      70%   81.01%    5.56     2.35    8.96\n",
      "Random            0%   84.62%    7.30     3.02   11.50\n",
      "Random           20%   82.92%    6.75     2.81   10.72\n",
      "Random           50%   81.73%    6.02     2.53    9.66\n",
      "Random           70%   81.02%    5.56     2.35    8.96\n",
      "\n",
      "🎉 All enhanced experiments completed!\n",
      "📁 Results saved to: /home/muis/thesis/github-repo/master-thesis/cnn/mobile_net_v2/results_mobilenetv2_cifar10_enhanced\n",
      "📁 Models saved to: /home/muis/thesis/github-repo/master-thesis/cnn/mobile_net_v2/base\n",
      "\n",
      "📊 Expected Performance Improvements:\n",
      "   • Baseline accuracy should reach 85-92% (vs previous ~70%)\n",
      "   • Better accuracy retention after pruning\n",
      "   • More stable training with enhanced optimizations\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
