{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# op",
   "id": "bc62a9146803bde3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:19:02.204930Z",
     "start_time": "2025-06-04T15:44:32.461051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_pruning as tp\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "import pandas as pd\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_BASE_NAME = \"mobilenet_v2\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "def get_data_loaders(data_dir_path='./data', batch_size=128, val_split=0.1, seed=42):\n",
    "    \"\"\"Load CIFAR-10 dataset with train/val/test splits and data augmentation\"\"\"\n",
    "    abs_data_dir = os.path.abspath(data_dir_path)\n",
    "    print(f\"Loading CIFAR-10 from: {abs_data_dir}\")\n",
    "\n",
    "    # Data augmentation for training\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "\n",
    "    # No augmentation for val/test\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "\n",
    "    # Load datasets (assuming pre-downloaded)\n",
    "    full_train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=abs_data_dir, train=True, download=False, transform=transform_train\n",
    "    )\n",
    "\n",
    "    # Create a version with test transforms for validation\n",
    "    full_train_dataset_val = torchvision.datasets.CIFAR10(\n",
    "        root=abs_data_dir, train=True, download=False, transform=transform_test\n",
    "    )\n",
    "\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=abs_data_dir, train=False, download=False, transform=transform_test\n",
    "    )\n",
    "\n",
    "    # Create train/validation split\n",
    "    val_size = int(len(full_train_dataset) * val_split)\n",
    "    train_size = len(full_train_dataset) - val_size\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "\n",
    "    train_indices, val_indices = torch.utils.data.random_split(\n",
    "        range(len(full_train_dataset)), [train_size, val_size], generator=generator\n",
    "    )\n",
    "\n",
    "    # Create subset datasets\n",
    "    train_dataset = torch.utils.data.Subset(full_train_dataset, train_indices.indices)\n",
    "    val_dataset = torch.utils.data.Subset(full_train_dataset_val, val_indices.indices)\n",
    "\n",
    "    # Create data loaders\n",
    "    num_workers = min(4, os.cpu_count() or 2)\n",
    "    pin_memory = True if DEVICE.type == 'cuda' else False\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                             num_workers=num_workers, pin_memory=pin_memory)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                           num_workers=num_workers, pin_memory=pin_memory)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                            num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    print(f\"DataLoaders created - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def get_mobilenetv2_model(num_classes=10, use_pretrained=True, width_mult=1.0):\n",
    "    \"\"\"Get MobileNetV2 model adapted for CIFAR-10 with proper initialization\"\"\"\n",
    "    if use_pretrained and os.path.exists('./mobilenet_v2-b0353104.pth'):\n",
    "        # Load pre-trained model\n",
    "        model = models.mobilenet_v2(weights=None)\n",
    "        pretrained_state = torch.load('./mobilenet_v2-b0353104.pth', map_location=DEVICE)\n",
    "\n",
    "        # Load all weights except the classifier\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_state.items()\n",
    "                          if k in model_dict and 'classifier' not in k}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "        print(\"‚úÖ Loaded MobileNetV2 with ImageNet pretrained features (excluding classifier)\")\n",
    "    else:\n",
    "        model = models.mobilenet_v2(weights=None)\n",
    "        print(\"‚úÖ Created MobileNetV2 without pretrained weights\")\n",
    "\n",
    "    # Replace classifier with a more suitable one for CIFAR-10\n",
    "    in_features = model.classifier[1].in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(in_features, num_classes)\n",
    "    )\n",
    "\n",
    "    # Initialize the new classifier properly\n",
    "    nn.init.xavier_uniform_(model.classifier[1].weight)\n",
    "    nn.init.zeros_(model.classifier[1].bias)\n",
    "\n",
    "    print(f\"‚úÖ Adapted classifier for {num_classes} classes\")\n",
    "    return model\n",
    "\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    \"\"\"Apply mixup augmentation\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"Mixup loss\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def get_ignored_layers(model):\n",
    "    \"\"\"Get layers to ignore during pruning (typically final classifier)\"\"\"\n",
    "    ignored_layers = []\n",
    "    if hasattr(model, 'classifier'):\n",
    "        if isinstance(model.classifier, nn.Sequential):\n",
    "            for layer in model.classifier:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    ignored_layers.append(layer)\n",
    "        elif isinstance(model.classifier, nn.Linear):\n",
    "            ignored_layers.append(model.classifier)\n",
    "    return ignored_layers\n",
    "\n",
    "def calculate_macs_params(model, example_input):\n",
    "    \"\"\"Calculate MACs and parameters using torch_pruning\"\"\"\n",
    "    model.eval()\n",
    "    target_device = example_input.device\n",
    "    model_on_device = model.to(target_device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        macs, params = tp.utils.count_ops_and_params(model_on_device, example_input)\n",
    "\n",
    "    return macs, params\n",
    "\n",
    "def save_model(model, save_path, example_input_cpu=None):\n",
    "    \"\"\"Save model state dict and optionally ONNX\"\"\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"‚úÖ Model saved to {save_path}\")\n",
    "\n",
    "    if example_input_cpu is not None:\n",
    "        onnx_path = save_path.replace('.pth', '.onnx')\n",
    "        try:\n",
    "            model_cpu = model.to('cpu')\n",
    "            torch.onnx.export(\n",
    "                model_cpu, example_input_cpu, onnx_path,\n",
    "                export_params=True, opset_version=13,\n",
    "                input_names=['input'], output_names=['output'],\n",
    "                dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "            )\n",
    "            print(f\"‚úÖ ONNX model saved to {onnx_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: ONNX export failed: {e}\")\n",
    "\n",
    "def evaluate_model(model, data_loader, example_input, criterion, device):\n",
    "    \"\"\"Evaluate model and return comprehensive metrics\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Calculate efficiency metrics\n",
    "    macs, params = calculate_macs_params(model, example_input.to(device))\n",
    "    model_size_mb = params * 4 / (1024 * 1024)  # Assuming float32\n",
    "\n",
    "    # Calculate accuracy and loss\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / total if total > 0 else float('nan')\n",
    "    accuracy = 100.0 * correct / total if total > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'loss': avg_loss,\n",
    "        'macs': macs,\n",
    "        'params': params,\n",
    "        'size_mb': model_size_mb\n",
    "    }\n",
    "\n",
    "def prune_model(model, strategy_config, sparsity_ratio, example_input, ignored_layers=None):\n",
    "    \"\"\"Apply structured pruning to model\"\"\"\n",
    "    if sparsity_ratio == 0.0:\n",
    "        print(\"No pruning needed (sparsity = 0.0)\")\n",
    "        return model\n",
    "\n",
    "    model.eval()\n",
    "    pruned_model = copy.deepcopy(model)\n",
    "    pruned_model.to(example_input.device)\n",
    "\n",
    "    # Calculate initial MACs\n",
    "    initial_macs, _ = calculate_macs_params(pruned_model, example_input)\n",
    "    print(f\"Initial MACs: {initial_macs / 1e6:.2f}M\")\n",
    "\n",
    "    ignored_layers = ignored_layers or []\n",
    "\n",
    "    # Create pruner based on strategy\n",
    "    pruner = strategy_config['pruner'](\n",
    "        pruned_model,\n",
    "        example_input,\n",
    "        importance=strategy_config['importance'],\n",
    "        iterative_steps=5,\n",
    "        ch_sparsity=sparsity_ratio,\n",
    "        root_module_types=[nn.Conv2d],\n",
    "        ignored_layers=ignored_layers\n",
    "    )\n",
    "\n",
    "    print(f\"Applying {strategy_config['importance'].__class__.__name__} pruning at {sparsity_ratio:.1%} sparsity...\")\n",
    "\n",
    "    # Apply pruning\n",
    "    pruner.step()\n",
    "\n",
    "    # Calculate final MACs\n",
    "    final_macs, _ = calculate_macs_params(pruned_model, example_input)\n",
    "    reduction = (initial_macs - final_macs) / initial_macs * 100 if initial_macs > 0 else 0\n",
    "    print(f\"Final MACs: {final_macs / 1e6:.2f}M (Reduction: {reduction:.1f}%)\")\n",
    "\n",
    "    return pruned_model\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs,\n",
    "                val_loader=None, patience=7, log_prefix=\"\", scheduler=None,\n",
    "                use_mixup=False, mixup_alpha=0.2):\n",
    "    \"\"\"Train model with early stopping and advanced techniques\"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Apply mixup if enabled\n",
    "            if use_mixup and epoch < num_epochs - 5:  # Disable mixup for last 5 epochs\n",
    "                data, target_a, target_b, lam = mixup_data(data, target, mixup_alpha)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = mixup_criterion(criterion, output, target_a, target_b, lam)\n",
    "            else:\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total += target.size(0)\n",
    "            if use_mixup and epoch < num_epochs - 5:\n",
    "                train_correct += (lam * predicted.eq(target_a).sum().item() +\n",
    "                                (1 - lam) * predicted.eq(target_b).sum().item())\n",
    "            else:\n",
    "                train_correct += (predicted == target).sum().item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_acc = 100.0 * train_correct / train_total\n",
    "\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "\n",
    "        log_msg = f\"Epoch {epoch+1}/{num_epochs} ({log_prefix}): Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%\"\n",
    "\n",
    "        # Validation phase\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for data, target in val_loader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = torch.max(output.data, 1)\n",
    "                    val_total += target.size(0)\n",
    "                    val_correct += (predicted == target).sum().item()\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_acc = 100.0 * val_correct / val_total\n",
    "\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            history['val_acc'].append(val_acc)\n",
    "\n",
    "            log_msg += f\", Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%\"\n",
    "\n",
    "            # Learning rate scheduling\n",
    "            if scheduler:\n",
    "                if isinstance(scheduler, ReduceLROnPlateau):\n",
    "                    scheduler.step(avg_val_loss)\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "\n",
    "            # Early stopping based on validation accuracy\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_val_loss = avg_val_loss\n",
    "                epochs_no_improve = 0\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "                log_msg += \" (Best)\"\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"{log_msg}\")\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "        else:\n",
    "            history['val_loss'].append(None)\n",
    "            history['val_acc'].append(None)\n",
    "            if scheduler and not isinstance(scheduler, ReduceLROnPlateau):\n",
    "                scheduler.step()\n",
    "\n",
    "        print(log_msg)\n",
    "\n",
    "    # Load best model state if available\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"Loaded best model state with val accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "def save_results_to_files(all_results, output_dir):\n",
    "    \"\"\"Save experimental results to JSON and CSV files\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save complete results as JSON\n",
    "    results_json_path = os.path.join(output_dir, 'complete_results.json')\n",
    "    with open(results_json_path, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2, default=str)\n",
    "    print(f\"‚úÖ Complete results saved to {results_json_path}\")\n",
    "\n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for strategy, strategy_results in all_results.items():\n",
    "        for sparsity, metrics in strategy_results.items():\n",
    "            row = {\n",
    "                'strategy': strategy,\n",
    "                'sparsity_ratio': sparsity,\n",
    "                'accuracy': metrics['accuracy'],\n",
    "                'loss': metrics['loss'],\n",
    "                'macs_millions': metrics['macs'] / 1e6,\n",
    "                'params_millions': metrics['params'] / 1e6,\n",
    "                'size_mb': metrics['size_mb']\n",
    "            }\n",
    "            summary_data.append(row)\n",
    "\n",
    "    # Save summary as CSV\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_csv_path = os.path.join(output_dir, 'summary_results.csv')\n",
    "    summary_df.to_csv(summary_csv_path, index=False)\n",
    "    print(f\"‚úÖ Summary results saved to {summary_csv_path}\")\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "def create_results_plots(summary_df, output_dir):\n",
    "    \"\"\"Create visualization plots\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    strategies = summary_df['strategy'].unique()\n",
    "    sparsity_levels = sorted(summary_df['sparsity_ratio'].unique())\n",
    "\n",
    "    # Plot 1: Accuracy vs Sparsity\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for strategy in strategies:\n",
    "        strategy_data = summary_df[summary_df['strategy'] == strategy].sort_values('sparsity_ratio')\n",
    "        plt.plot(strategy_data['sparsity_ratio'] * 100, strategy_data['accuracy'],\n",
    "                'o-', linewidth=2, markersize=8, label=strategy)\n",
    "\n",
    "    plt.xlabel('Sparsity (%)', fontsize=12)\n",
    "    plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "    plt.title('MobileNetV2: Accuracy vs Sparsity', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(output_dir, 'accuracy_vs_sparsity.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"‚úÖ Accuracy plot saved to {plot_path}\")\n",
    "\n",
    "    # Plot 2: Efficiency frontier (Accuracy vs MACs)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for strategy in strategies:\n",
    "        strategy_data = summary_df[summary_df['strategy'] == strategy].sort_values('sparsity_ratio')\n",
    "        plt.scatter(strategy_data['macs_millions'], strategy_data['accuracy'],\n",
    "                   s=100, label=strategy, alpha=0.8)\n",
    "        plt.plot(strategy_data['macs_millions'], strategy_data['accuracy'],\n",
    "                '--', alpha=0.6)\n",
    "\n",
    "    plt.xlabel('MACs (Millions)', fontsize=12)\n",
    "    plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "    plt.title('MobileNetV2: Efficiency Frontier (Accuracy vs MACs)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(output_dir, 'efficiency_frontier.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"‚úÖ Efficiency frontier plot saved to {plot_path}\")\n",
    "\n",
    "def print_results_table(summary_df):\n",
    "    \"\"\"Print formatted results table\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXPERIMENTAL RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Baseline results\n",
    "    baseline_results = summary_df[summary_df['sparsity_ratio'] == 0.0].iloc[0]\n",
    "    print(f\"\\nBaseline Performance:\")\n",
    "    print(f\"  Accuracy: {baseline_results['accuracy']:.2f}%\")\n",
    "    print(f\"  MACs: {baseline_results['macs_millions']:.2f}M\")\n",
    "    print(f\"  Parameters: {baseline_results['params_millions']:.2f}M\")\n",
    "    print(f\"  Model Size: {baseline_results['size_mb']:.2f}MB\")\n",
    "\n",
    "    # Strategy comparison at 50% sparsity\n",
    "    print(f\"\\nStrategy Comparison at 50% Sparsity:\")\n",
    "    sparsity_50_data = summary_df[summary_df['sparsity_ratio'] == 0.5]\n",
    "    for _, row in sparsity_50_data.iterrows():\n",
    "        degradation = baseline_results['accuracy'] - row['accuracy']\n",
    "        retention = (row['accuracy'] / baseline_results['accuracy']) * 100\n",
    "        print(f\"  {row['strategy']:>12}: {row['accuracy']:>6.2f}% accuracy ({degradation:>+5.2f}%, {retention:>5.1f}% retention)\")\n",
    "\n",
    "    # Complete results table\n",
    "    print(f\"\\nComplete Results Table:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Strategy':<12} {'Sparsity':<8} {'Accuracy':<8} {'MACs(M)':<8} {'Params(M)':<9} {'Size(MB)':<8}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for _, row in summary_df.sort_values(['strategy', 'sparsity_ratio']).iterrows():\n",
    "        print(f\"{row['strategy']:<12} {row['sparsity_ratio']*100:>6.0f}% \"\n",
    "              f\"{row['accuracy']:>7.2f}% {row['macs_millions']:>7.2f} \"\n",
    "              f\"{row['params_millions']:>8.2f} {row['size_mb']:>7.2f}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main experimental workflow\"\"\"\n",
    "    print(\"Starting Enhanced MobileNetV2 CIFAR-10 Pruning Experiments\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'strategies': {\n",
    "            'BNScale': {\n",
    "                'pruner': tp.pruner.BNScalePruner,\n",
    "                'importance': tp.importance.BNScaleImportance()\n",
    "            },\n",
    "            'MagnitudeL2': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.MagnitudeImportance(p=2)\n",
    "            },\n",
    "            'Random': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.RandomImportance()\n",
    "            },\n",
    "        },\n",
    "        'pruning_ratios': [0.0, 0.2, 0.5, 0.7],\n",
    "        'num_classes': 10,\n",
    "        'batch_size': 128,\n",
    "        'learning_rate': 0.001,  # Higher initial learning rate\n",
    "        'learning_rate_finetune': 0.001,  # Lower for fine-tuning\n",
    "        'epochs': 1000,  # More epochs for better training\n",
    "        'patience': 20,  # More patience\n",
    "        'weight_decay': 1e-4,  # Add weight decay\n",
    "        'use_mixup': True,  # Enable mixup\n",
    "        'mixup_alpha': 0.2,\n",
    "        'output_dir': './results_mobilenetv2_cifar10_enhanced',\n",
    "        'models_dir': './models_mobilenetv2_enhanced'\n",
    "    }\n",
    "\n",
    "    # Create output directories\n",
    "    os.makedirs(config['output_dir'], exist_ok=True)\n",
    "    os.makedirs(config['models_dir'], exist_ok=True)\n",
    "\n",
    "    # Load data with augmentation\n",
    "    print(\"Loading CIFAR-10 dataset with data augmentation...\")\n",
    "    train_loader, val_loader, test_loader = get_data_loaders(\n",
    "        batch_size=config['batch_size']\n",
    "    )\n",
    "\n",
    "    # Prepare inputs and criterion\n",
    "    example_input_cpu = torch.randn(1, 3, 32, 32)\n",
    "    example_input_device = example_input_cpu.to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Get baseline model and train it\n",
    "    print(\"\\nCreating and training baseline model...\")\n",
    "    model = get_mobilenetv2_model(num_classes=config['num_classes'], use_pretrained=True)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Train baseline model with enhanced settings\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'],\n",
    "                          weight_decay=config['weight_decay'])\n",
    "\n",
    "    # Use cosine annealing scheduler for better convergence\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=config['epochs'], eta_min=1e-6)\n",
    "\n",
    "    trained_model, training_history = train_model(\n",
    "        model, train_loader, criterion, optimizer, DEVICE,\n",
    "        config['epochs'], val_loader, config['patience'], \"Baseline Training\",\n",
    "        scheduler=scheduler, use_mixup=config['use_mixup'],\n",
    "        mixup_alpha=config['mixup_alpha']\n",
    "    )\n",
    "\n",
    "    # Save baseline model\n",
    "    baseline_model_path = os.path.join(config['models_dir'], 'baseline_model.pth')\n",
    "    save_model(trained_model, baseline_model_path, example_input_cpu)\n",
    "\n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(training_history['train_loss'], label='Train')\n",
    "    plt.plot(training_history['val_loss'], label='Validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training History - Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(training_history['train_acc'], label='Train')\n",
    "    plt.plot(training_history['val_acc'], label='Validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training History - Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config['output_dir'], 'training_history.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Evaluate baseline\n",
    "    print(\"\\nEvaluating baseline model...\")\n",
    "    baseline_metrics = evaluate_model(trained_model, test_loader, example_input_device, criterion, DEVICE)\n",
    "    print(f\"Baseline Results: Accuracy={baseline_metrics['accuracy']:.2f}%, \"\n",
    "          f\"MACs={baseline_metrics['macs']/1e6:.2f}M, \"\n",
    "          f\"Params={baseline_metrics['params']/1e6:.2f}M\")\n",
    "\n",
    "    # Initialize results storage\n",
    "    all_results = {}\n",
    "    for strategy_name in config['strategies'].keys():\n",
    "        all_results[strategy_name] = {0.0: baseline_metrics}\n",
    "\n",
    "    # Get ignored layers\n",
    "    ignored_layers = get_ignored_layers(trained_model)\n",
    "\n",
    "    # Run pruning experiments\n",
    "    print(\"\\nStarting pruning experiments...\")\n",
    "    for strategy_name, strategy_config in config['strategies'].items():\n",
    "        print(f\"\\n--- Strategy: {strategy_name} ---\")\n",
    "\n",
    "        for sparsity_ratio in config['pruning_ratios']:\n",
    "            if sparsity_ratio == 0.0:\n",
    "                continue  # Skip baseline (already done)\n",
    "\n",
    "            print(f\"\\nProcessing {strategy_name} at {sparsity_ratio:.1%} sparsity...\")\n",
    "\n",
    "            # Load fresh copy of trained baseline\n",
    "            model_copy = get_mobilenetv2_model(num_classes=config['num_classes'], use_pretrained=False)\n",
    "            model_copy.load_state_dict(torch.load(baseline_model_path, map_location=DEVICE))\n",
    "            model_copy.to(DEVICE)\n",
    "\n",
    "            # Apply pruning\n",
    "            pruned_model = prune_model(\n",
    "                model_copy, strategy_config, sparsity_ratio,\n",
    "                example_input_device, ignored_layers\n",
    "            )\n",
    "\n",
    "            # Fine-tune pruned model with lower learning rate\n",
    "            print(\"Fine-tuning pruned model...\")\n",
    "            optimizer_ft = optim.Adam(pruned_model.parameters(),\n",
    "                                     lr=config['learning_rate_finetune'],\n",
    "                                     weight_decay=config['weight_decay'])\n",
    "\n",
    "            scheduler_ft = CosineAnnealingLR(optimizer_ft, T_max=config['epochs'], eta_min=1e-7)\n",
    "\n",
    "            fine_tuned_model, ft_history = train_model(\n",
    "                pruned_model, train_loader, criterion, optimizer_ft, DEVICE,\n",
    "                config['epochs'], val_loader, config['patience'],\n",
    "                f\"{strategy_name}-{sparsity_ratio:.1%}\",\n",
    "                scheduler=scheduler_ft, use_mixup=False  # No mixup for fine-tuning\n",
    "            )\n",
    "\n",
    "            # Evaluate fine-tuned model\n",
    "            final_metrics = evaluate_model(fine_tuned_model, test_loader, example_input_device, criterion, DEVICE)\n",
    "            all_results[strategy_name][sparsity_ratio] = final_metrics\n",
    "\n",
    "            print(f\"Results: Accuracy={final_metrics['accuracy']:.2f}%, \"\n",
    "                  f\"MACs={final_metrics['macs']/1e6:.2f}M\")\n",
    "\n",
    "            # Save fine-tuned model\n",
    "            model_filename = f\"{strategy_name.lower()}_sparsity_{sparsity_ratio:.1f}.pth\"\n",
    "            model_path = os.path.join(config['models_dir'], model_filename)\n",
    "            save_model(fine_tuned_model, model_path, example_input_cpu)\n",
    "\n",
    "    # Save and analyze results\n",
    "    print(\"\\nSaving results...\")\n",
    "    summary_df = save_results_to_files(all_results, config['output_dir'])\n",
    "\n",
    "    # Create plots\n",
    "    print(\"Creating plots...\")\n",
    "    create_results_plots(summary_df, config['output_dir'])\n",
    "\n",
    "    # Print summary\n",
    "    print_results_table(summary_df)\n",
    "\n",
    "    print(f\"\\nüéâ All experiments completed!\")\n",
    "    print(f\"üìÅ Results saved to: {os.path.abspath(config['output_dir'])}\")\n",
    "    print(f\"üìÅ Models saved to: {os.path.abspath(config['models_dir'])}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "cf4ea7237f2ede7e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting Enhanced MobileNetV2 CIFAR-10 Pruning Experiments\n",
      "============================================================\n",
      "Loading CIFAR-10 dataset with data augmentation...\n",
      "Loading CIFAR-10 from: /home/muis/thesis/github-repo/master-thesis/cnn/mobile_net_v2/data\n",
      "DataLoaders created - Train: 45000, Val: 5000, Test: 10000\n",
      "\n",
      "Creating and training baseline model...\n",
      "‚úÖ Created MobileNetV2 without pretrained weights\n",
      "‚úÖ Adapted classifier for 10 classes\n",
      "Epoch 1/250 (Baseline Training): Train Loss: 2.2837, Train Acc: 17.79%, Val Loss: 2.0143, Val Acc: 26.04% (Best)\n",
      "Epoch 2/250 (Baseline Training): Train Loss: 1.9982, Train Acc: 26.11%, Val Loss: 1.7804, Val Acc: 33.08% (Best)\n",
      "Epoch 3/250 (Baseline Training): Train Loss: 1.8710, Train Acc: 32.05%, Val Loss: 1.6899, Val Acc: 37.88% (Best)\n",
      "Epoch 4/250 (Baseline Training): Train Loss: 1.7915, Train Acc: 35.89%, Val Loss: 1.5276, Val Acc: 43.54% (Best)\n",
      "Epoch 5/250 (Baseline Training): Train Loss: 1.7253, Train Acc: 38.98%, Val Loss: 1.5453, Val Acc: 45.70% (Best)\n",
      "Epoch 6/250 (Baseline Training): Train Loss: 1.7106, Train Acc: 39.98%, Val Loss: 2.1710, Val Acc: 34.18%\n",
      "Epoch 7/250 (Baseline Training): Train Loss: 1.6378, Train Acc: 42.88%, Val Loss: 1.3948, Val Acc: 50.94% (Best)\n",
      "Epoch 8/250 (Baseline Training): Train Loss: 1.6358, Train Acc: 43.48%, Val Loss: 1.3698, Val Acc: 51.68% (Best)\n",
      "Epoch 9/250 (Baseline Training): Train Loss: 1.5589, Train Acc: 46.59%, Val Loss: 1.4288, Val Acc: 51.18%\n",
      "Epoch 10/250 (Baseline Training): Train Loss: 1.6017, Train Acc: 44.80%, Val Loss: 1.3280, Val Acc: 52.82% (Best)\n",
      "Epoch 11/250 (Baseline Training): Train Loss: 1.5746, Train Acc: 46.16%, Val Loss: 1.3128, Val Acc: 54.74% (Best)\n",
      "Epoch 12/250 (Baseline Training): Train Loss: 1.5063, Train Acc: 49.42%, Val Loss: 1.2020, Val Acc: 58.12% (Best)\n",
      "Epoch 13/250 (Baseline Training): Train Loss: 1.4647, Train Acc: 50.85%, Val Loss: 1.1574, Val Acc: 59.50% (Best)\n",
      "Epoch 14/250 (Baseline Training): Train Loss: 1.4162, Train Acc: 52.97%, Val Loss: 1.1496, Val Acc: 59.76% (Best)\n",
      "Epoch 15/250 (Baseline Training): Train Loss: 1.3737, Train Acc: 54.51%, Val Loss: 1.0886, Val Acc: 62.38% (Best)\n",
      "Epoch 16/250 (Baseline Training): Train Loss: 1.3707, Train Acc: 54.86%, Val Loss: 1.0948, Val Acc: 62.12%\n",
      "Epoch 17/250 (Baseline Training): Train Loss: 1.3193, Train Acc: 56.49%, Val Loss: 1.0207, Val Acc: 65.80% (Best)\n",
      "Epoch 18/250 (Baseline Training): Train Loss: 1.3261, Train Acc: 56.75%, Val Loss: 0.9867, Val Acc: 65.24%\n",
      "Epoch 19/250 (Baseline Training): Train Loss: 1.2680, Train Acc: 58.58%, Val Loss: 1.0154, Val Acc: 66.60% (Best)\n",
      "Epoch 20/250 (Baseline Training): Train Loss: 1.3007, Train Acc: 58.03%, Val Loss: 0.9813, Val Acc: 67.28% (Best)\n",
      "Epoch 21/250 (Baseline Training): Train Loss: 1.1882, Train Acc: 61.99%, Val Loss: 0.9026, Val Acc: 69.28% (Best)\n",
      "Epoch 22/250 (Baseline Training): Train Loss: 1.2026, Train Acc: 61.16%, Val Loss: 0.9308, Val Acc: 66.96%\n",
      "Epoch 23/250 (Baseline Training): Train Loss: 1.2074, Train Acc: 61.55%, Val Loss: 0.8865, Val Acc: 71.06% (Best)\n",
      "Epoch 24/250 (Baseline Training): Train Loss: 1.1682, Train Acc: 63.09%, Val Loss: 0.8549, Val Acc: 71.78% (Best)\n",
      "Epoch 25/250 (Baseline Training): Train Loss: 1.1643, Train Acc: 63.37%, Val Loss: 0.8768, Val Acc: 71.80% (Best)\n",
      "Epoch 26/250 (Baseline Training): Train Loss: 1.1375, Train Acc: 64.29%, Val Loss: 0.8279, Val Acc: 71.26%\n",
      "Epoch 27/250 (Baseline Training): Train Loss: 1.0930, Train Acc: 65.75%, Val Loss: 0.8086, Val Acc: 72.26% (Best)\n",
      "Epoch 28/250 (Baseline Training): Train Loss: 1.0721, Train Acc: 66.64%, Val Loss: 0.7933, Val Acc: 74.94% (Best)\n",
      "Epoch 29/250 (Baseline Training): Train Loss: 1.1364, Train Acc: 64.95%, Val Loss: 0.8130, Val Acc: 73.96%\n",
      "Epoch 30/250 (Baseline Training): Train Loss: 1.0504, Train Acc: 67.45%, Val Loss: 0.7562, Val Acc: 73.52%\n",
      "Epoch 31/250 (Baseline Training): Train Loss: 1.0295, Train Acc: 68.30%, Val Loss: 0.7657, Val Acc: 74.10%\n",
      "Epoch 32/250 (Baseline Training): Train Loss: 1.0595, Train Acc: 67.47%, Val Loss: 0.7109, Val Acc: 76.84% (Best)\n",
      "Epoch 33/250 (Baseline Training): Train Loss: 1.0307, Train Acc: 68.49%, Val Loss: 0.7129, Val Acc: 77.16% (Best)\n",
      "Epoch 34/250 (Baseline Training): Train Loss: 1.0025, Train Acc: 69.40%, Val Loss: 0.7774, Val Acc: 75.74%\n",
      "Epoch 35/250 (Baseline Training): Train Loss: 0.9943, Train Acc: 69.82%, Val Loss: 0.6684, Val Acc: 78.72% (Best)\n",
      "Epoch 36/250 (Baseline Training): Train Loss: 0.9827, Train Acc: 70.05%, Val Loss: 0.7173, Val Acc: 77.06%\n",
      "Epoch 37/250 (Baseline Training): Train Loss: 1.0210, Train Acc: 69.06%, Val Loss: 0.7101, Val Acc: 76.14%\n",
      "Epoch 38/250 (Baseline Training): Train Loss: 0.9810, Train Acc: 70.19%, Val Loss: 0.6983, Val Acc: 78.06%\n",
      "Epoch 39/250 (Baseline Training): Train Loss: 0.9809, Train Acc: 70.55%, Val Loss: 0.7230, Val Acc: 76.08%\n",
      "Epoch 40/250 (Baseline Training): Train Loss: 0.9867, Train Acc: 70.35%, Val Loss: 0.6712, Val Acc: 78.24%\n",
      "Epoch 41/250 (Baseline Training): Train Loss: 0.9515, Train Acc: 71.33%, Val Loss: 0.6315, Val Acc: 79.32% (Best)\n",
      "Epoch 42/250 (Baseline Training): Train Loss: 0.9745, Train Acc: 70.73%, Val Loss: 0.6232, Val Acc: 79.84% (Best)\n",
      "Epoch 43/250 (Baseline Training): Train Loss: 0.9397, Train Acc: 71.55%, Val Loss: 0.6005, Val Acc: 79.78%\n",
      "Epoch 44/250 (Baseline Training): Train Loss: 0.9169, Train Acc: 72.30%, Val Loss: 0.6434, Val Acc: 79.28%\n",
      "Epoch 45/250 (Baseline Training): Train Loss: 0.9668, Train Acc: 71.04%, Val Loss: 0.6636, Val Acc: 78.80%\n",
      "Epoch 46/250 (Baseline Training): Train Loss: 0.9293, Train Acc: 72.16%, Val Loss: 0.6167, Val Acc: 79.96% (Best)\n",
      "Epoch 47/250 (Baseline Training): Train Loss: 0.9041, Train Acc: 72.92%, Val Loss: 0.5912, Val Acc: 80.10% (Best)\n",
      "Epoch 48/250 (Baseline Training): Train Loss: 0.9247, Train Acc: 72.36%, Val Loss: 0.6053, Val Acc: 80.14% (Best)\n",
      "Epoch 49/250 (Baseline Training): Train Loss: 0.9247, Train Acc: 72.19%, Val Loss: 0.6219, Val Acc: 79.40%\n",
      "Epoch 50/250 (Baseline Training): Train Loss: 0.9222, Train Acc: 72.22%, Val Loss: 0.5800, Val Acc: 81.04% (Best)\n",
      "Epoch 51/250 (Baseline Training): Train Loss: 0.9350, Train Acc: 71.98%, Val Loss: 0.6314, Val Acc: 80.30%\n",
      "Epoch 52/250 (Baseline Training): Train Loss: 0.8908, Train Acc: 73.36%, Val Loss: 0.5895, Val Acc: 81.60% (Best)\n",
      "Epoch 53/250 (Baseline Training): Train Loss: 0.9349, Train Acc: 72.03%, Val Loss: 0.6035, Val Acc: 80.50%\n",
      "Epoch 54/250 (Baseline Training): Train Loss: 0.8952, Train Acc: 73.06%, Val Loss: 0.5959, Val Acc: 81.18%\n",
      "Epoch 55/250 (Baseline Training): Train Loss: 0.8913, Train Acc: 73.12%, Val Loss: 0.5820, Val Acc: 80.30%\n",
      "Epoch 56/250 (Baseline Training): Train Loss: 0.9092, Train Acc: 72.46%, Val Loss: 0.6546, Val Acc: 79.20%\n",
      "Epoch 57/250 (Baseline Training): Train Loss: 0.8892, Train Acc: 73.17%, Val Loss: 0.5809, Val Acc: 81.00%\n",
      "Epoch 58/250 (Baseline Training): Train Loss: 0.9149, Train Acc: 72.47%, Val Loss: 0.5822, Val Acc: 80.98%\n",
      "Epoch 59/250 (Baseline Training): Train Loss: 0.8536, Train Acc: 74.58%, Val Loss: 0.5944, Val Acc: 80.98%\n",
      "Epoch 60/250 (Baseline Training): Train Loss: 0.9104, Train Acc: 72.79%, Val Loss: 0.5952, Val Acc: 80.30%\n",
      "Epoch 61/250 (Baseline Training): Train Loss: 0.8639, Train Acc: 74.28%, Val Loss: 0.5513, Val Acc: 82.20% (Best)\n",
      "Epoch 62/250 (Baseline Training): Train Loss: 0.8450, Train Acc: 74.50%, Val Loss: 0.5852, Val Acc: 81.46%\n",
      "Epoch 63/250 (Baseline Training): Train Loss: 0.8461, Train Acc: 74.55%, Val Loss: 0.5656, Val Acc: 81.78%\n",
      "Epoch 64/250 (Baseline Training): Train Loss: 0.8989, Train Acc: 72.88%, Val Loss: 0.5925, Val Acc: 82.38% (Best)\n",
      "Epoch 65/250 (Baseline Training): Train Loss: 0.8965, Train Acc: 72.82%, Val Loss: 0.5480, Val Acc: 82.68% (Best)\n",
      "Epoch 66/250 (Baseline Training): Train Loss: 0.8072, Train Acc: 75.97%, Val Loss: 0.5662, Val Acc: 81.56%\n",
      "Epoch 67/250 (Baseline Training): Train Loss: 0.8513, Train Acc: 74.66%, Val Loss: 0.6164, Val Acc: 82.18%\n",
      "Epoch 68/250 (Baseline Training): Train Loss: 0.8378, Train Acc: 74.98%, Val Loss: 0.5576, Val Acc: 81.38%\n",
      "Epoch 69/250 (Baseline Training): Train Loss: 0.8238, Train Acc: 75.02%, Val Loss: 0.6527, Val Acc: 79.56%\n",
      "Epoch 70/250 (Baseline Training): Train Loss: 0.8265, Train Acc: 75.22%, Val Loss: 0.5923, Val Acc: 80.90%\n",
      "Epoch 71/250 (Baseline Training): Train Loss: 0.8702, Train Acc: 73.84%, Val Loss: 0.5762, Val Acc: 82.06%\n",
      "Epoch 72/250 (Baseline Training): Train Loss: 0.8326, Train Acc: 75.12%, Val Loss: 0.5880, Val Acc: 81.64%\n",
      "Epoch 73/250 (Baseline Training): Train Loss: 0.8321, Train Acc: 75.09%, Val Loss: 0.5594, Val Acc: 82.66%\n",
      "Epoch 74/250 (Baseline Training): Train Loss: 0.8366, Train Acc: 74.92%, Val Loss: 0.5523, Val Acc: 82.20%\n",
      "Epoch 75/250 (Baseline Training): Train Loss: 0.8266, Train Acc: 74.92%, Val Loss: 0.6052, Val Acc: 81.68%\n",
      "Epoch 76/250 (Baseline Training): Train Loss: 0.8641, Train Acc: 74.00%, Val Loss: 0.5637, Val Acc: 82.66%\n",
      "Epoch 77/250 (Baseline Training): Train Loss: 0.8132, Train Acc: 75.34%, Val Loss: 0.5737, Val Acc: 82.90% (Best)\n",
      "Epoch 78/250 (Baseline Training): Train Loss: 0.7946, Train Acc: 76.14%, Val Loss: 0.4894, Val Acc: 83.64% (Best)\n",
      "Epoch 79/250 (Baseline Training): Train Loss: 0.7961, Train Acc: 76.14%, Val Loss: 0.5760, Val Acc: 81.36%\n",
      "Epoch 80/250 (Baseline Training): Train Loss: 0.7936, Train Acc: 76.25%, Val Loss: 0.5295, Val Acc: 82.96%\n",
      "Epoch 81/250 (Baseline Training): Train Loss: 0.8128, Train Acc: 75.91%, Val Loss: 0.5280, Val Acc: 83.36%\n",
      "Epoch 82/250 (Baseline Training): Train Loss: 0.8076, Train Acc: 76.17%, Val Loss: 0.5496, Val Acc: 84.00% (Best)\n",
      "Epoch 83/250 (Baseline Training): Train Loss: 0.8134, Train Acc: 75.87%, Val Loss: 0.5374, Val Acc: 83.44%\n",
      "Epoch 84/250 (Baseline Training): Train Loss: 0.8044, Train Acc: 76.01%, Val Loss: 0.5703, Val Acc: 83.36%\n",
      "Epoch 85/250 (Baseline Training): Train Loss: 0.8030, Train Acc: 76.01%, Val Loss: 0.5007, Val Acc: 84.02% (Best)\n",
      "Epoch 86/250 (Baseline Training): Train Loss: 0.7901, Train Acc: 76.37%, Val Loss: 0.5222, Val Acc: 83.28%\n",
      "Epoch 87/250 (Baseline Training): Train Loss: 0.7440, Train Acc: 77.59%, Val Loss: 0.4919, Val Acc: 83.44%\n",
      "Epoch 88/250 (Baseline Training): Train Loss: 0.7833, Train Acc: 76.38%, Val Loss: 0.4855, Val Acc: 83.90%\n",
      "Epoch 89/250 (Baseline Training): Train Loss: 0.7612, Train Acc: 77.08%, Val Loss: 0.4936, Val Acc: 83.58%\n",
      "Epoch 90/250 (Baseline Training): Train Loss: 0.7628, Train Acc: 77.21%, Val Loss: 0.5180, Val Acc: 83.92%\n",
      "Epoch 91/250 (Baseline Training): Train Loss: 0.7641, Train Acc: 76.95%, Val Loss: 0.5074, Val Acc: 83.94%\n",
      "Epoch 92/250 (Baseline Training): Train Loss: 0.7580, Train Acc: 77.35%, Val Loss: 0.5121, Val Acc: 83.36%\n",
      "Epoch 93/250 (Baseline Training): Train Loss: 0.7701, Train Acc: 76.92%, Val Loss: 0.5349, Val Acc: 84.06% (Best)\n",
      "Epoch 94/250 (Baseline Training): Train Loss: 0.7970, Train Acc: 76.53%, Val Loss: 0.4798, Val Acc: 84.74% (Best)\n",
      "Epoch 95/250 (Baseline Training): Train Loss: 0.8063, Train Acc: 75.80%, Val Loss: 0.5054, Val Acc: 84.46%\n",
      "Epoch 96/250 (Baseline Training): Train Loss: 0.7903, Train Acc: 76.44%, Val Loss: 0.5153, Val Acc: 84.24%\n",
      "Epoch 97/250 (Baseline Training): Train Loss: 0.7507, Train Acc: 77.52%, Val Loss: 0.4764, Val Acc: 84.40%\n",
      "Epoch 98/250 (Baseline Training): Train Loss: 0.7578, Train Acc: 77.28%, Val Loss: 0.4853, Val Acc: 83.94%\n",
      "Epoch 99/250 (Baseline Training): Train Loss: 0.7315, Train Acc: 78.06%, Val Loss: 0.5027, Val Acc: 84.80% (Best)\n",
      "Epoch 100/250 (Baseline Training): Train Loss: 0.7471, Train Acc: 77.37%, Val Loss: 0.5424, Val Acc: 83.50%\n",
      "Epoch 101/250 (Baseline Training): Train Loss: 0.7931, Train Acc: 76.27%, Val Loss: 0.5125, Val Acc: 84.60%\n",
      "Epoch 102/250 (Baseline Training): Train Loss: 0.7454, Train Acc: 77.76%, Val Loss: 0.4690, Val Acc: 83.88%\n",
      "Epoch 103/250 (Baseline Training): Train Loss: 0.7635, Train Acc: 77.21%, Val Loss: 0.4814, Val Acc: 84.90% (Best)\n",
      "Epoch 104/250 (Baseline Training): Train Loss: 0.7758, Train Acc: 76.94%, Val Loss: 0.4949, Val Acc: 84.44%\n",
      "Epoch 105/250 (Baseline Training): Train Loss: 0.7712, Train Acc: 77.09%, Val Loss: 0.4927, Val Acc: 84.54%\n",
      "Epoch 106/250 (Baseline Training): Train Loss: 0.7298, Train Acc: 78.22%, Val Loss: 0.4898, Val Acc: 84.24%\n",
      "Epoch 107/250 (Baseline Training): Train Loss: 0.7373, Train Acc: 77.97%, Val Loss: 0.4552, Val Acc: 84.90%\n",
      "Epoch 108/250 (Baseline Training): Train Loss: 0.7262, Train Acc: 78.60%, Val Loss: 0.5400, Val Acc: 83.76%\n",
      "Epoch 109/250 (Baseline Training): Train Loss: 0.7617, Train Acc: 77.36%, Val Loss: 0.5087, Val Acc: 84.20%\n",
      "Epoch 110/250 (Baseline Training): Train Loss: 0.7500, Train Acc: 77.24%, Val Loss: 0.4933, Val Acc: 85.12% (Best)\n",
      "Epoch 111/250 (Baseline Training): Train Loss: 0.7004, Train Acc: 78.95%, Val Loss: 0.5154, Val Acc: 84.94%\n",
      "Epoch 112/250 (Baseline Training): Train Loss: 0.7108, Train Acc: 78.90%, Val Loss: 0.5646, Val Acc: 84.06%\n",
      "Epoch 113/250 (Baseline Training): Train Loss: 0.7114, Train Acc: 78.93%, Val Loss: 0.5089, Val Acc: 83.66%\n",
      "Epoch 114/250 (Baseline Training): Train Loss: 0.7146, Train Acc: 79.04%, Val Loss: 0.5054, Val Acc: 84.48%\n",
      "Epoch 115/250 (Baseline Training): Train Loss: 0.7274, Train Acc: 78.35%, Val Loss: 0.4699, Val Acc: 85.06%\n",
      "Epoch 116/250 (Baseline Training): Train Loss: 0.7484, Train Acc: 77.72%, Val Loss: 0.4952, Val Acc: 85.38% (Best)\n",
      "Epoch 117/250 (Baseline Training): Train Loss: 0.6996, Train Acc: 79.02%, Val Loss: 0.5454, Val Acc: 84.18%\n",
      "Epoch 118/250 (Baseline Training): Train Loss: 0.7200, Train Acc: 78.32%, Val Loss: 0.4821, Val Acc: 84.24%\n",
      "Epoch 119/250 (Baseline Training): Train Loss: 0.7176, Train Acc: 78.93%, Val Loss: 0.5499, Val Acc: 84.04%\n",
      "Epoch 120/250 (Baseline Training): Train Loss: 0.7194, Train Acc: 78.81%, Val Loss: 0.4331, Val Acc: 85.46% (Best)\n",
      "Epoch 121/250 (Baseline Training): Train Loss: 0.7507, Train Acc: 77.91%, Val Loss: 0.4792, Val Acc: 84.98%\n",
      "Epoch 122/250 (Baseline Training): Train Loss: 0.7121, Train Acc: 79.24%, Val Loss: 0.4996, Val Acc: 84.90%\n",
      "Epoch 123/250 (Baseline Training): Train Loss: 0.6947, Train Acc: 79.37%, Val Loss: 0.4905, Val Acc: 84.82%\n",
      "Epoch 124/250 (Baseline Training): Train Loss: 0.7134, Train Acc: 78.98%, Val Loss: 0.4582, Val Acc: 85.22%\n",
      "Epoch 125/250 (Baseline Training): Train Loss: 0.7182, Train Acc: 78.74%, Val Loss: 0.4409, Val Acc: 85.56% (Best)\n",
      "Epoch 126/250 (Baseline Training): Train Loss: 0.7239, Train Acc: 78.85%, Val Loss: 0.5058, Val Acc: 84.42%\n",
      "Epoch 127/250 (Baseline Training): Train Loss: 0.6888, Train Acc: 79.51%, Val Loss: 0.4956, Val Acc: 85.90% (Best)\n",
      "Epoch 128/250 (Baseline Training): Train Loss: 0.7077, Train Acc: 78.93%, Val Loss: 0.4803, Val Acc: 85.36%\n",
      "Epoch 129/250 (Baseline Training): Train Loss: 0.7235, Train Acc: 78.83%, Val Loss: 0.4783, Val Acc: 85.88%\n",
      "Epoch 130/250 (Baseline Training): Train Loss: 0.6880, Train Acc: 80.10%, Val Loss: 0.4639, Val Acc: 85.66%\n",
      "Epoch 131/250 (Baseline Training): Train Loss: 0.7158, Train Acc: 78.77%, Val Loss: 0.4553, Val Acc: 86.22% (Best)\n",
      "Epoch 132/250 (Baseline Training): Train Loss: 0.6898, Train Acc: 79.55%, Val Loss: 0.4426, Val Acc: 85.52%\n",
      "Epoch 133/250 (Baseline Training): Train Loss: 0.7230, Train Acc: 78.47%, Val Loss: 0.4486, Val Acc: 85.42%\n",
      "Epoch 134/250 (Baseline Training): Train Loss: 0.6695, Train Acc: 80.43%, Val Loss: 0.5059, Val Acc: 85.46%\n",
      "Epoch 135/250 (Baseline Training): Train Loss: 0.6801, Train Acc: 79.93%, Val Loss: 0.4689, Val Acc: 86.00%\n",
      "Epoch 136/250 (Baseline Training): Train Loss: 0.6678, Train Acc: 80.35%, Val Loss: 0.4828, Val Acc: 85.24%\n",
      "Epoch 137/250 (Baseline Training): Train Loss: 0.7046, Train Acc: 79.32%, Val Loss: 0.4528, Val Acc: 85.78%\n",
      "Epoch 138/250 (Baseline Training): Train Loss: 0.7213, Train Acc: 78.50%, Val Loss: 0.4380, Val Acc: 85.62%\n",
      "Epoch 139/250 (Baseline Training): Train Loss: 0.6943, Train Acc: 80.40%, Val Loss: 0.4334, Val Acc: 86.22%\n",
      "Epoch 140/250 (Baseline Training): Train Loss: 0.6729, Train Acc: 80.30%, Val Loss: 0.4500, Val Acc: 85.78%\n",
      "Epoch 141/250 (Baseline Training): Train Loss: 0.6670, Train Acc: 80.54%, Val Loss: 0.4271, Val Acc: 85.78%\n",
      "Epoch 142/250 (Baseline Training): Train Loss: 0.6954, Train Acc: 79.47%, Val Loss: 0.4785, Val Acc: 85.76%\n",
      "Epoch 143/250 (Baseline Training): Train Loss: 0.7044, Train Acc: 79.64%, Val Loss: 0.4435, Val Acc: 86.14%\n",
      "Epoch 144/250 (Baseline Training): Train Loss: 0.6871, Train Acc: 80.18%, Val Loss: 0.4525, Val Acc: 85.82%\n",
      "Epoch 145/250 (Baseline Training): Train Loss: 0.6738, Train Acc: 80.47%, Val Loss: 0.4324, Val Acc: 86.22%\n",
      "Epoch 146/250 (Baseline Training): Train Loss: 0.6593, Train Acc: 80.51%, Val Loss: 0.4445, Val Acc: 86.18%\n",
      "Early stopping triggered after 146 epochs\n",
      "Loaded best model state with val accuracy: 86.22%\n",
      "‚úÖ Model saved to ./models_mobilenetv2_enhanced/baseline_model.pth\n",
      "‚úÖ ONNX model saved to ./models_mobilenetv2_enhanced/baseline_model.onnx\n",
      "\n",
      "Evaluating baseline model...\n",
      "Baseline Results: Accuracy=85.20%, MACs=6.52M, Params=2.24M\n",
      "\n",
      "Starting pruning experiments...\n",
      "\n",
      "--- Strategy: BNScale ---\n",
      "\n",
      "Processing BNScale at 20.0% sparsity...\n",
      "‚úÖ Created MobileNetV2 without pretrained weights\n",
      "‚úÖ Adapted classifier for 10 classes\n",
      "Initial MACs: 6.52M\n",
      "Applying BNScaleImportance pruning at 20.0% sparsity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch_pruning/pruner/algorithms/base_pruner.py:87: UserWarning: ch_sparsity is deprecated in v1.3.0. Please use pruning_ratio.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final MACs: 6.00M (Reduction: 8.0%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/250 (BNScale-20.0%): Train Loss: 0.3536, Train Acc: 88.23%, Val Loss: 0.4270, Val Acc: 85.46% (Best)\n",
      "Epoch 2/250 (BNScale-20.0%): Train Loss: 0.3145, Train Acc: 89.42%, Val Loss: 0.4210, Val Acc: 85.62% (Best)\n",
      "Epoch 3/250 (BNScale-20.0%): Train Loss: 0.3053, Train Acc: 89.57%, Val Loss: 0.4116, Val Acc: 85.94% (Best)\n",
      "Epoch 4/250 (BNScale-20.0%): Train Loss: 0.2946, Train Acc: 89.83%, Val Loss: 0.4068, Val Acc: 86.62% (Best)\n",
      "Epoch 5/250 (BNScale-20.0%): Train Loss: 0.2896, Train Acc: 90.04%, Val Loss: 0.4106, Val Acc: 86.44%\n",
      "Epoch 6/250 (BNScale-20.0%): Train Loss: 0.2822, Train Acc: 90.47%, Val Loss: 0.4060, Val Acc: 86.24%\n",
      "Epoch 7/250 (BNScale-20.0%): Train Loss: 0.2782, Train Acc: 90.56%, Val Loss: 0.4085, Val Acc: 86.28%\n",
      "Epoch 8/250 (BNScale-20.0%): Train Loss: 0.2704, Train Acc: 90.84%, Val Loss: 0.4144, Val Acc: 86.08%\n",
      "Epoch 9/250 (BNScale-20.0%): Train Loss: 0.2668, Train Acc: 90.81%, Val Loss: 0.4095, Val Acc: 86.42%\n",
      "Epoch 10/250 (BNScale-20.0%): Train Loss: 0.2611, Train Acc: 91.05%, Val Loss: 0.4131, Val Acc: 86.28%\n",
      "Epoch 11/250 (BNScale-20.0%): Train Loss: 0.2592, Train Acc: 91.08%, Val Loss: 0.4091, Val Acc: 86.22%\n",
      "Epoch 12/250 (BNScale-20.0%): Train Loss: 0.2612, Train Acc: 90.98%, Val Loss: 0.4066, Val Acc: 86.40%\n",
      "Epoch 13/250 (BNScale-20.0%): Train Loss: 0.2595, Train Acc: 91.14%, Val Loss: 0.3985, Val Acc: 86.40%\n",
      "Epoch 14/250 (BNScale-20.0%): Train Loss: 0.2516, Train Acc: 91.38%, Val Loss: 0.4022, Val Acc: 86.42%\n",
      "Epoch 15/250 (BNScale-20.0%): Train Loss: 0.2538, Train Acc: 91.15%, Val Loss: 0.4040, Val Acc: 86.18%\n",
      "Epoch 16/250 (BNScale-20.0%): Train Loss: 0.2517, Train Acc: 91.55%, Val Loss: 0.4033, Val Acc: 86.26%\n",
      "Epoch 17/250 (BNScale-20.0%): Train Loss: 0.2497, Train Acc: 91.34%, Val Loss: 0.3994, Val Acc: 86.44%\n",
      "Epoch 18/250 (BNScale-20.0%): Train Loss: 0.2480, Train Acc: 91.52%, Val Loss: 0.4075, Val Acc: 86.34%\n",
      "Epoch 19/250 (BNScale-20.0%): Train Loss: 0.2442, Train Acc: 91.51%, Val Loss: 0.4158, Val Acc: 86.30%\n",
      "Early stopping triggered after 19 epochs\n",
      "Loaded best model state with val accuracy: 86.62%\n",
      "Results: Accuracy=85.59%, MACs=6.00M\n",
      "‚úÖ Model saved to ./models_mobilenetv2_enhanced/bnscale_sparsity_0.2.pth\n",
      "‚úÖ ONNX model saved to ./models_mobilenetv2_enhanced/bnscale_sparsity_0.2.onnx\n",
      "\n",
      "Processing BNScale at 50.0% sparsity...\n",
      "‚úÖ Created MobileNetV2 without pretrained weights\n",
      "‚úÖ Adapted classifier for 10 classes\n",
      "Initial MACs: 6.52M\n",
      "Applying BNScaleImportance pruning at 50.0% sparsity...\n",
      "Final MACs: 5.30M (Reduction: 18.7%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/250 (BNScale-50.0%): Train Loss: 0.4766, Train Acc: 84.00%, Val Loss: 0.4806, Val Acc: 83.48% (Best)\n",
      "Epoch 2/250 (BNScale-50.0%): Train Loss: 0.4062, Train Acc: 86.06%, Val Loss: 0.4645, Val Acc: 83.94% (Best)\n",
      "Epoch 3/250 (BNScale-50.0%): Train Loss: 0.3741, Train Acc: 87.13%, Val Loss: 0.4436, Val Acc: 84.50% (Best)\n",
      "Epoch 4/250 (BNScale-50.0%): Train Loss: 0.3665, Train Acc: 87.46%, Val Loss: 0.4386, Val Acc: 84.58% (Best)\n",
      "Epoch 5/250 (BNScale-50.0%): Train Loss: 0.3540, Train Acc: 87.98%, Val Loss: 0.4426, Val Acc: 84.96% (Best)\n",
      "Epoch 6/250 (BNScale-50.0%): Train Loss: 0.3386, Train Acc: 88.28%, Val Loss: 0.4408, Val Acc: 84.88%\n",
      "Epoch 7/250 (BNScale-50.0%): Train Loss: 0.3320, Train Acc: 88.64%, Val Loss: 0.4390, Val Acc: 85.20% (Best)\n",
      "Epoch 8/250 (BNScale-50.0%): Train Loss: 0.3278, Train Acc: 88.74%, Val Loss: 0.4302, Val Acc: 85.62% (Best)\n",
      "Epoch 9/250 (BNScale-50.0%): Train Loss: 0.3202, Train Acc: 88.94%, Val Loss: 0.4350, Val Acc: 85.18%\n",
      "Epoch 10/250 (BNScale-50.0%): Train Loss: 0.3157, Train Acc: 89.15%, Val Loss: 0.4287, Val Acc: 85.22%\n",
      "Epoch 11/250 (BNScale-50.0%): Train Loss: 0.3121, Train Acc: 89.12%, Val Loss: 0.4300, Val Acc: 85.38%\n",
      "Epoch 12/250 (BNScale-50.0%): Train Loss: 0.3029, Train Acc: 89.40%, Val Loss: 0.4296, Val Acc: 85.44%\n",
      "Epoch 13/250 (BNScale-50.0%): Train Loss: 0.3004, Train Acc: 89.62%, Val Loss: 0.4257, Val Acc: 85.44%\n",
      "Epoch 14/250 (BNScale-50.0%): Train Loss: 0.2980, Train Acc: 89.73%, Val Loss: 0.4269, Val Acc: 85.52%\n",
      "Epoch 15/250 (BNScale-50.0%): Train Loss: 0.2973, Train Acc: 89.71%, Val Loss: 0.4257, Val Acc: 85.66% (Best)\n",
      "Epoch 16/250 (BNScale-50.0%): Train Loss: 0.2941, Train Acc: 89.77%, Val Loss: 0.4210, Val Acc: 85.24%\n",
      "Epoch 17/250 (BNScale-50.0%): Train Loss: 0.2946, Train Acc: 89.87%, Val Loss: 0.4215, Val Acc: 85.08%\n",
      "Epoch 18/250 (BNScale-50.0%): Train Loss: 0.2854, Train Acc: 90.09%, Val Loss: 0.4155, Val Acc: 85.66%\n",
      "Epoch 19/250 (BNScale-50.0%): Train Loss: 0.2830, Train Acc: 90.25%, Val Loss: 0.4190, Val Acc: 85.66%\n",
      "Epoch 20/250 (BNScale-50.0%): Train Loss: 0.2823, Train Acc: 90.27%, Val Loss: 0.4227, Val Acc: 85.34%\n",
      "Epoch 21/250 (BNScale-50.0%): Train Loss: 0.2803, Train Acc: 90.11%, Val Loss: 0.4209, Val Acc: 85.44%\n",
      "Epoch 22/250 (BNScale-50.0%): Train Loss: 0.2756, Train Acc: 90.18%, Val Loss: 0.4199, Val Acc: 85.70% (Best)\n",
      "Epoch 23/250 (BNScale-50.0%): Train Loss: 0.2745, Train Acc: 90.50%, Val Loss: 0.4231, Val Acc: 85.46%\n",
      "Epoch 24/250 (BNScale-50.0%): Train Loss: 0.2734, Train Acc: 90.33%, Val Loss: 0.4260, Val Acc: 85.72% (Best)\n",
      "Epoch 25/250 (BNScale-50.0%): Train Loss: 0.2701, Train Acc: 90.50%, Val Loss: 0.4205, Val Acc: 85.60%\n",
      "Epoch 26/250 (BNScale-50.0%): Train Loss: 0.2647, Train Acc: 90.86%, Val Loss: 0.4272, Val Acc: 85.90% (Best)\n",
      "Epoch 27/250 (BNScale-50.0%): Train Loss: 0.2676, Train Acc: 90.78%, Val Loss: 0.4216, Val Acc: 85.92% (Best)\n",
      "Epoch 28/250 (BNScale-50.0%): Train Loss: 0.2663, Train Acc: 90.72%, Val Loss: 0.4220, Val Acc: 85.94% (Best)\n",
      "Epoch 29/250 (BNScale-50.0%): Train Loss: 0.2641, Train Acc: 90.83%, Val Loss: 0.4238, Val Acc: 85.70%\n",
      "Epoch 30/250 (BNScale-50.0%): Train Loss: 0.2639, Train Acc: 90.78%, Val Loss: 0.4267, Val Acc: 85.58%\n",
      "Epoch 31/250 (BNScale-50.0%): Train Loss: 0.2642, Train Acc: 90.69%, Val Loss: 0.4253, Val Acc: 85.84%\n",
      "Epoch 32/250 (BNScale-50.0%): Train Loss: 0.2622, Train Acc: 90.80%, Val Loss: 0.4288, Val Acc: 85.46%\n",
      "Epoch 33/250 (BNScale-50.0%): Train Loss: 0.2615, Train Acc: 90.85%, Val Loss: 0.4323, Val Acc: 85.78%\n",
      "Epoch 34/250 (BNScale-50.0%): Train Loss: 0.2550, Train Acc: 91.00%, Val Loss: 0.4287, Val Acc: 85.64%\n",
      "Epoch 35/250 (BNScale-50.0%): Train Loss: 0.2556, Train Acc: 90.99%, Val Loss: 0.4386, Val Acc: 85.54%\n",
      "Epoch 36/250 (BNScale-50.0%): Train Loss: 0.2528, Train Acc: 91.18%, Val Loss: 0.4327, Val Acc: 85.56%\n",
      "Epoch 37/250 (BNScale-50.0%): Train Loss: 0.2495, Train Acc: 91.32%, Val Loss: 0.4308, Val Acc: 85.64%\n",
      "Epoch 38/250 (BNScale-50.0%): Train Loss: 0.2509, Train Acc: 91.30%, Val Loss: 0.4407, Val Acc: 85.52%\n",
      "Epoch 39/250 (BNScale-50.0%): Train Loss: 0.2464, Train Acc: 91.25%, Val Loss: 0.4375, Val Acc: 85.78%\n",
      "Epoch 40/250 (BNScale-50.0%): Train Loss: 0.2492, Train Acc: 91.30%, Val Loss: 0.4389, Val Acc: 85.78%\n",
      "Epoch 41/250 (BNScale-50.0%): Train Loss: 0.2435, Train Acc: 91.46%, Val Loss: 0.4342, Val Acc: 85.86%\n",
      "Epoch 42/250 (BNScale-50.0%): Train Loss: 0.2459, Train Acc: 91.33%, Val Loss: 0.4390, Val Acc: 85.96% (Best)\n",
      "Epoch 43/250 (BNScale-50.0%): Train Loss: 0.2480, Train Acc: 91.36%, Val Loss: 0.4390, Val Acc: 85.50%\n",
      "Epoch 44/250 (BNScale-50.0%): Train Loss: 0.2450, Train Acc: 91.40%, Val Loss: 0.4409, Val Acc: 85.46%\n",
      "Epoch 45/250 (BNScale-50.0%): Train Loss: 0.2401, Train Acc: 91.68%, Val Loss: 0.4367, Val Acc: 85.60%\n",
      "Epoch 46/250 (BNScale-50.0%): Train Loss: 0.2398, Train Acc: 91.60%, Val Loss: 0.4368, Val Acc: 85.62%\n",
      "Epoch 47/250 (BNScale-50.0%): Train Loss: 0.2408, Train Acc: 91.55%, Val Loss: 0.4467, Val Acc: 85.34%\n",
      "Epoch 48/250 (BNScale-50.0%): Train Loss: 0.2366, Train Acc: 91.68%, Val Loss: 0.4386, Val Acc: 85.44%\n",
      "Epoch 49/250 (BNScale-50.0%): Train Loss: 0.2355, Train Acc: 91.80%, Val Loss: 0.4560, Val Acc: 85.42%\n",
      "Epoch 50/250 (BNScale-50.0%): Train Loss: 0.2397, Train Acc: 91.60%, Val Loss: 0.4507, Val Acc: 85.64%\n",
      "Epoch 51/250 (BNScale-50.0%): Train Loss: 0.2353, Train Acc: 91.65%, Val Loss: 0.4498, Val Acc: 85.46%\n",
      "Epoch 52/250 (BNScale-50.0%): Train Loss: 0.2358, Train Acc: 91.76%, Val Loss: 0.4488, Val Acc: 85.76%\n",
      "Epoch 53/250 (BNScale-50.0%): Train Loss: 0.2334, Train Acc: 91.77%, Val Loss: 0.4466, Val Acc: 85.64%\n",
      "Epoch 54/250 (BNScale-50.0%): Train Loss: 0.2301, Train Acc: 92.03%, Val Loss: 0.4362, Val Acc: 85.40%\n",
      "Epoch 55/250 (BNScale-50.0%): Train Loss: 0.2330, Train Acc: 91.89%, Val Loss: 0.4420, Val Acc: 85.70%\n",
      "Epoch 56/250 (BNScale-50.0%): Train Loss: 0.2320, Train Acc: 91.85%, Val Loss: 0.4383, Val Acc: 85.86%\n",
      "Epoch 57/250 (BNScale-50.0%): Train Loss: 0.2256, Train Acc: 92.10%, Val Loss: 0.4432, Val Acc: 85.50%\n",
      "Early stopping triggered after 57 epochs\n",
      "Loaded best model state with val accuracy: 85.96%\n",
      "Results: Accuracy=85.74%, MACs=5.30M\n",
      "‚úÖ Model saved to ./models_mobilenetv2_enhanced/bnscale_sparsity_0.5.pth\n",
      "‚úÖ ONNX model saved to ./models_mobilenetv2_enhanced/bnscale_sparsity_0.5.onnx\n",
      "\n",
      "Processing BNScale at 70.0% sparsity...\n",
      "‚úÖ Created MobileNetV2 without pretrained weights\n",
      "‚úÖ Adapted classifier for 10 classes\n",
      "Initial MACs: 6.52M\n",
      "Applying BNScaleImportance pruning at 70.0% sparsity...\n",
      "Final MACs: 4.87M (Reduction: 25.3%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/250 (BNScale-70.0%): Train Loss: 0.6063, Train Acc: 79.77%, Val Loss: 0.5495, Val Acc: 81.20% (Best)\n",
      "Epoch 2/250 (BNScale-70.0%): Train Loss: 0.4816, Train Acc: 83.53%, Val Loss: 0.5011, Val Acc: 82.70% (Best)\n",
      "Epoch 3/250 (BNScale-70.0%): Train Loss: 0.4452, Train Acc: 84.77%, Val Loss: 0.4806, Val Acc: 83.50% (Best)\n",
      "Epoch 4/250 (BNScale-70.0%): Train Loss: 0.4213, Train Acc: 85.62%, Val Loss: 0.4752, Val Acc: 83.74% (Best)\n",
      "Epoch 5/250 (BNScale-70.0%): Train Loss: 0.4000, Train Acc: 86.40%, Val Loss: 0.4592, Val Acc: 83.92% (Best)\n",
      "Epoch 6/250 (BNScale-70.0%): Train Loss: 0.3879, Train Acc: 86.63%, Val Loss: 0.4556, Val Acc: 84.36% (Best)\n",
      "Epoch 7/250 (BNScale-70.0%): Train Loss: 0.3765, Train Acc: 87.21%, Val Loss: 0.4452, Val Acc: 84.44% (Best)\n",
      "Epoch 8/250 (BNScale-70.0%): Train Loss: 0.3674, Train Acc: 87.17%, Val Loss: 0.4403, Val Acc: 84.70% (Best)\n",
      "Epoch 9/250 (BNScale-70.0%): Train Loss: 0.3645, Train Acc: 87.35%, Val Loss: 0.4466, Val Acc: 84.22%\n",
      "Epoch 10/250 (BNScale-70.0%): Train Loss: 0.3488, Train Acc: 88.02%, Val Loss: 0.4361, Val Acc: 84.82% (Best)\n",
      "Epoch 11/250 (BNScale-70.0%): Train Loss: 0.3446, Train Acc: 87.96%, Val Loss: 0.4382, Val Acc: 85.18% (Best)\n",
      "Epoch 12/250 (BNScale-70.0%): Train Loss: 0.3407, Train Acc: 88.29%, Val Loss: 0.4358, Val Acc: 85.24% (Best)\n",
      "Epoch 13/250 (BNScale-70.0%): Train Loss: 0.3377, Train Acc: 88.42%, Val Loss: 0.4340, Val Acc: 84.98%\n",
      "Epoch 14/250 (BNScale-70.0%): Train Loss: 0.3257, Train Acc: 88.69%, Val Loss: 0.4318, Val Acc: 84.90%\n",
      "Epoch 15/250 (BNScale-70.0%): Train Loss: 0.3277, Train Acc: 88.64%, Val Loss: 0.4287, Val Acc: 85.18%\n",
      "Epoch 16/250 (BNScale-70.0%): Train Loss: 0.3241, Train Acc: 88.81%, Val Loss: 0.4233, Val Acc: 85.56% (Best)\n",
      "Epoch 17/250 (BNScale-70.0%): Train Loss: 0.3156, Train Acc: 89.02%, Val Loss: 0.4257, Val Acc: 85.46%\n",
      "Epoch 18/250 (BNScale-70.0%): Train Loss: 0.3163, Train Acc: 89.00%, Val Loss: 0.4276, Val Acc: 85.46%\n",
      "Epoch 19/250 (BNScale-70.0%): Train Loss: 0.3117, Train Acc: 89.17%, Val Loss: 0.4243, Val Acc: 85.08%\n",
      "Epoch 20/250 (BNScale-70.0%): Train Loss: 0.3085, Train Acc: 89.27%, Val Loss: 0.4274, Val Acc: 85.54%\n",
      "Epoch 21/250 (BNScale-70.0%): Train Loss: 0.3069, Train Acc: 89.25%, Val Loss: 0.4353, Val Acc: 85.26%\n",
      "Epoch 22/250 (BNScale-70.0%): Train Loss: 0.3045, Train Acc: 89.48%, Val Loss: 0.4359, Val Acc: 85.22%\n",
      "Epoch 23/250 (BNScale-70.0%): Train Loss: 0.2992, Train Acc: 89.60%, Val Loss: 0.4325, Val Acc: 85.70% (Best)\n",
      "Epoch 24/250 (BNScale-70.0%): Train Loss: 0.2996, Train Acc: 89.59%, Val Loss: 0.4332, Val Acc: 85.50%\n",
      "Epoch 25/250 (BNScale-70.0%): Train Loss: 0.2996, Train Acc: 89.54%, Val Loss: 0.4319, Val Acc: 85.50%\n",
      "Epoch 26/250 (BNScale-70.0%): Train Loss: 0.2963, Train Acc: 89.73%, Val Loss: 0.4304, Val Acc: 85.44%\n",
      "Epoch 27/250 (BNScale-70.0%): Train Loss: 0.2951, Train Acc: 89.76%, Val Loss: 0.4254, Val Acc: 85.76% (Best)\n",
      "Epoch 28/250 (BNScale-70.0%): Train Loss: 0.2852, Train Acc: 90.20%, Val Loss: 0.4315, Val Acc: 85.60%\n",
      "Epoch 29/250 (BNScale-70.0%): Train Loss: 0.2843, Train Acc: 89.96%, Val Loss: 0.4256, Val Acc: 85.72%\n",
      "Epoch 30/250 (BNScale-70.0%): Train Loss: 0.2842, Train Acc: 90.11%, Val Loss: 0.4313, Val Acc: 85.58%\n",
      "Epoch 31/250 (BNScale-70.0%): Train Loss: 0.2865, Train Acc: 89.95%, Val Loss: 0.4257, Val Acc: 85.72%\n",
      "Epoch 32/250 (BNScale-70.0%): Train Loss: 0.2841, Train Acc: 90.04%, Val Loss: 0.4234, Val Acc: 85.94% (Best)\n",
      "Epoch 33/250 (BNScale-70.0%): Train Loss: 0.2769, Train Acc: 90.42%, Val Loss: 0.4343, Val Acc: 85.64%\n",
      "Epoch 34/250 (BNScale-70.0%): Train Loss: 0.2742, Train Acc: 90.36%, Val Loss: 0.4351, Val Acc: 85.50%\n",
      "Epoch 35/250 (BNScale-70.0%): Train Loss: 0.2793, Train Acc: 90.17%, Val Loss: 0.4302, Val Acc: 85.90%\n",
      "Epoch 36/250 (BNScale-70.0%): Train Loss: 0.2755, Train Acc: 90.40%, Val Loss: 0.4281, Val Acc: 85.76%\n",
      "Epoch 37/250 (BNScale-70.0%): Train Loss: 0.2767, Train Acc: 90.28%, Val Loss: 0.4308, Val Acc: 85.76%\n",
      "Epoch 38/250 (BNScale-70.0%): Train Loss: 0.2711, Train Acc: 90.62%, Val Loss: 0.4375, Val Acc: 85.70%\n",
      "Epoch 39/250 (BNScale-70.0%): Train Loss: 0.2690, Train Acc: 90.56%, Val Loss: 0.4324, Val Acc: 85.78%\n",
      "Epoch 40/250 (BNScale-70.0%): Train Loss: 0.2701, Train Acc: 90.62%, Val Loss: 0.4288, Val Acc: 85.88%\n",
      "Epoch 41/250 (BNScale-70.0%): Train Loss: 0.2698, Train Acc: 90.63%, Val Loss: 0.4396, Val Acc: 85.58%\n",
      "Epoch 42/250 (BNScale-70.0%): Train Loss: 0.2655, Train Acc: 90.67%, Val Loss: 0.4403, Val Acc: 85.82%\n",
      "Epoch 43/250 (BNScale-70.0%): Train Loss: 0.2633, Train Acc: 90.79%, Val Loss: 0.4383, Val Acc: 85.58%\n",
      "Epoch 44/250 (BNScale-70.0%): Train Loss: 0.2638, Train Acc: 90.97%, Val Loss: 0.4379, Val Acc: 85.56%\n",
      "Epoch 45/250 (BNScale-70.0%): Train Loss: 0.2608, Train Acc: 90.76%, Val Loss: 0.4353, Val Acc: 85.68%\n",
      "Epoch 46/250 (BNScale-70.0%): Train Loss: 0.2591, Train Acc: 90.96%, Val Loss: 0.4411, Val Acc: 85.62%\n",
      "Epoch 47/250 (BNScale-70.0%): Train Loss: 0.2614, Train Acc: 90.84%, Val Loss: 0.4368, Val Acc: 85.76%\n",
      "Early stopping triggered after 47 epochs\n",
      "Loaded best model state with val accuracy: 85.94%\n",
      "Results: Accuracy=85.45%, MACs=4.87M\n",
      "‚úÖ Model saved to ./models_mobilenetv2_enhanced/bnscale_sparsity_0.7.pth\n",
      "‚úÖ ONNX model saved to ./models_mobilenetv2_enhanced/bnscale_sparsity_0.7.onnx\n",
      "\n",
      "--- Strategy: MagnitudeL2 ---\n",
      "\n",
      "Processing MagnitudeL2 at 20.0% sparsity...\n",
      "‚úÖ Created MobileNetV2 without pretrained weights\n",
      "‚úÖ Adapted classifier for 10 classes\n",
      "Initial MACs: 6.52M\n",
      "Applying MagnitudeImportance pruning at 20.0% sparsity...\n",
      "Final MACs: 6.00M (Reduction: 8.0%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/250 (MagnitudeL2-20.0%): Train Loss: 0.3421, Train Acc: 88.39%, Val Loss: 0.4323, Val Acc: 85.58% (Best)\n",
      "Epoch 2/250 (MagnitudeL2-20.0%): Train Loss: 0.3120, Train Acc: 89.52%, Val Loss: 0.4236, Val Acc: 85.94% (Best)\n",
      "Epoch 3/250 (MagnitudeL2-20.0%): Train Loss: 0.2954, Train Acc: 90.00%, Val Loss: 0.4186, Val Acc: 86.04% (Best)\n",
      "Epoch 4/250 (MagnitudeL2-20.0%): Train Loss: 0.2908, Train Acc: 90.11%, Val Loss: 0.4211, Val Acc: 85.90%\n",
      "Epoch 5/250 (MagnitudeL2-20.0%): Train Loss: 0.2817, Train Acc: 90.53%, Val Loss: 0.4186, Val Acc: 86.14% (Best)\n",
      "Epoch 6/250 (MagnitudeL2-20.0%): Train Loss: 0.2763, Train Acc: 90.60%, Val Loss: 0.4124, Val Acc: 85.84%\n",
      "Epoch 7/250 (MagnitudeL2-20.0%): Train Loss: 0.2771, Train Acc: 90.57%, Val Loss: 0.4140, Val Acc: 85.90%\n",
      "Epoch 8/250 (MagnitudeL2-20.0%): Train Loss: 0.2690, Train Acc: 90.87%, Val Loss: 0.4152, Val Acc: 86.14%\n",
      "Epoch 9/250 (MagnitudeL2-20.0%): Train Loss: 0.2654, Train Acc: 90.85%, Val Loss: 0.4107, Val Acc: 86.18% (Best)\n",
      "Epoch 10/250 (MagnitudeL2-20.0%): Train Loss: 0.2644, Train Acc: 90.96%, Val Loss: 0.4097, Val Acc: 86.32% (Best)\n",
      "Epoch 11/250 (MagnitudeL2-20.0%): Train Loss: 0.2621, Train Acc: 90.88%, Val Loss: 0.4068, Val Acc: 86.72% (Best)\n",
      "Epoch 12/250 (MagnitudeL2-20.0%): Train Loss: 0.2574, Train Acc: 91.21%, Val Loss: 0.4086, Val Acc: 86.46%\n",
      "Epoch 13/250 (MagnitudeL2-20.0%): Train Loss: 0.2565, Train Acc: 91.12%, Val Loss: 0.4101, Val Acc: 86.26%\n",
      "Epoch 14/250 (MagnitudeL2-20.0%): Train Loss: 0.2541, Train Acc: 91.30%, Val Loss: 0.4119, Val Acc: 86.24%\n",
      "Epoch 15/250 (MagnitudeL2-20.0%): Train Loss: 0.2515, Train Acc: 91.34%, Val Loss: 0.4090, Val Acc: 86.12%\n",
      "Epoch 16/250 (MagnitudeL2-20.0%): Train Loss: 0.2471, Train Acc: 91.42%, Val Loss: 0.4087, Val Acc: 86.56%\n",
      "Epoch 17/250 (MagnitudeL2-20.0%): Train Loss: 0.2484, Train Acc: 91.45%, Val Loss: 0.4126, Val Acc: 86.22%\n",
      "Epoch 18/250 (MagnitudeL2-20.0%): Train Loss: 0.2439, Train Acc: 91.66%, Val Loss: 0.4099, Val Acc: 86.84% (Best)\n",
      "Epoch 19/250 (MagnitudeL2-20.0%): Train Loss: 0.2434, Train Acc: 91.62%, Val Loss: 0.4072, Val Acc: 86.20%\n",
      "Epoch 20/250 (MagnitudeL2-20.0%): Train Loss: 0.2395, Train Acc: 91.66%, Val Loss: 0.4087, Val Acc: 86.50%\n",
      "Epoch 21/250 (MagnitudeL2-20.0%): Train Loss: 0.2409, Train Acc: 91.67%, Val Loss: 0.4123, Val Acc: 86.42%\n",
      "Epoch 22/250 (MagnitudeL2-20.0%): Train Loss: 0.2415, Train Acc: 91.66%, Val Loss: 0.4163, Val Acc: 86.44%\n",
      "Epoch 23/250 (MagnitudeL2-20.0%): Train Loss: 0.2371, Train Acc: 91.75%, Val Loss: 0.4112, Val Acc: 86.52%\n",
      "Epoch 24/250 (MagnitudeL2-20.0%): Train Loss: 0.2338, Train Acc: 91.69%, Val Loss: 0.4096, Val Acc: 86.52%\n",
      "Epoch 25/250 (MagnitudeL2-20.0%): Train Loss: 0.2311, Train Acc: 92.02%, Val Loss: 0.4127, Val Acc: 86.70%\n",
      "Epoch 26/250 (MagnitudeL2-20.0%): Train Loss: 0.2384, Train Acc: 91.59%, Val Loss: 0.4186, Val Acc: 86.38%\n",
      "Epoch 27/250 (MagnitudeL2-20.0%): Train Loss: 0.2331, Train Acc: 91.80%, Val Loss: 0.4154, Val Acc: 86.48%\n",
      "Epoch 28/250 (MagnitudeL2-20.0%): Train Loss: 0.2287, Train Acc: 92.06%, Val Loss: 0.4257, Val Acc: 86.40%\n",
      "Epoch 29/250 (MagnitudeL2-20.0%): Train Loss: 0.2309, Train Acc: 92.04%, Val Loss: 0.4211, Val Acc: 86.60%\n",
      "Epoch 30/250 (MagnitudeL2-20.0%): Train Loss: 0.2293, Train Acc: 92.08%, Val Loss: 0.4192, Val Acc: 86.60%\n",
      "Epoch 31/250 (MagnitudeL2-20.0%): Train Loss: 0.2230, Train Acc: 92.23%, Val Loss: 0.4176, Val Acc: 86.62%\n",
      "Epoch 32/250 (MagnitudeL2-20.0%): Train Loss: 0.2198, Train Acc: 92.32%, Val Loss: 0.4170, Val Acc: 86.56%\n",
      "Epoch 33/250 (MagnitudeL2-20.0%): Train Loss: 0.2217, Train Acc: 92.21%, Val Loss: 0.4148, Val Acc: 86.84%\n",
      "Early stopping triggered after 33 epochs\n",
      "Loaded best model state with val accuracy: 86.84%\n",
      "Results: Accuracy=85.95%, MACs=6.00M\n",
      "‚úÖ Model saved to ./models_mobilenetv2_enhanced/magnitudel2_sparsity_0.2.pth\n",
      "‚úÖ ONNX model saved to ./models_mobilenetv2_enhanced/magnitudel2_sparsity_0.2.onnx\n",
      "\n",
      "Processing MagnitudeL2 at 50.0% sparsity...\n",
      "‚úÖ Created MobileNetV2 without pretrained weights\n",
      "‚úÖ Adapted classifier for 10 classes\n",
      "Initial MACs: 6.52M\n",
      "Applying MagnitudeImportance pruning at 50.0% sparsity...\n",
      "Final MACs: 5.30M (Reduction: 18.7%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/250 (MagnitudeL2-50.0%): Train Loss: 0.4648, Train Acc: 84.37%, Val Loss: 0.4785, Val Acc: 83.68% (Best)\n",
      "Epoch 2/250 (MagnitudeL2-50.0%): Train Loss: 0.4019, Train Acc: 86.32%, Val Loss: 0.4639, Val Acc: 84.34% (Best)\n",
      "Epoch 3/250 (MagnitudeL2-50.0%): Train Loss: 0.3761, Train Acc: 87.09%, Val Loss: 0.4520, Val Acc: 84.54% (Best)\n",
      "Epoch 4/250 (MagnitudeL2-50.0%): Train Loss: 0.3612, Train Acc: 87.75%, Val Loss: 0.4418, Val Acc: 85.18% (Best)\n",
      "Epoch 5/250 (MagnitudeL2-50.0%): Train Loss: 0.3469, Train Acc: 88.13%, Val Loss: 0.4477, Val Acc: 84.68%\n",
      "Epoch 6/250 (MagnitudeL2-50.0%): Train Loss: 0.3431, Train Acc: 88.36%, Val Loss: 0.4380, Val Acc: 85.08%\n",
      "Epoch 7/250 (MagnitudeL2-50.0%): Train Loss: 0.3307, Train Acc: 88.66%, Val Loss: 0.4323, Val Acc: 85.26% (Best)\n",
      "Epoch 8/250 (MagnitudeL2-50.0%): Train Loss: 0.3273, Train Acc: 88.82%, Val Loss: 0.4303, Val Acc: 85.62% (Best)\n",
      "Epoch 9/250 (MagnitudeL2-50.0%): Train Loss: 0.3212, Train Acc: 89.00%, Val Loss: 0.4315, Val Acc: 85.60%\n",
      "Epoch 10/250 (MagnitudeL2-50.0%): Train Loss: 0.3201, Train Acc: 89.06%, Val Loss: 0.4288, Val Acc: 85.76% (Best)\n",
      "Epoch 11/250 (MagnitudeL2-50.0%): Train Loss: 0.3114, Train Acc: 89.24%, Val Loss: 0.4265, Val Acc: 85.38%\n",
      "Epoch 12/250 (MagnitudeL2-50.0%): Train Loss: 0.3027, Train Acc: 89.49%, Val Loss: 0.4246, Val Acc: 85.98% (Best)\n",
      "Epoch 13/250 (MagnitudeL2-50.0%): Train Loss: 0.3010, Train Acc: 89.49%, Val Loss: 0.4204, Val Acc: 86.12% (Best)\n",
      "Epoch 14/250 (MagnitudeL2-50.0%): Train Loss: 0.3007, Train Acc: 89.70%, Val Loss: 0.4174, Val Acc: 85.98%\n",
      "Epoch 15/250 (MagnitudeL2-50.0%): Train Loss: 0.2960, Train Acc: 89.87%, Val Loss: 0.4201, Val Acc: 85.66%\n",
      "Epoch 16/250 (MagnitudeL2-50.0%): Train Loss: 0.2913, Train Acc: 89.84%, Val Loss: 0.4217, Val Acc: 85.98%\n",
      "Epoch 17/250 (MagnitudeL2-50.0%): Train Loss: 0.2909, Train Acc: 89.92%, Val Loss: 0.4151, Val Acc: 86.38% (Best)\n",
      "Epoch 18/250 (MagnitudeL2-50.0%): Train Loss: 0.2861, Train Acc: 90.22%, Val Loss: 0.4218, Val Acc: 86.26%\n",
      "Epoch 19/250 (MagnitudeL2-50.0%): Train Loss: 0.2836, Train Acc: 90.02%, Val Loss: 0.4241, Val Acc: 85.76%\n",
      "Epoch 20/250 (MagnitudeL2-50.0%): Train Loss: 0.2836, Train Acc: 90.21%, Val Loss: 0.4222, Val Acc: 85.94%\n",
      "Epoch 21/250 (MagnitudeL2-50.0%): Train Loss: 0.2847, Train Acc: 90.06%, Val Loss: 0.4139, Val Acc: 86.28%\n",
      "Epoch 22/250 (MagnitudeL2-50.0%): Train Loss: 0.2782, Train Acc: 90.33%, Val Loss: 0.4166, Val Acc: 86.26%\n",
      "Epoch 23/250 (MagnitudeL2-50.0%): Train Loss: 0.2768, Train Acc: 90.34%, Val Loss: 0.4170, Val Acc: 85.74%\n",
      "Epoch 24/250 (MagnitudeL2-50.0%): Train Loss: 0.2789, Train Acc: 90.27%, Val Loss: 0.4218, Val Acc: 85.92%\n",
      "Epoch 25/250 (MagnitudeL2-50.0%): Train Loss: 0.2774, Train Acc: 90.34%, Val Loss: 0.4182, Val Acc: 85.84%\n",
      "Epoch 26/250 (MagnitudeL2-50.0%): Train Loss: 0.2690, Train Acc: 90.69%, Val Loss: 0.4217, Val Acc: 85.96%\n",
      "Epoch 27/250 (MagnitudeL2-50.0%): Train Loss: 0.2665, Train Acc: 90.77%, Val Loss: 0.4214, Val Acc: 85.86%\n",
      "Epoch 28/250 (MagnitudeL2-50.0%): Train Loss: 0.2683, Train Acc: 90.70%, Val Loss: 0.4189, Val Acc: 85.94%\n",
      "Epoch 29/250 (MagnitudeL2-50.0%): Train Loss: 0.2670, Train Acc: 90.67%, Val Loss: 0.4165, Val Acc: 86.30%\n",
      "Epoch 30/250 (MagnitudeL2-50.0%): Train Loss: 0.2693, Train Acc: 90.56%, Val Loss: 0.4158, Val Acc: 86.04%\n",
      "Epoch 31/250 (MagnitudeL2-50.0%): Train Loss: 0.2628, Train Acc: 90.68%, Val Loss: 0.4265, Val Acc: 86.14%\n",
      "Epoch 32/250 (MagnitudeL2-50.0%): Train Loss: 0.2613, Train Acc: 90.91%, Val Loss: 0.4230, Val Acc: 85.94%\n",
      "Early stopping triggered after 32 epochs\n",
      "Loaded best model state with val accuracy: 86.38%\n",
      "Results: Accuracy=85.35%, MACs=5.30M\n",
      "‚úÖ Model saved to ./models_mobilenetv2_enhanced/magnitudel2_sparsity_0.5.pth\n",
      "‚úÖ ONNX model saved to ./models_mobilenetv2_enhanced/magnitudel2_sparsity_0.5.onnx\n",
      "\n",
      "Processing MagnitudeL2 at 70.0% sparsity...\n",
      "‚úÖ Created MobileNetV2 without pretrained weights\n",
      "‚úÖ Adapted classifier for 10 classes\n",
      "Initial MACs: 6.52M\n",
      "Applying MagnitudeImportance pruning at 70.0% sparsity...\n",
      "Final MACs: 4.87M (Reduction: 25.3%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/250 (MagnitudeL2-70.0%): Train Loss: 0.5857, Train Acc: 80.37%, Val Loss: 0.5524, Val Acc: 81.14% (Best)\n",
      "Epoch 2/250 (MagnitudeL2-70.0%): Train Loss: 0.4867, Train Acc: 83.36%, Val Loss: 0.5127, Val Acc: 82.40% (Best)\n",
      "Epoch 3/250 (MagnitudeL2-70.0%): Train Loss: 0.4485, Train Acc: 84.85%, Val Loss: 0.4943, Val Acc: 82.72% (Best)\n",
      "Epoch 4/250 (MagnitudeL2-70.0%): Train Loss: 0.4293, Train Acc: 85.17%, Val Loss: 0.4785, Val Acc: 83.52% (Best)\n",
      "Epoch 5/250 (MagnitudeL2-70.0%): Train Loss: 0.4066, Train Acc: 86.12%, Val Loss: 0.4701, Val Acc: 83.84% (Best)\n",
      "Epoch 6/250 (MagnitudeL2-70.0%): Train Loss: 0.3970, Train Acc: 86.46%, Val Loss: 0.4634, Val Acc: 83.78%\n",
      "Epoch 7/250 (MagnitudeL2-70.0%): Train Loss: 0.3807, Train Acc: 86.66%, Val Loss: 0.4559, Val Acc: 84.30% (Best)\n",
      "Epoch 8/250 (MagnitudeL2-70.0%): Train Loss: 0.3752, Train Acc: 87.04%, Val Loss: 0.4559, Val Acc: 84.44% (Best)\n",
      "Epoch 9/250 (MagnitudeL2-70.0%): Train Loss: 0.3674, Train Acc: 87.29%, Val Loss: 0.4470, Val Acc: 84.40%\n",
      "Epoch 10/250 (MagnitudeL2-70.0%): Train Loss: 0.3575, Train Acc: 87.76%, Val Loss: 0.4463, Val Acc: 84.58% (Best)\n",
      "Epoch 11/250 (MagnitudeL2-70.0%): Train Loss: 0.3582, Train Acc: 87.72%, Val Loss: 0.4462, Val Acc: 84.76% (Best)\n",
      "Epoch 12/250 (MagnitudeL2-70.0%): Train Loss: 0.3428, Train Acc: 88.15%, Val Loss: 0.4433, Val Acc: 84.84% (Best)\n",
      "Epoch 13/250 (MagnitudeL2-70.0%): Train Loss: 0.3413, Train Acc: 88.17%, Val Loss: 0.4348, Val Acc: 85.16% (Best)\n",
      "Epoch 14/250 (MagnitudeL2-70.0%): Train Loss: 0.3345, Train Acc: 88.50%, Val Loss: 0.4371, Val Acc: 85.18% (Best)\n",
      "Epoch 15/250 (MagnitudeL2-70.0%): Train Loss: 0.3328, Train Acc: 88.44%, Val Loss: 0.4450, Val Acc: 85.22% (Best)\n",
      "Epoch 16/250 (MagnitudeL2-70.0%): Train Loss: 0.3231, Train Acc: 88.77%, Val Loss: 0.4309, Val Acc: 85.88% (Best)\n",
      "Epoch 17/250 (MagnitudeL2-70.0%): Train Loss: 0.3284, Train Acc: 88.61%, Val Loss: 0.4366, Val Acc: 85.56%\n",
      "Epoch 18/250 (MagnitudeL2-70.0%): Train Loss: 0.3187, Train Acc: 88.85%, Val Loss: 0.4350, Val Acc: 85.66%\n",
      "Epoch 19/250 (MagnitudeL2-70.0%): Train Loss: 0.3219, Train Acc: 88.82%, Val Loss: 0.4267, Val Acc: 85.96% (Best)\n",
      "Epoch 20/250 (MagnitudeL2-70.0%): Train Loss: 0.3125, Train Acc: 89.27%, Val Loss: 0.4312, Val Acc: 85.86%\n",
      "Epoch 21/250 (MagnitudeL2-70.0%): Train Loss: 0.3097, Train Acc: 89.17%, Val Loss: 0.4289, Val Acc: 85.86%\n",
      "Epoch 22/250 (MagnitudeL2-70.0%): Train Loss: 0.3088, Train Acc: 89.27%, Val Loss: 0.4311, Val Acc: 85.84%\n",
      "Epoch 23/250 (MagnitudeL2-70.0%): Train Loss: 0.3073, Train Acc: 89.25%, Val Loss: 0.4278, Val Acc: 85.74%\n",
      "Epoch 24/250 (MagnitudeL2-70.0%): Train Loss: 0.3043, Train Acc: 89.30%, Val Loss: 0.4298, Val Acc: 85.30%\n",
      "Epoch 25/250 (MagnitudeL2-70.0%): Train Loss: 0.2972, Train Acc: 89.61%, Val Loss: 0.4253, Val Acc: 85.78%\n",
      "Epoch 26/250 (MagnitudeL2-70.0%): Train Loss: 0.2992, Train Acc: 89.57%, Val Loss: 0.4288, Val Acc: 85.54%\n",
      "Epoch 27/250 (MagnitudeL2-70.0%): Train Loss: 0.2968, Train Acc: 89.67%, Val Loss: 0.4314, Val Acc: 85.32%\n",
      "Epoch 28/250 (MagnitudeL2-70.0%): Train Loss: 0.2931, Train Acc: 89.99%, Val Loss: 0.4365, Val Acc: 85.46%\n",
      "Epoch 29/250 (MagnitudeL2-70.0%): Train Loss: 0.2907, Train Acc: 89.94%, Val Loss: 0.4307, Val Acc: 85.50%\n",
      "Epoch 30/250 (MagnitudeL2-70.0%): Train Loss: 0.2891, Train Acc: 89.80%, Val Loss: 0.4321, Val Acc: 85.24%\n",
      "Epoch 31/250 (MagnitudeL2-70.0%): Train Loss: 0.2863, Train Acc: 90.06%, Val Loss: 0.4272, Val Acc: 85.46%\n",
      "Epoch 32/250 (MagnitudeL2-70.0%): Train Loss: 0.2855, Train Acc: 90.09%, Val Loss: 0.4366, Val Acc: 85.18%\n",
      "Epoch 33/250 (MagnitudeL2-70.0%): Train Loss: 0.2849, Train Acc: 89.90%, Val Loss: 0.4286, Val Acc: 85.60%\n",
      "Epoch 34/250 (MagnitudeL2-70.0%): Train Loss: 0.2816, Train Acc: 90.20%, Val Loss: 0.4275, Val Acc: 85.66%\n",
      "Early stopping triggered after 34 epochs\n",
      "Loaded best model state with val accuracy: 85.96%\n",
      "Results: Accuracy=85.03%, MACs=4.87M\n",
      "‚úÖ Model saved to ./models_mobilenetv2_enhanced/magnitudel2_sparsity_0.7.pth\n",
      "‚úÖ ONNX model saved to ./models_mobilenetv2_enhanced/magnitudel2_sparsity_0.7.onnx\n",
      "\n",
      "--- Strategy: Random ---\n",
      "\n",
      "Processing Random at 20.0% sparsity...\n",
      "‚úÖ Created MobileNetV2 without pretrained weights\n",
      "‚úÖ Adapted classifier for 10 classes\n",
      "Initial MACs: 6.52M\n",
      "Applying RandomImportance pruning at 20.0% sparsity...\n",
      "Final MACs: 6.00M (Reduction: 8.0%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/250 (Random-20.0%): Train Loss: 0.4140, Train Acc: 85.92%, Val Loss: 0.4570, Val Acc: 84.50% (Best)\n",
      "Epoch 2/250 (Random-20.0%): Train Loss: 0.3585, Train Acc: 87.74%, Val Loss: 0.4395, Val Acc: 85.22% (Best)\n",
      "Epoch 3/250 (Random-20.0%): Train Loss: 0.3406, Train Acc: 88.37%, Val Loss: 0.4283, Val Acc: 85.66% (Best)\n",
      "Epoch 4/250 (Random-20.0%): Train Loss: 0.3298, Train Acc: 88.73%, Val Loss: 0.4294, Val Acc: 85.50%\n",
      "Epoch 5/250 (Random-20.0%): Train Loss: 0.3162, Train Acc: 89.24%, Val Loss: 0.4270, Val Acc: 85.52%\n",
      "Epoch 6/250 (Random-20.0%): Train Loss: 0.3080, Train Acc: 89.45%, Val Loss: 0.4192, Val Acc: 86.06% (Best)\n",
      "Epoch 7/250 (Random-20.0%): Train Loss: 0.3071, Train Acc: 89.51%, Val Loss: 0.4150, Val Acc: 86.00%\n",
      "Epoch 8/250 (Random-20.0%): Train Loss: 0.2994, Train Acc: 89.82%, Val Loss: 0.4128, Val Acc: 86.02%\n",
      "Epoch 9/250 (Random-20.0%): Train Loss: 0.2974, Train Acc: 89.66%, Val Loss: 0.4178, Val Acc: 85.90%\n",
      "Epoch 10/250 (Random-20.0%): Train Loss: 0.2882, Train Acc: 90.08%, Val Loss: 0.4191, Val Acc: 85.88%\n",
      "Epoch 11/250 (Random-20.0%): Train Loss: 0.2844, Train Acc: 90.18%, Val Loss: 0.4191, Val Acc: 86.00%\n",
      "Epoch 12/250 (Random-20.0%): Train Loss: 0.2829, Train Acc: 90.21%, Val Loss: 0.4175, Val Acc: 85.94%\n",
      "Epoch 13/250 (Random-20.0%): Train Loss: 0.2787, Train Acc: 90.42%, Val Loss: 0.4210, Val Acc: 86.10% (Best)\n",
      "Epoch 14/250 (Random-20.0%): Train Loss: 0.2771, Train Acc: 90.49%, Val Loss: 0.4116, Val Acc: 85.96%\n",
      "Epoch 15/250 (Random-20.0%): Train Loss: 0.2758, Train Acc: 90.66%, Val Loss: 0.4181, Val Acc: 86.04%\n",
      "Epoch 16/250 (Random-20.0%): Train Loss: 0.2703, Train Acc: 90.73%, Val Loss: 0.4142, Val Acc: 86.06%\n",
      "Epoch 17/250 (Random-20.0%): Train Loss: 0.2700, Train Acc: 90.75%, Val Loss: 0.4209, Val Acc: 85.80%\n",
      "Epoch 18/250 (Random-20.0%): Train Loss: 0.2687, Train Acc: 90.73%, Val Loss: 0.4093, Val Acc: 86.40% (Best)\n",
      "Epoch 19/250 (Random-20.0%): Train Loss: 0.2661, Train Acc: 90.70%, Val Loss: 0.4088, Val Acc: 86.66% (Best)\n",
      "Epoch 20/250 (Random-20.0%): Train Loss: 0.2626, Train Acc: 90.94%, Val Loss: 0.4096, Val Acc: 86.18%\n",
      "Epoch 21/250 (Random-20.0%): Train Loss: 0.2582, Train Acc: 90.96%, Val Loss: 0.4136, Val Acc: 86.62%\n",
      "Epoch 22/250 (Random-20.0%): Train Loss: 0.2566, Train Acc: 91.09%, Val Loss: 0.4157, Val Acc: 86.24%\n",
      "Epoch 23/250 (Random-20.0%): Train Loss: 0.2568, Train Acc: 91.16%, Val Loss: 0.4167, Val Acc: 86.38%\n",
      "Epoch 24/250 (Random-20.0%): Train Loss: 0.2548, Train Acc: 91.12%, Val Loss: 0.4211, Val Acc: 85.88%\n",
      "Epoch 25/250 (Random-20.0%): Train Loss: 0.2520, Train Acc: 91.35%, Val Loss: 0.4171, Val Acc: 86.18%\n",
      "Epoch 26/250 (Random-20.0%): Train Loss: 0.2541, Train Acc: 91.28%, Val Loss: 0.4190, Val Acc: 86.04%\n",
      "Epoch 27/250 (Random-20.0%): Train Loss: 0.2483, Train Acc: 91.34%, Val Loss: 0.4218, Val Acc: 86.08%\n",
      "Epoch 28/250 (Random-20.0%): Train Loss: 0.2506, Train Acc: 91.20%, Val Loss: 0.4166, Val Acc: 86.50%\n",
      "Epoch 29/250 (Random-20.0%): Train Loss: 0.2456, Train Acc: 91.42%, Val Loss: 0.4245, Val Acc: 86.22%\n",
      "Epoch 30/250 (Random-20.0%): Train Loss: 0.2504, Train Acc: 91.29%, Val Loss: 0.4212, Val Acc: 86.20%\n",
      "Epoch 31/250 (Random-20.0%): Train Loss: 0.2467, Train Acc: 91.38%, Val Loss: 0.4210, Val Acc: 86.18%\n",
      "Epoch 32/250 (Random-20.0%): Train Loss: 0.2452, Train Acc: 91.49%, Val Loss: 0.4254, Val Acc: 86.32%\n",
      "Epoch 33/250 (Random-20.0%): Train Loss: 0.2407, Train Acc: 91.60%, Val Loss: 0.4250, Val Acc: 86.14%\n",
      "Epoch 34/250 (Random-20.0%): Train Loss: 0.2404, Train Acc: 91.67%, Val Loss: 0.4259, Val Acc: 86.22%\n",
      "Early stopping triggered after 34 epochs\n",
      "Loaded best model state with val accuracy: 86.66%\n",
      "Results: Accuracy=85.89%, MACs=6.00M\n",
      "‚úÖ Model saved to ./models_mobilenetv2_enhanced/random_sparsity_0.2.pth\n",
      "‚úÖ ONNX model saved to ./models_mobilenetv2_enhanced/random_sparsity_0.2.onnx\n",
      "\n",
      "Processing Random at 50.0% sparsity...\n",
      "‚úÖ Created MobileNetV2 without pretrained weights\n",
      "‚úÖ Adapted classifier for 10 classes\n",
      "Initial MACs: 6.52M\n",
      "Applying RandomImportance pruning at 50.0% sparsity...\n",
      "Final MACs: 5.30M (Reduction: 18.7%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/250 (Random-50.0%): Train Loss: 0.8136, Train Acc: 72.84%, Val Loss: 0.6674, Val Acc: 77.12% (Best)\n",
      "Epoch 2/250 (Random-50.0%): Train Loss: 0.6218, Train Acc: 79.06%, Val Loss: 0.5949, Val Acc: 79.28% (Best)\n",
      "Epoch 3/250 (Random-50.0%): Train Loss: 0.5560, Train Acc: 80.97%, Val Loss: 0.5596, Val Acc: 80.78% (Best)\n",
      "Epoch 4/250 (Random-50.0%): Train Loss: 0.5204, Train Acc: 82.21%, Val Loss: 0.5364, Val Acc: 81.46% (Best)\n",
      "Epoch 5/250 (Random-50.0%): Train Loss: 0.5017, Train Acc: 82.93%, Val Loss: 0.5239, Val Acc: 81.80% (Best)\n",
      "Epoch 6/250 (Random-50.0%): Train Loss: 0.4711, Train Acc: 83.91%, Val Loss: 0.5094, Val Acc: 82.40% (Best)\n",
      "Epoch 7/250 (Random-50.0%): Train Loss: 0.4606, Train Acc: 84.11%, Val Loss: 0.5021, Val Acc: 82.66% (Best)\n",
      "Epoch 8/250 (Random-50.0%): Train Loss: 0.4446, Train Acc: 84.83%, Val Loss: 0.4967, Val Acc: 83.04% (Best)\n",
      "Epoch 9/250 (Random-50.0%): Train Loss: 0.4334, Train Acc: 85.16%, Val Loss: 0.4898, Val Acc: 83.06% (Best)\n",
      "Epoch 10/250 (Random-50.0%): Train Loss: 0.4171, Train Acc: 85.57%, Val Loss: 0.4889, Val Acc: 83.16% (Best)\n",
      "Epoch 11/250 (Random-50.0%): Train Loss: 0.4097, Train Acc: 85.79%, Val Loss: 0.4800, Val Acc: 83.46% (Best)\n",
      "Epoch 12/250 (Random-50.0%): Train Loss: 0.4034, Train Acc: 86.18%, Val Loss: 0.4739, Val Acc: 83.62% (Best)\n",
      "Epoch 13/250 (Random-50.0%): Train Loss: 0.3919, Train Acc: 86.33%, Val Loss: 0.4791, Val Acc: 83.64% (Best)\n",
      "Epoch 14/250 (Random-50.0%): Train Loss: 0.3923, Train Acc: 86.53%, Val Loss: 0.4733, Val Acc: 83.80% (Best)\n",
      "Epoch 15/250 (Random-50.0%): Train Loss: 0.3796, Train Acc: 86.81%, Val Loss: 0.4759, Val Acc: 83.86% (Best)\n",
      "Epoch 16/250 (Random-50.0%): Train Loss: 0.3764, Train Acc: 87.09%, Val Loss: 0.4701, Val Acc: 84.24% (Best)\n",
      "Epoch 17/250 (Random-50.0%): Train Loss: 0.3772, Train Acc: 86.94%, Val Loss: 0.4642, Val Acc: 84.08%\n",
      "Epoch 18/250 (Random-50.0%): Train Loss: 0.3671, Train Acc: 87.25%, Val Loss: 0.4608, Val Acc: 84.12%\n",
      "Epoch 19/250 (Random-50.0%): Train Loss: 0.3611, Train Acc: 87.40%, Val Loss: 0.4609, Val Acc: 84.34% (Best)\n",
      "Epoch 20/250 (Random-50.0%): Train Loss: 0.3632, Train Acc: 87.39%, Val Loss: 0.4667, Val Acc: 84.20%\n",
      "Epoch 21/250 (Random-50.0%): Train Loss: 0.3540, Train Acc: 87.79%, Val Loss: 0.4645, Val Acc: 84.38% (Best)\n",
      "Epoch 22/250 (Random-50.0%): Train Loss: 0.3519, Train Acc: 87.84%, Val Loss: 0.4536, Val Acc: 84.78% (Best)\n",
      "Epoch 23/250 (Random-50.0%): Train Loss: 0.3499, Train Acc: 87.90%, Val Loss: 0.4509, Val Acc: 84.78%\n",
      "Epoch 24/250 (Random-50.0%): Train Loss: 0.3430, Train Acc: 88.10%, Val Loss: 0.4513, Val Acc: 84.88% (Best)\n",
      "Epoch 25/250 (Random-50.0%): Train Loss: 0.3368, Train Acc: 88.28%, Val Loss: 0.4572, Val Acc: 84.48%\n",
      "Epoch 26/250 (Random-50.0%): Train Loss: 0.3396, Train Acc: 88.18%, Val Loss: 0.4523, Val Acc: 84.56%\n",
      "Epoch 27/250 (Random-50.0%): Train Loss: 0.3390, Train Acc: 88.36%, Val Loss: 0.4544, Val Acc: 84.28%\n",
      "Epoch 28/250 (Random-50.0%): Train Loss: 0.3343, Train Acc: 88.28%, Val Loss: 0.4435, Val Acc: 84.76%\n",
      "Epoch 29/250 (Random-50.0%): Train Loss: 0.3276, Train Acc: 88.60%, Val Loss: 0.4585, Val Acc: 84.70%\n",
      "Epoch 30/250 (Random-50.0%): Train Loss: 0.3276, Train Acc: 88.61%, Val Loss: 0.4534, Val Acc: 84.60%\n",
      "Epoch 31/250 (Random-50.0%): Train Loss: 0.3247, Train Acc: 88.54%, Val Loss: 0.4444, Val Acc: 84.86%\n",
      "Epoch 32/250 (Random-50.0%): Train Loss: 0.3241, Train Acc: 88.86%, Val Loss: 0.4478, Val Acc: 84.88%\n",
      "Epoch 33/250 (Random-50.0%): Train Loss: 0.3203, Train Acc: 89.01%, Val Loss: 0.4439, Val Acc: 85.26% (Best)\n",
      "Epoch 34/250 (Random-50.0%): Train Loss: 0.3166, Train Acc: 88.92%, Val Loss: 0.4409, Val Acc: 85.12%\n",
      "Epoch 35/250 (Random-50.0%): Train Loss: 0.3164, Train Acc: 88.96%, Val Loss: 0.4472, Val Acc: 85.00%\n",
      "Epoch 36/250 (Random-50.0%): Train Loss: 0.3151, Train Acc: 88.97%, Val Loss: 0.4459, Val Acc: 85.26%\n",
      "Epoch 37/250 (Random-50.0%): Train Loss: 0.3155, Train Acc: 89.12%, Val Loss: 0.4427, Val Acc: 84.76%\n",
      "Epoch 38/250 (Random-50.0%): Train Loss: 0.3118, Train Acc: 89.21%, Val Loss: 0.4491, Val Acc: 84.80%\n",
      "Epoch 39/250 (Random-50.0%): Train Loss: 0.3084, Train Acc: 89.22%, Val Loss: 0.4458, Val Acc: 84.76%\n",
      "Epoch 40/250 (Random-50.0%): Train Loss: 0.3099, Train Acc: 89.22%, Val Loss: 0.4424, Val Acc: 85.06%\n",
      "Epoch 41/250 (Random-50.0%): Train Loss: 0.3024, Train Acc: 89.57%, Val Loss: 0.4496, Val Acc: 84.92%\n",
      "Epoch 42/250 (Random-50.0%): Train Loss: 0.3002, Train Acc: 89.48%, Val Loss: 0.4470, Val Acc: 84.48%\n",
      "Epoch 43/250 (Random-50.0%): Train Loss: 0.2992, Train Acc: 89.53%, Val Loss: 0.4471, Val Acc: 84.90%\n",
      "Epoch 44/250 (Random-50.0%): Train Loss: 0.2985, Train Acc: 89.90%, Val Loss: 0.4453, Val Acc: 84.68%\n",
      "Epoch 45/250 (Random-50.0%): Train Loss: 0.2977, Train Acc: 89.71%, Val Loss: 0.4496, Val Acc: 84.60%\n",
      "Epoch 46/250 (Random-50.0%): Train Loss: 0.2985, Train Acc: 89.55%, Val Loss: 0.4505, Val Acc: 85.02%\n",
      "Epoch 47/250 (Random-50.0%): Train Loss: 0.2914, Train Acc: 89.82%, Val Loss: 0.4482, Val Acc: 84.90%\n",
      "Epoch 48/250 (Random-50.0%): Train Loss: 0.2926, Train Acc: 89.70%, Val Loss: 0.4465, Val Acc: 84.80%\n",
      "Early stopping triggered after 48 epochs\n",
      "Loaded best model state with val accuracy: 85.26%\n",
      "Results: Accuracy=84.72%, MACs=5.30M\n",
      "‚úÖ Model saved to ./models_mobilenetv2_enhanced/random_sparsity_0.5.pth\n",
      "‚úÖ ONNX model saved to ./models_mobilenetv2_enhanced/random_sparsity_0.5.onnx\n",
      "\n",
      "Processing Random at 70.0% sparsity...\n",
      "‚úÖ Created MobileNetV2 without pretrained weights\n",
      "‚úÖ Adapted classifier for 10 classes\n",
      "Initial MACs: 6.52M\n",
      "Applying RandomImportance pruning at 70.0% sparsity...\n",
      "Final MACs: 4.87M (Reduction: 25.3%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/250 (Random-70.0%): Train Loss: 0.9701, Train Acc: 67.44%, Val Loss: 0.7530, Val Acc: 73.96% (Best)\n",
      "Epoch 2/250 (Random-70.0%): Train Loss: 0.7290, Train Acc: 75.69%, Val Loss: 0.6571, Val Acc: 77.26% (Best)\n",
      "Epoch 3/250 (Random-70.0%): Train Loss: 0.6449, Train Acc: 78.06%, Val Loss: 0.6105, Val Acc: 79.10% (Best)\n",
      "Epoch 4/250 (Random-70.0%): Train Loss: 0.5991, Train Acc: 79.78%, Val Loss: 0.5880, Val Acc: 79.74% (Best)\n",
      "Epoch 5/250 (Random-70.0%): Train Loss: 0.5664, Train Acc: 80.81%, Val Loss: 0.5670, Val Acc: 80.26% (Best)\n",
      "Epoch 6/250 (Random-70.0%): Train Loss: 0.5398, Train Acc: 81.60%, Val Loss: 0.5615, Val Acc: 80.78% (Best)\n",
      "Epoch 7/250 (Random-70.0%): Train Loss: 0.5264, Train Acc: 82.05%, Val Loss: 0.5447, Val Acc: 81.20% (Best)\n",
      "Epoch 8/250 (Random-70.0%): Train Loss: 0.5017, Train Acc: 82.60%, Val Loss: 0.5336, Val Acc: 81.44% (Best)\n",
      "Epoch 9/250 (Random-70.0%): Train Loss: 0.4933, Train Acc: 82.97%, Val Loss: 0.5241, Val Acc: 81.74% (Best)\n",
      "Epoch 10/250 (Random-70.0%): Train Loss: 0.4744, Train Acc: 83.74%, Val Loss: 0.5225, Val Acc: 82.06% (Best)\n",
      "Epoch 11/250 (Random-70.0%): Train Loss: 0.4658, Train Acc: 83.98%, Val Loss: 0.5119, Val Acc: 82.02%\n",
      "Epoch 12/250 (Random-70.0%): Train Loss: 0.4582, Train Acc: 84.30%, Val Loss: 0.5106, Val Acc: 82.40% (Best)\n",
      "Epoch 13/250 (Random-70.0%): Train Loss: 0.4492, Train Acc: 84.52%, Val Loss: 0.5068, Val Acc: 82.18%\n",
      "Epoch 14/250 (Random-70.0%): Train Loss: 0.4405, Train Acc: 84.80%, Val Loss: 0.4981, Val Acc: 82.80% (Best)\n",
      "Epoch 15/250 (Random-70.0%): Train Loss: 0.4328, Train Acc: 85.07%, Val Loss: 0.5023, Val Acc: 82.94% (Best)\n",
      "Epoch 16/250 (Random-70.0%): Train Loss: 0.4271, Train Acc: 85.25%, Val Loss: 0.4922, Val Acc: 82.98% (Best)\n",
      "Epoch 17/250 (Random-70.0%): Train Loss: 0.4226, Train Acc: 85.45%, Val Loss: 0.4861, Val Acc: 83.36% (Best)\n",
      "Epoch 18/250 (Random-70.0%): Train Loss: 0.4138, Train Acc: 85.64%, Val Loss: 0.4893, Val Acc: 83.20%\n",
      "Epoch 19/250 (Random-70.0%): Train Loss: 0.4046, Train Acc: 86.17%, Val Loss: 0.4815, Val Acc: 83.54% (Best)\n",
      "Epoch 20/250 (Random-70.0%): Train Loss: 0.4012, Train Acc: 86.14%, Val Loss: 0.4850, Val Acc: 83.02%\n",
      "Epoch 21/250 (Random-70.0%): Train Loss: 0.3996, Train Acc: 86.15%, Val Loss: 0.4761, Val Acc: 83.42%\n",
      "Epoch 22/250 (Random-70.0%): Train Loss: 0.3910, Train Acc: 86.62%, Val Loss: 0.4801, Val Acc: 83.52%\n",
      "Epoch 23/250 (Random-70.0%): Train Loss: 0.3885, Train Acc: 86.66%, Val Loss: 0.4835, Val Acc: 83.78% (Best)\n",
      "Epoch 24/250 (Random-70.0%): Train Loss: 0.3843, Train Acc: 86.80%, Val Loss: 0.4802, Val Acc: 83.82% (Best)\n",
      "Epoch 25/250 (Random-70.0%): Train Loss: 0.3799, Train Acc: 86.87%, Val Loss: 0.4777, Val Acc: 83.58%\n",
      "Epoch 26/250 (Random-70.0%): Train Loss: 0.3792, Train Acc: 86.86%, Val Loss: 0.4724, Val Acc: 84.20% (Best)\n",
      "Epoch 27/250 (Random-70.0%): Train Loss: 0.3707, Train Acc: 87.07%, Val Loss: 0.4716, Val Acc: 83.80%\n",
      "Epoch 28/250 (Random-70.0%): Train Loss: 0.3693, Train Acc: 87.15%, Val Loss: 0.4730, Val Acc: 84.06%\n",
      "Epoch 29/250 (Random-70.0%): Train Loss: 0.3655, Train Acc: 87.27%, Val Loss: 0.4671, Val Acc: 84.04%\n",
      "Epoch 30/250 (Random-70.0%): Train Loss: 0.3598, Train Acc: 87.48%, Val Loss: 0.4725, Val Acc: 83.98%\n",
      "Epoch 31/250 (Random-70.0%): Train Loss: 0.3617, Train Acc: 87.42%, Val Loss: 0.4737, Val Acc: 84.14%\n",
      "Epoch 32/250 (Random-70.0%): Train Loss: 0.3538, Train Acc: 87.69%, Val Loss: 0.4753, Val Acc: 84.14%\n",
      "Epoch 33/250 (Random-70.0%): Train Loss: 0.3572, Train Acc: 87.54%, Val Loss: 0.4714, Val Acc: 84.18%\n",
      "Epoch 34/250 (Random-70.0%): Train Loss: 0.3500, Train Acc: 87.87%, Val Loss: 0.4722, Val Acc: 84.30% (Best)\n",
      "Epoch 35/250 (Random-70.0%): Train Loss: 0.3465, Train Acc: 87.80%, Val Loss: 0.4714, Val Acc: 84.18%\n",
      "Epoch 36/250 (Random-70.0%): Train Loss: 0.3481, Train Acc: 87.83%, Val Loss: 0.4638, Val Acc: 84.28%\n",
      "Epoch 37/250 (Random-70.0%): Train Loss: 0.3380, Train Acc: 88.31%, Val Loss: 0.4663, Val Acc: 84.42% (Best)\n",
      "Epoch 38/250 (Random-70.0%): Train Loss: 0.3448, Train Acc: 87.95%, Val Loss: 0.4662, Val Acc: 84.42%\n",
      "Epoch 39/250 (Random-70.0%): Train Loss: 0.3391, Train Acc: 88.13%, Val Loss: 0.4695, Val Acc: 84.52% (Best)\n",
      "Epoch 40/250 (Random-70.0%): Train Loss: 0.3381, Train Acc: 88.15%, Val Loss: 0.4623, Val Acc: 84.38%\n",
      "Epoch 41/250 (Random-70.0%): Train Loss: 0.3390, Train Acc: 88.19%, Val Loss: 0.4643, Val Acc: 84.38%\n",
      "Epoch 42/250 (Random-70.0%): Train Loss: 0.3322, Train Acc: 88.63%, Val Loss: 0.4672, Val Acc: 84.20%\n",
      "Epoch 43/250 (Random-70.0%): Train Loss: 0.3305, Train Acc: 88.59%, Val Loss: 0.4683, Val Acc: 84.32%\n",
      "Epoch 44/250 (Random-70.0%): Train Loss: 0.3297, Train Acc: 88.48%, Val Loss: 0.4621, Val Acc: 84.68% (Best)\n",
      "Epoch 45/250 (Random-70.0%): Train Loss: 0.3265, Train Acc: 88.55%, Val Loss: 0.4619, Val Acc: 84.74% (Best)\n",
      "Epoch 46/250 (Random-70.0%): Train Loss: 0.3264, Train Acc: 88.65%, Val Loss: 0.4635, Val Acc: 84.92% (Best)\n",
      "Epoch 47/250 (Random-70.0%): Train Loss: 0.3257, Train Acc: 88.81%, Val Loss: 0.4688, Val Acc: 84.46%\n",
      "Epoch 48/250 (Random-70.0%): Train Loss: 0.3207, Train Acc: 88.76%, Val Loss: 0.4666, Val Acc: 84.42%\n",
      "Epoch 49/250 (Random-70.0%): Train Loss: 0.3222, Train Acc: 88.53%, Val Loss: 0.4684, Val Acc: 84.62%\n",
      "Epoch 50/250 (Random-70.0%): Train Loss: 0.3176, Train Acc: 88.86%, Val Loss: 0.4565, Val Acc: 84.72%\n",
      "Epoch 51/250 (Random-70.0%): Train Loss: 0.3198, Train Acc: 88.93%, Val Loss: 0.4648, Val Acc: 84.86%\n",
      "Epoch 52/250 (Random-70.0%): Train Loss: 0.3188, Train Acc: 88.94%, Val Loss: 0.4598, Val Acc: 84.72%\n",
      "Epoch 53/250 (Random-70.0%): Train Loss: 0.3113, Train Acc: 89.26%, Val Loss: 0.4629, Val Acc: 85.04% (Best)\n",
      "Epoch 54/250 (Random-70.0%): Train Loss: 0.3125, Train Acc: 88.99%, Val Loss: 0.4584, Val Acc: 84.70%\n",
      "Epoch 55/250 (Random-70.0%): Train Loss: 0.3090, Train Acc: 89.23%, Val Loss: 0.4631, Val Acc: 84.82%\n",
      "Epoch 56/250 (Random-70.0%): Train Loss: 0.3050, Train Acc: 89.30%, Val Loss: 0.4619, Val Acc: 84.70%\n",
      "Epoch 57/250 (Random-70.0%): Train Loss: 0.3023, Train Acc: 89.45%, Val Loss: 0.4598, Val Acc: 84.96%\n",
      "Epoch 58/250 (Random-70.0%): Train Loss: 0.3057, Train Acc: 89.14%, Val Loss: 0.4623, Val Acc: 85.12% (Best)\n",
      "Epoch 59/250 (Random-70.0%): Train Loss: 0.3071, Train Acc: 89.22%, Val Loss: 0.4579, Val Acc: 84.90%\n",
      "Epoch 60/250 (Random-70.0%): Train Loss: 0.3018, Train Acc: 89.43%, Val Loss: 0.4557, Val Acc: 84.96%\n",
      "Epoch 61/250 (Random-70.0%): Train Loss: 0.3102, Train Acc: 89.23%, Val Loss: 0.4667, Val Acc: 84.80%\n",
      "Epoch 62/250 (Random-70.0%): Train Loss: 0.3024, Train Acc: 89.40%, Val Loss: 0.4607, Val Acc: 85.22% (Best)\n",
      "Epoch 63/250 (Random-70.0%): Train Loss: 0.2982, Train Acc: 89.54%, Val Loss: 0.4625, Val Acc: 85.02%\n",
      "Epoch 64/250 (Random-70.0%): Train Loss: 0.2977, Train Acc: 89.52%, Val Loss: 0.4566, Val Acc: 85.32% (Best)\n",
      "Epoch 65/250 (Random-70.0%): Train Loss: 0.2967, Train Acc: 89.63%, Val Loss: 0.4632, Val Acc: 84.92%\n",
      "Epoch 66/250 (Random-70.0%): Train Loss: 0.2940, Train Acc: 89.74%, Val Loss: 0.4629, Val Acc: 85.26%\n",
      "Epoch 67/250 (Random-70.0%): Train Loss: 0.2926, Train Acc: 89.76%, Val Loss: 0.4578, Val Acc: 85.14%\n",
      "Epoch 68/250 (Random-70.0%): Train Loss: 0.2939, Train Acc: 89.78%, Val Loss: 0.4644, Val Acc: 84.88%\n",
      "Epoch 69/250 (Random-70.0%): Train Loss: 0.2945, Train Acc: 89.62%, Val Loss: 0.4608, Val Acc: 85.28%\n",
      "Epoch 70/250 (Random-70.0%): Train Loss: 0.2906, Train Acc: 89.82%, Val Loss: 0.4638, Val Acc: 85.18%\n",
      "Epoch 71/250 (Random-70.0%): Train Loss: 0.2911, Train Acc: 89.78%, Val Loss: 0.4576, Val Acc: 85.22%\n",
      "Epoch 72/250 (Random-70.0%): Train Loss: 0.2904, Train Acc: 89.65%, Val Loss: 0.4657, Val Acc: 84.82%\n",
      "Epoch 73/250 (Random-70.0%): Train Loss: 0.2838, Train Acc: 90.16%, Val Loss: 0.4691, Val Acc: 84.58%\n",
      "Epoch 74/250 (Random-70.0%): Train Loss: 0.2908, Train Acc: 89.91%, Val Loss: 0.4645, Val Acc: 84.84%\n",
      "Epoch 75/250 (Random-70.0%): Train Loss: 0.2820, Train Acc: 90.17%, Val Loss: 0.4737, Val Acc: 85.06%\n",
      "Epoch 76/250 (Random-70.0%): Train Loss: 0.2851, Train Acc: 90.10%, Val Loss: 0.4645, Val Acc: 85.30%\n",
      "Epoch 77/250 (Random-70.0%): Train Loss: 0.2857, Train Acc: 89.92%, Val Loss: 0.4678, Val Acc: 85.42% (Best)\n",
      "Epoch 78/250 (Random-70.0%): Train Loss: 0.2809, Train Acc: 90.05%, Val Loss: 0.4620, Val Acc: 85.30%\n",
      "Epoch 79/250 (Random-70.0%): Train Loss: 0.2804, Train Acc: 90.23%, Val Loss: 0.4638, Val Acc: 85.24%\n",
      "Epoch 80/250 (Random-70.0%): Train Loss: 0.2826, Train Acc: 90.07%, Val Loss: 0.4636, Val Acc: 85.02%\n",
      "Epoch 81/250 (Random-70.0%): Train Loss: 0.2836, Train Acc: 90.10%, Val Loss: 0.4663, Val Acc: 84.86%\n",
      "Epoch 82/250 (Random-70.0%): Train Loss: 0.2741, Train Acc: 90.45%, Val Loss: 0.4633, Val Acc: 85.34%\n",
      "Epoch 83/250 (Random-70.0%): Train Loss: 0.2782, Train Acc: 90.27%, Val Loss: 0.4599, Val Acc: 85.30%\n",
      "Epoch 84/250 (Random-70.0%): Train Loss: 0.2758, Train Acc: 90.44%, Val Loss: 0.4610, Val Acc: 85.34%\n",
      "Epoch 85/250 (Random-70.0%): Train Loss: 0.2741, Train Acc: 90.48%, Val Loss: 0.4684, Val Acc: 85.06%\n",
      "Epoch 86/250 (Random-70.0%): Train Loss: 0.2733, Train Acc: 90.36%, Val Loss: 0.4690, Val Acc: 84.96%\n",
      "Epoch 87/250 (Random-70.0%): Train Loss: 0.2712, Train Acc: 90.59%, Val Loss: 0.4746, Val Acc: 85.10%\n",
      "Epoch 88/250 (Random-70.0%): Train Loss: 0.2709, Train Acc: 90.44%, Val Loss: 0.4691, Val Acc: 85.18%\n",
      "Epoch 89/250 (Random-70.0%): Train Loss: 0.2681, Train Acc: 90.52%, Val Loss: 0.4779, Val Acc: 85.30%\n",
      "Epoch 90/250 (Random-70.0%): Train Loss: 0.2623, Train Acc: 90.62%, Val Loss: 0.4682, Val Acc: 85.02%\n",
      "Epoch 91/250 (Random-70.0%): Train Loss: 0.2734, Train Acc: 90.46%, Val Loss: 0.4632, Val Acc: 85.18%\n",
      "Epoch 92/250 (Random-70.0%): Train Loss: 0.2627, Train Acc: 90.77%, Val Loss: 0.4708, Val Acc: 85.04%\n",
      "Early stopping triggered after 92 epochs\n",
      "Loaded best model state with val accuracy: 85.42%\n",
      "Results: Accuracy=84.69%, MACs=4.87M\n",
      "‚úÖ Model saved to ./models_mobilenetv2_enhanced/random_sparsity_0.7.pth\n",
      "‚úÖ ONNX model saved to ./models_mobilenetv2_enhanced/random_sparsity_0.7.onnx\n",
      "\n",
      "Saving results...\n",
      "‚úÖ Complete results saved to ./results_mobilenetv2_cifar10_enhanced/complete_results.json\n",
      "‚úÖ Summary results saved to ./results_mobilenetv2_cifar10_enhanced/summary_results.csv\n",
      "Creating plots...\n",
      "‚úÖ Accuracy plot saved to ./results_mobilenetv2_cifar10_enhanced/accuracy_vs_sparsity.png\n",
      "‚úÖ Efficiency frontier plot saved to ./results_mobilenetv2_cifar10_enhanced/efficiency_frontier.png\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENTAL RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Baseline Performance:\n",
      "  Accuracy: 85.20%\n",
      "  MACs: 6.52M\n",
      "  Parameters: 2.24M\n",
      "  Model Size: 8.53MB\n",
      "\n",
      "Strategy Comparison at 50% Sparsity:\n",
      "       BNScale:  85.74% accuracy (-0.54%, 100.6% retention)\n",
      "   MagnitudeL2:  85.35% accuracy (-0.15%, 100.2% retention)\n",
      "        Random:  84.72% accuracy (+0.48%,  99.4% retention)\n",
      "\n",
      "Complete Results Table:\n",
      "--------------------------------------------------------------------------------\n",
      "Strategy     Sparsity Accuracy MACs(M)  Params(M) Size(MB)\n",
      "--------------------------------------------------------------------------------\n",
      "BNScale           0%   85.20%    6.52     2.24    8.53\n",
      "BNScale          20%   85.59%    6.00     2.06    7.85\n",
      "BNScale          50%   85.74%    5.30     1.82    6.93\n",
      "BNScale          70%   85.45%    4.87     1.66    6.34\n",
      "MagnitudeL2       0%   85.20%    6.52     2.24    8.53\n",
      "MagnitudeL2      20%   85.95%    6.00     2.06    7.85\n",
      "MagnitudeL2      50%   85.35%    5.30     1.82    6.93\n",
      "MagnitudeL2      70%   85.03%    4.87     1.66    6.34\n",
      "Random            0%   85.20%    6.52     2.24    8.53\n",
      "Random           20%   85.89%    6.00     2.06    7.85\n",
      "Random           50%   84.72%    5.30     1.82    6.93\n",
      "Random           70%   84.69%    4.87     1.66    6.34\n",
      "\n",
      "üéâ All experiments completed!\n",
      "üìÅ Results saved to: /home/muis/thesis/github-repo/master-thesis/cnn/mobile_net_v2/results_mobilenetv2_cifar10_enhanced\n",
      "üìÅ Models saved to: /home/muis/thesis/github-repo/master-thesis/cnn/mobile_net_v2/models_mobilenetv2_enhanced\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
