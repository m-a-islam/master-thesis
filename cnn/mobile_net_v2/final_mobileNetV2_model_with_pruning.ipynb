{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T07:14:39.098422Z",
     "start_time": "2025-06-03T07:07:31.351272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_pruning as tp\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR, CosineAnnealingLR\n",
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_BASE_NAME = \"mobilenet_v2\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "def get_optimized_data_loaders(data_dir_path='./data', batch_size=128, val_split=0.1, seed=42):\n",
    "    \"\"\"Optimized data loading for maximum CIFAR-10 performance\"\"\"\n",
    "    abs_data_dir = os.path.abspath(data_dir_path)\n",
    "    print(f\"Loading CIFAR-10 from: {abs_data_dir}\")\n",
    "\n",
    "    # CIFAR-10 mean and std (verified values)\n",
    "    mean = [0.4914, 0.4822, 0.4465]\n",
    "    std = [0.2023, 0.1994, 0.2010]\n",
    "\n",
    "    # Enhanced training transforms for maximum performance\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4, padding_mode='reflect'),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "        transforms.RandomErasing(p=0.1, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)\n",
    "    ])\n",
    "\n",
    "    # Clean test transform\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "    # Load datasets\n",
    "    full_train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=abs_data_dir, train=True, download=False, transform=train_transform\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=abs_data_dir, train=False, download=False, transform=test_transform\n",
    "    )\n",
    "\n",
    "    # Create train/validation split\n",
    "    val_size = int(len(full_train_dataset) * val_split)\n",
    "    train_size = len(full_train_dataset) - val_size\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_train_dataset, [train_size, val_size], generator=generator\n",
    "    )\n",
    "\n",
    "    # Apply test transform to validation set\n",
    "    val_dataset.dataset = copy.deepcopy(full_train_dataset)\n",
    "    val_dataset.dataset.transform = test_transform\n",
    "\n",
    "    # Optimized data loaders\n",
    "    num_workers = min(4, os.cpu_count() or 2)\n",
    "    pin_memory = True if DEVICE.type == 'cuda' else False\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=num_workers, pin_memory=pin_memory, drop_last=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers, pin_memory=pin_memory\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers, pin_memory=pin_memory\n",
    "    )\n",
    "\n",
    "    print(f\"Optimized DataLoaders - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "def get_optimized_mobilenetv2_model(num_classes=10, pretrained_path='./base/mobilenet_v2-b0353104.pth'):\n",
    "    \"\"\"Create properly optimized MobileNetV2 for 92%+ accuracy\"\"\"\n",
    "\n",
    "    # Create model without weights first\n",
    "    model = models.mobilenet_v2(weights=None)\n",
    "\n",
    "    # Load ImageNet pretrained weights more carefully\n",
    "    if os.path.exists(pretrained_path):\n",
    "        print(f\"Loading optimized pretrained weights from: {pretrained_path}\")\n",
    "        try:\n",
    "            # Load the state dict\n",
    "            checkpoint = torch.load(pretrained_path, map_location='cpu')\n",
    "\n",
    "            # Get model's current state dict\n",
    "            model_dict = model.state_dict()\n",
    "\n",
    "            # Filter pretrained dict to match model structure (exclude classifier)\n",
    "            filtered_dict = {}\n",
    "            for k, v in checkpoint.items():\n",
    "                # Skip classifier weights and ensure shape matches\n",
    "                if 'classifier' not in k and k in model_dict:\n",
    "                    if model_dict[k].shape == v.shape:\n",
    "                        filtered_dict[k] = v\n",
    "                    else:\n",
    "                        print(f\"Shape mismatch for {k}: model={model_dict[k].shape}, pretrained={v.shape}\")\n",
    "\n",
    "            print(f\"Loading {len(filtered_dict)}/{len(model_dict)} pretrained weights\")\n",
    "\n",
    "            # Update model dict and load\n",
    "            model_dict.update(filtered_dict)\n",
    "            model.load_state_dict(model_dict, strict=False)\n",
    "            print(\"âœ… Optimized ImageNet pretrained weights loaded successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading pretrained weights: {e}\")\n",
    "            print(\"Training from scratch...\")\n",
    "    else:\n",
    "        print(\"âŒ No pretrained weights found - training from scratch\")\n",
    "\n",
    "    # Enhanced classifier for better CIFAR-10 performance\n",
    "    num_features = model.classifier[1].in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(num_features, 512),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(512, num_classes)\n",
    "    )\n",
    "\n",
    "    # Initialize classifier weights properly\n",
    "    for m in model.classifier.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm1d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    print(f\"âœ… Optimized MobileNetV2 model created for {num_classes} classes\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_optimized_model(model, train_loader, val_loader, device, config):\n",
    "    \"\"\"Optimized training function for 92%+ accuracy with early stopping\"\"\"\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "    # Optimized AdamW optimizer (works better than SGD for transfer learning)\n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': model.features.parameters(), 'lr': config['learning_rate'] * 0.1,\n",
    "         'weight_decay': config['weight_decay']},\n",
    "        {'params': model.classifier.parameters(), 'lr': config['learning_rate'],\n",
    "         'weight_decay': config['weight_decay'] * 0.1}\n",
    "    ], eps=1e-8)\n",
    "\n",
    "    # Cosine annealing scheduler for smooth convergence\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=config['epochs'], eta_min=config['learning_rate'] * 0.001)\n",
    "\n",
    "    # Training state\n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'learning_rates': []\n",
    "    }\n",
    "\n",
    "    print(f\"ðŸš€ Starting optimized training for 92%+ accuracy...\")\n",
    "    print(f\"   Epochs: {config['epochs']}, Early stopping patience: {config['patience']}\")\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total += target.size(0)\n",
    "            train_correct += (predicted == target).sum().item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                val_total += target.size(0)\n",
    "                val_correct += (predicted == target).sum().item()\n",
    "\n",
    "        # Calculate metrics\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_acc = 100.0 * train_correct / train_total\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100.0 * val_correct / val_total\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # Update scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improvement = \" âœ…\"\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            improvement = \"\"\n",
    "\n",
    "        # Record history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['learning_rates'].append(current_lr)\n",
    "\n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0 or epoch < 20:\n",
    "            print(f\"Epoch {epoch + 1:3d}/{config['epochs']}: \"\n",
    "                  f\"Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%, \"\n",
    "                  f\"LR: {current_lr:.6f}{improvement}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if epochs_no_improve >= config['patience']:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1} (Best Val Acc: {best_val_acc:.2f}%)\")\n",
    "            break\n",
    "\n",
    "        # Stop if we achieve excellent performance\n",
    "        if val_acc >= 95.0:\n",
    "            print(f\"ðŸŽ‰ Excellent performance achieved: {val_acc:.2f}% at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"âœ… Loaded best model state (Best Val Acc: {best_val_acc:.2f}%)\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def calculate_macs_params(model, example_input):\n",
    "    \"\"\"Calculate MACs and parameters using torch_pruning\"\"\"\n",
    "    model.eval()\n",
    "    target_device = example_input.device\n",
    "    model_on_device = model.to(target_device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        macs, params = tp.utils.count_ops_and_params(model_on_device, example_input)\n",
    "\n",
    "    return macs, params\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, example_input, criterion, device):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Calculate efficiency metrics\n",
    "    macs, params = calculate_macs_params(model, example_input.to(device))\n",
    "    model_size_mb = params * 4 / (1024 * 1024)  # Assuming float32\n",
    "\n",
    "    # Performance evaluation\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / total if total > 0 else float('nan')\n",
    "    accuracy = 100.0 * correct / total if total > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'loss': avg_loss,\n",
    "        'macs': macs,\n",
    "        'params': params,\n",
    "        'size_mb': model_size_mb\n",
    "    }\n",
    "\n",
    "\n",
    "def get_ignored_layers(model):\n",
    "    \"\"\"Get layers to ignore during pruning\"\"\"\n",
    "    ignored_layers = []\n",
    "    if hasattr(model, 'classifier'):\n",
    "        if isinstance(model.classifier, nn.Sequential):\n",
    "            for layer in model.classifier:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    ignored_layers.append(layer)\n",
    "        elif isinstance(model.classifier, nn.Linear):\n",
    "            ignored_layers.append(model.classifier)\n",
    "    return ignored_layers\n",
    "\n",
    "\n",
    "def prune_model(model, strategy_config, sparsity_ratio, example_input, ignored_layers=None):\n",
    "    \"\"\"Apply structured pruning to model\"\"\"\n",
    "    if sparsity_ratio == 0.0:\n",
    "        print(\"No pruning needed (sparsity = 0.0)\")\n",
    "        return model\n",
    "\n",
    "    model.eval()\n",
    "    pruned_model = copy.deepcopy(model)\n",
    "    pruned_model.to(example_input.device)\n",
    "\n",
    "    # Calculate initial MACs\n",
    "    initial_macs, _ = calculate_macs_params(pruned_model, example_input)\n",
    "    print(f\"Initial MACs: {initial_macs / 1e6:.2f}M\")\n",
    "\n",
    "    ignored_layers = ignored_layers or []\n",
    "\n",
    "    # Create pruner based on strategy\n",
    "    pruner = strategy_config['pruner'](\n",
    "        pruned_model,\n",
    "        example_input,\n",
    "        importance=strategy_config['importance'],\n",
    "        iterative_steps=5,\n",
    "        ch_sparsity=sparsity_ratio,\n",
    "        root_module_types=[nn.Conv2d],\n",
    "        ignored_layers=ignored_layers\n",
    "    )\n",
    "\n",
    "    print(f\"Applying {strategy_config['importance'].__class__.__name__} pruning at {sparsity_ratio:.1%} sparsity...\")\n",
    "\n",
    "    # Apply pruning\n",
    "    pruner.step()\n",
    "\n",
    "    # Calculate final MACs\n",
    "    final_macs, _ = calculate_macs_params(pruned_model, example_input)\n",
    "    reduction = (initial_macs - final_macs) / initial_macs * 100 if initial_macs > 0 else 0\n",
    "    print(f\"Final MACs: {final_macs / 1e6:.2f}M (Reduction: {reduction:.1f}%)\")\n",
    "\n",
    "    return pruned_model\n",
    "\n",
    "\n",
    "def fine_tune_pruned_model(model, train_loader, val_loader, device, config):\n",
    "    \"\"\"Fine-tune pruned model with optimized settings\"\"\"\n",
    "\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Fine-tuning optimizer with lower learning rate\n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': model.features.parameters(), 'lr': config['learning_rate'] * 0.05,\n",
    "         'weight_decay': config['weight_decay']},\n",
    "        {'params': model.classifier.parameters(), 'lr': config['learning_rate'] * 0.3,\n",
    "         'weight_decay': config['weight_decay'] * 0.1}\n",
    "    ])\n",
    "\n",
    "    # Scheduler for fine-tuning\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=config['ft_epochs'], eta_min=config['learning_rate'] * 0.0001)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    print(f\"ðŸ”§ Fine-tuning for {config['ft_epochs']} epochs with patience {config['ft_patience']}\")\n",
    "\n",
    "    for epoch in range(config['ft_epochs']):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total += target.size(0)\n",
    "            train_correct += (predicted == target).sum().item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                val_total += target.size(0)\n",
    "                val_correct += (predicted == target).sum().item()\n",
    "\n",
    "        # Calculate metrics\n",
    "        train_acc = 100.0 * train_correct / train_total\n",
    "        val_acc = 100.0 * val_correct / val_total\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            epochs_no_improve = 0\n",
    "            improvement = \" âœ…\"\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            improvement = \"\"\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"FT Epoch {epoch + 1:2d}/{config['ft_epochs']}: \"\n",
    "                  f\"Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%{improvement}\")\n",
    "\n",
    "        if epochs_no_improve >= config['ft_patience']:\n",
    "            print(f\"Fine-tuning early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"âœ… Fine-tuning complete (Best Val Acc: {best_val_acc:.2f}%)\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_model(model, save_path, example_input_cpu=None):\n",
    "    \"\"\"Save model state dict and optionally ONNX\"\"\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"âœ… Model saved to {save_path}\")\n",
    "\n",
    "    if example_input_cpu is not None:\n",
    "        onnx_path = save_path.replace('.pth', '.onnx')\n",
    "        try:\n",
    "            model_cpu = model.to('cpu')\n",
    "            torch.onnx.export(\n",
    "                model_cpu, example_input_cpu, onnx_path,\n",
    "                export_params=True, opset_version=13,\n",
    "                input_names=['input'], output_names=['output'],\n",
    "                dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "            )\n",
    "            print(f\"âœ… ONNX model saved to {onnx_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: ONNX export failed: {e}\")\n",
    "\n",
    "\n",
    "def save_results_to_files(all_results, output_dir):\n",
    "    \"\"\"Save experimental results to JSON and CSV files\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save complete results as JSON\n",
    "    results_json_path = os.path.join(output_dir, 'optimized_complete_results.json')\n",
    "    with open(results_json_path, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2, default=str)\n",
    "    print(f\"âœ… Complete results saved to {results_json_path}\")\n",
    "\n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for strategy, strategy_results in all_results.items():\n",
    "        for sparsity, metrics in strategy_results.items():\n",
    "            row = {\n",
    "                'strategy': strategy,\n",
    "                'sparsity_ratio': sparsity,\n",
    "                'accuracy': metrics['accuracy'],\n",
    "                'loss': metrics['loss'],\n",
    "                'macs_millions': metrics['macs'] / 1e6,\n",
    "                'params_millions': metrics['params'] / 1e6,\n",
    "                'size_mb': metrics['size_mb']\n",
    "            }\n",
    "            summary_data.append(row)\n",
    "\n",
    "    # Save summary as CSV\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_csv_path = os.path.join(output_dir, 'optimized_summary_results.csv')\n",
    "    summary_df.to_csv(summary_csv_path, index=False)\n",
    "    print(f\"âœ… Summary results saved to {summary_csv_path}\")\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "def create_results_plots(summary_df, output_dir):\n",
    "    \"\"\"Create visualization plots\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    strategies = summary_df['strategy'].unique()\n",
    "\n",
    "    # Plot 1: Accuracy vs Sparsity\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for strategy in strategies:\n",
    "        strategy_data = summary_df[summary_df['strategy'] == strategy].sort_values('sparsity_ratio')\n",
    "        plt.plot(strategy_data['sparsity_ratio'] * 100, strategy_data['accuracy'],\n",
    "                 'o-', linewidth=3, markersize=10, label=strategy)\n",
    "\n",
    "    plt.xlabel('Sparsity (%)', fontsize=14)\n",
    "    plt.ylabel('Accuracy (%)', fontsize=14)\n",
    "    plt.title('Optimized MobileNetV2: Accuracy vs Sparsity', fontsize=16, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(output_dir, 'optimized_accuracy_vs_sparsity.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"âœ… Accuracy plot saved to {plot_path}\")\n",
    "\n",
    "    # Plot 2: Efficiency frontier (Accuracy vs MACs)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for strategy in strategies:\n",
    "        strategy_data = summary_df[summary_df['strategy'] == strategy].sort_values('sparsity_ratio')\n",
    "        plt.scatter(strategy_data['macs_millions'], strategy_data['accuracy'],\n",
    "                    s=100, label=strategy, alpha=0.8)\n",
    "        plt.plot(strategy_data['macs_millions'], strategy_data['accuracy'],\n",
    "                 '--', alpha=0.6)\n",
    "\n",
    "    plt.xlabel('MACs (Millions)', fontsize=14)\n",
    "    plt.ylabel('Accuracy (%)', fontsize=14)\n",
    "    plt.title('Optimized MobileNetV2: Efficiency Frontier (Accuracy vs MACs)', fontsize=16, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(output_dir, 'optimized_efficiency_frontier.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"âœ… Efficiency frontier plot saved to {plot_path}\")\n",
    "\n",
    "\n",
    "def print_results_table(summary_df):\n",
    "    \"\"\"Print formatted results table with ALL pruning strategies\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"OPTIMIZED MobileNetV2 EXPERIMENTAL RESULTS SUMMARY\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    # Baseline results\n",
    "    baseline_results = summary_df[summary_df['sparsity_ratio'] == 0.0].iloc[0]\n",
    "    baseline_acc = baseline_results['accuracy']\n",
    "\n",
    "    print(f\"\\nðŸŽ¯ BASELINE PERFORMANCE:\")\n",
    "    print(\n",
    "        f\"  Accuracy: {baseline_acc:.2f}% {'ðŸŽ‰ EXCELLENT!' if baseline_acc >= 92 else 'âœ… Good' if baseline_acc >= 85 else 'âŒ Below target'}\")\n",
    "    print(f\"  MACs: {baseline_results['macs_millions']:.2f}M\")\n",
    "    print(f\"  Parameters: {baseline_results['params_millions']:.2f}M\")\n",
    "    print(f\"  Model Size: {baseline_results['size_mb']:.2f}MB\")\n",
    "\n",
    "    # Strategy comparison at different sparsity levels\n",
    "    for sparsity_level in [0.2, 0.5, 0.7]:\n",
    "        print(f\"\\nðŸ“Š STRATEGY COMPARISON AT {sparsity_level * 100:.0f}% SPARSITY:\")\n",
    "        sparsity_data = summary_df[summary_df['sparsity_ratio'] == sparsity_level]\n",
    "        if len(sparsity_data) > 0:\n",
    "            for _, row in sparsity_data.iterrows():\n",
    "                degradation = baseline_acc - row['accuracy']\n",
    "                retention = (row['accuracy'] / baseline_acc) * 100\n",
    "                mac_reduction = (1 - row['macs_millions'] / baseline_results['macs_millions']) * 100\n",
    "                size_reduction = (1 - row['size_mb'] / baseline_results['size_mb']) * 100\n",
    "\n",
    "                status = \"ðŸŽ‰\" if row['accuracy'] >= 90 else \"âœ…\" if row['accuracy'] >= 85 else \"âš ï¸\" if row[\n",
    "                                                                                                         'accuracy'] >= 80 else \"âŒ\"\n",
    "\n",
    "                print(f\"  {row['strategy']:>12}: {row['accuracy']:>6.2f}% (â†“{degradation:>4.1f}%, \"\n",
    "                      f\"Retention: {retention:>5.1f}%, MACsâ†“{mac_reduction:>4.1f}%, Sizeâ†“{size_reduction:>4.1f}%) {status}\")\n",
    "\n",
    "    # Complete results table\n",
    "    print(f\"\\nðŸ“‹ COMPLETE RESULTS TABLE:\")\n",
    "    print(\"-\" * 100)\n",
    "    print(\n",
    "        f\"{'Strategy':<12} {'Sparsity':<8} {'Accuracy':<8} {'Loss':<8} {'MACs(M)':<8} {'Params(M)':<9} {'Size(MB)':<8}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    for _, row in summary_df.sort_values(['strategy', 'sparsity_ratio']).iterrows():\n",
    "        print(f\"{row['strategy']:<12} {row['sparsity_ratio'] * 100:>6.0f}% \"\n",
    "              f\"{row['accuracy']:>7.2f}% {row['loss']:>7.4f} {row['macs_millions']:>7.2f} \"\n",
    "              f\"{row['params_millions']:>8.2f} {row['size_mb']:>7.2f}\")\n",
    "\n",
    "    # Best performers\n",
    "    print(f\"\\nðŸ† BEST PERFORMERS BY SPARSITY LEVEL:\")\n",
    "    for sparsity_level in [0.2, 0.5, 0.7]:\n",
    "        sparsity_data = summary_df[summary_df['sparsity_ratio'] == sparsity_level]\n",
    "        if len(sparsity_data) > 0:\n",
    "            best = sparsity_data.loc[sparsity_data['accuracy'].idxmax()]\n",
    "            print(f\"  {sparsity_level * 100:.0f}% Sparsity: {best['strategy']} with {best['accuracy']:.2f}% accuracy\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main experimental workflow with 1000 epochs and early stopping\"\"\"\n",
    "    print(\"ðŸš€ OPTIMIZED MobileNetV2 CIFAR-10 Experiments for 92%+ Baseline\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Optimized configuration\n",
    "    config = {\n",
    "        'strategies': {\n",
    "            'BNScale': {\n",
    "                'pruner': tp.pruner.BNScalePruner,\n",
    "                'importance': tp.importance.BNScaleImportance()\n",
    "            },\n",
    "            'MagnitudeL2': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.MagnitudeImportance(p=2)\n",
    "            },\n",
    "            'Random': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.RandomImportance()\n",
    "            },\n",
    "        },\n",
    "        'pruning_ratios': [0.0, 0.2, 0.5, 0.7],\n",
    "        'num_classes': 10,\n",
    "        'batch_size': 128,\n",
    "        'learning_rate': 0.002,  # Optimized learning rate\n",
    "        'weight_decay': 1e-4,  # L2 regularization\n",
    "        'epochs': 1000,  # Maximum epochs as requested\n",
    "        'patience': 20,  # Early stopping patience as requested\n",
    "        'ft_epochs': 50,  # Fine-tuning epochs\n",
    "        'ft_patience': 10,  # Fine-tuning patience\n",
    "        'output_dir': './results_optimized_mobilenetv2_1000ep',\n",
    "        'models_dir': './base',\n",
    "        'pretrained_path': './base/mobilenet_v2-b0353104.pth'\n",
    "    }\n",
    "\n",
    "    # Create output directories\n",
    "    os.makedirs(config['output_dir'], exist_ok=True)\n",
    "    os.makedirs(config['models_dir'], exist_ok=True)\n",
    "\n",
    "    # Load optimized data\n",
    "    print(\"ðŸ“Š Loading CIFAR-10 with optimized preprocessing...\")\n",
    "    train_loader, val_loader, test_loader = get_optimized_data_loaders(\n",
    "        batch_size=config['batch_size']\n",
    "    )\n",
    "\n",
    "    # Prepare inputs and criterion\n",
    "    example_input_cpu = torch.randn(1, 3, 32, 32)\n",
    "    example_input_device = example_input_cpu.to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Get optimized baseline model\n",
    "    print(\"\\nðŸ—ï¸ Creating optimized MobileNetV2 model...\")\n",
    "    model = get_optimized_mobilenetv2_model(\n",
    "        num_classes=config['num_classes'],\n",
    "        pretrained_path=config['pretrained_path']\n",
    "    )\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Train optimized baseline model\n",
    "    print(\"\\nðŸš€ Training optimized baseline model (1000 epochs, early stopping=20)...\")\n",
    "    trained_model, training_history = train_optimized_model(\n",
    "        model, train_loader, val_loader, DEVICE, config\n",
    "    )\n",
    "\n",
    "    # Save baseline model\n",
    "    baseline_model_path = os.path.join(config['models_dir'], 'optimized_baseline_1000ep.pth')\n",
    "    save_model(trained_model, baseline_model_path, example_input_cpu)\n",
    "\n",
    "    # Evaluate baseline\n",
    "    print(\"\\nðŸ“Š Evaluating optimized baseline model...\")\n",
    "    baseline_metrics = evaluate_model(trained_model, test_loader, example_input_device, criterion, DEVICE)\n",
    "    baseline_acc = baseline_metrics['accuracy']\n",
    "\n",
    "    print(f\"\\nðŸŽ¯ OPTIMIZED BASELINE RESULTS:\")\n",
    "    print(\n",
    "        f\"   Test Accuracy: {baseline_acc:.2f}% {'ðŸŽ‰ EXCELLENT!' if baseline_acc >= 92 else 'âœ… Good' if baseline_acc >= 85 else 'âŒ Below target'}\")\n",
    "    print(f\"   MACs: {baseline_metrics['macs'] / 1e6:.2f}M\")\n",
    "    print(f\"   Parameters: {baseline_metrics['params'] / 1e6:.2f}M\")\n",
    "    print(f\"   Model Size: {baseline_metrics['size_mb']:.2f}MB\")\n",
    "\n",
    "    # Initialize results storage for ALL strategies\n",
    "    all_results = {}\n",
    "    for strategy_name in config['strategies'].keys():\n",
    "        all_results[strategy_name] = {0.0: baseline_metrics}\n",
    "\n",
    "    # Get ignored layers\n",
    "    ignored_layers = get_ignored_layers(trained_model)\n",
    "\n",
    "    # Run pruning experiments for ALL strategies\n",
    "    print(\"\\nðŸ”¬ Starting pruning experiments for ALL strategies...\")\n",
    "    for strategy_name, strategy_config in config['strategies'].items():\n",
    "        print(f\"\\n--- Strategy: {strategy_name} ---\")\n",
    "\n",
    "        for sparsity_ratio in config['pruning_ratios']:\n",
    "            if sparsity_ratio == 0.0:\n",
    "                continue  # Skip baseline (already done)\n",
    "\n",
    "            print(f\"\\nâš™ï¸ Processing {strategy_name} at {sparsity_ratio:.1%} sparsity...\")\n",
    "\n",
    "            # Load fresh copy of trained baseline\n",
    "            model_copy = get_optimized_mobilenetv2_model(\n",
    "                num_classes=config['num_classes'],\n",
    "                pretrained_path=None  # Don't reload pretrained weights\n",
    "            )\n",
    "            model_copy.load_state_dict(torch.load(baseline_model_path, map_location=DEVICE))\n",
    "            model_copy.to(DEVICE)\n",
    "\n",
    "            # Apply pruning\n",
    "            pruned_model = prune_model(\n",
    "                model_copy, strategy_config, sparsity_ratio,\n",
    "                example_input_device, ignored_layers\n",
    "            )\n",
    "\n",
    "            # Fine-tune pruned model\n",
    "            print(\"ðŸ”§ Fine-tuning pruned model...\")\n",
    "            fine_tuned_model = fine_tune_pruned_model(\n",
    "                pruned_model, train_loader, val_loader, DEVICE, config\n",
    "            )\n",
    "\n",
    "            # Evaluate fine-tuned model\n",
    "            final_metrics = evaluate_model(fine_tuned_model, test_loader, example_input_device, criterion, DEVICE)\n",
    "            all_results[strategy_name][sparsity_ratio] = final_metrics\n",
    "\n",
    "            final_acc = final_metrics['accuracy']\n",
    "            retention = (final_acc / baseline_acc) * 100\n",
    "            mac_reduction = (1 - final_metrics['macs'] / baseline_metrics['macs']) * 100\n",
    "\n",
    "            print(f\"ðŸ“ˆ Results: Accuracy={final_acc:.2f}% (Retention: {retention:.1f}%), \"\n",
    "                  f\"MACs={final_metrics['macs'] / 1e6:.2f}M (Reduction: {mac_reduction:.1f}%)\")\n",
    "\n",
    "            # Save fine-tuned model\n",
    "            model_filename = f\"optimized_{strategy_name.lower()}_sparsity_{sparsity_ratio:.1f}.pth\"\n",
    "            model_path = os.path.join(config['models_dir'], model_filename)\n",
    "            save_model(fine_tuned_model, model_path, example_input_cpu)\n",
    "\n",
    "    # Save and analyze results\n",
    "    print(\"\\nðŸ’¾ Saving comprehensive results...\")\n",
    "    summary_df = save_results_to_files(all_results, config['output_dir'])\n",
    "\n",
    "    # Create plots\n",
    "    print(\"ðŸ“Š Creating result plots...\")\n",
    "    create_results_plots(summary_df, config['output_dir'])\n",
    "\n",
    "    # Print comprehensive summary with ALL strategies\n",
    "    print_results_table(summary_df)\n",
    "\n",
    "    # Final summary\n",
    "    print(f\"\\nðŸ OPTIMIZED EXPERIMENTS COMPLETED!\")\n",
    "    print(f\"ðŸ“ Results saved to: {os.path.abspath(config['output_dir'])}\")\n",
    "    print(f\"ðŸ“ Models saved to: {os.path.abspath(config['models_dir'])}\")\n",
    "\n",
    "    # Performance analysis\n",
    "    if baseline_acc >= 92:\n",
    "        print(f\"\\nðŸŽ‰ SUCCESS: Baseline accuracy {baseline_acc:.2f}% achieved 92%+ target!\")\n",
    "        print(f\"ðŸ”¥ Key optimizations applied:\")\n",
    "        print(f\"   â€¢ Enhanced data augmentation with RandomErasing\")\n",
    "        print(f\"   â€¢ Improved classifier with BatchNorm\")\n",
    "        print(f\"   â€¢ AdamW optimizer with differential learning rates\")\n",
    "        print(f\"   â€¢ CosineAnnealingLR scheduler\")\n",
    "        print(f\"   â€¢ Label smoothing (0.1) for better generalization\")\n",
    "        print(f\"   â€¢ Gradient clipping for training stability\")\n",
    "    elif baseline_acc >= 85:\n",
    "        print(f\"\\nâœ… Good baseline: {baseline_acc:.2f}% - Close to target!\")\n",
    "        print(f\"ðŸ”§ Suggestions for further improvement:\")\n",
    "        print(f\"   â€¢ Try longer training with reduced learning rate\")\n",
    "        print(f\"   â€¢ Experiment with different augmentation strategies\")\n",
    "        print(f\"   â€¢ Consider ensemble methods\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸ Baseline: {baseline_acc:.2f}% - Still below target\")\n",
    "        print(f\"ðŸ”§ Debugging suggestions:\")\n",
    "        print(f\"   â€¢ Check if pretrained weights loaded correctly\")\n",
    "        print(f\"   â€¢ Verify CIFAR-10 dataset integrity\")\n",
    "        print(f\"   â€¢ Try training from scratch without pretrained weights\")\n",
    "        print(f\"   â€¢ Increase model capacity or try different architecture\")\n",
    "\n",
    "    # Show best performing strategy per sparsity level\n",
    "    print(f\"\\nðŸ† BEST STRATEGIES SUMMARY:\")\n",
    "    for sparsity in [0.2, 0.5, 0.7]:\n",
    "        sparsity_data = summary_df[summary_df['sparsity_ratio'] == sparsity]\n",
    "        if len(sparsity_data) > 0:\n",
    "            best = sparsity_data.loc[sparsity_data['accuracy'].idxmax()]\n",
    "            retention = (best['accuracy'] / baseline_acc) * 100\n",
    "            print(\n",
    "                f\"   {sparsity * 100:.0f}% Sparsity: {best['strategy']} â†’ {best['accuracy']:.2f}% ({retention:.1f}% retention)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "10ba42bbb036a1fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "ðŸš€ OPTIMIZED MobileNetV2 CIFAR-10 Experiments for 92%+ Baseline\n",
      "================================================================================\n",
      "ðŸ“Š Loading CIFAR-10 with optimized preprocessing...\n",
      "Loading CIFAR-10 from: /home/muis/thesis/github-repo/master-thesis/cnn/mobile_net_v2/data\n",
      "Optimized DataLoaders - Train: 45000, Val: 5000, Test: 10000\n",
      "\n",
      "ðŸ—ï¸ Creating optimized MobileNetV2 model...\n",
      "Loading optimized pretrained weights from: ./base/mobilenet_v2-b0353104.pth\n",
      "Loading 312/314 pretrained weights\n",
      "âœ… Optimized ImageNet pretrained weights loaded successfully\n",
      "âœ… Optimized MobileNetV2 model created for 10 classes\n",
      "\n",
      "ðŸš€ Training optimized baseline model (1000 epochs, early stopping=20)...\n",
      "ðŸš€ Starting optimized training for 92%+ accuracy...\n",
      "   Epochs: 1000, Early stopping patience: 20\n",
      "Epoch   1/1000: Train Acc: 47.05%, Val Acc: 65.66%, LR: 0.000200 âœ…\n",
      "Epoch   2/1000: Train Acc: 64.69%, Val Acc: 73.00%, LR: 0.000200 âœ…\n",
      "Epoch   3/1000: Train Acc: 69.50%, Val Acc: 76.58%, LR: 0.000200 âœ…\n",
      "Epoch   4/1000: Train Acc: 72.19%, Val Acc: 78.08%, LR: 0.000200 âœ…\n",
      "Epoch   5/1000: Train Acc: 74.46%, Val Acc: 78.76%, LR: 0.000200 âœ…\n",
      "Epoch   6/1000: Train Acc: 75.98%, Val Acc: 80.82%, LR: 0.000200 âœ…\n",
      "Epoch   7/1000: Train Acc: 77.27%, Val Acc: 80.62%, LR: 0.000200\n",
      "Epoch   8/1000: Train Acc: 78.07%, Val Acc: 82.00%, LR: 0.000200 âœ…\n",
      "Epoch   9/1000: Train Acc: 79.05%, Val Acc: 82.62%, LR: 0.000200 âœ…\n",
      "Epoch  10/1000: Train Acc: 79.74%, Val Acc: 82.82%, LR: 0.000200 âœ…\n",
      "Epoch  11/1000: Train Acc: 80.47%, Val Acc: 82.92%, LR: 0.000200 âœ…\n",
      "Epoch  12/1000: Train Acc: 80.97%, Val Acc: 83.08%, LR: 0.000200 âœ…\n",
      "Epoch  13/1000: Train Acc: 81.66%, Val Acc: 83.88%, LR: 0.000200 âœ…\n",
      "Epoch  14/1000: Train Acc: 82.09%, Val Acc: 83.80%, LR: 0.000200\n",
      "Epoch  15/1000: Train Acc: 82.33%, Val Acc: 83.94%, LR: 0.000200 âœ…\n",
      "Epoch  16/1000: Train Acc: 82.92%, Val Acc: 84.30%, LR: 0.000200 âœ…\n",
      "Epoch  17/1000: Train Acc: 83.19%, Val Acc: 85.04%, LR: 0.000200 âœ…\n",
      "Epoch  18/1000: Train Acc: 83.74%, Val Acc: 84.90%, LR: 0.000200\n",
      "Epoch  19/1000: Train Acc: 83.93%, Val Acc: 84.26%, LR: 0.000200\n",
      "Epoch  20/1000: Train Acc: 84.27%, Val Acc: 84.78%, LR: 0.000200\n",
      "Epoch  30/1000: Train Acc: 86.91%, Val Acc: 85.56%, LR: 0.000200\n",
      "Epoch  40/1000: Train Acc: 88.40%, Val Acc: 86.24%, LR: 0.000199\n",
      "Epoch  50/1000: Train Acc: 89.70%, Val Acc: 86.40%, LR: 0.000199\n",
      "Epoch  60/1000: Train Acc: 90.80%, Val Acc: 86.70%, LR: 0.000198\n",
      "Epoch  70/1000: Train Acc: 91.51%, Val Acc: 86.56%, LR: 0.000198\n",
      "Epoch  80/1000: Train Acc: 92.70%, Val Acc: 86.58%, LR: 0.000197\n",
      "Early stopping triggered at epoch 81 (Best Val Acc: 87.02%)\n",
      "âœ… Loaded best model state (Best Val Acc: 87.02%)\n",
      "âœ… Model saved to ./base/optimized_baseline_1000ep.pth\n",
      "âœ… ONNX model saved to ./base/optimized_baseline_1000ep.onnx\n",
      "\n",
      "ðŸ“Š Evaluating optimized baseline model...\n",
      "\n",
      "ðŸŽ¯ OPTIMIZED BASELINE RESULTS:\n",
      "   Test Accuracy: 86.20% âœ… Good\n",
      "   MACs: 7.17M\n",
      "   Parameters: 2.89M\n",
      "   Model Size: 11.01MB\n",
      "\n",
      "ðŸ”¬ Starting pruning experiments for ALL strategies...\n",
      "\n",
      "--- Strategy: BNScale ---\n",
      "\n",
      "âš™ï¸ Processing BNScale at 20.0% sparsity...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "stat: path should be string, bytes, os.PathLike or integer, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 821\u001B[39m\n\u001B[32m    816\u001B[39m             \u001B[38;5;28mprint\u001B[39m(\n\u001B[32m    817\u001B[39m                 \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m   \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msparsity\u001B[38;5;250m \u001B[39m*\u001B[38;5;250m \u001B[39m\u001B[32m100\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.0f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m% Sparsity: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbest[\u001B[33m'\u001B[39m\u001B[33mstrategy\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m â†’ \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbest[\u001B[33m'\u001B[39m\u001B[33maccuracy\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m% (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mretention\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.1f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m% retention)\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    820\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m821\u001B[39m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 734\u001B[39m, in \u001B[36mmain\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    731\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mâš™ï¸ Processing \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstrategy_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msparsity_ratio\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.1%\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m sparsity...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    733\u001B[39m \u001B[38;5;66;03m# Load fresh copy of trained baseline\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m734\u001B[39m model_copy = \u001B[43mget_optimized_mobilenetv2_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    735\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnum_classes\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mnum_classes\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    736\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpretrained_path\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Don't reload pretrained weights\u001B[39;49;00m\n\u001B[32m    737\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    738\u001B[39m model_copy.load_state_dict(torch.load(baseline_model_path, map_location=DEVICE))\n\u001B[32m    739\u001B[39m model_copy.to(DEVICE)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 97\u001B[39m, in \u001B[36mget_optimized_mobilenetv2_model\u001B[39m\u001B[34m(num_classes, pretrained_path)\u001B[39m\n\u001B[32m     94\u001B[39m model = models.mobilenet_v2(weights=\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m     96\u001B[39m \u001B[38;5;66;03m# Load ImageNet pretrained weights more carefully\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m97\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mos\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m.\u001B[49m\u001B[43mexists\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_path\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[32m     98\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mLoading optimized pretrained weights from: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     99\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    100\u001B[39m         \u001B[38;5;66;03m# Load the state dict\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen genericpath>:19\u001B[39m, in \u001B[36mexists\u001B[39m\u001B[34m(path)\u001B[39m\n",
      "\u001B[31mTypeError\u001B[39m: stat: path should be string, bytes, os.PathLike or integer, not NoneType"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T14:02:48.160566Z",
     "start_time": "2025-06-04T13:32:33.679088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_pruning as tp\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR, CosineAnnealingLR\n",
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_BASE_NAME = \"mobilenet_v2\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "def get_optimized_data_loaders(data_dir_path='./data', batch_size=128, val_split=0.1, seed=42):\n",
    "    \"\"\"Optimized data loading for maximum CIFAR-10 performance\"\"\"\n",
    "    abs_data_dir = os.path.abspath(data_dir_path)\n",
    "    print(f\"Loading CIFAR-10 from: {abs_data_dir}\")\n",
    "\n",
    "    # CIFAR-10 mean and std (verified values)\n",
    "    mean = [0.4914, 0.4822, 0.4465]\n",
    "    std = [0.2023, 0.1994, 0.2010]\n",
    "\n",
    "    # Enhanced training transforms for maximum performance\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4, padding_mode='reflect'),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "        transforms.RandomErasing(p=0.1, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)\n",
    "    ])\n",
    "\n",
    "    # Clean test transform\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "    # Load datasets\n",
    "    full_train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=abs_data_dir, train=True, download=False, transform=train_transform\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=abs_data_dir, train=False, download=False, transform=test_transform\n",
    "    )\n",
    "\n",
    "    # Create train/validation split\n",
    "    val_size = int(len(full_train_dataset) * val_split)\n",
    "    train_size = len(full_train_dataset) - val_size\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_train_dataset, [train_size, val_size], generator=generator\n",
    "    )\n",
    "\n",
    "    # Apply test transform to validation set\n",
    "    val_dataset.dataset = copy.deepcopy(full_train_dataset)\n",
    "    val_dataset.dataset.transform = test_transform\n",
    "\n",
    "    # Optimized data loaders\n",
    "    num_workers = min(4, os.cpu_count() or 2)\n",
    "    pin_memory = True if DEVICE.type == 'cuda' else False\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=num_workers, pin_memory=pin_memory, drop_last=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers, pin_memory=pin_memory\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers, pin_memory=pin_memory\n",
    "    )\n",
    "\n",
    "    print(f\"Optimized DataLoaders - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "def get_optimized_mobilenetv2_model(num_classes=10, pretrained_path='./base/mobilenet_v2-b0353104.pth'):\n",
    "    \"\"\"Create anti-overfitting MobileNetV2 for 92%+ test accuracy\"\"\"\n",
    "\n",
    "    # Create model without weights first\n",
    "    model = models.mobilenet_v2(weights=None)\n",
    "\n",
    "    # Load ImageNet pretrained weights more carefully\n",
    "    if pretrained_path is not None and os.path.exists(pretrained_path):\n",
    "        print(f\"Loading optimized pretrained weights from: {pretrained_path}\")\n",
    "        try:\n",
    "            # Load the state dict\n",
    "            checkpoint = torch.load(pretrained_path, map_location='cpu')\n",
    "\n",
    "            # Get model's current state dict\n",
    "            model_dict = model.state_dict()\n",
    "\n",
    "            # Filter pretrained dict to match model structure (exclude classifier)\n",
    "            filtered_dict = {}\n",
    "            for k, v in checkpoint.items():\n",
    "                # Skip classifier weights and ensure shape matches\n",
    "                if 'classifier' not in k and k in model_dict:\n",
    "                    if model_dict[k].shape == v.shape:\n",
    "                        filtered_dict[k] = v\n",
    "                    else:\n",
    "                        print(f\"Shape mismatch for {k}: model={model_dict[k].shape}, pretrained={v.shape}\")\n",
    "\n",
    "            print(f\"Loading {len(filtered_dict)}/{len(model_dict)} pretrained weights\")\n",
    "\n",
    "            # Update model dict and load\n",
    "            model_dict.update(filtered_dict)\n",
    "            model.load_state_dict(model_dict, strict=False)\n",
    "            print(\"âœ… Optimized ImageNet pretrained weights loaded successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading pretrained weights: {e}\")\n",
    "            print(\"Training from scratch...\")\n",
    "    else:\n",
    "        if pretrained_path is None:\n",
    "            print(\"âœ… Creating model without pretrained weights (as requested)\")\n",
    "        else:\n",
    "            print(f\"âŒ No pretrained weights found at {pretrained_path} - training from scratch\")\n",
    "\n",
    "    # Anti-overfitting classifier - simpler and more regularized\n",
    "    num_features = model.classifier[1].in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.5),  # Increased dropout\n",
    "        nn.Linear(num_features, 256),  # Smaller hidden layer\n",
    "        nn.BatchNorm1d(256),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(0.4),  # More dropout\n",
    "        nn.Linear(256, num_classes)\n",
    "    )\n",
    "\n",
    "    # Initialize classifier weights properly\n",
    "    for m in model.classifier.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm1d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    print(f\"âœ… Anti-overfitting MobileNetV2 model created for {num_classes} classes\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_optimized_model(model, train_loader, val_loader, device, config):\n",
    "    \"\"\"Anti-overfitting training function for 92%+ test accuracy\"\"\"\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Loss function with stronger label smoothing\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.15)\n",
    "\n",
    "    # More conservative optimizer to prevent overfitting\n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': model.features.parameters(), 'lr': config['learning_rate'] * 0.05, 'weight_decay': config['weight_decay'] * 2},  # Lower LR, higher WD\n",
    "        {'params': model.classifier.parameters(), 'lr': config['learning_rate'] * 0.5, 'weight_decay': config['weight_decay']}  # Reduced LR\n",
    "    ], eps=1e-8)\n",
    "\n",
    "    # More aggressive scheduler to prevent overfitting\n",
    "    from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True, min_lr=1e-7)\n",
    "\n",
    "    # Training state\n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'learning_rates': []\n",
    "    }\n",
    "\n",
    "    print(f\"ðŸš€ Starting optimized training for 92%+ accuracy...\")\n",
    "    print(f\"   Epochs: {config['epochs']}, Early stopping patience: {config['patience']}\")\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total += target.size(0)\n",
    "            train_correct += (predicted == target).sum().item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                val_total += target.size(0)\n",
    "                val_correct += (predicted == target).sum().item()\n",
    "\n",
    "        # Calculate metrics\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_acc = 100.0 * train_correct / train_total\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100.0 * val_correct / val_total\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # Update scheduler based on validation accuracy (anti-overfitting)\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            improvement = \" âœ…\"\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            improvement = \"\"\n",
    "\n",
    "        # Record history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['learning_rates'].append(current_lr)\n",
    "\n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0 or epoch < 20:\n",
    "            print(f\"Epoch {epoch+1:3d}/{config['epochs']}: \"\n",
    "                  f\"Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%, \"\n",
    "                  f\"LR: {current_lr:.6f}{improvement}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if epochs_no_improve >= config['patience']:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1} (Best Val Acc: {best_val_acc:.2f}%)\")\n",
    "            break\n",
    "\n",
    "        # Stop if we achieve excellent performance\n",
    "        if val_acc >= 95.0:\n",
    "            print(f\"ðŸŽ‰ Excellent performance achieved: {val_acc:.2f}% at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"âœ… Loaded best model state (Best Val Acc: {best_val_acc:.2f}%)\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def calculate_macs_params(model, example_input):\n",
    "    \"\"\"Calculate MACs and parameters using torch_pruning\"\"\"\n",
    "    model.eval()\n",
    "    target_device = example_input.device\n",
    "    model_on_device = model.to(target_device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        macs, params = tp.utils.count_ops_and_params(model_on_device, example_input)\n",
    "\n",
    "    return macs, params\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, example_input, criterion, device):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Calculate efficiency metrics\n",
    "    macs, params = calculate_macs_params(model, example_input.to(device))\n",
    "    model_size_mb = params * 4 / (1024 * 1024)  # Assuming float32\n",
    "\n",
    "    # Performance evaluation\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / total if total > 0 else float('nan')\n",
    "    accuracy = 100.0 * correct / total if total > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'loss': avg_loss,\n",
    "        'macs': macs,\n",
    "        'params': params,\n",
    "        'size_mb': model_size_mb\n",
    "    }\n",
    "\n",
    "\n",
    "def get_ignored_layers(model):\n",
    "    \"\"\"Get layers to ignore during pruning\"\"\"\n",
    "    ignored_layers = []\n",
    "    if hasattr(model, 'classifier'):\n",
    "        if isinstance(model.classifier, nn.Sequential):\n",
    "            for layer in model.classifier:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    ignored_layers.append(layer)\n",
    "        elif isinstance(model.classifier, nn.Linear):\n",
    "            ignored_layers.append(model.classifier)\n",
    "    return ignored_layers\n",
    "\n",
    "\n",
    "def prune_model(model, strategy_config, sparsity_ratio, example_input, ignored_layers=None):\n",
    "    \"\"\"Apply structured pruning to model\"\"\"\n",
    "    if sparsity_ratio == 0.0:\n",
    "        print(\"No pruning needed (sparsity = 0.0)\")\n",
    "        return model\n",
    "\n",
    "    model.eval()\n",
    "    pruned_model = copy.deepcopy(model)\n",
    "    pruned_model.to(example_input.device)\n",
    "\n",
    "    # Calculate initial MACs\n",
    "    initial_macs, _ = calculate_macs_params(pruned_model, example_input)\n",
    "    print(f\"Initial MACs: {initial_macs / 1e6:.2f}M\")\n",
    "\n",
    "    ignored_layers = ignored_layers or []\n",
    "\n",
    "    # Create pruner based on strategy\n",
    "    pruner = strategy_config['pruner'](\n",
    "        pruned_model,\n",
    "        example_input,\n",
    "        importance=strategy_config['importance'],\n",
    "        iterative_steps=5,\n",
    "        ch_sparsity=sparsity_ratio,\n",
    "        root_module_types=[nn.Conv2d],\n",
    "        ignored_layers=ignored_layers\n",
    "    )\n",
    "\n",
    "    print(f\"Applying {strategy_config['importance'].__class__.__name__} pruning at {sparsity_ratio:.1%} sparsity...\")\n",
    "\n",
    "    # Apply pruning\n",
    "    pruner.step()\n",
    "\n",
    "    # Calculate final MACs\n",
    "    final_macs, _ = calculate_macs_params(pruned_model, example_input)\n",
    "    reduction = (initial_macs - final_macs) / initial_macs * 100 if initial_macs > 0 else 0\n",
    "    print(f\"Final MACs: {final_macs / 1e6:.2f}M (Reduction: {reduction:.1f}%)\")\n",
    "\n",
    "    return pruned_model\n",
    "\n",
    "\n",
    "def fine_tune_pruned_model(model, train_loader, val_loader, device, config):\n",
    "    \"\"\"Fine-tune pruned model with optimized settings\"\"\"\n",
    "\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Fine-tuning optimizer with lower learning rate\n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': model.features.parameters(), 'lr': config['learning_rate'] * 0.05, 'weight_decay': config['weight_decay']},\n",
    "        {'params': model.classifier.parameters(), 'lr': config['learning_rate'] * 0.3, 'weight_decay': config['weight_decay'] * 0.1}\n",
    "    ])\n",
    "\n",
    "    # Scheduler for fine-tuning\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=config['ft_epochs'], eta_min=config['learning_rate'] * 0.0001)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    print(f\"ðŸ”§ Fine-tuning for {config['ft_epochs']} epochs with patience {config['ft_patience']}\")\n",
    "\n",
    "    for epoch in range(config['ft_epochs']):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total += target.size(0)\n",
    "            train_correct += (predicted == target).sum().item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                val_total += target.size(0)\n",
    "                val_correct += (predicted == target).sum().item()\n",
    "\n",
    "        # Calculate metrics\n",
    "        train_acc = 100.0 * train_correct / train_total\n",
    "        val_acc = 100.0 * val_correct / val_total\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            epochs_no_improve = 0\n",
    "            improvement = \" âœ…\"\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            improvement = \"\"\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"FT Epoch {epoch+1:2d}/{config['ft_epochs']}: \"\n",
    "                  f\"Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%{improvement}\")\n",
    "\n",
    "        if epochs_no_improve >= config['ft_patience']:\n",
    "            print(f\"Fine-tuning early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"âœ… Fine-tuning complete (Best Val Acc: {best_val_acc:.2f}%)\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_model(model, save_path, example_input_cpu=None):\n",
    "    \"\"\"Save model state dict and optionally ONNX\"\"\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"âœ… Model saved to {save_path}\")\n",
    "\n",
    "    if example_input_cpu is not None:\n",
    "        onnx_path = save_path.replace('.pth', '.onnx')\n",
    "        try:\n",
    "            model_cpu = model.to('cpu')\n",
    "            torch.onnx.export(\n",
    "                model_cpu, example_input_cpu, onnx_path,\n",
    "                export_params=True, opset_version=13,\n",
    "                input_names=['input'], output_names=['output'],\n",
    "                dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "            )\n",
    "            print(f\"âœ… ONNX model saved to {onnx_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: ONNX export failed: {e}\")\n",
    "\n",
    "\n",
    "def save_results_to_files(all_results, output_dir):\n",
    "    \"\"\"Save experimental results to JSON and CSV files\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save complete results as JSON\n",
    "    results_json_path = os.path.join(output_dir, 'optimized_complete_results.json')\n",
    "    with open(results_json_path, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2, default=str)\n",
    "    print(f\"âœ… Complete results saved to {results_json_path}\")\n",
    "\n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for strategy, strategy_results in all_results.items():\n",
    "        for sparsity, metrics in strategy_results.items():\n",
    "            row = {\n",
    "                'strategy': strategy,\n",
    "                'sparsity_ratio': sparsity,\n",
    "                'accuracy': metrics['accuracy'],\n",
    "                'loss': metrics['loss'],\n",
    "                'macs_millions': metrics['macs'] / 1e6,\n",
    "                'params_millions': metrics['params'] / 1e6,\n",
    "                'size_mb': metrics['size_mb']\n",
    "            }\n",
    "            summary_data.append(row)\n",
    "\n",
    "    # Save summary as CSV\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_csv_path = os.path.join(output_dir, 'optimized_summary_results.csv')\n",
    "    summary_df.to_csv(summary_csv_path, index=False)\n",
    "    print(f\"âœ… Summary results saved to {summary_csv_path}\")\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "def create_results_plots(summary_df, output_dir):\n",
    "    \"\"\"Create visualization plots\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    strategies = summary_df['strategy'].unique()\n",
    "\n",
    "    # Plot 1: Accuracy vs Sparsity\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for strategy in strategies:\n",
    "        strategy_data = summary_df[summary_df['strategy'] == strategy].sort_values('sparsity_ratio')\n",
    "        plt.plot(strategy_data['sparsity_ratio'] * 100, strategy_data['accuracy'],\n",
    "                 'o-', linewidth=3, markersize=10, label=strategy)\n",
    "\n",
    "    plt.xlabel('Sparsity (%)', fontsize=14)\n",
    "    plt.ylabel('Accuracy (%)', fontsize=14)\n",
    "    plt.title('Optimized MobileNetV2: Accuracy vs Sparsity', fontsize=16, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(output_dir, 'optimized_accuracy_vs_sparsity.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"âœ… Accuracy plot saved to {plot_path}\")\n",
    "\n",
    "    # Plot 2: Efficiency frontier (Accuracy vs MACs)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for strategy in strategies:\n",
    "        strategy_data = summary_df[summary_df['strategy'] == strategy].sort_values('sparsity_ratio')\n",
    "        plt.scatter(strategy_data['macs_millions'], strategy_data['accuracy'],\n",
    "                    s=100, label=strategy, alpha=0.8)\n",
    "        plt.plot(strategy_data['macs_millions'], strategy_data['accuracy'],\n",
    "                 '--', alpha=0.6)\n",
    "\n",
    "    plt.xlabel('MACs (Millions)', fontsize=14)\n",
    "    plt.ylabel('Accuracy (%)', fontsize=14)\n",
    "    plt.title('Optimized MobileNetV2: Efficiency Frontier (Accuracy vs MACs)', fontsize=16, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(output_dir, 'optimized_efficiency_frontier.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"âœ… Efficiency frontier plot saved to {plot_path}\")\n",
    "\n",
    "\n",
    "def print_results_table(summary_df):\n",
    "    \"\"\"Print formatted results table with ALL pruning strategies\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"OPTIMIZED MobileNetV2 EXPERIMENTAL RESULTS SUMMARY\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    # Baseline results\n",
    "    baseline_results = summary_df[summary_df['sparsity_ratio'] == 0.0].iloc[0]\n",
    "    baseline_acc = baseline_results['accuracy']\n",
    "\n",
    "    print(f\"\\nðŸŽ¯ BASELINE PERFORMANCE:\")\n",
    "    print(f\"  Accuracy: {baseline_acc:.2f}% {'ðŸŽ‰ EXCELLENT!' if baseline_acc >= 92 else 'âœ… Good' if baseline_acc >= 85 else 'âŒ Below target'}\")\n",
    "    print(f\"  MACs: {baseline_results['macs_millions']:.2f}M\")\n",
    "    print(f\"  Parameters: {baseline_results['params_millions']:.2f}M\")\n",
    "    print(f\"  Model Size: {baseline_results['size_mb']:.2f}MB\")\n",
    "\n",
    "    # Strategy comparison at different sparsity levels\n",
    "    for sparsity_level in [0.2, 0.5, 0.7]:\n",
    "        print(f\"\\nðŸ“Š STRATEGY COMPARISON AT {sparsity_level*100:.0f}% SPARSITY:\")\n",
    "        sparsity_data = summary_df[summary_df['sparsity_ratio'] == sparsity_level]\n",
    "        if len(sparsity_data) > 0:\n",
    "            for _, row in sparsity_data.iterrows():\n",
    "                degradation = baseline_acc - row['accuracy']\n",
    "                retention = (row['accuracy'] / baseline_acc) * 100\n",
    "                mac_reduction = (1 - row['macs_millions'] / baseline_results['macs_millions']) * 100\n",
    "                size_reduction = (1 - row['size_mb'] / baseline_results['size_mb']) * 100\n",
    "\n",
    "                status = \"ðŸŽ‰\" if row['accuracy'] >= 90 else \"âœ…\" if row['accuracy'] >= 85 else \"âš ï¸\" if row['accuracy'] >= 80 else \"âŒ\"\n",
    "\n",
    "                print(f\"  {row['strategy']:>12}: {row['accuracy']:>6.2f}% (â†“{degradation:>4.1f}%, \"\n",
    "                      f\"Retention: {retention:>5.1f}%, MACsâ†“{mac_reduction:>4.1f}%, Sizeâ†“{size_reduction:>4.1f}%) {status}\")\n",
    "\n",
    "    # Complete results table\n",
    "    print(f\"\\nðŸ“‹ COMPLETE RESULTS TABLE:\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"{'Strategy':<12} {'Sparsity':<8} {'Accuracy':<8} {'Loss':<8} {'MACs(M)':<8} {'Params(M)':<9} {'Size(MB)':<8}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    for _, row in summary_df.sort_values(['strategy', 'sparsity_ratio']).iterrows():\n",
    "        print(f\"{row['strategy']:<12} {row['sparsity_ratio'] * 100:>6.0f}% \"\n",
    "              f\"{row['accuracy']:>7.2f}% {row['loss']:>7.4f} {row['macs_millions']:>7.2f} \"\n",
    "              f\"{row['params_millions']:>8.2f} {row['size_mb']:>7.2f}\")\n",
    "\n",
    "    # Best performers\n",
    "    print(f\"\\nðŸ† BEST PERFORMERS BY SPARSITY LEVEL:\")\n",
    "    for sparsity_level in [0.2, 0.5, 0.7]:\n",
    "        sparsity_data = summary_df[summary_df['sparsity_ratio'] == sparsity_level]\n",
    "        if len(sparsity_data) > 0:\n",
    "            best = sparsity_data.loc[sparsity_data['accuracy'].idxmax()]\n",
    "            print(f\"  {sparsity_level*100:.0f}% Sparsity: {best['strategy']} with {best['accuracy']:.2f}% accuracy\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main experimental workflow with 1000 epochs and early stopping\"\"\"\n",
    "    print(\"ðŸš€ ANTI-OVERFITTING MobileNetV2 CIFAR-10 Experiments for 92%+ Test Accuracy\")\n",
    "    print(\"=\" * 90)\n",
    "\n",
    "    print(\"\\nðŸ”§ ANTI-OVERFITTING STRATEGIES APPLIED:\")\n",
    "    print(\"   â€¢ Reduced learning rates to prevent overfitting\")\n",
    "    print(\"   â€¢ Increased weight decay (2e-4)\")\n",
    "    print(\"   â€¢ Higher dropout rates (0.5, 0.4)\")\n",
    "    print(\"   â€¢ Simpler classifier (256 vs 512 neurons)\")\n",
    "    print(\"   â€¢ ReduceLROnPlateau scheduler\")\n",
    "    print(\"   â€¢ Stronger label smoothing (0.15)\")\n",
    "    print(\"   â€¢ Earlier patience (15 vs 20)\")\n",
    "\n",
    "    # Optimized configuration to prevent overfitting\n",
    "    config = {\n",
    "        'strategies': {\n",
    "            'BNScale': {\n",
    "                'pruner': tp.pruner.BNScalePruner,\n",
    "                'importance': tp.importance.BNScaleImportance()\n",
    "            },\n",
    "            'MagnitudeL2': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.MagnitudeImportance(p=2)\n",
    "            },\n",
    "            'Random': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.RandomImportance()\n",
    "            },\n",
    "        },\n",
    "        'pruning_ratios': [0.0, 0.2, 0.5, 0.7],\n",
    "        'num_classes': 10,\n",
    "        'batch_size': 128,\n",
    "        'learning_rate': 0.001,          # Reduced to prevent overfitting\n",
    "        'weight_decay': 2e-4,            # Increased regularization\n",
    "        'epochs': 1000,                  # Maximum epochs as requested\n",
    "        'patience': 15,                  # Reduced patience to stop overfitting earlier\n",
    "        'ft_epochs': 30,                 # Reduced fine-tuning epochs\n",
    "        'ft_patience': 8,                # Reduced fine-tuning patience\n",
    "        'output_dir': './results_optimized_mobilenetv2_1000ep',\n",
    "        'models_dir': './base',\n",
    "        'pretrained_path': './base/mobilenet_v2-b0353104.pth'\n",
    "    }\n",
    "\n",
    "    # Create output directories\n",
    "    os.makedirs(config['output_dir'], exist_ok=True)\n",
    "    os.makedirs(config['models_dir'], exist_ok=True)\n",
    "\n",
    "    # Load optimized data\n",
    "    print(\"ðŸ“Š Loading CIFAR-10 with optimized preprocessing...\")\n",
    "    train_loader, val_loader, test_loader = get_optimized_data_loaders(\n",
    "        batch_size=config['batch_size']\n",
    "    )\n",
    "\n",
    "    # Prepare inputs and criterion\n",
    "    example_input_cpu = torch.randn(1, 3, 32, 32)\n",
    "    example_input_device = example_input_cpu.to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Get anti-overfitting baseline model\n",
    "    print(\"\\nðŸ—ï¸ Creating anti-overfitting MobileNetV2 model...\")\n",
    "    model = get_optimized_mobilenetv2_model(\n",
    "        num_classes=config['num_classes'],\n",
    "        pretrained_path=config['pretrained_path']\n",
    "    )\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Train anti-overfitting baseline model\n",
    "    print(\"\\nðŸš€ Training anti-overfitting baseline model (1000 epochs, early stopping=15)...\")\n",
    "    trained_model, training_history = train_optimized_model(\n",
    "        model, train_loader, val_loader, DEVICE, config\n",
    "    )\n",
    "\n",
    "    # Save baseline model\n",
    "    baseline_model_path = os.path.join(config['models_dir'], 'anti_overfitting_baseline_1000ep.pth')\n",
    "    save_model(trained_model, baseline_model_path, example_input_cpu)\n",
    "\n",
    "    # Evaluate baseline\n",
    "    print(\"\\nðŸ“Š Evaluating anti-overfitting baseline model...\")\n",
    "    baseline_metrics = evaluate_model(trained_model, test_loader, example_input_device, criterion, DEVICE)\n",
    "    baseline_acc = baseline_metrics['accuracy']\n",
    "\n",
    "    print(f\"\\nðŸŽ¯ ANTI-OVERFITTING BASELINE RESULTS:\")\n",
    "    print(f\"   Test Accuracy: {baseline_acc:.2f}% {'ðŸŽ‰ EXCELLENT!' if baseline_acc >= 92 else 'âœ… Good' if baseline_acc >= 88 else 'âš ï¸ Needs improvement' if baseline_acc >= 85 else 'âŒ Below target'}\")\n",
    "    print(f\"   MACs: {baseline_metrics['macs'] / 1e6:.2f}M\")\n",
    "    print(f\"   Parameters: {baseline_metrics['params'] / 1e6:.2f}M\")\n",
    "    print(f\"   Model Size: {baseline_metrics['size_mb']:.2f}MB\")\n",
    "\n",
    "    # Check training vs test gap\n",
    "    if len(training_history['train_acc']) > 0:\n",
    "        final_train_acc = training_history['train_acc'][-1]\n",
    "        gap = final_train_acc - baseline_acc\n",
    "        print(f\"   Train-Test Gap: {gap:.2f}% {'âœ… Good' if gap < 5 else 'âš ï¸ Moderate' if gap < 10 else 'âŒ High overfitting'}\")\n",
    "\n",
    "        if gap > 10:\n",
    "            print(f\"   ðŸ”§ Overfitting detected! Consider:\")\n",
    "            print(f\"      â€¢ Further increase dropout\")\n",
    "            print(f\"      â€¢ Reduce model complexity\")\n",
    "            print(f\"      â€¢ More data augmentation\")\n",
    "        elif gap < 3:\n",
    "            print(f\"   ðŸŽ¯ Excellent generalization!\")\n",
    "        else:\n",
    "            print(f\"   âœ… Good balance between training and test performance\")\n",
    "\n",
    "    # Initialize results storage for ALL strategies\n",
    "    all_results = {}\n",
    "    for strategy_name in config['strategies'].keys():\n",
    "        all_results[strategy_name] = {0.0: baseline_metrics}\n",
    "\n",
    "    # Get ignored layers\n",
    "    ignored_layers = get_ignored_layers(trained_model)\n",
    "\n",
    "    # Run pruning experiments for ALL strategies\n",
    "    print(\"\\nðŸ”¬ Starting pruning experiments for ALL strategies...\")\n",
    "    for strategy_name, strategy_config in config['strategies'].items():\n",
    "        print(f\"\\n--- Strategy: {strategy_name} ---\")\n",
    "\n",
    "        for sparsity_ratio in config['pruning_ratios']:\n",
    "            if sparsity_ratio == 0.0:\n",
    "                continue  # Skip baseline (already done)\n",
    "\n",
    "            print(f\"\\nâš™ï¸ Processing {strategy_name} at {sparsity_ratio:.1%} sparsity...\")\n",
    "\n",
    "            # Load fresh copy of trained baseline\n",
    "            model_copy = get_optimized_mobilenetv2_model(\n",
    "                num_classes=config['num_classes'],\n",
    "                pretrained_path=None  # Don't reload pretrained weights\n",
    "            )\n",
    "            model_copy.load_state_dict(torch.load(baseline_model_path, map_location=DEVICE))\n",
    "            model_copy.to(DEVICE)\n",
    "\n",
    "            # Apply pruning\n",
    "            pruned_model = prune_model(\n",
    "                model_copy, strategy_config, sparsity_ratio,\n",
    "                example_input_device, ignored_layers\n",
    "            )\n",
    "\n",
    "            # Fine-tune pruned model\n",
    "            print(\"ðŸ”§ Fine-tuning pruned model...\")\n",
    "            fine_tuned_model = fine_tune_pruned_model(\n",
    "                pruned_model, train_loader, val_loader, DEVICE, config\n",
    "            )\n",
    "\n",
    "            # Evaluate fine-tuned model\n",
    "            final_metrics = evaluate_model(fine_tuned_model, test_loader, example_input_device, criterion, DEVICE)\n",
    "            all_results[strategy_name][sparsity_ratio] = final_metrics\n",
    "\n",
    "            final_acc = final_metrics['accuracy']\n",
    "            retention = (final_acc / baseline_acc) * 100\n",
    "            mac_reduction = (1 - final_metrics['macs'] / baseline_metrics['macs']) * 100\n",
    "\n",
    "            print(f\"ðŸ“ˆ Results: Accuracy={final_acc:.2f}% (Retention: {retention:.1f}%), \"\n",
    "                  f\"MACs={final_metrics['macs'] / 1e6:.2f}M (Reduction: {mac_reduction:.1f}%)\")\n",
    "\n",
    "            # Save fine-tuned model\n",
    "            model_filename = f\"optimized_{strategy_name.lower()}_sparsity_{sparsity_ratio:.1f}.pth\"\n",
    "            model_path = os.path.join(config['models_dir'], model_filename)\n",
    "            save_model(fine_tuned_model, model_path, example_input_cpu)\n",
    "\n",
    "    # Save and analyze results\n",
    "    print(\"\\nðŸ’¾ Saving comprehensive results...\")\n",
    "    summary_df = save_results_to_files(all_results, config['output_dir'])\n",
    "\n",
    "    # Create plots\n",
    "    print(\"ðŸ“Š Creating result plots...\")\n",
    "    create_results_plots(summary_df, config['output_dir'])\n",
    "\n",
    "    # Print comprehensive summary with ALL strategies\n",
    "    print_results_table(summary_df)\n",
    "\n",
    "    # Final summary\n",
    "    print(f\"\\nðŸ ANTI-OVERFITTING EXPERIMENTS COMPLETED!\")\n",
    "    print(f\"ðŸ“ Results saved to: {os.path.abspath(config['output_dir'])}\")\n",
    "    print(f\"ðŸ“ Models saved to: {os.path.abspath(config['models_dir'])}\")\n",
    "\n",
    "    # Performance analysis with overfitting awareness\n",
    "    if baseline_acc >= 92:\n",
    "        print(f\"\\nðŸŽ‰ SUCCESS: Test accuracy {baseline_acc:.2f}% achieved 92%+ target!\")\n",
    "        print(f\"ðŸ”¥ Anti-overfitting strategies worked:\")\n",
    "        print(f\"   â€¢ Reduced learning rates prevented overfitting\")\n",
    "        print(f\"   â€¢ Higher regularization improved generalization\")\n",
    "        print(f\"   â€¢ ReduceLROnPlateau scheduler adapted to validation\")\n",
    "        print(f\"   â€¢ Stronger label smoothing and dropout\")\n",
    "    elif baseline_acc >= 88:\n",
    "        print(f\"\\nâœ… Very good baseline: {baseline_acc:.2f}% - Close to target!\")\n",
    "        print(f\"ðŸ”§ Suggestions for reaching 92%+:\")\n",
    "        print(f\"   â€¢ Try even lower learning rates\")\n",
    "        print(f\"   â€¢ Increase data augmentation strength\")\n",
    "        print(f\"   â€¢ Consider mixup or cutmix augmentation\")\n",
    "    elif baseline_acc >= 85:\n",
    "        print(f\"\\nâš ï¸ Good baseline: {baseline_acc:.2f}% - Needs improvement\")\n",
    "        print(f\"ðŸ”§ Debugging suggestions:\")\n",
    "        print(f\"   â€¢ Check if train-test gap is reasonable\")\n",
    "        print(f\"   â€¢ Try different augmentation strategies\")\n",
    "        print(f\"   â€¢ Consider longer training with lower LR\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ Baseline: {baseline_acc:.2f}% - Below target\")\n",
    "        print(f\"ðŸ”§ Major debugging needed:\")\n",
    "        print(f\"   â€¢ Check pretrained weights loading\")\n",
    "        print(f\"   â€¢ Verify dataset integrity\")\n",
    "        print(f\"   â€¢ Try different model architecture\")\n",
    "        print(f\"   â€¢ Consider training from scratch\")\n",
    "\n",
    "    # Show best performing strategy per sparsity level\n",
    "    print(f\"\\nðŸ† BEST STRATEGIES SUMMARY:\")\n",
    "    for sparsity in [0.2, 0.5, 0.7]:\n",
    "        sparsity_data = summary_df[summary_df['sparsity_ratio'] == sparsity]\n",
    "        if len(sparsity_data) > 0:\n",
    "            best = sparsity_data.loc[sparsity_data['accuracy'].idxmax()]\n",
    "            retention = (best['accuracy'] / baseline_acc) * 100\n",
    "            print(f\"   {sparsity*100:.0f}% Sparsity: {best['strategy']} â†’ {best['accuracy']:.2f}% ({retention:.1f}% retention)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "46ddcdf118df051f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "ðŸš€ ANTI-OVERFITTING MobileNetV2 CIFAR-10 Experiments for 92%+ Test Accuracy\n",
      "==========================================================================================\n",
      "\n",
      "ðŸ”§ ANTI-OVERFITTING STRATEGIES APPLIED:\n",
      "   â€¢ Reduced learning rates to prevent overfitting\n",
      "   â€¢ Increased weight decay (2e-4)\n",
      "   â€¢ Higher dropout rates (0.5, 0.4)\n",
      "   â€¢ Simpler classifier (256 vs 512 neurons)\n",
      "   â€¢ ReduceLROnPlateau scheduler\n",
      "   â€¢ Stronger label smoothing (0.15)\n",
      "   â€¢ Earlier patience (15 vs 20)\n",
      "ðŸ“Š Loading CIFAR-10 with optimized preprocessing...\n",
      "Loading CIFAR-10 from: /home/muis/thesis/github-repo/master-thesis/cnn/mobile_net_v2/data\n",
      "Optimized DataLoaders - Train: 45000, Val: 5000, Test: 10000\n",
      "\n",
      "ðŸ—ï¸ Creating anti-overfitting MobileNetV2 model...\n",
      "Loading optimized pretrained weights from: ./base/mobilenet_v2-b0353104.pth\n",
      "Loading 312/314 pretrained weights\n",
      "âœ… Optimized ImageNet pretrained weights loaded successfully\n",
      "âœ… Anti-overfitting MobileNetV2 model created for 10 classes\n",
      "\n",
      "ðŸš€ Training anti-overfitting baseline model (1000 epochs, early stopping=15)...\n",
      "ðŸš€ Starting optimized training for 92%+ accuracy...\n",
      "   Epochs: 1000, Early stopping patience: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/1000: Train Acc: 21.80%, Val Acc: 48.72%, LR: 0.000050 âœ…\n",
      "Epoch   2/1000: Train Acc: 44.41%, Val Acc: 61.38%, LR: 0.000050 âœ…\n",
      "Epoch   3/1000: Train Acc: 57.31%, Val Acc: 67.38%, LR: 0.000050 âœ…\n",
      "Epoch   4/1000: Train Acc: 62.59%, Val Acc: 69.44%, LR: 0.000050 âœ…\n",
      "Epoch   5/1000: Train Acc: 66.10%, Val Acc: 72.06%, LR: 0.000050 âœ…\n",
      "Epoch   6/1000: Train Acc: 68.10%, Val Acc: 74.04%, LR: 0.000050 âœ…\n",
      "Epoch   7/1000: Train Acc: 69.84%, Val Acc: 75.34%, LR: 0.000050 âœ…\n",
      "Epoch   8/1000: Train Acc: 71.47%, Val Acc: 76.56%, LR: 0.000050 âœ…\n",
      "Epoch   9/1000: Train Acc: 72.46%, Val Acc: 77.50%, LR: 0.000050 âœ…\n",
      "Epoch  10/1000: Train Acc: 73.65%, Val Acc: 78.02%, LR: 0.000050 âœ…\n",
      "Epoch  11/1000: Train Acc: 74.53%, Val Acc: 78.46%, LR: 0.000050 âœ…\n",
      "Epoch  12/1000: Train Acc: 75.57%, Val Acc: 78.98%, LR: 0.000050 âœ…\n",
      "Epoch  13/1000: Train Acc: 75.91%, Val Acc: 79.80%, LR: 0.000050 âœ…\n",
      "Epoch  14/1000: Train Acc: 76.31%, Val Acc: 80.04%, LR: 0.000050 âœ…\n",
      "Epoch  15/1000: Train Acc: 76.97%, Val Acc: 80.90%, LR: 0.000050 âœ…\n",
      "Epoch  16/1000: Train Acc: 77.57%, Val Acc: 80.98%, LR: 0.000050 âœ…\n",
      "Epoch  17/1000: Train Acc: 78.10%, Val Acc: 81.50%, LR: 0.000050 âœ…\n",
      "Epoch  18/1000: Train Acc: 78.44%, Val Acc: 81.54%, LR: 0.000050 âœ…\n",
      "Epoch  19/1000: Train Acc: 78.60%, Val Acc: 81.46%, LR: 0.000050\n",
      "Epoch  20/1000: Train Acc: 79.18%, Val Acc: 82.04%, LR: 0.000050 âœ…\n",
      "Epoch  30/1000: Train Acc: 82.29%, Val Acc: 83.26%, LR: 0.000050\n",
      "Epoch  40/1000: Train Acc: 85.11%, Val Acc: 84.52%, LR: 0.000025 âœ…\n",
      "Epoch  50/1000: Train Acc: 86.28%, Val Acc: 84.36%, LR: 0.000013\n",
      "Epoch  60/1000: Train Acc: 87.22%, Val Acc: 84.92%, LR: 0.000013\n",
      "Epoch  70/1000: Train Acc: 87.39%, Val Acc: 84.18%, LR: 0.000003\n",
      "Early stopping triggered at epoch 71 (Best Val Acc: 85.08%)\n",
      "âœ… Loaded best model state (Best Val Acc: 85.08%)\n",
      "âœ… Model saved to ./base/anti_overfitting_baseline_1000ep.pth\n",
      "âœ… ONNX model saved to ./base/anti_overfitting_baseline_1000ep.onnx\n",
      "\n",
      "ðŸ“Š Evaluating anti-overfitting baseline model...\n",
      "\n",
      "ðŸŽ¯ ANTI-OVERFITTING BASELINE RESULTS:\n",
      "   Test Accuracy: 84.40% âŒ Below target\n",
      "   MACs: 6.84M\n",
      "   Parameters: 2.55M\n",
      "   Model Size: 9.75MB\n",
      "   Train-Test Gap: 3.07% âœ… Good\n",
      "   âœ… Good balance between training and test performance\n",
      "\n",
      "ðŸ”¬ Starting pruning experiments for ALL strategies...\n",
      "\n",
      "--- Strategy: BNScale ---\n",
      "\n",
      "âš™ï¸ Processing BNScale at 20.0% sparsity...\n",
      "âœ… Creating model without pretrained weights (as requested)\n",
      "âœ… Anti-overfitting MobileNetV2 model created for 10 classes\n",
      "Initial MACs: 6.84M\n",
      "Applying BNScaleImportance pruning at 20.0% sparsity...\n",
      "Final MACs: 6.30M (Reduction: 7.9%)\n",
      "ðŸ”§ Fine-tuning pruned model...\n",
      "ðŸ”§ Fine-tuning for 30 epochs with patience 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch_pruning/pruner/algorithms/base_pruner.py:87: UserWarning: ch_sparsity is deprecated in v1.3.0. Please use pruning_ratio.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FT Epoch  5/30: Train Acc: 81.94%, Val Acc: 81.72%\n",
      "FT Epoch 10/30: Train Acc: 83.36%, Val Acc: 83.12%\n",
      "FT Epoch 15/30: Train Acc: 84.80%, Val Acc: 83.32% âœ…\n",
      "FT Epoch 20/30: Train Acc: 85.61%, Val Acc: 83.46%\n",
      "FT Epoch 25/30: Train Acc: 86.08%, Val Acc: 83.66%\n",
      "FT Epoch 30/30: Train Acc: 86.01%, Val Acc: 83.62%\n",
      "Fine-tuning early stopping at epoch 30\n",
      "âœ… Fine-tuning complete (Best Val Acc: 83.72%)\n",
      "ðŸ“ˆ Results: Accuracy=83.96% (Retention: 99.5%), MACs=6.30M (Reduction: 7.9%)\n",
      "âœ… Model saved to ./base/optimized_bnscale_sparsity_0.2.pth\n",
      "âœ… ONNX model saved to ./base/optimized_bnscale_sparsity_0.2.onnx\n",
      "\n",
      "âš™ï¸ Processing BNScale at 50.0% sparsity...\n",
      "âœ… Creating model without pretrained weights (as requested)\n",
      "âœ… Anti-overfitting MobileNetV2 model created for 10 classes\n",
      "Initial MACs: 6.84M\n",
      "Applying BNScaleImportance pruning at 50.0% sparsity...\n",
      "Final MACs: 5.59M (Reduction: 18.3%)\n",
      "ðŸ”§ Fine-tuning pruned model...\n",
      "ðŸ”§ Fine-tuning for 30 epochs with patience 8\n",
      "FT Epoch  5/30: Train Acc: 75.73%, Val Acc: 79.58% âœ…\n",
      "FT Epoch 10/30: Train Acc: 78.97%, Val Acc: 81.04%\n",
      "FT Epoch 15/30: Train Acc: 80.48%, Val Acc: 81.38%\n",
      "FT Epoch 20/30: Train Acc: 81.38%, Val Acc: 82.04%\n",
      "FT Epoch 25/30: Train Acc: 81.81%, Val Acc: 82.42%\n",
      "FT Epoch 30/30: Train Acc: 81.86%, Val Acc: 82.42%\n",
      "Fine-tuning early stopping at epoch 30\n",
      "âœ… Fine-tuning complete (Best Val Acc: 82.46%)\n",
      "ðŸ“ˆ Results: Accuracy=82.08% (Retention: 97.3%), MACs=5.59M (Reduction: 18.3%)\n",
      "âœ… Model saved to ./base/optimized_bnscale_sparsity_0.5.pth\n",
      "âœ… ONNX model saved to ./base/optimized_bnscale_sparsity_0.5.onnx\n",
      "\n",
      "âš™ï¸ Processing BNScale at 70.0% sparsity...\n",
      "âœ… Creating model without pretrained weights (as requested)\n",
      "âœ… Anti-overfitting MobileNetV2 model created for 10 classes\n",
      "Initial MACs: 6.84M\n",
      "Applying BNScaleImportance pruning at 70.0% sparsity...\n",
      "Final MACs: 5.15M (Reduction: 24.7%)\n",
      "ðŸ”§ Fine-tuning pruned model...\n",
      "ðŸ”§ Fine-tuning for 30 epochs with patience 8\n",
      "FT Epoch  5/30: Train Acc: 72.52%, Val Acc: 77.54% âœ…\n",
      "FT Epoch 10/30: Train Acc: 76.09%, Val Acc: 79.80% âœ…\n",
      "FT Epoch 15/30: Train Acc: 78.34%, Val Acc: 80.76%\n",
      "FT Epoch 20/30: Train Acc: 79.05%, Val Acc: 81.12% âœ…\n",
      "FT Epoch 25/30: Train Acc: 79.51%, Val Acc: 81.56% âœ…\n",
      "FT Epoch 30/30: Train Acc: 79.73%, Val Acc: 81.42%\n",
      "âœ… Fine-tuning complete (Best Val Acc: 81.72%)\n",
      "ðŸ“ˆ Results: Accuracy=81.13% (Retention: 96.1%), MACs=5.15M (Reduction: 24.7%)\n",
      "âœ… Model saved to ./base/optimized_bnscale_sparsity_0.7.pth\n",
      "âœ… ONNX model saved to ./base/optimized_bnscale_sparsity_0.7.onnx\n",
      "\n",
      "--- Strategy: MagnitudeL2 ---\n",
      "\n",
      "âš™ï¸ Processing MagnitudeL2 at 20.0% sparsity...\n",
      "âœ… Creating model without pretrained weights (as requested)\n",
      "âœ… Anti-overfitting MobileNetV2 model created for 10 classes\n",
      "Initial MACs: 6.84M\n",
      "Applying MagnitudeImportance pruning at 20.0% sparsity...\n",
      "Final MACs: 6.30M (Reduction: 7.9%)\n",
      "ðŸ”§ Fine-tuning pruned model...\n",
      "ðŸ”§ Fine-tuning for 30 epochs with patience 8\n",
      "FT Epoch  5/30: Train Acc: 83.21%, Val Acc: 83.48% âœ…\n",
      "FT Epoch 10/30: Train Acc: 84.54%, Val Acc: 83.70% âœ…\n",
      "FT Epoch 15/30: Train Acc: 85.47%, Val Acc: 84.06% âœ…\n",
      "FT Epoch 20/30: Train Acc: 86.23%, Val Acc: 84.04%\n",
      "FT Epoch 25/30: Train Acc: 86.83%, Val Acc: 84.60% âœ…\n",
      "FT Epoch 30/30: Train Acc: 86.91%, Val Acc: 84.50%\n",
      "âœ… Fine-tuning complete (Best Val Acc: 84.68%)\n",
      "ðŸ“ˆ Results: Accuracy=83.94% (Retention: 99.5%), MACs=6.30M (Reduction: 7.9%)\n",
      "âœ… Model saved to ./base/optimized_magnitudel2_sparsity_0.2.pth\n",
      "âœ… ONNX model saved to ./base/optimized_magnitudel2_sparsity_0.2.onnx\n",
      "\n",
      "âš™ï¸ Processing MagnitudeL2 at 50.0% sparsity...\n",
      "âœ… Creating model without pretrained weights (as requested)\n",
      "âœ… Anti-overfitting MobileNetV2 model created for 10 classes\n",
      "Initial MACs: 6.84M\n",
      "Applying MagnitudeImportance pruning at 50.0% sparsity...\n",
      "Final MACs: 5.59M (Reduction: 18.3%)\n",
      "ðŸ”§ Fine-tuning pruned model...\n",
      "ðŸ”§ Fine-tuning for 30 epochs with patience 8\n",
      "FT Epoch  5/30: Train Acc: 79.17%, Val Acc: 81.00% âœ…\n",
      "FT Epoch 10/30: Train Acc: 81.05%, Val Acc: 82.44% âœ…\n",
      "FT Epoch 15/30: Train Acc: 82.41%, Val Acc: 82.52%\n",
      "FT Epoch 20/30: Train Acc: 83.51%, Val Acc: 83.22%\n",
      "FT Epoch 25/30: Train Acc: 83.86%, Val Acc: 83.64% âœ…\n",
      "FT Epoch 30/30: Train Acc: 83.91%, Val Acc: 83.72% âœ…\n",
      "âœ… Fine-tuning complete (Best Val Acc: 83.72%)\n",
      "ðŸ“ˆ Results: Accuracy=82.78% (Retention: 98.1%), MACs=5.59M (Reduction: 18.3%)\n",
      "âœ… Model saved to ./base/optimized_magnitudel2_sparsity_0.5.pth\n",
      "âœ… ONNX model saved to ./base/optimized_magnitudel2_sparsity_0.5.onnx\n",
      "\n",
      "âš™ï¸ Processing MagnitudeL2 at 70.0% sparsity...\n",
      "âœ… Creating model without pretrained weights (as requested)\n",
      "âœ… Anti-overfitting MobileNetV2 model created for 10 classes\n",
      "Initial MACs: 6.84M\n",
      "Applying MagnitudeImportance pruning at 70.0% sparsity...\n",
      "Final MACs: 5.15M (Reduction: 24.7%)\n",
      "ðŸ”§ Fine-tuning pruned model...\n",
      "ðŸ”§ Fine-tuning for 30 epochs with patience 8\n",
      "FT Epoch  5/30: Train Acc: 76.17%, Val Acc: 79.62% âœ…\n",
      "FT Epoch 10/30: Train Acc: 79.27%, Val Acc: 81.48% âœ…\n",
      "FT Epoch 15/30: Train Acc: 80.42%, Val Acc: 81.90%\n",
      "FT Epoch 20/30: Train Acc: 81.35%, Val Acc: 82.04%\n",
      "FT Epoch 25/30: Train Acc: 82.19%, Val Acc: 81.90%\n",
      "FT Epoch 30/30: Train Acc: 82.04%, Val Acc: 81.88%\n",
      "âœ… Fine-tuning complete (Best Val Acc: 82.26%)\n",
      "ðŸ“ˆ Results: Accuracy=82.61% (Retention: 97.9%), MACs=5.15M (Reduction: 24.7%)\n",
      "âœ… Model saved to ./base/optimized_magnitudel2_sparsity_0.7.pth\n",
      "âœ… ONNX model saved to ./base/optimized_magnitudel2_sparsity_0.7.onnx\n",
      "\n",
      "--- Strategy: Random ---\n",
      "\n",
      "âš™ï¸ Processing Random at 20.0% sparsity...\n",
      "âœ… Creating model without pretrained weights (as requested)\n",
      "âœ… Anti-overfitting MobileNetV2 model created for 10 classes\n",
      "Initial MACs: 6.84M\n",
      "Applying RandomImportance pruning at 20.0% sparsity...\n",
      "Final MACs: 6.30M (Reduction: 7.9%)\n",
      "ðŸ”§ Fine-tuning pruned model...\n",
      "ðŸ”§ Fine-tuning for 30 epochs with patience 8\n",
      "FT Epoch  5/30: Train Acc: 80.87%, Val Acc: 82.26% âœ…\n",
      "FT Epoch 10/30: Train Acc: 83.15%, Val Acc: 83.36%\n",
      "FT Epoch 15/30: Train Acc: 84.09%, Val Acc: 83.96%\n",
      "FT Epoch 20/30: Train Acc: 85.00%, Val Acc: 84.32% âœ…\n",
      "FT Epoch 25/30: Train Acc: 85.37%, Val Acc: 84.34% âœ…\n",
      "FT Epoch 30/30: Train Acc: 85.55%, Val Acc: 84.26%\n",
      "âœ… Fine-tuning complete (Best Val Acc: 84.44%)\n",
      "ðŸ“ˆ Results: Accuracy=83.94% (Retention: 99.5%), MACs=6.30M (Reduction: 7.9%)\n",
      "âœ… Model saved to ./base/optimized_random_sparsity_0.2.pth\n",
      "âœ… ONNX model saved to ./base/optimized_random_sparsity_0.2.onnx\n",
      "\n",
      "âš™ï¸ Processing Random at 50.0% sparsity...\n",
      "âœ… Creating model without pretrained weights (as requested)\n",
      "âœ… Anti-overfitting MobileNetV2 model created for 10 classes\n",
      "Initial MACs: 6.84M\n",
      "Applying RandomImportance pruning at 50.0% sparsity...\n",
      "Final MACs: 5.59M (Reduction: 18.3%)\n",
      "ðŸ”§ Fine-tuning pruned model...\n",
      "ðŸ”§ Fine-tuning for 30 epochs with patience 8\n",
      "FT Epoch  5/30: Train Acc: 73.91%, Val Acc: 78.38% âœ…\n",
      "FT Epoch 10/30: Train Acc: 77.40%, Val Acc: 80.52% âœ…\n",
      "FT Epoch 15/30: Train Acc: 78.57%, Val Acc: 81.08%\n",
      "FT Epoch 20/30: Train Acc: 80.03%, Val Acc: 81.88%\n",
      "FT Epoch 25/30: Train Acc: 80.75%, Val Acc: 82.12%\n",
      "FT Epoch 30/30: Train Acc: 80.67%, Val Acc: 81.98%\n",
      "âœ… Fine-tuning complete (Best Val Acc: 82.30%)\n",
      "ðŸ“ˆ Results: Accuracy=81.32% (Retention: 96.4%), MACs=5.59M (Reduction: 18.3%)\n",
      "âœ… Model saved to ./base/optimized_random_sparsity_0.5.pth\n",
      "âœ… ONNX model saved to ./base/optimized_random_sparsity_0.5.onnx\n",
      "\n",
      "âš™ï¸ Processing Random at 70.0% sparsity...\n",
      "âœ… Creating model without pretrained weights (as requested)\n",
      "âœ… Anti-overfitting MobileNetV2 model created for 10 classes\n",
      "Initial MACs: 6.84M\n",
      "Applying RandomImportance pruning at 70.0% sparsity...\n",
      "Final MACs: 5.15M (Reduction: 24.7%)\n",
      "ðŸ”§ Fine-tuning pruned model...\n",
      "ðŸ”§ Fine-tuning for 30 epochs with patience 8\n",
      "FT Epoch  5/30: Train Acc: 69.65%, Val Acc: 73.70% âœ…\n",
      "FT Epoch 10/30: Train Acc: 73.54%, Val Acc: 77.16% âœ…\n",
      "FT Epoch 15/30: Train Acc: 75.93%, Val Acc: 78.94% âœ…\n",
      "FT Epoch 20/30: Train Acc: 77.05%, Val Acc: 79.50% âœ…\n",
      "FT Epoch 25/30: Train Acc: 77.57%, Val Acc: 79.96% âœ…\n",
      "FT Epoch 30/30: Train Acc: 77.62%, Val Acc: 79.82%\n",
      "âœ… Fine-tuning complete (Best Val Acc: 80.06%)\n",
      "ðŸ“ˆ Results: Accuracy=79.40% (Retention: 94.1%), MACs=5.15M (Reduction: 24.7%)\n",
      "âœ… Model saved to ./base/optimized_random_sparsity_0.7.pth\n",
      "âœ… ONNX model saved to ./base/optimized_random_sparsity_0.7.onnx\n",
      "\n",
      "ðŸ’¾ Saving comprehensive results...\n",
      "âœ… Complete results saved to ./results_optimized_mobilenetv2_1000ep/optimized_complete_results.json\n",
      "âœ… Summary results saved to ./results_optimized_mobilenetv2_1000ep/optimized_summary_results.csv\n",
      "ðŸ“Š Creating result plots...\n",
      "âœ… Accuracy plot saved to ./results_optimized_mobilenetv2_1000ep/optimized_accuracy_vs_sparsity.png\n",
      "âœ… Efficiency frontier plot saved to ./results_optimized_mobilenetv2_1000ep/optimized_efficiency_frontier.png\n",
      "\n",
      "====================================================================================================\n",
      "OPTIMIZED MobileNetV2 EXPERIMENTAL RESULTS SUMMARY\n",
      "====================================================================================================\n",
      "\n",
      "ðŸŽ¯ BASELINE PERFORMANCE:\n",
      "  Accuracy: 84.40% âŒ Below target\n",
      "  MACs: 6.84M\n",
      "  Parameters: 2.55M\n",
      "  Model Size: 9.75MB\n",
      "\n",
      "ðŸ“Š STRATEGY COMPARISON AT 20% SPARSITY:\n",
      "       BNScale:  83.96% (â†“ 0.4%, Retention:  99.5%, MACsâ†“ 7.9%, Sizeâ†“ 7.5%) âš ï¸\n",
      "   MagnitudeL2:  83.94% (â†“ 0.5%, Retention:  99.5%, MACsâ†“ 7.9%, Sizeâ†“ 7.5%) âš ï¸\n",
      "        Random:  83.94% (â†“ 0.5%, Retention:  99.5%, MACsâ†“ 7.9%, Sizeâ†“ 7.5%) âš ï¸\n",
      "\n",
      "ðŸ“Š STRATEGY COMPARISON AT 50% SPARSITY:\n",
      "       BNScale:  82.08% (â†“ 2.3%, Retention:  97.3%, MACsâ†“18.3%, Sizeâ†“17.7%) âš ï¸\n",
      "   MagnitudeL2:  82.78% (â†“ 1.6%, Retention:  98.1%, MACsâ†“18.3%, Sizeâ†“17.7%) âš ï¸\n",
      "        Random:  81.32% (â†“ 3.1%, Retention:  96.4%, MACsâ†“18.3%, Sizeâ†“17.7%) âš ï¸\n",
      "\n",
      "ðŸ“Š STRATEGY COMPARISON AT 70% SPARSITY:\n",
      "       BNScale:  81.13% (â†“ 3.3%, Retention:  96.1%, MACsâ†“24.7%, Sizeâ†“24.3%) âš ï¸\n",
      "   MagnitudeL2:  82.61% (â†“ 1.8%, Retention:  97.9%, MACsâ†“24.7%, Sizeâ†“24.3%) âš ï¸\n",
      "        Random:  79.40% (â†“ 5.0%, Retention:  94.1%, MACsâ†“24.7%, Sizeâ†“24.3%) âŒ\n",
      "\n",
      "ðŸ“‹ COMPLETE RESULTS TABLE:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Strategy     Sparsity Accuracy Loss     MACs(M)  Params(M) Size(MB)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "BNScale           0%   84.40%  0.5594    6.84     2.55    9.75\n",
      "BNScale          20%   83.96%  0.4989    6.30     2.36    9.01\n",
      "BNScale          50%   82.08%  0.5366    5.59     2.10    8.03\n",
      "BNScale          70%   81.13%  0.5537    5.15     1.93    7.38\n",
      "MagnitudeL2       0%   84.40%  0.5594    6.84     2.55    9.75\n",
      "MagnitudeL2      20%   83.94%  0.4943    6.30     2.36    9.01\n",
      "MagnitudeL2      50%   82.78%  0.5142    5.59     2.10    8.03\n",
      "MagnitudeL2      70%   82.61%  0.5133    5.15     1.93    7.38\n",
      "Random            0%   84.40%  0.5594    6.84     2.55    9.75\n",
      "Random           20%   83.94%  0.4885    6.30     2.36    9.01\n",
      "Random           50%   81.32%  0.5460    5.59     2.10    8.03\n",
      "Random           70%   79.40%  0.5890    5.15     1.93    7.38\n",
      "\n",
      "ðŸ† BEST PERFORMERS BY SPARSITY LEVEL:\n",
      "  20% Sparsity: BNScale with 83.96% accuracy\n",
      "  50% Sparsity: MagnitudeL2 with 82.78% accuracy\n",
      "  70% Sparsity: MagnitudeL2 with 82.61% accuracy\n",
      "\n",
      "ðŸ ANTI-OVERFITTING EXPERIMENTS COMPLETED!\n",
      "ðŸ“ Results saved to: /home/muis/thesis/github-repo/master-thesis/cnn/mobile_net_v2/results_optimized_mobilenetv2_1000ep\n",
      "ðŸ“ Models saved to: /home/muis/thesis/github-repo/master-thesis/cnn/mobile_net_v2/base\n",
      "\n",
      "âŒ Baseline: 84.40% - Below target\n",
      "ðŸ”§ Major debugging needed:\n",
      "   â€¢ Check pretrained weights loading\n",
      "   â€¢ Verify dataset integrity\n",
      "   â€¢ Try different model architecture\n",
      "   â€¢ Consider training from scratch\n",
      "\n",
      "ðŸ† BEST STRATEGIES SUMMARY:\n",
      "   20% Sparsity: BNScale â†’ 83.96% (99.5% retention)\n",
      "   50% Sparsity: MagnitudeL2 â†’ 82.78% (98.1% retention)\n",
      "   70% Sparsity: MagnitudeL2 â†’ 82.61% (97.9% retention)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fd30a41345a7f9d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# op",
   "id": "bc62a9146803bde3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:19:02.204930Z",
     "start_time": "2025-06-04T15:44:32.461051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_pruning as tp\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "import pandas as pd\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_BASE_NAME = \"mobilenet_v2\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "def get_data_loaders(data_dir_path='./data', batch_size=128, val_split=0.1, seed=42):\n",
    "    \"\"\"Load CIFAR-10 dataset with train/val/test splits and data augmentation\"\"\"\n",
    "    abs_data_dir = os.path.abspath(data_dir_path)\n",
    "    print(f\"Loading CIFAR-10 from: {abs_data_dir}\")\n",
    "\n",
    "    # Data augmentation for training\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "\n",
    "    # No augmentation for val/test\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "\n",
    "    # Load datasets (assuming pre-downloaded)\n",
    "    full_train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=abs_data_dir, train=True, download=False, transform=transform_train\n",
    "    )\n",
    "\n",
    "    # Create a version with test transforms for validation\n",
    "    full_train_dataset_val = torchvision.datasets.CIFAR10(\n",
    "        root=abs_data_dir, train=True, download=False, transform=transform_test\n",
    "    )\n",
    "\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=abs_data_dir, train=False, download=False, transform=transform_test\n",
    "    )\n",
    "\n",
    "    # Create train/validation split\n",
    "    val_size = int(len(full_train_dataset) * val_split)\n",
    "    train_size = len(full_train_dataset) - val_size\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "\n",
    "    train_indices, val_indices = torch.utils.data.random_split(\n",
    "        range(len(full_train_dataset)), [train_size, val_size], generator=generator\n",
    "    )\n",
    "\n",
    "    # Create subset datasets\n",
    "    train_dataset = torch.utils.data.Subset(full_train_dataset, train_indices.indices)\n",
    "    val_dataset = torch.utils.data.Subset(full_train_dataset_val, val_indices.indices)\n",
    "\n",
    "    # Create data loaders\n",
    "    num_workers = min(4, os.cpu_count() or 2)\n",
    "    pin_memory = True if DEVICE.type == 'cuda' else False\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                             num_workers=num_workers, pin_memory=pin_memory)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                           num_workers=num_workers, pin_memory=pin_memory)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                            num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    print(f\"DataLoaders created - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def get_mobilenetv2_model(num_classes=10, use_pretrained=True, width_mult=1.0):\n",
    "    \"\"\"Get MobileNetV2 model adapted for CIFAR-10 with proper initialization\"\"\"\n",
    "    if use_pretrained and os.path.exists('./mobilenet_v2-b0353104.pth'):\n",
    "        # Load pre-trained model\n",
    "        model = models.mobilenet_v2(weights=None)\n",
    "        pretrained_state = torch.load('./mobilenet_v2-b0353104.pth', map_location='cpu')\n",
    "\n",
    "        # Load all weights except the classifier\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_state.items()\n",
    "                          if k in model_dict and 'classifier' not in k}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "        print(\"âœ… Loaded MobileNetV2 with ImageNet pretrained features (excluding classifier)\")\n",
    "    else:\n",
    "        model = models.mobilenet_v2(weights=None)\n",
    "        print(\"âœ… Created MobileNetV2 without pretrained weights\")\n",
    "\n",
    "    # Replace classifier with a more suitable one for CIFAR-10\n",
    "    in_features = model.classifier[1].in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(in_features, num_classes)\n",
    "    )\n",
    "\n",
    "    # Initialize the new classifier properly\n",
    "    nn.init.xavier_uniform_(model.classifier[1].weight)\n",
    "    nn.init.zeros_(model.classifier[1].bias)\n",
    "\n",
    "    print(f\"âœ… Adapted classifier for {num_classes} classes\")\n",
    "    return model\n",
    "\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    \"\"\"Apply mixup augmentation\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"Mixup loss\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def get_ignored_layers(model):\n",
    "    \"\"\"Get layers to ignore during pruning (typically final classifier)\"\"\"\n",
    "    ignored_layers = []\n",
    "    if hasattr(model, 'classifier'):\n",
    "        if isinstance(model.classifier, nn.Sequential):\n",
    "            for layer in model.classifier:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    ignored_layers.append(layer)\n",
    "        elif isinstance(model.classifier, nn.Linear):\n",
    "            ignored_layers.append(model.classifier)\n",
    "    return ignored_layers\n",
    "\n",
    "def calculate_macs_params(model, example_input):\n",
    "    \"\"\"Calculate MACs and parameters using torch_pruning\"\"\"\n",
    "    model.eval()\n",
    "    target_device = example_input.device\n",
    "    model_on_device = model.to(target_device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        macs, params = tp.utils.count_ops_and_params(model_on_device, example_input)\n",
    "\n",
    "    return macs, params\n",
    "\n",
    "def save_model(model, save_path, example_input_cpu=None):\n",
    "    \"\"\"Save model state dict and optionally ONNX\"\"\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"âœ… Model saved to {save_path}\")\n",
    "\n",
    "    if example_input_cpu is not None:\n",
    "        onnx_path = save_path.replace('.pth', '.onnx')\n",
    "        try:\n",
    "            model_cpu = model.to('cpu')\n",
    "            torch.onnx.export(\n",
    "                model_cpu, example_input_cpu, onnx_path,\n",
    "                export_params=True, opset_version=13,\n",
    "                input_names=['input'], output_names=['output'],\n",
    "                dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "            )\n",
    "            print(f\"âœ… ONNX model saved to {onnx_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: ONNX export failed: {e}\")\n",
    "\n",
    "def evaluate_model(model, data_loader, example_input, criterion, device):\n",
    "    \"\"\"Evaluate model and return comprehensive metrics\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Calculate efficiency metrics\n",
    "    macs, params = calculate_macs_params(model, example_input.to(device))\n",
    "    model_size_mb = params * 4 / (1024 * 1024)  # Assuming float32\n",
    "\n",
    "    # Calculate accuracy and loss\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / total if total > 0 else float('nan')\n",
    "    accuracy = 100.0 * correct / total if total > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'loss': avg_loss,\n",
    "        'macs': macs,\n",
    "        'params': params,\n",
    "        'size_mb': model_size_mb\n",
    "    }\n",
    "\n",
    "def prune_model(model, strategy_config, sparsity_ratio, example_input, ignored_layers=None):\n",
    "    \"\"\"Apply structured pruning to model\"\"\"\n",
    "    if sparsity_ratio == 0.0:\n",
    "        print(\"No pruning needed (sparsity = 0.0)\")\n",
    "        return model\n",
    "\n",
    "    model.eval()\n",
    "    pruned_model = copy.deepcopy(model)\n",
    "    pruned_model.to(example_input.device)\n",
    "\n",
    "    # Calculate initial MACs\n",
    "    initial_macs, _ = calculate_macs_params(pruned_model, example_input)\n",
    "    print(f\"Initial MACs: {initial_macs / 1e6:.2f}M\")\n",
    "\n",
    "    ignored_layers = ignored_layers or []\n",
    "\n",
    "    # Create pruner based on strategy\n",
    "    pruner = strategy_config['pruner'](\n",
    "        pruned_model,\n",
    "        example_input,\n",
    "        importance=strategy_config['importance'],\n",
    "        iterative_steps=5,\n",
    "        ch_sparsity=sparsity_ratio,\n",
    "        root_module_types=[nn.Conv2d],\n",
    "        ignored_layers=ignored_layers\n",
    "    )\n",
    "\n",
    "    print(f\"Applying {strategy_config['importance'].__class__.__name__} pruning at {sparsity_ratio:.1%} sparsity...\")\n",
    "\n",
    "    # Apply pruning\n",
    "    pruner.step()\n",
    "\n",
    "    # Calculate final MACs\n",
    "    final_macs, _ = calculate_macs_params(pruned_model, example_input)\n",
    "    reduction = (initial_macs - final_macs) / initial_macs * 100 if initial_macs > 0 else 0\n",
    "    print(f\"Final MACs: {final_macs / 1e6:.2f}M (Reduction: {reduction:.1f}%)\")\n",
    "\n",
    "    return pruned_model\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs,\n",
    "                val_loader=None, patience=7, log_prefix=\"\", scheduler=None,\n",
    "                use_mixup=False, mixup_alpha=0.2):\n",
    "    \"\"\"Train model with early stopping and advanced techniques\"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Apply mixup if enabled\n",
    "            if use_mixup and epoch < num_epochs - 5:  # Disable mixup for last 5 epochs\n",
    "                data, target_a, target_b, lam = mixup_data(data, target, mixup_alpha)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = mixup_criterion(criterion, output, target_a, target_b, lam)\n",
    "            else:\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total += target.size(0)\n",
    "            if use_mixup and epoch < num_epochs - 5:\n",
    "                train_correct += (lam * predicted.eq(target_a).sum().item() +\n",
    "                                (1 - lam) * predicted.eq(target_b).sum().item())\n",
    "            else:\n",
    "                train_correct += (predicted == target).sum().item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_acc = 100.0 * train_correct / train_total\n",
    "\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "\n",
    "        log_msg = f\"Epoch {epoch+1}/{num_epochs} ({log_prefix}): Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%\"\n",
    "\n",
    "        # Validation phase\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for data, target in val_loader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = torch.max(output.data, 1)\n",
    "                    val_total += target.size(0)\n",
    "                    val_correct += (predicted == target).sum().item()\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_acc = 100.0 * val_correct / val_total\n",
    "\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            history['val_acc'].append(val_acc)\n",
    "\n",
    "            log_msg += f\", Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%\"\n",
    "\n",
    "            # Learning rate scheduling\n",
    "            if scheduler:\n",
    "                if isinstance(scheduler, ReduceLROnPlateau):\n",
    "                    scheduler.step(avg_val_loss)\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "\n",
    "            # Early stopping based on validation accuracy\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_val_loss = avg_val_loss\n",
    "                epochs_no_improve = 0\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "                log_msg += \" (Best)\"\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"{log_msg}\")\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "        else:\n",
    "            history['val_loss'].append(None)\n",
    "            history['val_acc'].append(None)\n",
    "            if scheduler and not isinstance(scheduler, ReduceLROnPlateau):\n",
    "                scheduler.step()\n",
    "\n",
    "        print(log_msg)\n",
    "\n",
    "    # Load best model state if available\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"Loaded best model state with val accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "def save_results_to_files(all_results, output_dir):\n",
    "    \"\"\"Save experimental results to JSON and CSV files\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save complete results as JSON\n",
    "    results_json_path = os.path.join(output_dir, 'complete_results.json')\n",
    "    with open(results_json_path, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2, default=str)\n",
    "    print(f\"âœ… Complete results saved to {results_json_path}\")\n",
    "\n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for strategy, strategy_results in all_results.items():\n",
    "        for sparsity, metrics in strategy_results.items():\n",
    "            row = {\n",
    "                'strategy': strategy,\n",
    "                'sparsity_ratio': sparsity,\n",
    "                'accuracy': metrics['accuracy'],\n",
    "                'loss': metrics['loss'],\n",
    "                'macs_millions': metrics['macs'] / 1e6,\n",
    "                'params_millions': metrics['params'] / 1e6,\n",
    "                'size_mb': metrics['size_mb']\n",
    "            }\n",
    "            summary_data.append(row)\n",
    "\n",
    "    # Save summary as CSV\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_csv_path = os.path.join(output_dir, 'summary_results.csv')\n",
    "    summary_df.to_csv(summary_csv_path, index=False)\n",
    "    print(f\"âœ… Summary results saved to {summary_csv_path}\")\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "def create_results_plots(summary_df, output_dir):\n",
    "    \"\"\"Create visualization plots\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    strategies = summary_df['strategy'].unique()\n",
    "    sparsity_levels = sorted(summary_df['sparsity_ratio'].unique())\n",
    "\n",
    "    # Plot 1: Accuracy vs Sparsity\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for strategy in strategies:\n",
    "        strategy_data = summary_df[summary_df['strategy'] == strategy].sort_values('sparsity_ratio')\n",
    "        plt.plot(strategy_data['sparsity_ratio'] * 100, strategy_data['accuracy'],\n",
    "                'o-', linewidth=2, markersize=8, label=strategy)\n",
    "\n",
    "    plt.xlabel('Sparsity (%)', fontsize=12)\n",
    "    plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "    plt.title('MobileNetV2: Accuracy vs Sparsity', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(output_dir, 'accuracy_vs_sparsity.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"âœ… Accuracy plot saved to {plot_path}\")\n",
    "\n",
    "    # Plot 2: Efficiency frontier (Accuracy vs MACs)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for strategy in strategies:\n",
    "        strategy_data = summary_df[summary_df['strategy'] == strategy].sort_values('sparsity_ratio')\n",
    "        plt.scatter(strategy_data['macs_millions'], strategy_data['accuracy'],\n",
    "                   s=100, label=strategy, alpha=0.8)\n",
    "        plt.plot(strategy_data['macs_millions'], strategy_data['accuracy'],\n",
    "                '--', alpha=0.6)\n",
    "\n",
    "    plt.xlabel('MACs (Millions)', fontsize=12)\n",
    "    plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "    plt.title('MobileNetV2: Efficiency Frontier (Accuracy vs MACs)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(output_dir, 'efficiency_frontier.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"âœ… Efficiency frontier plot saved to {plot_path}\")\n",
    "\n",
    "def print_results_table(summary_df):\n",
    "    \"\"\"Print formatted results table\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXPERIMENTAL RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Baseline results\n",
    "    baseline_results = summary_df[summary_df['sparsity_ratio'] == 0.0].iloc[0]\n",
    "    print(f\"\\nBaseline Performance:\")\n",
    "    print(f\"  Accuracy: {baseline_results['accuracy']:.2f}%\")\n",
    "    print(f\"  MACs: {baseline_results['macs_millions']:.2f}M\")\n",
    "    print(f\"  Parameters: {baseline_results['params_millions']:.2f}M\")\n",
    "    print(f\"  Model Size: {baseline_results['size_mb']:.2f}MB\")\n",
    "\n",
    "    # Strategy comparison at 50% sparsity\n",
    "    print(f\"\\nStrategy Comparison at 50% Sparsity:\")\n",
    "    sparsity_50_data = summary_df[summary_df['sparsity_ratio'] == 0.5]\n",
    "    for _, row in sparsity_50_data.iterrows():\n",
    "        degradation = baseline_results['accuracy'] - row['accuracy']\n",
    "        retention = (row['accuracy'] / baseline_results['accuracy']) * 100\n",
    "        print(f\"  {row['strategy']:>12}: {row['accuracy']:>6.2f}% accuracy ({degradation:>+5.2f}%, {retention:>5.1f}% retention)\")\n",
    "\n",
    "    # Complete results table\n",
    "    print(f\"\\nComplete Results Table:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Strategy':<12} {'Sparsity':<8} {'Accuracy':<8} {'MACs(M)':<8} {'Params(M)':<9} {'Size(MB)':<8}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for _, row in summary_df.sort_values(['strategy', 'sparsity_ratio']).iterrows():\n",
    "        print(f\"{row['strategy']:<12} {row['sparsity_ratio']*100:>6.0f}% \"\n",
    "              f\"{row['accuracy']:>7.2f}% {row['macs_millions']:>7.2f} \"\n",
    "              f\"{row['params_millions']:>8.2f} {row['size_mb']:>7.2f}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main experimental workflow\"\"\"\n",
    "    print(\"Starting Enhanced MobileNetV2 CIFAR-10 Pruning Experiments\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'strategies': {\n",
    "            'BNScale': {\n",
    "                'pruner': tp.pruner.BNScalePruner,\n",
    "                'importance': tp.importance.BNScaleImportance()\n",
    "            },\n",
    "            'MagnitudeL2': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.MagnitudeImportance(p=2)\n",
    "            },\n",
    "            'Random': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.RandomImportance()\n",
    "            },\n",
    "        },\n",
    "        'pruning_ratios': [0.0, 0.2, 0.5, 0.7],\n",
    "        'num_classes': 10,\n",
    "        'batch_size': 128,\n",
    "        'learning_rate': 0.001,  # Higher initial learning rate\n",
    "        'learning_rate_finetune': 0.0001,  # Lower for fine-tuning\n",
    "        'epochs': 250,  # More epochs for better training\n",
    "        'patience': 15,  # More patience\n",
    "        'weight_decay': 1e-4,  # Add weight decay\n",
    "        'use_mixup': True,  # Enable mixup\n",
    "        'mixup_alpha': 0.2,\n",
    "        'output_dir': './results_mobilenetv2_cifar10_enhanced',\n",
    "        'models_dir': './models_mobilenetv2_enhanced'\n",
    "    }\n",
    "\n",
    "    # Create output directories\n",
    "    os.makedirs(config['output_dir'], exist_ok=True)\n",
    "    os.makedirs(config['models_dir'], exist_ok=True)\n",
    "\n",
    "    # Load data with augmentation\n",
    "    print(\"Loading CIFAR-10 dataset with data augmentation...\")\n",
    "    train_loader, val_loader, test_loader = get_data_loaders(\n",
    "        batch_size=config['batch_size']\n",
    "    )\n",
    "\n",
    "    # Prepare inputs and criterion\n",
    "    example_input_cpu = torch.randn(1, 3, 32, 32)\n",
    "    example_input_device = example_input_cpu.to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Get baseline model and train it\n",
    "    print(\"\\nCreating and training baseline model...\")\n",
    "    model = get_mobilenetv2_model(num_classes=config['num_classes'], use_pretrained=True)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Train baseline model with enhanced settings\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'],\n",
    "                          weight_decay=config['weight_decay'])\n",
    "\n",
    "    # Use cosine annealing scheduler for better convergence\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=config['epochs'], eta_min=1e-6)\n",
    "\n",
    "    trained_model, training_history = train_model(\n",
    "        model, train_loader, criterion, optimizer, DEVICE,\n",
    "        config['epochs'], val_loader, config['patience'], \"Baseline Training\",\n",
    "        scheduler=scheduler, use_mixup=config['use_mixup'],\n",
    "        mixup_alpha=config['mixup_alpha']\n",
    "    )\n",
    "\n",
    "    # Save baseline model\n",
    "    baseline_model_path = os.path.join(config['models_dir'], 'baseline_model.pth')\n",
    "    save_model(trained_model, baseline_model_path, example_input_cpu)\n",
    "\n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(training_history['train_loss'], label='Train')\n",
    "    plt.plot(training_history['val_loss'], label='Validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training History - Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(training_history['train_acc'], label='Train')\n",
    "    plt.plot(training_history['val_acc'], label='Validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training History - Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config['output_dir'], 'training_history.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Evaluate baseline\n",
    "    print(\"\\nEvaluating baseline model...\")\n",
    "    baseline_metrics = evaluate_model(trained_model, test_loader, example_input_device, criterion, DEVICE)\n",
    "    print(f\"Baseline Results: Accuracy={baseline_metrics['accuracy']:.2f}%, \"\n",
    "          f\"MACs={baseline_metrics['macs']/1e6:.2f}M, \"\n",
    "          f\"Params={baseline_metrics['params']/1e6:.2f}M\")\n",
    "\n",
    "    # Initialize results storage\n",
    "    all_results = {}\n",
    "    for strategy_name in config['strategies'].keys():\n",
    "        all_results[strategy_name] = {0.0: baseline_metrics}\n",
    "\n",
    "    # Get ignored layers\n",
    "    ignored_layers = get_ignored_layers(trained_model)\n",
    "\n",
    "    # Run pruning experiments\n",
    "    print(\"\\nStarting pruning experiments...\")\n",
    "    for strategy_name, strategy_config in config['strategies'].items():\n",
    "        print(f\"\\n--- Strategy: {strategy_name} ---\")\n",
    "\n",
    "        for sparsity_ratio in config['pruning_ratios']:\n",
    "            if sparsity_ratio == 0.0:\n",
    "                continue  # Skip baseline (already done)\n",
    "\n",
    "            print(f\"\\nProcessing {strategy_name} at {sparsity_ratio:.1%} sparsity...\")\n",
    "\n",
    "            # Load fresh copy of trained baseline\n",
    "            model_copy = get_mobilenetv2_model(num_classes=config['num_classes'], use_pretrained=False)\n",
    "            model_copy.load_state_dict(torch.load(baseline_model_path, map_location=DEVICE))\n",
    "            model_copy.to(DEVICE)\n",
    "\n",
    "            # Apply pruning\n",
    "            pruned_model = prune_model(\n",
    "                model_copy, strategy_config, sparsity_ratio,\n",
    "                example_input_device, ignored_layers\n",
    "            )\n",
    "\n",
    "            # Fine-tune pruned model with lower learning rate\n",
    "            print(\"Fine-tuning pruned model...\")\n",
    "            optimizer_ft = optim.Adam(pruned_model.parameters(),\n",
    "                                     lr=config['learning_rate_finetune'],\n",
    "                                     weight_decay=config['weight_decay'])\n",
    "\n",
    "            scheduler_ft = CosineAnnealingLR(optimizer_ft, T_max=config['epochs'], eta_min=1e-7)\n",
    "\n",
    "            fine_tuned_model, ft_history = train_model(\n",
    "                pruned_model, train_loader, criterion, optimizer_ft, DEVICE,\n",
    "                config['epochs'], val_loader, config['patience'],\n",
    "                f\"{strategy_name}-{sparsity_ratio:.1%}\",\n",
    "                scheduler=scheduler_ft, use_mixup=False  # No mixup for fine-tuning\n",
    "            )\n",
    "\n",
    "            # Evaluate fine-tuned model\n",
    "            final_metrics = evaluate_model(fine_tuned_model, test_loader, example_input_device, criterion, DEVICE)\n",
    "            all_results[strategy_name][sparsity_ratio] = final_metrics\n",
    "\n",
    "            print(f\"Results: Accuracy={final_metrics['accuracy']:.2f}%, \"\n",
    "                  f\"MACs={final_metrics['macs']/1e6:.2f}M\")\n",
    "\n",
    "            # Save fine-tuned model\n",
    "            model_filename = f\"{strategy_name.lower()}_sparsity_{sparsity_ratio:.1f}.pth\"\n",
    "            model_path = os.path.join(config['models_dir'], model_filename)\n",
    "            save_model(fine_tuned_model, model_path, example_input_cpu)\n",
    "\n",
    "    # Save and analyze results\n",
    "    print(\"\\nSaving results...\")\n",
    "    summary_df = save_results_to_files(all_results, config['output_dir'])\n",
    "\n",
    "    # Create plots\n",
    "    print(\"Creating plots...\")\n",
    "    create_results_plots(summary_df, config['output_dir'])\n",
    "\n",
    "    # Print summary\n",
    "    print_results_table(summary_df)\n",
    "\n",
    "    print(f\"\\nðŸŽ‰ All experiments completed!\")\n",
    "    print(f\"ðŸ“ Results saved to: {os.path.abspath(config['output_dir'])}\")\n",
    "    print(f\"ðŸ“ Models saved to: {os.path.abspath(config['models_dir'])}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "cf4ea7237f2ede7e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting Enhanced MobileNetV2 CIFAR-10 Pruning Experiments\n",
      "============================================================\n",
      "Loading CIFAR-10 dataset with data augmentation...\n",
      "Loading CIFAR-10 from: /home/muis/thesis/github-repo/master-thesis/cnn/mobile_net_v2/data\n",
      "DataLoaders created - Train: 45000, Val: 5000, Test: 10000\n",
      "\n",
      "Creating and training baseline model...\n",
      "âœ… Created MobileNetV2 without pretrained weights\n",
      "âœ… Adapted classifier for 10 classes\n",
      "Epoch 1/250 (Baseline Training): Train Loss: 2.2837, Train Acc: 17.79%, Val Loss: 2.0143, Val Acc: 26.04% (Best)\n",
      "Epoch 2/250 (Baseline Training): Train Loss: 1.9982, Train Acc: 26.11%, Val Loss: 1.7804, Val Acc: 33.08% (Best)\n",
      "Epoch 3/250 (Baseline Training): Train Loss: 1.8710, Train Acc: 32.05%, Val Loss: 1.6899, Val Acc: 37.88% (Best)\n",
      "Epoch 4/250 (Baseline Training): Train Loss: 1.7915, Train Acc: 35.89%, Val Loss: 1.5276, Val Acc: 43.54% (Best)\n",
      "Epoch 5/250 (Baseline Training): Train Loss: 1.7253, Train Acc: 38.98%, Val Loss: 1.5453, Val Acc: 45.70% (Best)\n",
      "Epoch 6/250 (Baseline Training): Train Loss: 1.7106, Train Acc: 39.98%, Val Loss: 2.1710, Val Acc: 34.18%\n",
      "Epoch 7/250 (Baseline Training): Train Loss: 1.6378, Train Acc: 42.88%, Val Loss: 1.3948, Val Acc: 50.94% (Best)\n",
      "Epoch 8/250 (Baseline Training): Train Loss: 1.6358, Train Acc: 43.48%, Val Loss: 1.3698, Val Acc: 51.68% (Best)\n",
      "Epoch 9/250 (Baseline Training): Train Loss: 1.5589, Train Acc: 46.59%, Val Loss: 1.4288, Val Acc: 51.18%\n",
      "Epoch 10/250 (Baseline Training): Train Loss: 1.6017, Train Acc: 44.80%, Val Loss: 1.3280, Val Acc: 52.82% (Best)\n",
      "Epoch 11/250 (Baseline Training): Train Loss: 1.5746, Train Acc: 46.16%, Val Loss: 1.3128, Val Acc: 54.74% (Best)\n",
      "Epoch 12/250 (Baseline Training): Train Loss: 1.5063, Train Acc: 49.42%, Val Loss: 1.2020, Val Acc: 58.12% (Best)\n",
      "Epoch 13/250 (Baseline Training): Train Loss: 1.4647, Train Acc: 50.85%, Val Loss: 1.1574, Val Acc: 59.50% (Best)\n",
      "Epoch 14/250 (Baseline Training): Train Loss: 1.4162, Train Acc: 52.97%, Val Loss: 1.1496, Val Acc: 59.76% (Best)\n",
      "Epoch 15/250 (Baseline Training): Train Loss: 1.3737, Train Acc: 54.51%, Val Loss: 1.0886, Val Acc: 62.38% (Best)\n",
      "Epoch 16/250 (Baseline Training): Train Loss: 1.3707, Train Acc: 54.86%, Val Loss: 1.0948, Val Acc: 62.12%\n",
      "Epoch 17/250 (Baseline Training): Train Loss: 1.3193, Train Acc: 56.49%, Val Loss: 1.0207, Val Acc: 65.80% (Best)\n",
      "Epoch 18/250 (Baseline Training): Train Loss: 1.3261, Train Acc: 56.75%, Val Loss: 0.9867, Val Acc: 65.24%\n",
      "Epoch 19/250 (Baseline Training): Train Loss: 1.2680, Train Acc: 58.58%, Val Loss: 1.0154, Val Acc: 66.60% (Best)\n",
      "Epoch 20/250 (Baseline Training): Train Loss: 1.3007, Train Acc: 58.03%, Val Loss: 0.9813, Val Acc: 67.28% (Best)\n",
      "Epoch 21/250 (Baseline Training): Train Loss: 1.1882, Train Acc: 61.99%, Val Loss: 0.9026, Val Acc: 69.28% (Best)\n",
      "Epoch 22/250 (Baseline Training): Train Loss: 1.2026, Train Acc: 61.16%, Val Loss: 0.9308, Val Acc: 66.96%\n",
      "Epoch 23/250 (Baseline Training): Train Loss: 1.2074, Train Acc: 61.55%, Val Loss: 0.8865, Val Acc: 71.06% (Best)\n",
      "Epoch 24/250 (Baseline Training): Train Loss: 1.1682, Train Acc: 63.09%, Val Loss: 0.8549, Val Acc: 71.78% (Best)\n",
      "Epoch 25/250 (Baseline Training): Train Loss: 1.1643, Train Acc: 63.37%, Val Loss: 0.8768, Val Acc: 71.80% (Best)\n",
      "Epoch 26/250 (Baseline Training): Train Loss: 1.1375, Train Acc: 64.29%, Val Loss: 0.8279, Val Acc: 71.26%\n",
      "Epoch 27/250 (Baseline Training): Train Loss: 1.0930, Train Acc: 65.75%, Val Loss: 0.8086, Val Acc: 72.26% (Best)\n",
      "Epoch 28/250 (Baseline Training): Train Loss: 1.0721, Train Acc: 66.64%, Val Loss: 0.7933, Val Acc: 74.94% (Best)\n",
      "Epoch 29/250 (Baseline Training): Train Loss: 1.1364, Train Acc: 64.95%, Val Loss: 0.8130, Val Acc: 73.96%\n",
      "Epoch 30/250 (Baseline Training): Train Loss: 1.0504, Train Acc: 67.45%, Val Loss: 0.7562, Val Acc: 73.52%\n",
      "Epoch 31/250 (Baseline Training): Train Loss: 1.0295, Train Acc: 68.30%, Val Loss: 0.7657, Val Acc: 74.10%\n",
      "Epoch 32/250 (Baseline Training): Train Loss: 1.0595, Train Acc: 67.47%, Val Loss: 0.7109, Val Acc: 76.84% (Best)\n",
      "Epoch 33/250 (Baseline Training): Train Loss: 1.0307, Train Acc: 68.49%, Val Loss: 0.7129, Val Acc: 77.16% (Best)\n",
      "Epoch 34/250 (Baseline Training): Train Loss: 1.0025, Train Acc: 69.40%, Val Loss: 0.7774, Val Acc: 75.74%\n",
      "Epoch 35/250 (Baseline Training): Train Loss: 0.9943, Train Acc: 69.82%, Val Loss: 0.6684, Val Acc: 78.72% (Best)\n",
      "Epoch 36/250 (Baseline Training): Train Loss: 0.9827, Train Acc: 70.05%, Val Loss: 0.7173, Val Acc: 77.06%\n",
      "Epoch 37/250 (Baseline Training): Train Loss: 1.0210, Train Acc: 69.06%, Val Loss: 0.7101, Val Acc: 76.14%\n",
      "Epoch 38/250 (Baseline Training): Train Loss: 0.9810, Train Acc: 70.19%, Val Loss: 0.6983, Val Acc: 78.06%\n",
      "Epoch 39/250 (Baseline Training): Train Loss: 0.9809, Train Acc: 70.55%, Val Loss: 0.7230, Val Acc: 76.08%\n",
      "Epoch 40/250 (Baseline Training): Train Loss: 0.9867, Train Acc: 70.35%, Val Loss: 0.6712, Val Acc: 78.24%\n",
      "Epoch 41/250 (Baseline Training): Train Loss: 0.9515, Train Acc: 71.33%, Val Loss: 0.6315, Val Acc: 79.32% (Best)\n",
      "Epoch 42/250 (Baseline Training): Train Loss: 0.9745, Train Acc: 70.73%, Val Loss: 0.6232, Val Acc: 79.84% (Best)\n",
      "Epoch 43/250 (Baseline Training): Train Loss: 0.9397, Train Acc: 71.55%, Val Loss: 0.6005, Val Acc: 79.78%\n",
      "Epoch 44/250 (Baseline Training): Train Loss: 0.9169, Train Acc: 72.30%, Val Loss: 0.6434, Val Acc: 79.28%\n",
      "Epoch 45/250 (Baseline Training): Train Loss: 0.9668, Train Acc: 71.04%, Val Loss: 0.6636, Val Acc: 78.80%\n",
      "Epoch 46/250 (Baseline Training): Train Loss: 0.9293, Train Acc: 72.16%, Val Loss: 0.6167, Val Acc: 79.96% (Best)\n",
      "Epoch 47/250 (Baseline Training): Train Loss: 0.9041, Train Acc: 72.92%, Val Loss: 0.5912, Val Acc: 80.10% (Best)\n",
      "Epoch 48/250 (Baseline Training): Train Loss: 0.9247, Train Acc: 72.36%, Val Loss: 0.6053, Val Acc: 80.14% (Best)\n",
      "Epoch 49/250 (Baseline Training): Train Loss: 0.9247, Train Acc: 72.19%, Val Loss: 0.6219, Val Acc: 79.40%\n",
      "Epoch 50/250 (Baseline Training): Train Loss: 0.9222, Train Acc: 72.22%, Val Loss: 0.5800, Val Acc: 81.04% (Best)\n",
      "Epoch 51/250 (Baseline Training): Train Loss: 0.9350, Train Acc: 71.98%, Val Loss: 0.6314, Val Acc: 80.30%\n",
      "Epoch 52/250 (Baseline Training): Train Loss: 0.8908, Train Acc: 73.36%, Val Loss: 0.5895, Val Acc: 81.60% (Best)\n",
      "Epoch 53/250 (Baseline Training): Train Loss: 0.9349, Train Acc: 72.03%, Val Loss: 0.6035, Val Acc: 80.50%\n",
      "Epoch 54/250 (Baseline Training): Train Loss: 0.8952, Train Acc: 73.06%, Val Loss: 0.5959, Val Acc: 81.18%\n",
      "Epoch 55/250 (Baseline Training): Train Loss: 0.8913, Train Acc: 73.12%, Val Loss: 0.5820, Val Acc: 80.30%\n",
      "Epoch 56/250 (Baseline Training): Train Loss: 0.9092, Train Acc: 72.46%, Val Loss: 0.6546, Val Acc: 79.20%\n",
      "Epoch 57/250 (Baseline Training): Train Loss: 0.8892, Train Acc: 73.17%, Val Loss: 0.5809, Val Acc: 81.00%\n",
      "Epoch 58/250 (Baseline Training): Train Loss: 0.9149, Train Acc: 72.47%, Val Loss: 0.5822, Val Acc: 80.98%\n",
      "Epoch 59/250 (Baseline Training): Train Loss: 0.8536, Train Acc: 74.58%, Val Loss: 0.5944, Val Acc: 80.98%\n",
      "Epoch 60/250 (Baseline Training): Train Loss: 0.9104, Train Acc: 72.79%, Val Loss: 0.5952, Val Acc: 80.30%\n",
      "Epoch 61/250 (Baseline Training): Train Loss: 0.8639, Train Acc: 74.28%, Val Loss: 0.5513, Val Acc: 82.20% (Best)\n",
      "Epoch 62/250 (Baseline Training): Train Loss: 0.8450, Train Acc: 74.50%, Val Loss: 0.5852, Val Acc: 81.46%\n",
      "Epoch 63/250 (Baseline Training): Train Loss: 0.8461, Train Acc: 74.55%, Val Loss: 0.5656, Val Acc: 81.78%\n",
      "Epoch 64/250 (Baseline Training): Train Loss: 0.8989, Train Acc: 72.88%, Val Loss: 0.5925, Val Acc: 82.38% (Best)\n",
      "Epoch 65/250 (Baseline Training): Train Loss: 0.8965, Train Acc: 72.82%, Val Loss: 0.5480, Val Acc: 82.68% (Best)\n",
      "Epoch 66/250 (Baseline Training): Train Loss: 0.8072, Train Acc: 75.97%, Val Loss: 0.5662, Val Acc: 81.56%\n",
      "Epoch 67/250 (Baseline Training): Train Loss: 0.8513, Train Acc: 74.66%, Val Loss: 0.6164, Val Acc: 82.18%\n",
      "Epoch 68/250 (Baseline Training): Train Loss: 0.8378, Train Acc: 74.98%, Val Loss: 0.5576, Val Acc: 81.38%\n",
      "Epoch 69/250 (Baseline Training): Train Loss: 0.8238, Train Acc: 75.02%, Val Loss: 0.6527, Val Acc: 79.56%\n",
      "Epoch 70/250 (Baseline Training): Train Loss: 0.8265, Train Acc: 75.22%, Val Loss: 0.5923, Val Acc: 80.90%\n",
      "Epoch 71/250 (Baseline Training): Train Loss: 0.8702, Train Acc: 73.84%, Val Loss: 0.5762, Val Acc: 82.06%\n",
      "Epoch 72/250 (Baseline Training): Train Loss: 0.8326, Train Acc: 75.12%, Val Loss: 0.5880, Val Acc: 81.64%\n",
      "Epoch 73/250 (Baseline Training): Train Loss: 0.8321, Train Acc: 75.09%, Val Loss: 0.5594, Val Acc: 82.66%\n",
      "Epoch 74/250 (Baseline Training): Train Loss: 0.8366, Train Acc: 74.92%, Val Loss: 0.5523, Val Acc: 82.20%\n",
      "Epoch 75/250 (Baseline Training): Train Loss: 0.8266, Train Acc: 74.92%, Val Loss: 0.6052, Val Acc: 81.68%\n",
      "Epoch 76/250 (Baseline Training): Train Loss: 0.8641, Train Acc: 74.00%, Val Loss: 0.5637, Val Acc: 82.66%\n",
      "Epoch 77/250 (Baseline Training): Train Loss: 0.8132, Train Acc: 75.34%, Val Loss: 0.5737, Val Acc: 82.90% (Best)\n",
      "Epoch 78/250 (Baseline Training): Train Loss: 0.7946, Train Acc: 76.14%, Val Loss: 0.4894, Val Acc: 83.64% (Best)\n",
      "Epoch 79/250 (Baseline Training): Train Loss: 0.7961, Train Acc: 76.14%, Val Loss: 0.5760, Val Acc: 81.36%\n",
      "Epoch 80/250 (Baseline Training): Train Loss: 0.7936, Train Acc: 76.25%, Val Loss: 0.5295, Val Acc: 82.96%\n",
      "Epoch 81/250 (Baseline Training): Train Loss: 0.8128, Train Acc: 75.91%, Val Loss: 0.5280, Val Acc: 83.36%\n",
      "Epoch 82/250 (Baseline Training): Train Loss: 0.8076, Train Acc: 76.17%, Val Loss: 0.5496, Val Acc: 84.00% (Best)\n",
      "Epoch 83/250 (Baseline Training): Train Loss: 0.8134, Train Acc: 75.87%, Val Loss: 0.5374, Val Acc: 83.44%\n",
      "Epoch 84/250 (Baseline Training): Train Loss: 0.8044, Train Acc: 76.01%, Val Loss: 0.5703, Val Acc: 83.36%\n",
      "Epoch 85/250 (Baseline Training): Train Loss: 0.8030, Train Acc: 76.01%, Val Loss: 0.5007, Val Acc: 84.02% (Best)\n",
      "Epoch 86/250 (Baseline Training): Train Loss: 0.7901, Train Acc: 76.37%, Val Loss: 0.5222, Val Acc: 83.28%\n",
      "Epoch 87/250 (Baseline Training): Train Loss: 0.7440, Train Acc: 77.59%, Val Loss: 0.4919, Val Acc: 83.44%\n",
      "Epoch 88/250 (Baseline Training): Train Loss: 0.7833, Train Acc: 76.38%, Val Loss: 0.4855, Val Acc: 83.90%\n",
      "Epoch 89/250 (Baseline Training): Train Loss: 0.7612, Train Acc: 77.08%, Val Loss: 0.4936, Val Acc: 83.58%\n",
      "Epoch 90/250 (Baseline Training): Train Loss: 0.7628, Train Acc: 77.21%, Val Loss: 0.5180, Val Acc: 83.92%\n",
      "Epoch 91/250 (Baseline Training): Train Loss: 0.7641, Train Acc: 76.95%, Val Loss: 0.5074, Val Acc: 83.94%\n",
      "Epoch 92/250 (Baseline Training): Train Loss: 0.7580, Train Acc: 77.35%, Val Loss: 0.5121, Val Acc: 83.36%\n",
      "Epoch 93/250 (Baseline Training): Train Loss: 0.7701, Train Acc: 76.92%, Val Loss: 0.5349, Val Acc: 84.06% (Best)\n",
      "Epoch 94/250 (Baseline Training): Train Loss: 0.7970, Train Acc: 76.53%, Val Loss: 0.4798, Val Acc: 84.74% (Best)\n",
      "Epoch 95/250 (Baseline Training): Train Loss: 0.8063, Train Acc: 75.80%, Val Loss: 0.5054, Val Acc: 84.46%\n",
      "Epoch 96/250 (Baseline Training): Train Loss: 0.7903, Train Acc: 76.44%, Val Loss: 0.5153, Val Acc: 84.24%\n",
      "Epoch 97/250 (Baseline Training): Train Loss: 0.7507, Train Acc: 77.52%, Val Loss: 0.4764, Val Acc: 84.40%\n",
      "Epoch 98/250 (Baseline Training): Train Loss: 0.7578, Train Acc: 77.28%, Val Loss: 0.4853, Val Acc: 83.94%\n",
      "Epoch 99/250 (Baseline Training): Train Loss: 0.7315, Train Acc: 78.06%, Val Loss: 0.5027, Val Acc: 84.80% (Best)\n",
      "Epoch 100/250 (Baseline Training): Train Loss: 0.7471, Train Acc: 77.37%, Val Loss: 0.5424, Val Acc: 83.50%\n",
      "Epoch 101/250 (Baseline Training): Train Loss: 0.7931, Train Acc: 76.27%, Val Loss: 0.5125, Val Acc: 84.60%\n",
      "Epoch 102/250 (Baseline Training): Train Loss: 0.7454, Train Acc: 77.76%, Val Loss: 0.4690, Val Acc: 83.88%\n",
      "Epoch 103/250 (Baseline Training): Train Loss: 0.7635, Train Acc: 77.21%, Val Loss: 0.4814, Val Acc: 84.90% (Best)\n",
      "Epoch 104/250 (Baseline Training): Train Loss: 0.7758, Train Acc: 76.94%, Val Loss: 0.4949, Val Acc: 84.44%\n",
      "Epoch 105/250 (Baseline Training): Train Loss: 0.7712, Train Acc: 77.09%, Val Loss: 0.4927, Val Acc: 84.54%\n",
      "Epoch 106/250 (Baseline Training): Train Loss: 0.7298, Train Acc: 78.22%, Val Loss: 0.4898, Val Acc: 84.24%\n",
      "Epoch 107/250 (Baseline Training): Train Loss: 0.7373, Train Acc: 77.97%, Val Loss: 0.4552, Val Acc: 84.90%\n",
      "Epoch 108/250 (Baseline Training): Train Loss: 0.7262, Train Acc: 78.60%, Val Loss: 0.5400, Val Acc: 83.76%\n",
      "Epoch 109/250 (Baseline Training): Train Loss: 0.7617, Train Acc: 77.36%, Val Loss: 0.5087, Val Acc: 84.20%\n",
      "Epoch 110/250 (Baseline Training): Train Loss: 0.7500, Train Acc: 77.24%, Val Loss: 0.4933, Val Acc: 85.12% (Best)\n",
      "Epoch 111/250 (Baseline Training): Train Loss: 0.7004, Train Acc: 78.95%, Val Loss: 0.5154, Val Acc: 84.94%\n",
      "Epoch 112/250 (Baseline Training): Train Loss: 0.7108, Train Acc: 78.90%, Val Loss: 0.5646, Val Acc: 84.06%\n",
      "Epoch 113/250 (Baseline Training): Train Loss: 0.7114, Train Acc: 78.93%, Val Loss: 0.5089, Val Acc: 83.66%\n",
      "Epoch 114/250 (Baseline Training): Train Loss: 0.7146, Train Acc: 79.04%, Val Loss: 0.5054, Val Acc: 84.48%\n",
      "Epoch 115/250 (Baseline Training): Train Loss: 0.7274, Train Acc: 78.35%, Val Loss: 0.4699, Val Acc: 85.06%\n",
      "Epoch 116/250 (Baseline Training): Train Loss: 0.7484, Train Acc: 77.72%, Val Loss: 0.4952, Val Acc: 85.38% (Best)\n",
      "Epoch 117/250 (Baseline Training): Train Loss: 0.6996, Train Acc: 79.02%, Val Loss: 0.5454, Val Acc: 84.18%\n",
      "Epoch 118/250 (Baseline Training): Train Loss: 0.7200, Train Acc: 78.32%, Val Loss: 0.4821, Val Acc: 84.24%\n",
      "Epoch 119/250 (Baseline Training): Train Loss: 0.7176, Train Acc: 78.93%, Val Loss: 0.5499, Val Acc: 84.04%\n",
      "Epoch 120/250 (Baseline Training): Train Loss: 0.7194, Train Acc: 78.81%, Val Loss: 0.4331, Val Acc: 85.46% (Best)\n",
      "Epoch 121/250 (Baseline Training): Train Loss: 0.7507, Train Acc: 77.91%, Val Loss: 0.4792, Val Acc: 84.98%\n",
      "Epoch 122/250 (Baseline Training): Train Loss: 0.7121, Train Acc: 79.24%, Val Loss: 0.4996, Val Acc: 84.90%\n",
      "Epoch 123/250 (Baseline Training): Train Loss: 0.6947, Train Acc: 79.37%, Val Loss: 0.4905, Val Acc: 84.82%\n",
      "Epoch 124/250 (Baseline Training): Train Loss: 0.7134, Train Acc: 78.98%, Val Loss: 0.4582, Val Acc: 85.22%\n",
      "Epoch 125/250 (Baseline Training): Train Loss: 0.7182, Train Acc: 78.74%, Val Loss: 0.4409, Val Acc: 85.56% (Best)\n",
      "Epoch 126/250 (Baseline Training): Train Loss: 0.7239, Train Acc: 78.85%, Val Loss: 0.5058, Val Acc: 84.42%\n",
      "Epoch 127/250 (Baseline Training): Train Loss: 0.6888, Train Acc: 79.51%, Val Loss: 0.4956, Val Acc: 85.90% (Best)\n",
      "Epoch 128/250 (Baseline Training): Train Loss: 0.7077, Train Acc: 78.93%, Val Loss: 0.4803, Val Acc: 85.36%\n",
      "Epoch 129/250 (Baseline Training): Train Loss: 0.7235, Train Acc: 78.83%, Val Loss: 0.4783, Val Acc: 85.88%\n",
      "Epoch 130/250 (Baseline Training): Train Loss: 0.6880, Train Acc: 80.10%, Val Loss: 0.4639, Val Acc: 85.66%\n",
      "Epoch 131/250 (Baseline Training): Train Loss: 0.7158, Train Acc: 78.77%, Val Loss: 0.4553, Val Acc: 86.22% (Best)\n",
      "Epoch 132/250 (Baseline Training): Train Loss: 0.6898, Train Acc: 79.55%, Val Loss: 0.4426, Val Acc: 85.52%\n",
      "Epoch 133/250 (Baseline Training): Train Loss: 0.7230, Train Acc: 78.47%, Val Loss: 0.4486, Val Acc: 85.42%\n",
      "Epoch 134/250 (Baseline Training): Train Loss: 0.6695, Train Acc: 80.43%, Val Loss: 0.5059, Val Acc: 85.46%\n",
      "Epoch 135/250 (Baseline Training): Train Loss: 0.6801, Train Acc: 79.93%, Val Loss: 0.4689, Val Acc: 86.00%\n",
      "Epoch 136/250 (Baseline Training): Train Loss: 0.6678, Train Acc: 80.35%, Val Loss: 0.4828, Val Acc: 85.24%\n",
      "Epoch 137/250 (Baseline Training): Train Loss: 0.7046, Train Acc: 79.32%, Val Loss: 0.4528, Val Acc: 85.78%\n",
      "Epoch 138/250 (Baseline Training): Train Loss: 0.7213, Train Acc: 78.50%, Val Loss: 0.4380, Val Acc: 85.62%\n",
      "Epoch 139/250 (Baseline Training): Train Loss: 0.6943, Train Acc: 80.40%, Val Loss: 0.4334, Val Acc: 86.22%\n",
      "Epoch 140/250 (Baseline Training): Train Loss: 0.6729, Train Acc: 80.30%, Val Loss: 0.4500, Val Acc: 85.78%\n",
      "Epoch 141/250 (Baseline Training): Train Loss: 0.6670, Train Acc: 80.54%, Val Loss: 0.4271, Val Acc: 85.78%\n",
      "Epoch 142/250 (Baseline Training): Train Loss: 0.6954, Train Acc: 79.47%, Val Loss: 0.4785, Val Acc: 85.76%\n",
      "Epoch 143/250 (Baseline Training): Train Loss: 0.7044, Train Acc: 79.64%, Val Loss: 0.4435, Val Acc: 86.14%\n",
      "Epoch 144/250 (Baseline Training): Train Loss: 0.6871, Train Acc: 80.18%, Val Loss: 0.4525, Val Acc: 85.82%\n",
      "Epoch 145/250 (Baseline Training): Train Loss: 0.6738, Train Acc: 80.47%, Val Loss: 0.4324, Val Acc: 86.22%\n",
      "Epoch 146/250 (Baseline Training): Train Loss: 0.6593, Train Acc: 80.51%, Val Loss: 0.4445, Val Acc: 86.18%\n",
      "Early stopping triggered after 146 epochs\n",
      "Loaded best model state with val accuracy: 86.22%\n",
      "âœ… Model saved to ./models_mobilenetv2_enhanced/baseline_model.pth\n",
      "âœ… ONNX model saved to ./models_mobilenetv2_enhanced/baseline_model.onnx\n",
      "\n",
      "Evaluating baseline model...\n",
      "Baseline Results: Accuracy=85.20%, MACs=6.52M, Params=2.24M\n",
      "\n",
      "Starting pruning experiments...\n",
      "\n",
      "--- Strategy: BNScale ---\n",
      "\n",
      "Processing BNScale at 20.0% sparsity...\n",
      "âœ… Created MobileNetV2 without pretrained weights\n",
      "âœ… Adapted classifier for 10 classes\n",
      "Initial MACs: 6.52M\n",
      "Applying BNScaleImportance pruning at 20.0% sparsity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch_pruning/pruner/algorithms/base_pruner.py:87: UserWarning: ch_sparsity is deprecated in v1.3.0. Please use pruning_ratio.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final MACs: 6.00M (Reduction: 8.0%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/250 (BNScale-20.0%): Train Loss: 0.3536, Train Acc: 88.23%, Val Loss: 0.4270, Val Acc: 85.46% (Best)\n",
      "Epoch 2/250 (BNScale-20.0%): Train Loss: 0.3145, Train Acc: 89.42%, Val Loss: 0.4210, Val Acc: 85.62% (Best)\n",
      "Epoch 3/250 (BNScale-20.0%): Train Loss: 0.3053, Train Acc: 89.57%, Val Loss: 0.4116, Val Acc: 85.94% (Best)\n",
      "Epoch 4/250 (BNScale-20.0%): Train Loss: 0.2946, Train Acc: 89.83%, Val Loss: 0.4068, Val Acc: 86.62% (Best)\n",
      "Epoch 5/250 (BNScale-20.0%): Train Loss: 0.2896, Train Acc: 90.04%, Val Loss: 0.4106, Val Acc: 86.44%\n",
      "Epoch 6/250 (BNScale-20.0%): Train Loss: 0.2822, Train Acc: 90.47%, Val Loss: 0.4060, Val Acc: 86.24%\n",
      "Epoch 7/250 (BNScale-20.0%): Train Loss: 0.2782, Train Acc: 90.56%, Val Loss: 0.4085, Val Acc: 86.28%\n",
      "Epoch 8/250 (BNScale-20.0%): Train Loss: 0.2704, Train Acc: 90.84%, Val Loss: 0.4144, Val Acc: 86.08%\n",
      "Epoch 9/250 (BNScale-20.0%): Train Loss: 0.2668, Train Acc: 90.81%, Val Loss: 0.4095, Val Acc: 86.42%\n",
      "Epoch 10/250 (BNScale-20.0%): Train Loss: 0.2611, Train Acc: 91.05%, Val Loss: 0.4131, Val Acc: 86.28%\n",
      "Epoch 11/250 (BNScale-20.0%): Train Loss: 0.2592, Train Acc: 91.08%, Val Loss: 0.4091, Val Acc: 86.22%\n",
      "Epoch 12/250 (BNScale-20.0%): Train Loss: 0.2612, Train Acc: 90.98%, Val Loss: 0.4066, Val Acc: 86.40%\n",
      "Epoch 13/250 (BNScale-20.0%): Train Loss: 0.2595, Train Acc: 91.14%, Val Loss: 0.3985, Val Acc: 86.40%\n",
      "Epoch 14/250 (BNScale-20.0%): Train Loss: 0.2516, Train Acc: 91.38%, Val Loss: 0.4022, Val Acc: 86.42%\n",
      "Epoch 15/250 (BNScale-20.0%): Train Loss: 0.2538, Train Acc: 91.15%, Val Loss: 0.4040, Val Acc: 86.18%\n",
      "Epoch 16/250 (BNScale-20.0%): Train Loss: 0.2517, Train Acc: 91.55%, Val Loss: 0.4033, Val Acc: 86.26%\n",
      "Epoch 17/250 (BNScale-20.0%): Train Loss: 0.2497, Train Acc: 91.34%, Val Loss: 0.3994, Val Acc: 86.44%\n",
      "Epoch 18/250 (BNScale-20.0%): Train Loss: 0.2480, Train Acc: 91.52%, Val Loss: 0.4075, Val Acc: 86.34%\n",
      "Epoch 19/250 (BNScale-20.0%): Train Loss: 0.2442, Train Acc: 91.51%, Val Loss: 0.4158, Val Acc: 86.30%\n",
      "Early stopping triggered after 19 epochs\n",
      "Loaded best model state with val accuracy: 86.62%\n",
      "Results: Accuracy=85.59%, MACs=6.00M\n",
      "âœ… Model saved to ./models_mobilenetv2_enhanced/bnscale_sparsity_0.2.pth\n",
      "âœ… ONNX model saved to ./models_mobilenetv2_enhanced/bnscale_sparsity_0.2.onnx\n",
      "\n",
      "Processing BNScale at 50.0% sparsity...\n",
      "âœ… Created MobileNetV2 without pretrained weights\n",
      "âœ… Adapted classifier for 10 classes\n",
      "Initial MACs: 6.52M\n",
      "Applying BNScaleImportance pruning at 50.0% sparsity...\n",
      "Final MACs: 5.30M (Reduction: 18.7%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/250 (BNScale-50.0%): Train Loss: 0.4766, Train Acc: 84.00%, Val Loss: 0.4806, Val Acc: 83.48% (Best)\n",
      "Epoch 2/250 (BNScale-50.0%): Train Loss: 0.4062, Train Acc: 86.06%, Val Loss: 0.4645, Val Acc: 83.94% (Best)\n",
      "Epoch 3/250 (BNScale-50.0%): Train Loss: 0.3741, Train Acc: 87.13%, Val Loss: 0.4436, Val Acc: 84.50% (Best)\n",
      "Epoch 4/250 (BNScale-50.0%): Train Loss: 0.3665, Train Acc: 87.46%, Val Loss: 0.4386, Val Acc: 84.58% (Best)\n",
      "Epoch 5/250 (BNScale-50.0%): Train Loss: 0.3540, Train Acc: 87.98%, Val Loss: 0.4426, Val Acc: 84.96% (Best)\n",
      "Epoch 6/250 (BNScale-50.0%): Train Loss: 0.3386, Train Acc: 88.28%, Val Loss: 0.4408, Val Acc: 84.88%\n",
      "Epoch 7/250 (BNScale-50.0%): Train Loss: 0.3320, Train Acc: 88.64%, Val Loss: 0.4390, Val Acc: 85.20% (Best)\n",
      "Epoch 8/250 (BNScale-50.0%): Train Loss: 0.3278, Train Acc: 88.74%, Val Loss: 0.4302, Val Acc: 85.62% (Best)\n",
      "Epoch 9/250 (BNScale-50.0%): Train Loss: 0.3202, Train Acc: 88.94%, Val Loss: 0.4350, Val Acc: 85.18%\n",
      "Epoch 10/250 (BNScale-50.0%): Train Loss: 0.3157, Train Acc: 89.15%, Val Loss: 0.4287, Val Acc: 85.22%\n",
      "Epoch 11/250 (BNScale-50.0%): Train Loss: 0.3121, Train Acc: 89.12%, Val Loss: 0.4300, Val Acc: 85.38%\n",
      "Epoch 12/250 (BNScale-50.0%): Train Loss: 0.3029, Train Acc: 89.40%, Val Loss: 0.4296, Val Acc: 85.44%\n",
      "Epoch 13/250 (BNScale-50.0%): Train Loss: 0.3004, Train Acc: 89.62%, Val Loss: 0.4257, Val Acc: 85.44%\n",
      "Epoch 14/250 (BNScale-50.0%): Train Loss: 0.2980, Train Acc: 89.73%, Val Loss: 0.4269, Val Acc: 85.52%\n",
      "Epoch 15/250 (BNScale-50.0%): Train Loss: 0.2973, Train Acc: 89.71%, Val Loss: 0.4257, Val Acc: 85.66% (Best)\n",
      "Epoch 16/250 (BNScale-50.0%): Train Loss: 0.2941, Train Acc: 89.77%, Val Loss: 0.4210, Val Acc: 85.24%\n",
      "Epoch 17/250 (BNScale-50.0%): Train Loss: 0.2946, Train Acc: 89.87%, Val Loss: 0.4215, Val Acc: 85.08%\n",
      "Epoch 18/250 (BNScale-50.0%): Train Loss: 0.2854, Train Acc: 90.09%, Val Loss: 0.4155, Val Acc: 85.66%\n",
      "Epoch 19/250 (BNScale-50.0%): Train Loss: 0.2830, Train Acc: 90.25%, Val Loss: 0.4190, Val Acc: 85.66%\n",
      "Epoch 20/250 (BNScale-50.0%): Train Loss: 0.2823, Train Acc: 90.27%, Val Loss: 0.4227, Val Acc: 85.34%\n",
      "Epoch 21/250 (BNScale-50.0%): Train Loss: 0.2803, Train Acc: 90.11%, Val Loss: 0.4209, Val Acc: 85.44%\n",
      "Epoch 22/250 (BNScale-50.0%): Train Loss: 0.2756, Train Acc: 90.18%, Val Loss: 0.4199, Val Acc: 85.70% (Best)\n",
      "Epoch 23/250 (BNScale-50.0%): Train Loss: 0.2745, Train Acc: 90.50%, Val Loss: 0.4231, Val Acc: 85.46%\n",
      "Epoch 24/250 (BNScale-50.0%): Train Loss: 0.2734, Train Acc: 90.33%, Val Loss: 0.4260, Val Acc: 85.72% (Best)\n",
      "Epoch 25/250 (BNScale-50.0%): Train Loss: 0.2701, Train Acc: 90.50%, Val Loss: 0.4205, Val Acc: 85.60%\n",
      "Epoch 26/250 (BNScale-50.0%): Train Loss: 0.2647, Train Acc: 90.86%, Val Loss: 0.4272, Val Acc: 85.90% (Best)\n",
      "Epoch 27/250 (BNScale-50.0%): Train Loss: 0.2676, Train Acc: 90.78%, Val Loss: 0.4216, Val Acc: 85.92% (Best)\n",
      "Epoch 28/250 (BNScale-50.0%): Train Loss: 0.2663, Train Acc: 90.72%, Val Loss: 0.4220, Val Acc: 85.94% (Best)\n",
      "Epoch 29/250 (BNScale-50.0%): Train Loss: 0.2641, Train Acc: 90.83%, Val Loss: 0.4238, Val Acc: 85.70%\n",
      "Epoch 30/250 (BNScale-50.0%): Train Loss: 0.2639, Train Acc: 90.78%, Val Loss: 0.4267, Val Acc: 85.58%\n",
      "Epoch 31/250 (BNScale-50.0%): Train Loss: 0.2642, Train Acc: 90.69%, Val Loss: 0.4253, Val Acc: 85.84%\n",
      "Epoch 32/250 (BNScale-50.0%): Train Loss: 0.2622, Train Acc: 90.80%, Val Loss: 0.4288, Val Acc: 85.46%\n",
      "Epoch 33/250 (BNScale-50.0%): Train Loss: 0.2615, Train Acc: 90.85%, Val Loss: 0.4323, Val Acc: 85.78%\n",
      "Epoch 34/250 (BNScale-50.0%): Train Loss: 0.2550, Train Acc: 91.00%, Val Loss: 0.4287, Val Acc: 85.64%\n",
      "Epoch 35/250 (BNScale-50.0%): Train Loss: 0.2556, Train Acc: 90.99%, Val Loss: 0.4386, Val Acc: 85.54%\n",
      "Epoch 36/250 (BNScale-50.0%): Train Loss: 0.2528, Train Acc: 91.18%, Val Loss: 0.4327, Val Acc: 85.56%\n",
      "Epoch 37/250 (BNScale-50.0%): Train Loss: 0.2495, Train Acc: 91.32%, Val Loss: 0.4308, Val Acc: 85.64%\n",
      "Epoch 38/250 (BNScale-50.0%): Train Loss: 0.2509, Train Acc: 91.30%, Val Loss: 0.4407, Val Acc: 85.52%\n",
      "Epoch 39/250 (BNScale-50.0%): Train Loss: 0.2464, Train Acc: 91.25%, Val Loss: 0.4375, Val Acc: 85.78%\n",
      "Epoch 40/250 (BNScale-50.0%): Train Loss: 0.2492, Train Acc: 91.30%, Val Loss: 0.4389, Val Acc: 85.78%\n",
      "Epoch 41/250 (BNScale-50.0%): Train Loss: 0.2435, Train Acc: 91.46%, Val Loss: 0.4342, Val Acc: 85.86%\n",
      "Epoch 42/250 (BNScale-50.0%): Train Loss: 0.2459, Train Acc: 91.33%, Val Loss: 0.4390, Val Acc: 85.96% (Best)\n",
      "Epoch 43/250 (BNScale-50.0%): Train Loss: 0.2480, Train Acc: 91.36%, Val Loss: 0.4390, Val Acc: 85.50%\n",
      "Epoch 44/250 (BNScale-50.0%): Train Loss: 0.2450, Train Acc: 91.40%, Val Loss: 0.4409, Val Acc: 85.46%\n",
      "Epoch 45/250 (BNScale-50.0%): Train Loss: 0.2401, Train Acc: 91.68%, Val Loss: 0.4367, Val Acc: 85.60%\n",
      "Epoch 46/250 (BNScale-50.0%): Train Loss: 0.2398, Train Acc: 91.60%, Val Loss: 0.4368, Val Acc: 85.62%\n",
      "Epoch 47/250 (BNScale-50.0%): Train Loss: 0.2408, Train Acc: 91.55%, Val Loss: 0.4467, Val Acc: 85.34%\n",
      "Epoch 48/250 (BNScale-50.0%): Train Loss: 0.2366, Train Acc: 91.68%, Val Loss: 0.4386, Val Acc: 85.44%\n",
      "Epoch 49/250 (BNScale-50.0%): Train Loss: 0.2355, Train Acc: 91.80%, Val Loss: 0.4560, Val Acc: 85.42%\n",
      "Epoch 50/250 (BNScale-50.0%): Train Loss: 0.2397, Train Acc: 91.60%, Val Loss: 0.4507, Val Acc: 85.64%\n",
      "Epoch 51/250 (BNScale-50.0%): Train Loss: 0.2353, Train Acc: 91.65%, Val Loss: 0.4498, Val Acc: 85.46%\n",
      "Epoch 52/250 (BNScale-50.0%): Train Loss: 0.2358, Train Acc: 91.76%, Val Loss: 0.4488, Val Acc: 85.76%\n",
      "Epoch 53/250 (BNScale-50.0%): Train Loss: 0.2334, Train Acc: 91.77%, Val Loss: 0.4466, Val Acc: 85.64%\n",
      "Epoch 54/250 (BNScale-50.0%): Train Loss: 0.2301, Train Acc: 92.03%, Val Loss: 0.4362, Val Acc: 85.40%\n",
      "Epoch 55/250 (BNScale-50.0%): Train Loss: 0.2330, Train Acc: 91.89%, Val Loss: 0.4420, Val Acc: 85.70%\n",
      "Epoch 56/250 (BNScale-50.0%): Train Loss: 0.2320, Train Acc: 91.85%, Val Loss: 0.4383, Val Acc: 85.86%\n",
      "Epoch 57/250 (BNScale-50.0%): Train Loss: 0.2256, Train Acc: 92.10%, Val Loss: 0.4432, Val Acc: 85.50%\n",
      "Early stopping triggered after 57 epochs\n",
      "Loaded best model state with val accuracy: 85.96%\n",
      "Results: Accuracy=85.74%, MACs=5.30M\n",
      "âœ… Model saved to ./models_mobilenetv2_enhanced/bnscale_sparsity_0.5.pth\n",
      "âœ… ONNX model saved to ./models_mobilenetv2_enhanced/bnscale_sparsity_0.5.onnx\n",
      "\n",
      "Processing BNScale at 70.0% sparsity...\n",
      "âœ… Created MobileNetV2 without pretrained weights\n",
      "âœ… Adapted classifier for 10 classes\n",
      "Initial MACs: 6.52M\n",
      "Applying BNScaleImportance pruning at 70.0% sparsity...\n",
      "Final MACs: 4.87M (Reduction: 25.3%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/250 (BNScale-70.0%): Train Loss: 0.6063, Train Acc: 79.77%, Val Loss: 0.5495, Val Acc: 81.20% (Best)\n",
      "Epoch 2/250 (BNScale-70.0%): Train Loss: 0.4816, Train Acc: 83.53%, Val Loss: 0.5011, Val Acc: 82.70% (Best)\n",
      "Epoch 3/250 (BNScale-70.0%): Train Loss: 0.4452, Train Acc: 84.77%, Val Loss: 0.4806, Val Acc: 83.50% (Best)\n",
      "Epoch 4/250 (BNScale-70.0%): Train Loss: 0.4213, Train Acc: 85.62%, Val Loss: 0.4752, Val Acc: 83.74% (Best)\n",
      "Epoch 5/250 (BNScale-70.0%): Train Loss: 0.4000, Train Acc: 86.40%, Val Loss: 0.4592, Val Acc: 83.92% (Best)\n",
      "Epoch 6/250 (BNScale-70.0%): Train Loss: 0.3879, Train Acc: 86.63%, Val Loss: 0.4556, Val Acc: 84.36% (Best)\n",
      "Epoch 7/250 (BNScale-70.0%): Train Loss: 0.3765, Train Acc: 87.21%, Val Loss: 0.4452, Val Acc: 84.44% (Best)\n",
      "Epoch 8/250 (BNScale-70.0%): Train Loss: 0.3674, Train Acc: 87.17%, Val Loss: 0.4403, Val Acc: 84.70% (Best)\n",
      "Epoch 9/250 (BNScale-70.0%): Train Loss: 0.3645, Train Acc: 87.35%, Val Loss: 0.4466, Val Acc: 84.22%\n",
      "Epoch 10/250 (BNScale-70.0%): Train Loss: 0.3488, Train Acc: 88.02%, Val Loss: 0.4361, Val Acc: 84.82% (Best)\n",
      "Epoch 11/250 (BNScale-70.0%): Train Loss: 0.3446, Train Acc: 87.96%, Val Loss: 0.4382, Val Acc: 85.18% (Best)\n",
      "Epoch 12/250 (BNScale-70.0%): Train Loss: 0.3407, Train Acc: 88.29%, Val Loss: 0.4358, Val Acc: 85.24% (Best)\n",
      "Epoch 13/250 (BNScale-70.0%): Train Loss: 0.3377, Train Acc: 88.42%, Val Loss: 0.4340, Val Acc: 84.98%\n",
      "Epoch 14/250 (BNScale-70.0%): Train Loss: 0.3257, Train Acc: 88.69%, Val Loss: 0.4318, Val Acc: 84.90%\n",
      "Epoch 15/250 (BNScale-70.0%): Train Loss: 0.3277, Train Acc: 88.64%, Val Loss: 0.4287, Val Acc: 85.18%\n",
      "Epoch 16/250 (BNScale-70.0%): Train Loss: 0.3241, Train Acc: 88.81%, Val Loss: 0.4233, Val Acc: 85.56% (Best)\n",
      "Epoch 17/250 (BNScale-70.0%): Train Loss: 0.3156, Train Acc: 89.02%, Val Loss: 0.4257, Val Acc: 85.46%\n",
      "Epoch 18/250 (BNScale-70.0%): Train Loss: 0.3163, Train Acc: 89.00%, Val Loss: 0.4276, Val Acc: 85.46%\n",
      "Epoch 19/250 (BNScale-70.0%): Train Loss: 0.3117, Train Acc: 89.17%, Val Loss: 0.4243, Val Acc: 85.08%\n",
      "Epoch 20/250 (BNScale-70.0%): Train Loss: 0.3085, Train Acc: 89.27%, Val Loss: 0.4274, Val Acc: 85.54%\n",
      "Epoch 21/250 (BNScale-70.0%): Train Loss: 0.3069, Train Acc: 89.25%, Val Loss: 0.4353, Val Acc: 85.26%\n",
      "Epoch 22/250 (BNScale-70.0%): Train Loss: 0.3045, Train Acc: 89.48%, Val Loss: 0.4359, Val Acc: 85.22%\n",
      "Epoch 23/250 (BNScale-70.0%): Train Loss: 0.2992, Train Acc: 89.60%, Val Loss: 0.4325, Val Acc: 85.70% (Best)\n",
      "Epoch 24/250 (BNScale-70.0%): Train Loss: 0.2996, Train Acc: 89.59%, Val Loss: 0.4332, Val Acc: 85.50%\n",
      "Epoch 25/250 (BNScale-70.0%): Train Loss: 0.2996, Train Acc: 89.54%, Val Loss: 0.4319, Val Acc: 85.50%\n",
      "Epoch 26/250 (BNScale-70.0%): Train Loss: 0.2963, Train Acc: 89.73%, Val Loss: 0.4304, Val Acc: 85.44%\n",
      "Epoch 27/250 (BNScale-70.0%): Train Loss: 0.2951, Train Acc: 89.76%, Val Loss: 0.4254, Val Acc: 85.76% (Best)\n",
      "Epoch 28/250 (BNScale-70.0%): Train Loss: 0.2852, Train Acc: 90.20%, Val Loss: 0.4315, Val Acc: 85.60%\n",
      "Epoch 29/250 (BNScale-70.0%): Train Loss: 0.2843, Train Acc: 89.96%, Val Loss: 0.4256, Val Acc: 85.72%\n",
      "Epoch 30/250 (BNScale-70.0%): Train Loss: 0.2842, Train Acc: 90.11%, Val Loss: 0.4313, Val Acc: 85.58%\n",
      "Epoch 31/250 (BNScale-70.0%): Train Loss: 0.2865, Train Acc: 89.95%, Val Loss: 0.4257, Val Acc: 85.72%\n",
      "Epoch 32/250 (BNScale-70.0%): Train Loss: 0.2841, Train Acc: 90.04%, Val Loss: 0.4234, Val Acc: 85.94% (Best)\n",
      "Epoch 33/250 (BNScale-70.0%): Train Loss: 0.2769, Train Acc: 90.42%, Val Loss: 0.4343, Val Acc: 85.64%\n",
      "Epoch 34/250 (BNScale-70.0%): Train Loss: 0.2742, Train Acc: 90.36%, Val Loss: 0.4351, Val Acc: 85.50%\n",
      "Epoch 35/250 (BNScale-70.0%): Train Loss: 0.2793, Train Acc: 90.17%, Val Loss: 0.4302, Val Acc: 85.90%\n",
      "Epoch 36/250 (BNScale-70.0%): Train Loss: 0.2755, Train Acc: 90.40%, Val Loss: 0.4281, Val Acc: 85.76%\n",
      "Epoch 37/250 (BNScale-70.0%): Train Loss: 0.2767, Train Acc: 90.28%, Val Loss: 0.4308, Val Acc: 85.76%\n",
      "Epoch 38/250 (BNScale-70.0%): Train Loss: 0.2711, Train Acc: 90.62%, Val Loss: 0.4375, Val Acc: 85.70%\n",
      "Epoch 39/250 (BNScale-70.0%): Train Loss: 0.2690, Train Acc: 90.56%, Val Loss: 0.4324, Val Acc: 85.78%\n",
      "Epoch 40/250 (BNScale-70.0%): Train Loss: 0.2701, Train Acc: 90.62%, Val Loss: 0.4288, Val Acc: 85.88%\n",
      "Epoch 41/250 (BNScale-70.0%): Train Loss: 0.2698, Train Acc: 90.63%, Val Loss: 0.4396, Val Acc: 85.58%\n",
      "Epoch 42/250 (BNScale-70.0%): Train Loss: 0.2655, Train Acc: 90.67%, Val Loss: 0.4403, Val Acc: 85.82%\n",
      "Epoch 43/250 (BNScale-70.0%): Train Loss: 0.2633, Train Acc: 90.79%, Val Loss: 0.4383, Val Acc: 85.58%\n",
      "Epoch 44/250 (BNScale-70.0%): Train Loss: 0.2638, Train Acc: 90.97%, Val Loss: 0.4379, Val Acc: 85.56%\n",
      "Epoch 45/250 (BNScale-70.0%): Train Loss: 0.2608, Train Acc: 90.76%, Val Loss: 0.4353, Val Acc: 85.68%\n",
      "Epoch 46/250 (BNScale-70.0%): Train Loss: 0.2591, Train Acc: 90.96%, Val Loss: 0.4411, Val Acc: 85.62%\n",
      "Epoch 47/250 (BNScale-70.0%): Train Loss: 0.2614, Train Acc: 90.84%, Val Loss: 0.4368, Val Acc: 85.76%\n",
      "Early stopping triggered after 47 epochs\n",
      "Loaded best model state with val accuracy: 85.94%\n",
      "Results: Accuracy=85.45%, MACs=4.87M\n",
      "âœ… Model saved to ./models_mobilenetv2_enhanced/bnscale_sparsity_0.7.pth\n",
      "âœ… ONNX model saved to ./models_mobilenetv2_enhanced/bnscale_sparsity_0.7.onnx\n",
      "\n",
      "--- Strategy: MagnitudeL2 ---\n",
      "\n",
      "Processing MagnitudeL2 at 20.0% sparsity...\n",
      "âœ… Created MobileNetV2 without pretrained weights\n",
      "âœ… Adapted classifier for 10 classes\n",
      "Initial MACs: 6.52M\n",
      "Applying MagnitudeImportance pruning at 20.0% sparsity...\n",
      "Final MACs: 6.00M (Reduction: 8.0%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/250 (MagnitudeL2-20.0%): Train Loss: 0.3421, Train Acc: 88.39%, Val Loss: 0.4323, Val Acc: 85.58% (Best)\n",
      "Epoch 2/250 (MagnitudeL2-20.0%): Train Loss: 0.3120, Train Acc: 89.52%, Val Loss: 0.4236, Val Acc: 85.94% (Best)\n",
      "Epoch 3/250 (MagnitudeL2-20.0%): Train Loss: 0.2954, Train Acc: 90.00%, Val Loss: 0.4186, Val Acc: 86.04% (Best)\n",
      "Epoch 4/250 (MagnitudeL2-20.0%): Train Loss: 0.2908, Train Acc: 90.11%, Val Loss: 0.4211, Val Acc: 85.90%\n",
      "Epoch 5/250 (MagnitudeL2-20.0%): Train Loss: 0.2817, Train Acc: 90.53%, Val Loss: 0.4186, Val Acc: 86.14% (Best)\n",
      "Epoch 6/250 (MagnitudeL2-20.0%): Train Loss: 0.2763, Train Acc: 90.60%, Val Loss: 0.4124, Val Acc: 85.84%\n",
      "Epoch 7/250 (MagnitudeL2-20.0%): Train Loss: 0.2771, Train Acc: 90.57%, Val Loss: 0.4140, Val Acc: 85.90%\n",
      "Epoch 8/250 (MagnitudeL2-20.0%): Train Loss: 0.2690, Train Acc: 90.87%, Val Loss: 0.4152, Val Acc: 86.14%\n",
      "Epoch 9/250 (MagnitudeL2-20.0%): Train Loss: 0.2654, Train Acc: 90.85%, Val Loss: 0.4107, Val Acc: 86.18% (Best)\n",
      "Epoch 10/250 (MagnitudeL2-20.0%): Train Loss: 0.2644, Train Acc: 90.96%, Val Loss: 0.4097, Val Acc: 86.32% (Best)\n",
      "Epoch 11/250 (MagnitudeL2-20.0%): Train Loss: 0.2621, Train Acc: 90.88%, Val Loss: 0.4068, Val Acc: 86.72% (Best)\n",
      "Epoch 12/250 (MagnitudeL2-20.0%): Train Loss: 0.2574, Train Acc: 91.21%, Val Loss: 0.4086, Val Acc: 86.46%\n",
      "Epoch 13/250 (MagnitudeL2-20.0%): Train Loss: 0.2565, Train Acc: 91.12%, Val Loss: 0.4101, Val Acc: 86.26%\n",
      "Epoch 14/250 (MagnitudeL2-20.0%): Train Loss: 0.2541, Train Acc: 91.30%, Val Loss: 0.4119, Val Acc: 86.24%\n",
      "Epoch 15/250 (MagnitudeL2-20.0%): Train Loss: 0.2515, Train Acc: 91.34%, Val Loss: 0.4090, Val Acc: 86.12%\n",
      "Epoch 16/250 (MagnitudeL2-20.0%): Train Loss: 0.2471, Train Acc: 91.42%, Val Loss: 0.4087, Val Acc: 86.56%\n",
      "Epoch 17/250 (MagnitudeL2-20.0%): Train Loss: 0.2484, Train Acc: 91.45%, Val Loss: 0.4126, Val Acc: 86.22%\n",
      "Epoch 18/250 (MagnitudeL2-20.0%): Train Loss: 0.2439, Train Acc: 91.66%, Val Loss: 0.4099, Val Acc: 86.84% (Best)\n",
      "Epoch 19/250 (MagnitudeL2-20.0%): Train Loss: 0.2434, Train Acc: 91.62%, Val Loss: 0.4072, Val Acc: 86.20%\n",
      "Epoch 20/250 (MagnitudeL2-20.0%): Train Loss: 0.2395, Train Acc: 91.66%, Val Loss: 0.4087, Val Acc: 86.50%\n",
      "Epoch 21/250 (MagnitudeL2-20.0%): Train Loss: 0.2409, Train Acc: 91.67%, Val Loss: 0.4123, Val Acc: 86.42%\n",
      "Epoch 22/250 (MagnitudeL2-20.0%): Train Loss: 0.2415, Train Acc: 91.66%, Val Loss: 0.4163, Val Acc: 86.44%\n",
      "Epoch 23/250 (MagnitudeL2-20.0%): Train Loss: 0.2371, Train Acc: 91.75%, Val Loss: 0.4112, Val Acc: 86.52%\n",
      "Epoch 24/250 (MagnitudeL2-20.0%): Train Loss: 0.2338, Train Acc: 91.69%, Val Loss: 0.4096, Val Acc: 86.52%\n",
      "Epoch 25/250 (MagnitudeL2-20.0%): Train Loss: 0.2311, Train Acc: 92.02%, Val Loss: 0.4127, Val Acc: 86.70%\n",
      "Epoch 26/250 (MagnitudeL2-20.0%): Train Loss: 0.2384, Train Acc: 91.59%, Val Loss: 0.4186, Val Acc: 86.38%\n",
      "Epoch 27/250 (MagnitudeL2-20.0%): Train Loss: 0.2331, Train Acc: 91.80%, Val Loss: 0.4154, Val Acc: 86.48%\n",
      "Epoch 28/250 (MagnitudeL2-20.0%): Train Loss: 0.2287, Train Acc: 92.06%, Val Loss: 0.4257, Val Acc: 86.40%\n",
      "Epoch 29/250 (MagnitudeL2-20.0%): Train Loss: 0.2309, Train Acc: 92.04%, Val Loss: 0.4211, Val Acc: 86.60%\n",
      "Epoch 30/250 (MagnitudeL2-20.0%): Train Loss: 0.2293, Train Acc: 92.08%, Val Loss: 0.4192, Val Acc: 86.60%\n",
      "Epoch 31/250 (MagnitudeL2-20.0%): Train Loss: 0.2230, Train Acc: 92.23%, Val Loss: 0.4176, Val Acc: 86.62%\n",
      "Epoch 32/250 (MagnitudeL2-20.0%): Train Loss: 0.2198, Train Acc: 92.32%, Val Loss: 0.4170, Val Acc: 86.56%\n",
      "Epoch 33/250 (MagnitudeL2-20.0%): Train Loss: 0.2217, Train Acc: 92.21%, Val Loss: 0.4148, Val Acc: 86.84%\n",
      "Early stopping triggered after 33 epochs\n",
      "Loaded best model state with val accuracy: 86.84%\n",
      "Results: Accuracy=85.95%, MACs=6.00M\n",
      "âœ… Model saved to ./models_mobilenetv2_enhanced/magnitudel2_sparsity_0.2.pth\n",
      "âœ… ONNX model saved to ./models_mobilenetv2_enhanced/magnitudel2_sparsity_0.2.onnx\n",
      "\n",
      "Processing MagnitudeL2 at 50.0% sparsity...\n",
      "âœ… Created MobileNetV2 without pretrained weights\n",
      "âœ… Adapted classifier for 10 classes\n",
      "Initial MACs: 6.52M\n",
      "Applying MagnitudeImportance pruning at 50.0% sparsity...\n",
      "Final MACs: 5.30M (Reduction: 18.7%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/250 (MagnitudeL2-50.0%): Train Loss: 0.4648, Train Acc: 84.37%, Val Loss: 0.4785, Val Acc: 83.68% (Best)\n",
      "Epoch 2/250 (MagnitudeL2-50.0%): Train Loss: 0.4019, Train Acc: 86.32%, Val Loss: 0.4639, Val Acc: 84.34% (Best)\n",
      "Epoch 3/250 (MagnitudeL2-50.0%): Train Loss: 0.3761, Train Acc: 87.09%, Val Loss: 0.4520, Val Acc: 84.54% (Best)\n",
      "Epoch 4/250 (MagnitudeL2-50.0%): Train Loss: 0.3612, Train Acc: 87.75%, Val Loss: 0.4418, Val Acc: 85.18% (Best)\n",
      "Epoch 5/250 (MagnitudeL2-50.0%): Train Loss: 0.3469, Train Acc: 88.13%, Val Loss: 0.4477, Val Acc: 84.68%\n",
      "Epoch 6/250 (MagnitudeL2-50.0%): Train Loss: 0.3431, Train Acc: 88.36%, Val Loss: 0.4380, Val Acc: 85.08%\n",
      "Epoch 7/250 (MagnitudeL2-50.0%): Train Loss: 0.3307, Train Acc: 88.66%, Val Loss: 0.4323, Val Acc: 85.26% (Best)\n",
      "Epoch 8/250 (MagnitudeL2-50.0%): Train Loss: 0.3273, Train Acc: 88.82%, Val Loss: 0.4303, Val Acc: 85.62% (Best)\n",
      "Epoch 9/250 (MagnitudeL2-50.0%): Train Loss: 0.3212, Train Acc: 89.00%, Val Loss: 0.4315, Val Acc: 85.60%\n",
      "Epoch 10/250 (MagnitudeL2-50.0%): Train Loss: 0.3201, Train Acc: 89.06%, Val Loss: 0.4288, Val Acc: 85.76% (Best)\n",
      "Epoch 11/250 (MagnitudeL2-50.0%): Train Loss: 0.3114, Train Acc: 89.24%, Val Loss: 0.4265, Val Acc: 85.38%\n",
      "Epoch 12/250 (MagnitudeL2-50.0%): Train Loss: 0.3027, Train Acc: 89.49%, Val Loss: 0.4246, Val Acc: 85.98% (Best)\n",
      "Epoch 13/250 (MagnitudeL2-50.0%): Train Loss: 0.3010, Train Acc: 89.49%, Val Loss: 0.4204, Val Acc: 86.12% (Best)\n",
      "Epoch 14/250 (MagnitudeL2-50.0%): Train Loss: 0.3007, Train Acc: 89.70%, Val Loss: 0.4174, Val Acc: 85.98%\n",
      "Epoch 15/250 (MagnitudeL2-50.0%): Train Loss: 0.2960, Train Acc: 89.87%, Val Loss: 0.4201, Val Acc: 85.66%\n",
      "Epoch 16/250 (MagnitudeL2-50.0%): Train Loss: 0.2913, Train Acc: 89.84%, Val Loss: 0.4217, Val Acc: 85.98%\n",
      "Epoch 17/250 (MagnitudeL2-50.0%): Train Loss: 0.2909, Train Acc: 89.92%, Val Loss: 0.4151, Val Acc: 86.38% (Best)\n",
      "Epoch 18/250 (MagnitudeL2-50.0%): Train Loss: 0.2861, Train Acc: 90.22%, Val Loss: 0.4218, Val Acc: 86.26%\n",
      "Epoch 19/250 (MagnitudeL2-50.0%): Train Loss: 0.2836, Train Acc: 90.02%, Val Loss: 0.4241, Val Acc: 85.76%\n",
      "Epoch 20/250 (MagnitudeL2-50.0%): Train Loss: 0.2836, Train Acc: 90.21%, Val Loss: 0.4222, Val Acc: 85.94%\n",
      "Epoch 21/250 (MagnitudeL2-50.0%): Train Loss: 0.2847, Train Acc: 90.06%, Val Loss: 0.4139, Val Acc: 86.28%\n",
      "Epoch 22/250 (MagnitudeL2-50.0%): Train Loss: 0.2782, Train Acc: 90.33%, Val Loss: 0.4166, Val Acc: 86.26%\n",
      "Epoch 23/250 (MagnitudeL2-50.0%): Train Loss: 0.2768, Train Acc: 90.34%, Val Loss: 0.4170, Val Acc: 85.74%\n",
      "Epoch 24/250 (MagnitudeL2-50.0%): Train Loss: 0.2789, Train Acc: 90.27%, Val Loss: 0.4218, Val Acc: 85.92%\n",
      "Epoch 25/250 (MagnitudeL2-50.0%): Train Loss: 0.2774, Train Acc: 90.34%, Val Loss: 0.4182, Val Acc: 85.84%\n",
      "Epoch 26/250 (MagnitudeL2-50.0%): Train Loss: 0.2690, Train Acc: 90.69%, Val Loss: 0.4217, Val Acc: 85.96%\n",
      "Epoch 27/250 (MagnitudeL2-50.0%): Train Loss: 0.2665, Train Acc: 90.77%, Val Loss: 0.4214, Val Acc: 85.86%\n",
      "Epoch 28/250 (MagnitudeL2-50.0%): Train Loss: 0.2683, Train Acc: 90.70%, Val Loss: 0.4189, Val Acc: 85.94%\n",
      "Epoch 29/250 (MagnitudeL2-50.0%): Train Loss: 0.2670, Train Acc: 90.67%, Val Loss: 0.4165, Val Acc: 86.30%\n",
      "Epoch 30/250 (MagnitudeL2-50.0%): Train Loss: 0.2693, Train Acc: 90.56%, Val Loss: 0.4158, Val Acc: 86.04%\n",
      "Epoch 31/250 (MagnitudeL2-50.0%): Train Loss: 0.2628, Train Acc: 90.68%, Val Loss: 0.4265, Val Acc: 86.14%\n",
      "Epoch 32/250 (MagnitudeL2-50.0%): Train Loss: 0.2613, Train Acc: 90.91%, Val Loss: 0.4230, Val Acc: 85.94%\n",
      "Early stopping triggered after 32 epochs\n",
      "Loaded best model state with val accuracy: 86.38%\n",
      "Results: Accuracy=85.35%, MACs=5.30M\n",
      "âœ… Model saved to ./models_mobilenetv2_enhanced/magnitudel2_sparsity_0.5.pth\n",
      "âœ… ONNX model saved to ./models_mobilenetv2_enhanced/magnitudel2_sparsity_0.5.onnx\n",
      "\n",
      "Processing MagnitudeL2 at 70.0% sparsity...\n",
      "âœ… Created MobileNetV2 without pretrained weights\n",
      "âœ… Adapted classifier for 10 classes\n",
      "Initial MACs: 6.52M\n",
      "Applying MagnitudeImportance pruning at 70.0% sparsity...\n",
      "Final MACs: 4.87M (Reduction: 25.3%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/250 (MagnitudeL2-70.0%): Train Loss: 0.5857, Train Acc: 80.37%, Val Loss: 0.5524, Val Acc: 81.14% (Best)\n",
      "Epoch 2/250 (MagnitudeL2-70.0%): Train Loss: 0.4867, Train Acc: 83.36%, Val Loss: 0.5127, Val Acc: 82.40% (Best)\n",
      "Epoch 3/250 (MagnitudeL2-70.0%): Train Loss: 0.4485, Train Acc: 84.85%, Val Loss: 0.4943, Val Acc: 82.72% (Best)\n",
      "Epoch 4/250 (MagnitudeL2-70.0%): Train Loss: 0.4293, Train Acc: 85.17%, Val Loss: 0.4785, Val Acc: 83.52% (Best)\n",
      "Epoch 5/250 (MagnitudeL2-70.0%): Train Loss: 0.4066, Train Acc: 86.12%, Val Loss: 0.4701, Val Acc: 83.84% (Best)\n",
      "Epoch 6/250 (MagnitudeL2-70.0%): Train Loss: 0.3970, Train Acc: 86.46%, Val Loss: 0.4634, Val Acc: 83.78%\n",
      "Epoch 7/250 (MagnitudeL2-70.0%): Train Loss: 0.3807, Train Acc: 86.66%, Val Loss: 0.4559, Val Acc: 84.30% (Best)\n",
      "Epoch 8/250 (MagnitudeL2-70.0%): Train Loss: 0.3752, Train Acc: 87.04%, Val Loss: 0.4559, Val Acc: 84.44% (Best)\n",
      "Epoch 9/250 (MagnitudeL2-70.0%): Train Loss: 0.3674, Train Acc: 87.29%, Val Loss: 0.4470, Val Acc: 84.40%\n",
      "Epoch 10/250 (MagnitudeL2-70.0%): Train Loss: 0.3575, Train Acc: 87.76%, Val Loss: 0.4463, Val Acc: 84.58% (Best)\n",
      "Epoch 11/250 (MagnitudeL2-70.0%): Train Loss: 0.3582, Train Acc: 87.72%, Val Loss: 0.4462, Val Acc: 84.76% (Best)\n",
      "Epoch 12/250 (MagnitudeL2-70.0%): Train Loss: 0.3428, Train Acc: 88.15%, Val Loss: 0.4433, Val Acc: 84.84% (Best)\n",
      "Epoch 13/250 (MagnitudeL2-70.0%): Train Loss: 0.3413, Train Acc: 88.17%, Val Loss: 0.4348, Val Acc: 85.16% (Best)\n",
      "Epoch 14/250 (MagnitudeL2-70.0%): Train Loss: 0.3345, Train Acc: 88.50%, Val Loss: 0.4371, Val Acc: 85.18% (Best)\n",
      "Epoch 15/250 (MagnitudeL2-70.0%): Train Loss: 0.3328, Train Acc: 88.44%, Val Loss: 0.4450, Val Acc: 85.22% (Best)\n",
      "Epoch 16/250 (MagnitudeL2-70.0%): Train Loss: 0.3231, Train Acc: 88.77%, Val Loss: 0.4309, Val Acc: 85.88% (Best)\n",
      "Epoch 17/250 (MagnitudeL2-70.0%): Train Loss: 0.3284, Train Acc: 88.61%, Val Loss: 0.4366, Val Acc: 85.56%\n",
      "Epoch 18/250 (MagnitudeL2-70.0%): Train Loss: 0.3187, Train Acc: 88.85%, Val Loss: 0.4350, Val Acc: 85.66%\n",
      "Epoch 19/250 (MagnitudeL2-70.0%): Train Loss: 0.3219, Train Acc: 88.82%, Val Loss: 0.4267, Val Acc: 85.96% (Best)\n",
      "Epoch 20/250 (MagnitudeL2-70.0%): Train Loss: 0.3125, Train Acc: 89.27%, Val Loss: 0.4312, Val Acc: 85.86%\n",
      "Epoch 21/250 (MagnitudeL2-70.0%): Train Loss: 0.3097, Train Acc: 89.17%, Val Loss: 0.4289, Val Acc: 85.86%\n",
      "Epoch 22/250 (MagnitudeL2-70.0%): Train Loss: 0.3088, Train Acc: 89.27%, Val Loss: 0.4311, Val Acc: 85.84%\n",
      "Epoch 23/250 (MagnitudeL2-70.0%): Train Loss: 0.3073, Train Acc: 89.25%, Val Loss: 0.4278, Val Acc: 85.74%\n",
      "Epoch 24/250 (MagnitudeL2-70.0%): Train Loss: 0.3043, Train Acc: 89.30%, Val Loss: 0.4298, Val Acc: 85.30%\n",
      "Epoch 25/250 (MagnitudeL2-70.0%): Train Loss: 0.2972, Train Acc: 89.61%, Val Loss: 0.4253, Val Acc: 85.78%\n",
      "Epoch 26/250 (MagnitudeL2-70.0%): Train Loss: 0.2992, Train Acc: 89.57%, Val Loss: 0.4288, Val Acc: 85.54%\n",
      "Epoch 27/250 (MagnitudeL2-70.0%): Train Loss: 0.2968, Train Acc: 89.67%, Val Loss: 0.4314, Val Acc: 85.32%\n",
      "Epoch 28/250 (MagnitudeL2-70.0%): Train Loss: 0.2931, Train Acc: 89.99%, Val Loss: 0.4365, Val Acc: 85.46%\n",
      "Epoch 29/250 (MagnitudeL2-70.0%): Train Loss: 0.2907, Train Acc: 89.94%, Val Loss: 0.4307, Val Acc: 85.50%\n",
      "Epoch 30/250 (MagnitudeL2-70.0%): Train Loss: 0.2891, Train Acc: 89.80%, Val Loss: 0.4321, Val Acc: 85.24%\n",
      "Epoch 31/250 (MagnitudeL2-70.0%): Train Loss: 0.2863, Train Acc: 90.06%, Val Loss: 0.4272, Val Acc: 85.46%\n",
      "Epoch 32/250 (MagnitudeL2-70.0%): Train Loss: 0.2855, Train Acc: 90.09%, Val Loss: 0.4366, Val Acc: 85.18%\n",
      "Epoch 33/250 (MagnitudeL2-70.0%): Train Loss: 0.2849, Train Acc: 89.90%, Val Loss: 0.4286, Val Acc: 85.60%\n",
      "Epoch 34/250 (MagnitudeL2-70.0%): Train Loss: 0.2816, Train Acc: 90.20%, Val Loss: 0.4275, Val Acc: 85.66%\n",
      "Early stopping triggered after 34 epochs\n",
      "Loaded best model state with val accuracy: 85.96%\n",
      "Results: Accuracy=85.03%, MACs=4.87M\n",
      "âœ… Model saved to ./models_mobilenetv2_enhanced/magnitudel2_sparsity_0.7.pth\n",
      "âœ… ONNX model saved to ./models_mobilenetv2_enhanced/magnitudel2_sparsity_0.7.onnx\n",
      "\n",
      "--- Strategy: Random ---\n",
      "\n",
      "Processing Random at 20.0% sparsity...\n",
      "âœ… Created MobileNetV2 without pretrained weights\n",
      "âœ… Adapted classifier for 10 classes\n",
      "Initial MACs: 6.52M\n",
      "Applying RandomImportance pruning at 20.0% sparsity...\n",
      "Final MACs: 6.00M (Reduction: 8.0%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/250 (Random-20.0%): Train Loss: 0.4140, Train Acc: 85.92%, Val Loss: 0.4570, Val Acc: 84.50% (Best)\n",
      "Epoch 2/250 (Random-20.0%): Train Loss: 0.3585, Train Acc: 87.74%, Val Loss: 0.4395, Val Acc: 85.22% (Best)\n",
      "Epoch 3/250 (Random-20.0%): Train Loss: 0.3406, Train Acc: 88.37%, Val Loss: 0.4283, Val Acc: 85.66% (Best)\n",
      "Epoch 4/250 (Random-20.0%): Train Loss: 0.3298, Train Acc: 88.73%, Val Loss: 0.4294, Val Acc: 85.50%\n",
      "Epoch 5/250 (Random-20.0%): Train Loss: 0.3162, Train Acc: 89.24%, Val Loss: 0.4270, Val Acc: 85.52%\n",
      "Epoch 6/250 (Random-20.0%): Train Loss: 0.3080, Train Acc: 89.45%, Val Loss: 0.4192, Val Acc: 86.06% (Best)\n",
      "Epoch 7/250 (Random-20.0%): Train Loss: 0.3071, Train Acc: 89.51%, Val Loss: 0.4150, Val Acc: 86.00%\n",
      "Epoch 8/250 (Random-20.0%): Train Loss: 0.2994, Train Acc: 89.82%, Val Loss: 0.4128, Val Acc: 86.02%\n",
      "Epoch 9/250 (Random-20.0%): Train Loss: 0.2974, Train Acc: 89.66%, Val Loss: 0.4178, Val Acc: 85.90%\n",
      "Epoch 10/250 (Random-20.0%): Train Loss: 0.2882, Train Acc: 90.08%, Val Loss: 0.4191, Val Acc: 85.88%\n",
      "Epoch 11/250 (Random-20.0%): Train Loss: 0.2844, Train Acc: 90.18%, Val Loss: 0.4191, Val Acc: 86.00%\n",
      "Epoch 12/250 (Random-20.0%): Train Loss: 0.2829, Train Acc: 90.21%, Val Loss: 0.4175, Val Acc: 85.94%\n",
      "Epoch 13/250 (Random-20.0%): Train Loss: 0.2787, Train Acc: 90.42%, Val Loss: 0.4210, Val Acc: 86.10% (Best)\n",
      "Epoch 14/250 (Random-20.0%): Train Loss: 0.2771, Train Acc: 90.49%, Val Loss: 0.4116, Val Acc: 85.96%\n",
      "Epoch 15/250 (Random-20.0%): Train Loss: 0.2758, Train Acc: 90.66%, Val Loss: 0.4181, Val Acc: 86.04%\n",
      "Epoch 16/250 (Random-20.0%): Train Loss: 0.2703, Train Acc: 90.73%, Val Loss: 0.4142, Val Acc: 86.06%\n",
      "Epoch 17/250 (Random-20.0%): Train Loss: 0.2700, Train Acc: 90.75%, Val Loss: 0.4209, Val Acc: 85.80%\n",
      "Epoch 18/250 (Random-20.0%): Train Loss: 0.2687, Train Acc: 90.73%, Val Loss: 0.4093, Val Acc: 86.40% (Best)\n",
      "Epoch 19/250 (Random-20.0%): Train Loss: 0.2661, Train Acc: 90.70%, Val Loss: 0.4088, Val Acc: 86.66% (Best)\n",
      "Epoch 20/250 (Random-20.0%): Train Loss: 0.2626, Train Acc: 90.94%, Val Loss: 0.4096, Val Acc: 86.18%\n",
      "Epoch 21/250 (Random-20.0%): Train Loss: 0.2582, Train Acc: 90.96%, Val Loss: 0.4136, Val Acc: 86.62%\n",
      "Epoch 22/250 (Random-20.0%): Train Loss: 0.2566, Train Acc: 91.09%, Val Loss: 0.4157, Val Acc: 86.24%\n",
      "Epoch 23/250 (Random-20.0%): Train Loss: 0.2568, Train Acc: 91.16%, Val Loss: 0.4167, Val Acc: 86.38%\n",
      "Epoch 24/250 (Random-20.0%): Train Loss: 0.2548, Train Acc: 91.12%, Val Loss: 0.4211, Val Acc: 85.88%\n",
      "Epoch 25/250 (Random-20.0%): Train Loss: 0.2520, Train Acc: 91.35%, Val Loss: 0.4171, Val Acc: 86.18%\n",
      "Epoch 26/250 (Random-20.0%): Train Loss: 0.2541, Train Acc: 91.28%, Val Loss: 0.4190, Val Acc: 86.04%\n",
      "Epoch 27/250 (Random-20.0%): Train Loss: 0.2483, Train Acc: 91.34%, Val Loss: 0.4218, Val Acc: 86.08%\n",
      "Epoch 28/250 (Random-20.0%): Train Loss: 0.2506, Train Acc: 91.20%, Val Loss: 0.4166, Val Acc: 86.50%\n",
      "Epoch 29/250 (Random-20.0%): Train Loss: 0.2456, Train Acc: 91.42%, Val Loss: 0.4245, Val Acc: 86.22%\n",
      "Epoch 30/250 (Random-20.0%): Train Loss: 0.2504, Train Acc: 91.29%, Val Loss: 0.4212, Val Acc: 86.20%\n",
      "Epoch 31/250 (Random-20.0%): Train Loss: 0.2467, Train Acc: 91.38%, Val Loss: 0.4210, Val Acc: 86.18%\n",
      "Epoch 32/250 (Random-20.0%): Train Loss: 0.2452, Train Acc: 91.49%, Val Loss: 0.4254, Val Acc: 86.32%\n",
      "Epoch 33/250 (Random-20.0%): Train Loss: 0.2407, Train Acc: 91.60%, Val Loss: 0.4250, Val Acc: 86.14%\n",
      "Epoch 34/250 (Random-20.0%): Train Loss: 0.2404, Train Acc: 91.67%, Val Loss: 0.4259, Val Acc: 86.22%\n",
      "Early stopping triggered after 34 epochs\n",
      "Loaded best model state with val accuracy: 86.66%\n",
      "Results: Accuracy=85.89%, MACs=6.00M\n",
      "âœ… Model saved to ./models_mobilenetv2_enhanced/random_sparsity_0.2.pth\n",
      "âœ… ONNX model saved to ./models_mobilenetv2_enhanced/random_sparsity_0.2.onnx\n",
      "\n",
      "Processing Random at 50.0% sparsity...\n",
      "âœ… Created MobileNetV2 without pretrained weights\n",
      "âœ… Adapted classifier for 10 classes\n",
      "Initial MACs: 6.52M\n",
      "Applying RandomImportance pruning at 50.0% sparsity...\n",
      "Final MACs: 5.30M (Reduction: 18.7%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/250 (Random-50.0%): Train Loss: 0.8136, Train Acc: 72.84%, Val Loss: 0.6674, Val Acc: 77.12% (Best)\n",
      "Epoch 2/250 (Random-50.0%): Train Loss: 0.6218, Train Acc: 79.06%, Val Loss: 0.5949, Val Acc: 79.28% (Best)\n",
      "Epoch 3/250 (Random-50.0%): Train Loss: 0.5560, Train Acc: 80.97%, Val Loss: 0.5596, Val Acc: 80.78% (Best)\n",
      "Epoch 4/250 (Random-50.0%): Train Loss: 0.5204, Train Acc: 82.21%, Val Loss: 0.5364, Val Acc: 81.46% (Best)\n",
      "Epoch 5/250 (Random-50.0%): Train Loss: 0.5017, Train Acc: 82.93%, Val Loss: 0.5239, Val Acc: 81.80% (Best)\n",
      "Epoch 6/250 (Random-50.0%): Train Loss: 0.4711, Train Acc: 83.91%, Val Loss: 0.5094, Val Acc: 82.40% (Best)\n",
      "Epoch 7/250 (Random-50.0%): Train Loss: 0.4606, Train Acc: 84.11%, Val Loss: 0.5021, Val Acc: 82.66% (Best)\n",
      "Epoch 8/250 (Random-50.0%): Train Loss: 0.4446, Train Acc: 84.83%, Val Loss: 0.4967, Val Acc: 83.04% (Best)\n",
      "Epoch 9/250 (Random-50.0%): Train Loss: 0.4334, Train Acc: 85.16%, Val Loss: 0.4898, Val Acc: 83.06% (Best)\n",
      "Epoch 10/250 (Random-50.0%): Train Loss: 0.4171, Train Acc: 85.57%, Val Loss: 0.4889, Val Acc: 83.16% (Best)\n",
      "Epoch 11/250 (Random-50.0%): Train Loss: 0.4097, Train Acc: 85.79%, Val Loss: 0.4800, Val Acc: 83.46% (Best)\n",
      "Epoch 12/250 (Random-50.0%): Train Loss: 0.4034, Train Acc: 86.18%, Val Loss: 0.4739, Val Acc: 83.62% (Best)\n",
      "Epoch 13/250 (Random-50.0%): Train Loss: 0.3919, Train Acc: 86.33%, Val Loss: 0.4791, Val Acc: 83.64% (Best)\n",
      "Epoch 14/250 (Random-50.0%): Train Loss: 0.3923, Train Acc: 86.53%, Val Loss: 0.4733, Val Acc: 83.80% (Best)\n",
      "Epoch 15/250 (Random-50.0%): Train Loss: 0.3796, Train Acc: 86.81%, Val Loss: 0.4759, Val Acc: 83.86% (Best)\n",
      "Epoch 16/250 (Random-50.0%): Train Loss: 0.3764, Train Acc: 87.09%, Val Loss: 0.4701, Val Acc: 84.24% (Best)\n",
      "Epoch 17/250 (Random-50.0%): Train Loss: 0.3772, Train Acc: 86.94%, Val Loss: 0.4642, Val Acc: 84.08%\n",
      "Epoch 18/250 (Random-50.0%): Train Loss: 0.3671, Train Acc: 87.25%, Val Loss: 0.4608, Val Acc: 84.12%\n",
      "Epoch 19/250 (Random-50.0%): Train Loss: 0.3611, Train Acc: 87.40%, Val Loss: 0.4609, Val Acc: 84.34% (Best)\n",
      "Epoch 20/250 (Random-50.0%): Train Loss: 0.3632, Train Acc: 87.39%, Val Loss: 0.4667, Val Acc: 84.20%\n",
      "Epoch 21/250 (Random-50.0%): Train Loss: 0.3540, Train Acc: 87.79%, Val Loss: 0.4645, Val Acc: 84.38% (Best)\n",
      "Epoch 22/250 (Random-50.0%): Train Loss: 0.3519, Train Acc: 87.84%, Val Loss: 0.4536, Val Acc: 84.78% (Best)\n",
      "Epoch 23/250 (Random-50.0%): Train Loss: 0.3499, Train Acc: 87.90%, Val Loss: 0.4509, Val Acc: 84.78%\n",
      "Epoch 24/250 (Random-50.0%): Train Loss: 0.3430, Train Acc: 88.10%, Val Loss: 0.4513, Val Acc: 84.88% (Best)\n",
      "Epoch 25/250 (Random-50.0%): Train Loss: 0.3368, Train Acc: 88.28%, Val Loss: 0.4572, Val Acc: 84.48%\n",
      "Epoch 26/250 (Random-50.0%): Train Loss: 0.3396, Train Acc: 88.18%, Val Loss: 0.4523, Val Acc: 84.56%\n",
      "Epoch 27/250 (Random-50.0%): Train Loss: 0.3390, Train Acc: 88.36%, Val Loss: 0.4544, Val Acc: 84.28%\n",
      "Epoch 28/250 (Random-50.0%): Train Loss: 0.3343, Train Acc: 88.28%, Val Loss: 0.4435, Val Acc: 84.76%\n",
      "Epoch 29/250 (Random-50.0%): Train Loss: 0.3276, Train Acc: 88.60%, Val Loss: 0.4585, Val Acc: 84.70%\n",
      "Epoch 30/250 (Random-50.0%): Train Loss: 0.3276, Train Acc: 88.61%, Val Loss: 0.4534, Val Acc: 84.60%\n",
      "Epoch 31/250 (Random-50.0%): Train Loss: 0.3247, Train Acc: 88.54%, Val Loss: 0.4444, Val Acc: 84.86%\n",
      "Epoch 32/250 (Random-50.0%): Train Loss: 0.3241, Train Acc: 88.86%, Val Loss: 0.4478, Val Acc: 84.88%\n",
      "Epoch 33/250 (Random-50.0%): Train Loss: 0.3203, Train Acc: 89.01%, Val Loss: 0.4439, Val Acc: 85.26% (Best)\n",
      "Epoch 34/250 (Random-50.0%): Train Loss: 0.3166, Train Acc: 88.92%, Val Loss: 0.4409, Val Acc: 85.12%\n",
      "Epoch 35/250 (Random-50.0%): Train Loss: 0.3164, Train Acc: 88.96%, Val Loss: 0.4472, Val Acc: 85.00%\n",
      "Epoch 36/250 (Random-50.0%): Train Loss: 0.3151, Train Acc: 88.97%, Val Loss: 0.4459, Val Acc: 85.26%\n",
      "Epoch 37/250 (Random-50.0%): Train Loss: 0.3155, Train Acc: 89.12%, Val Loss: 0.4427, Val Acc: 84.76%\n",
      "Epoch 38/250 (Random-50.0%): Train Loss: 0.3118, Train Acc: 89.21%, Val Loss: 0.4491, Val Acc: 84.80%\n",
      "Epoch 39/250 (Random-50.0%): Train Loss: 0.3084, Train Acc: 89.22%, Val Loss: 0.4458, Val Acc: 84.76%\n",
      "Epoch 40/250 (Random-50.0%): Train Loss: 0.3099, Train Acc: 89.22%, Val Loss: 0.4424, Val Acc: 85.06%\n",
      "Epoch 41/250 (Random-50.0%): Train Loss: 0.3024, Train Acc: 89.57%, Val Loss: 0.4496, Val Acc: 84.92%\n",
      "Epoch 42/250 (Random-50.0%): Train Loss: 0.3002, Train Acc: 89.48%, Val Loss: 0.4470, Val Acc: 84.48%\n",
      "Epoch 43/250 (Random-50.0%): Train Loss: 0.2992, Train Acc: 89.53%, Val Loss: 0.4471, Val Acc: 84.90%\n",
      "Epoch 44/250 (Random-50.0%): Train Loss: 0.2985, Train Acc: 89.90%, Val Loss: 0.4453, Val Acc: 84.68%\n",
      "Epoch 45/250 (Random-50.0%): Train Loss: 0.2977, Train Acc: 89.71%, Val Loss: 0.4496, Val Acc: 84.60%\n",
      "Epoch 46/250 (Random-50.0%): Train Loss: 0.2985, Train Acc: 89.55%, Val Loss: 0.4505, Val Acc: 85.02%\n",
      "Epoch 47/250 (Random-50.0%): Train Loss: 0.2914, Train Acc: 89.82%, Val Loss: 0.4482, Val Acc: 84.90%\n",
      "Epoch 48/250 (Random-50.0%): Train Loss: 0.2926, Train Acc: 89.70%, Val Loss: 0.4465, Val Acc: 84.80%\n",
      "Early stopping triggered after 48 epochs\n",
      "Loaded best model state with val accuracy: 85.26%\n",
      "Results: Accuracy=84.72%, MACs=5.30M\n",
      "âœ… Model saved to ./models_mobilenetv2_enhanced/random_sparsity_0.5.pth\n",
      "âœ… ONNX model saved to ./models_mobilenetv2_enhanced/random_sparsity_0.5.onnx\n",
      "\n",
      "Processing Random at 70.0% sparsity...\n",
      "âœ… Created MobileNetV2 without pretrained weights\n",
      "âœ… Adapted classifier for 10 classes\n",
      "Initial MACs: 6.52M\n",
      "Applying RandomImportance pruning at 70.0% sparsity...\n",
      "Final MACs: 4.87M (Reduction: 25.3%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/250 (Random-70.0%): Train Loss: 0.9701, Train Acc: 67.44%, Val Loss: 0.7530, Val Acc: 73.96% (Best)\n",
      "Epoch 2/250 (Random-70.0%): Train Loss: 0.7290, Train Acc: 75.69%, Val Loss: 0.6571, Val Acc: 77.26% (Best)\n",
      "Epoch 3/250 (Random-70.0%): Train Loss: 0.6449, Train Acc: 78.06%, Val Loss: 0.6105, Val Acc: 79.10% (Best)\n",
      "Epoch 4/250 (Random-70.0%): Train Loss: 0.5991, Train Acc: 79.78%, Val Loss: 0.5880, Val Acc: 79.74% (Best)\n",
      "Epoch 5/250 (Random-70.0%): Train Loss: 0.5664, Train Acc: 80.81%, Val Loss: 0.5670, Val Acc: 80.26% (Best)\n",
      "Epoch 6/250 (Random-70.0%): Train Loss: 0.5398, Train Acc: 81.60%, Val Loss: 0.5615, Val Acc: 80.78% (Best)\n",
      "Epoch 7/250 (Random-70.0%): Train Loss: 0.5264, Train Acc: 82.05%, Val Loss: 0.5447, Val Acc: 81.20% (Best)\n",
      "Epoch 8/250 (Random-70.0%): Train Loss: 0.5017, Train Acc: 82.60%, Val Loss: 0.5336, Val Acc: 81.44% (Best)\n",
      "Epoch 9/250 (Random-70.0%): Train Loss: 0.4933, Train Acc: 82.97%, Val Loss: 0.5241, Val Acc: 81.74% (Best)\n",
      "Epoch 10/250 (Random-70.0%): Train Loss: 0.4744, Train Acc: 83.74%, Val Loss: 0.5225, Val Acc: 82.06% (Best)\n",
      "Epoch 11/250 (Random-70.0%): Train Loss: 0.4658, Train Acc: 83.98%, Val Loss: 0.5119, Val Acc: 82.02%\n",
      "Epoch 12/250 (Random-70.0%): Train Loss: 0.4582, Train Acc: 84.30%, Val Loss: 0.5106, Val Acc: 82.40% (Best)\n",
      "Epoch 13/250 (Random-70.0%): Train Loss: 0.4492, Train Acc: 84.52%, Val Loss: 0.5068, Val Acc: 82.18%\n",
      "Epoch 14/250 (Random-70.0%): Train Loss: 0.4405, Train Acc: 84.80%, Val Loss: 0.4981, Val Acc: 82.80% (Best)\n",
      "Epoch 15/250 (Random-70.0%): Train Loss: 0.4328, Train Acc: 85.07%, Val Loss: 0.5023, Val Acc: 82.94% (Best)\n",
      "Epoch 16/250 (Random-70.0%): Train Loss: 0.4271, Train Acc: 85.25%, Val Loss: 0.4922, Val Acc: 82.98% (Best)\n",
      "Epoch 17/250 (Random-70.0%): Train Loss: 0.4226, Train Acc: 85.45%, Val Loss: 0.4861, Val Acc: 83.36% (Best)\n",
      "Epoch 18/250 (Random-70.0%): Train Loss: 0.4138, Train Acc: 85.64%, Val Loss: 0.4893, Val Acc: 83.20%\n",
      "Epoch 19/250 (Random-70.0%): Train Loss: 0.4046, Train Acc: 86.17%, Val Loss: 0.4815, Val Acc: 83.54% (Best)\n",
      "Epoch 20/250 (Random-70.0%): Train Loss: 0.4012, Train Acc: 86.14%, Val Loss: 0.4850, Val Acc: 83.02%\n",
      "Epoch 21/250 (Random-70.0%): Train Loss: 0.3996, Train Acc: 86.15%, Val Loss: 0.4761, Val Acc: 83.42%\n",
      "Epoch 22/250 (Random-70.0%): Train Loss: 0.3910, Train Acc: 86.62%, Val Loss: 0.4801, Val Acc: 83.52%\n",
      "Epoch 23/250 (Random-70.0%): Train Loss: 0.3885, Train Acc: 86.66%, Val Loss: 0.4835, Val Acc: 83.78% (Best)\n",
      "Epoch 24/250 (Random-70.0%): Train Loss: 0.3843, Train Acc: 86.80%, Val Loss: 0.4802, Val Acc: 83.82% (Best)\n",
      "Epoch 25/250 (Random-70.0%): Train Loss: 0.3799, Train Acc: 86.87%, Val Loss: 0.4777, Val Acc: 83.58%\n",
      "Epoch 26/250 (Random-70.0%): Train Loss: 0.3792, Train Acc: 86.86%, Val Loss: 0.4724, Val Acc: 84.20% (Best)\n",
      "Epoch 27/250 (Random-70.0%): Train Loss: 0.3707, Train Acc: 87.07%, Val Loss: 0.4716, Val Acc: 83.80%\n",
      "Epoch 28/250 (Random-70.0%): Train Loss: 0.3693, Train Acc: 87.15%, Val Loss: 0.4730, Val Acc: 84.06%\n",
      "Epoch 29/250 (Random-70.0%): Train Loss: 0.3655, Train Acc: 87.27%, Val Loss: 0.4671, Val Acc: 84.04%\n",
      "Epoch 30/250 (Random-70.0%): Train Loss: 0.3598, Train Acc: 87.48%, Val Loss: 0.4725, Val Acc: 83.98%\n",
      "Epoch 31/250 (Random-70.0%): Train Loss: 0.3617, Train Acc: 87.42%, Val Loss: 0.4737, Val Acc: 84.14%\n",
      "Epoch 32/250 (Random-70.0%): Train Loss: 0.3538, Train Acc: 87.69%, Val Loss: 0.4753, Val Acc: 84.14%\n",
      "Epoch 33/250 (Random-70.0%): Train Loss: 0.3572, Train Acc: 87.54%, Val Loss: 0.4714, Val Acc: 84.18%\n",
      "Epoch 34/250 (Random-70.0%): Train Loss: 0.3500, Train Acc: 87.87%, Val Loss: 0.4722, Val Acc: 84.30% (Best)\n",
      "Epoch 35/250 (Random-70.0%): Train Loss: 0.3465, Train Acc: 87.80%, Val Loss: 0.4714, Val Acc: 84.18%\n",
      "Epoch 36/250 (Random-70.0%): Train Loss: 0.3481, Train Acc: 87.83%, Val Loss: 0.4638, Val Acc: 84.28%\n",
      "Epoch 37/250 (Random-70.0%): Train Loss: 0.3380, Train Acc: 88.31%, Val Loss: 0.4663, Val Acc: 84.42% (Best)\n",
      "Epoch 38/250 (Random-70.0%): Train Loss: 0.3448, Train Acc: 87.95%, Val Loss: 0.4662, Val Acc: 84.42%\n",
      "Epoch 39/250 (Random-70.0%): Train Loss: 0.3391, Train Acc: 88.13%, Val Loss: 0.4695, Val Acc: 84.52% (Best)\n",
      "Epoch 40/250 (Random-70.0%): Train Loss: 0.3381, Train Acc: 88.15%, Val Loss: 0.4623, Val Acc: 84.38%\n",
      "Epoch 41/250 (Random-70.0%): Train Loss: 0.3390, Train Acc: 88.19%, Val Loss: 0.4643, Val Acc: 84.38%\n",
      "Epoch 42/250 (Random-70.0%): Train Loss: 0.3322, Train Acc: 88.63%, Val Loss: 0.4672, Val Acc: 84.20%\n",
      "Epoch 43/250 (Random-70.0%): Train Loss: 0.3305, Train Acc: 88.59%, Val Loss: 0.4683, Val Acc: 84.32%\n",
      "Epoch 44/250 (Random-70.0%): Train Loss: 0.3297, Train Acc: 88.48%, Val Loss: 0.4621, Val Acc: 84.68% (Best)\n",
      "Epoch 45/250 (Random-70.0%): Train Loss: 0.3265, Train Acc: 88.55%, Val Loss: 0.4619, Val Acc: 84.74% (Best)\n",
      "Epoch 46/250 (Random-70.0%): Train Loss: 0.3264, Train Acc: 88.65%, Val Loss: 0.4635, Val Acc: 84.92% (Best)\n",
      "Epoch 47/250 (Random-70.0%): Train Loss: 0.3257, Train Acc: 88.81%, Val Loss: 0.4688, Val Acc: 84.46%\n",
      "Epoch 48/250 (Random-70.0%): Train Loss: 0.3207, Train Acc: 88.76%, Val Loss: 0.4666, Val Acc: 84.42%\n",
      "Epoch 49/250 (Random-70.0%): Train Loss: 0.3222, Train Acc: 88.53%, Val Loss: 0.4684, Val Acc: 84.62%\n",
      "Epoch 50/250 (Random-70.0%): Train Loss: 0.3176, Train Acc: 88.86%, Val Loss: 0.4565, Val Acc: 84.72%\n",
      "Epoch 51/250 (Random-70.0%): Train Loss: 0.3198, Train Acc: 88.93%, Val Loss: 0.4648, Val Acc: 84.86%\n",
      "Epoch 52/250 (Random-70.0%): Train Loss: 0.3188, Train Acc: 88.94%, Val Loss: 0.4598, Val Acc: 84.72%\n",
      "Epoch 53/250 (Random-70.0%): Train Loss: 0.3113, Train Acc: 89.26%, Val Loss: 0.4629, Val Acc: 85.04% (Best)\n",
      "Epoch 54/250 (Random-70.0%): Train Loss: 0.3125, Train Acc: 88.99%, Val Loss: 0.4584, Val Acc: 84.70%\n",
      "Epoch 55/250 (Random-70.0%): Train Loss: 0.3090, Train Acc: 89.23%, Val Loss: 0.4631, Val Acc: 84.82%\n",
      "Epoch 56/250 (Random-70.0%): Train Loss: 0.3050, Train Acc: 89.30%, Val Loss: 0.4619, Val Acc: 84.70%\n",
      "Epoch 57/250 (Random-70.0%): Train Loss: 0.3023, Train Acc: 89.45%, Val Loss: 0.4598, Val Acc: 84.96%\n",
      "Epoch 58/250 (Random-70.0%): Train Loss: 0.3057, Train Acc: 89.14%, Val Loss: 0.4623, Val Acc: 85.12% (Best)\n",
      "Epoch 59/250 (Random-70.0%): Train Loss: 0.3071, Train Acc: 89.22%, Val Loss: 0.4579, Val Acc: 84.90%\n",
      "Epoch 60/250 (Random-70.0%): Train Loss: 0.3018, Train Acc: 89.43%, Val Loss: 0.4557, Val Acc: 84.96%\n",
      "Epoch 61/250 (Random-70.0%): Train Loss: 0.3102, Train Acc: 89.23%, Val Loss: 0.4667, Val Acc: 84.80%\n",
      "Epoch 62/250 (Random-70.0%): Train Loss: 0.3024, Train Acc: 89.40%, Val Loss: 0.4607, Val Acc: 85.22% (Best)\n",
      "Epoch 63/250 (Random-70.0%): Train Loss: 0.2982, Train Acc: 89.54%, Val Loss: 0.4625, Val Acc: 85.02%\n",
      "Epoch 64/250 (Random-70.0%): Train Loss: 0.2977, Train Acc: 89.52%, Val Loss: 0.4566, Val Acc: 85.32% (Best)\n",
      "Epoch 65/250 (Random-70.0%): Train Loss: 0.2967, Train Acc: 89.63%, Val Loss: 0.4632, Val Acc: 84.92%\n",
      "Epoch 66/250 (Random-70.0%): Train Loss: 0.2940, Train Acc: 89.74%, Val Loss: 0.4629, Val Acc: 85.26%\n",
      "Epoch 67/250 (Random-70.0%): Train Loss: 0.2926, Train Acc: 89.76%, Val Loss: 0.4578, Val Acc: 85.14%\n",
      "Epoch 68/250 (Random-70.0%): Train Loss: 0.2939, Train Acc: 89.78%, Val Loss: 0.4644, Val Acc: 84.88%\n",
      "Epoch 69/250 (Random-70.0%): Train Loss: 0.2945, Train Acc: 89.62%, Val Loss: 0.4608, Val Acc: 85.28%\n",
      "Epoch 70/250 (Random-70.0%): Train Loss: 0.2906, Train Acc: 89.82%, Val Loss: 0.4638, Val Acc: 85.18%\n",
      "Epoch 71/250 (Random-70.0%): Train Loss: 0.2911, Train Acc: 89.78%, Val Loss: 0.4576, Val Acc: 85.22%\n",
      "Epoch 72/250 (Random-70.0%): Train Loss: 0.2904, Train Acc: 89.65%, Val Loss: 0.4657, Val Acc: 84.82%\n",
      "Epoch 73/250 (Random-70.0%): Train Loss: 0.2838, Train Acc: 90.16%, Val Loss: 0.4691, Val Acc: 84.58%\n",
      "Epoch 74/250 (Random-70.0%): Train Loss: 0.2908, Train Acc: 89.91%, Val Loss: 0.4645, Val Acc: 84.84%\n",
      "Epoch 75/250 (Random-70.0%): Train Loss: 0.2820, Train Acc: 90.17%, Val Loss: 0.4737, Val Acc: 85.06%\n",
      "Epoch 76/250 (Random-70.0%): Train Loss: 0.2851, Train Acc: 90.10%, Val Loss: 0.4645, Val Acc: 85.30%\n",
      "Epoch 77/250 (Random-70.0%): Train Loss: 0.2857, Train Acc: 89.92%, Val Loss: 0.4678, Val Acc: 85.42% (Best)\n",
      "Epoch 78/250 (Random-70.0%): Train Loss: 0.2809, Train Acc: 90.05%, Val Loss: 0.4620, Val Acc: 85.30%\n",
      "Epoch 79/250 (Random-70.0%): Train Loss: 0.2804, Train Acc: 90.23%, Val Loss: 0.4638, Val Acc: 85.24%\n",
      "Epoch 80/250 (Random-70.0%): Train Loss: 0.2826, Train Acc: 90.07%, Val Loss: 0.4636, Val Acc: 85.02%\n",
      "Epoch 81/250 (Random-70.0%): Train Loss: 0.2836, Train Acc: 90.10%, Val Loss: 0.4663, Val Acc: 84.86%\n",
      "Epoch 82/250 (Random-70.0%): Train Loss: 0.2741, Train Acc: 90.45%, Val Loss: 0.4633, Val Acc: 85.34%\n",
      "Epoch 83/250 (Random-70.0%): Train Loss: 0.2782, Train Acc: 90.27%, Val Loss: 0.4599, Val Acc: 85.30%\n",
      "Epoch 84/250 (Random-70.0%): Train Loss: 0.2758, Train Acc: 90.44%, Val Loss: 0.4610, Val Acc: 85.34%\n",
      "Epoch 85/250 (Random-70.0%): Train Loss: 0.2741, Train Acc: 90.48%, Val Loss: 0.4684, Val Acc: 85.06%\n",
      "Epoch 86/250 (Random-70.0%): Train Loss: 0.2733, Train Acc: 90.36%, Val Loss: 0.4690, Val Acc: 84.96%\n",
      "Epoch 87/250 (Random-70.0%): Train Loss: 0.2712, Train Acc: 90.59%, Val Loss: 0.4746, Val Acc: 85.10%\n",
      "Epoch 88/250 (Random-70.0%): Train Loss: 0.2709, Train Acc: 90.44%, Val Loss: 0.4691, Val Acc: 85.18%\n",
      "Epoch 89/250 (Random-70.0%): Train Loss: 0.2681, Train Acc: 90.52%, Val Loss: 0.4779, Val Acc: 85.30%\n",
      "Epoch 90/250 (Random-70.0%): Train Loss: 0.2623, Train Acc: 90.62%, Val Loss: 0.4682, Val Acc: 85.02%\n",
      "Epoch 91/250 (Random-70.0%): Train Loss: 0.2734, Train Acc: 90.46%, Val Loss: 0.4632, Val Acc: 85.18%\n",
      "Epoch 92/250 (Random-70.0%): Train Loss: 0.2627, Train Acc: 90.77%, Val Loss: 0.4708, Val Acc: 85.04%\n",
      "Early stopping triggered after 92 epochs\n",
      "Loaded best model state with val accuracy: 85.42%\n",
      "Results: Accuracy=84.69%, MACs=4.87M\n",
      "âœ… Model saved to ./models_mobilenetv2_enhanced/random_sparsity_0.7.pth\n",
      "âœ… ONNX model saved to ./models_mobilenetv2_enhanced/random_sparsity_0.7.onnx\n",
      "\n",
      "Saving results...\n",
      "âœ… Complete results saved to ./results_mobilenetv2_cifar10_enhanced/complete_results.json\n",
      "âœ… Summary results saved to ./results_mobilenetv2_cifar10_enhanced/summary_results.csv\n",
      "Creating plots...\n",
      "âœ… Accuracy plot saved to ./results_mobilenetv2_cifar10_enhanced/accuracy_vs_sparsity.png\n",
      "âœ… Efficiency frontier plot saved to ./results_mobilenetv2_cifar10_enhanced/efficiency_frontier.png\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENTAL RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Baseline Performance:\n",
      "  Accuracy: 85.20%\n",
      "  MACs: 6.52M\n",
      "  Parameters: 2.24M\n",
      "  Model Size: 8.53MB\n",
      "\n",
      "Strategy Comparison at 50% Sparsity:\n",
      "       BNScale:  85.74% accuracy (-0.54%, 100.6% retention)\n",
      "   MagnitudeL2:  85.35% accuracy (-0.15%, 100.2% retention)\n",
      "        Random:  84.72% accuracy (+0.48%,  99.4% retention)\n",
      "\n",
      "Complete Results Table:\n",
      "--------------------------------------------------------------------------------\n",
      "Strategy     Sparsity Accuracy MACs(M)  Params(M) Size(MB)\n",
      "--------------------------------------------------------------------------------\n",
      "BNScale           0%   85.20%    6.52     2.24    8.53\n",
      "BNScale          20%   85.59%    6.00     2.06    7.85\n",
      "BNScale          50%   85.74%    5.30     1.82    6.93\n",
      "BNScale          70%   85.45%    4.87     1.66    6.34\n",
      "MagnitudeL2       0%   85.20%    6.52     2.24    8.53\n",
      "MagnitudeL2      20%   85.95%    6.00     2.06    7.85\n",
      "MagnitudeL2      50%   85.35%    5.30     1.82    6.93\n",
      "MagnitudeL2      70%   85.03%    4.87     1.66    6.34\n",
      "Random            0%   85.20%    6.52     2.24    8.53\n",
      "Random           20%   85.89%    6.00     2.06    7.85\n",
      "Random           50%   84.72%    5.30     1.82    6.93\n",
      "Random           70%   84.69%    4.87     1.66    6.34\n",
      "\n",
      "ðŸŽ‰ All experiments completed!\n",
      "ðŸ“ Results saved to: /home/muis/thesis/github-repo/master-thesis/cnn/mobile_net_v2/results_mobilenetv2_cifar10_enhanced\n",
      "ðŸ“ Models saved to: /home/muis/thesis/github-repo/master-thesis/cnn/mobile_net_v2/models_mobilenetv2_enhanced\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
