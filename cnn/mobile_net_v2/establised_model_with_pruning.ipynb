{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-23T09:41:39.149070Z",
     "start_time": "2025-05-23T09:41:37.765370Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_pruning as tp\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T09:41:39.196633Z",
     "start_time": "2025-05-23T09:41:39.158768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_BASE_NAME = \"mobilenet_v2-b0353104\""
   ],
   "id": "704546a9c7c93888",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Loaders (Using your provided function structure)",
   "id": "10ee584e0136f444"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T09:41:39.261620Z",
     "start_time": "2025-05-23T09:41:39.256272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_data_loaders(data_dir_path, batch_size=64, val_split=0.1, seed=42):\n",
    "    abs_data_dir = os.path.abspath(data_dir_path)\n",
    "    print(f\"Attempting to load CIFAR-10 from pre-downloaded directory: {abs_data_dir}\")\n",
    "    expected_cifar_folder = os.path.join(abs_data_dir, 'cifar-10-batches-py')\n",
    "    if not os.path.exists(expected_cifar_folder):\n",
    "        print(f\"ERROR: Expected CIFAR-10 data folder '{expected_cifar_folder}' not found!\")\n",
    "        raise FileNotFoundError(f\"CIFAR-10 data not found at {expected_cifar_folder}\")\n",
    "\n",
    "    transform_cifar = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "    try:\n",
    "        full_train_dataset = torchvision.datasets.CIFAR10(root=abs_data_dir, train=True, download=False, transform=transform_cifar)\n",
    "        test_dataset = torchvision.datasets.CIFAR10(root=abs_data_dir, train=False, download=False, transform=transform_cifar)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"ERROR: Failed to load CIFAR-10 from {abs_data_dir}. Error: {e}\"); raise\n",
    "    val_size = int(len(full_train_dataset) * val_split); train_size = len(full_train_dataset) - val_size\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(full_train_dataset, [train_size, val_size], generator=generator)\n",
    "    num_avail_cpus = len(os.sched_getaffinity(0)) if hasattr(os, 'sched_getaffinity') else os.cpu_count()\n",
    "    num_workers_val = min(num_avail_cpus, 4) if num_avail_cpus is not None else 2\n",
    "    pin_memory_flag = True if DEVICE.type == 'cuda' else False\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers_val, pin_memory=pin_memory_flag)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers_val, pin_memory=pin_memory_flag)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers_val, pin_memory=pin_memory_flag)\n",
    "    print(f\"DataLoaders: Train {len(train_dataset)}, Val {len(val_dataset)}, Test {len(test_dataset)}\")\n",
    "    return train_loader, val_loader, test_loader"
   ],
   "id": "a1eb571858ed5260",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Torchvision MobileNetV2 Model Helper",
   "id": "72c4b8df3840979d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T09:41:39.309652Z",
     "start_time": "2025-05-23T09:41:39.303735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_torchvision_mobilenetv2_structure(num_classes_local, device_to_map=DEVICE):\n",
    "    \"\"\"\n",
    "    Creates the torchvision.models.mobilenet_v2 STRUCTURE (weights=None)\n",
    "    and adapts its classifier for the given number of classes.\n",
    "    This is used as a template to load a state_dict.\n",
    "    \"\"\"\n",
    "    # print(f\"Creating torchvision MobileNetV2 structure (weights=None) for {num_classes_local} classes.\")\n",
    "    tv_model = models.mobilenet_v2(weights=None, num_classes=num_classes_local) # num_classes adapts final layer\n",
    "\n",
    "    # Ensure the classifier is correctly adapted if num_classes was for something else initially\n",
    "    # For models.mobilenet_v2(weights=None, num_classes=X), the classifier is already X.\n",
    "    # This re-adaptation might be redundant if num_classes in constructor works as expected.\n",
    "    if isinstance(tv_model.classifier, nn.Sequential) and len(tv_model.classifier) > 0:\n",
    "        last_layer_index = -1 # Typically the Linear layer is last or second last\n",
    "        if isinstance(tv_model.classifier[last_layer_index], nn.Linear):\n",
    "             current_out_features = tv_model.classifier[last_layer_index].out_features\n",
    "             if current_out_features != num_classes_local:\n",
    "                 print(f\"Adapting classifier output from {current_out_features} to {num_classes_local} (should be redundant if constructor worked).\")\n",
    "                 num_ftrs_tv = tv_model.classifier[last_layer_index].in_features\n",
    "                 tv_model.classifier[last_layer_index] = torch.nn.Linear(num_ftrs_tv, num_classes_local)\n",
    "        elif len(tv_model.classifier) > 1 and isinstance(tv_model.classifier[-2], nn.Linear): # Check if last is dropout\n",
    "            current_out_features = tv_model.classifier[-2].out_features\n",
    "            if current_out_features != num_classes_local:\n",
    "                print(f\"Adapting classifier output from {current_out_features} to {num_classes_local} (should be redundant if constructor worked).\")\n",
    "                num_ftrs_tv = tv_model.classifier[-2].in_features\n",
    "                tv_model.classifier[-2] = torch.nn.Linear(num_ftrs_tv, num_classes_local)\n",
    "    return tv_model.to(device_to_map)\n",
    "\n",
    "\n",
    "def get_ignored_layer_for_torchvision_mobilenetv2(tv_model):\n",
    "    if isinstance(tv_model.classifier, nn.Sequential) and len(tv_model.classifier) > 0:\n",
    "        if isinstance(tv_model.classifier[-1], nn.Linear): return [tv_model.classifier[-1]]\n",
    "        elif len(tv_model.classifier) > 1 and isinstance(tv_model.classifier[-2], nn.Linear): return [tv_model.classifier[-2]]\n",
    "    elif isinstance(tv_model.classifier, nn.Linear): return [tv_model.classifier]\n",
    "    print(\"Warning: Could not identify classifier layer in get_ignored_layer_for_torchvision_mobilenetv2.\"); return []\n",
    "\n"
   ],
   "id": "c7a59f758a88fc2f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Utility Functions (Save/Load, MACs, Evaluate)",
   "id": "b8e77be324d6bfb1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T09:41:39.363501Z",
     "start_time": "2025-05-23T09:41:39.356354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_model_as_onnx(model_to_save, example_input_cpu_for_onnx, output_path_onnx):\n",
    "    model_to_save.eval()\n",
    "    model_cpu_for_export = model_to_save.to('cpu')\n",
    "    torch.onnx.export(\n",
    "        model_cpu_for_export, example_input_cpu_for_onnx, output_path_onnx, export_params=True, opset_version=13,\n",
    "        input_names=['input'], output_names=['output'],\n",
    "        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "    )\n",
    "    print(f\"✅ Model saved as ONNX to {output_path_onnx}\")\n",
    "\n",
    "def load_model_state(model_structure_provider_func, path_to_load_pth, device_to_load, num_classes_for_model_struct):\n",
    "    model_shell = model_structure_provider_func(num_classes_local=num_classes_for_model_struct, device_to_map=device_to_load)\n",
    "    model_shell.load_state_dict(torch.load(path_to_load_pth, map_location=device_to_load))\n",
    "    print(f\"✅ State loaded into model structure from: {path_to_load_pth}\")\n",
    "    return model_shell\n",
    "\n",
    "\n",
    "def calculate_macs_params(model_to_calc, example_input_for_calc):\n",
    "    # Ensure model and input are on the same device for tp.utils\n",
    "    target_device = example_input_for_calc.device\n",
    "    model_on_target_device = model_to_calc.to(target_device)\n",
    "    macs, params = tp.utils.count_ops_and_params(model_on_target_device, example_input_for_calc)\n",
    "    return macs, params\n",
    "\n",
    "def save_model(model_to_save, path_to_save_pth, example_input_cpu_for_onnx_save=None):\n",
    "    os.makedirs(os.path.dirname(path_to_save_pth), exist_ok=True)\n",
    "    torch.save(model_to_save.state_dict(), path_to_save_pth)\n",
    "    print(f\"✅ Model state_dict saved to {path_to_save_pth}\")\n",
    "    if example_input_cpu_for_onnx_save is not None:\n",
    "        onnx_output_path = path_to_save_pth.replace('.pth', '.onnx')\n",
    "        save_model_as_onnx(model_to_save, example_input_cpu_for_onnx_save, onnx_output_path)\n",
    "\n",
    "def load_torchvision_model_state(model_constructor_func, path_to_load_pth, device_to_load, num_classes_for_model):\n",
    "    # model_constructor_func here is 'get_torchvision_mobilenetv2'\n",
    "    # It needs num_classes and use_pretrained_weights=False to get the correct structure before loading state_dict\n",
    "    loaded_model = model_constructor_func(num_classes=num_classes_for_model, use_pretrained_weights=False, device_to_map=device_to_load)\n",
    "    loaded_model.load_state_dict(torch.load(path_to_load_pth, map_location=device_to_load))\n",
    "    print(f\"✅ Torchvision model state loaded from {path_to_load_pth} to {device_to_load}\")\n",
    "    return loaded_model\n",
    "\n",
    "def evaluate_model_with_loss(model_to_eval, data_loader_for_eval, example_input_for_macs, criterion_for_loss, device_for_eval):\n",
    "    model_to_eval.eval()\n",
    "    model_to_eval.to(device_for_eval)\n",
    "    macs_val, params_val = calculate_macs_params(model_to_eval, example_input_for_macs.to(device_for_eval))\n",
    "    size_mib_val = params_val * 4 / (1024 * 1024) # Assuming float32\n",
    "    correct_preds = 0; total_samples_eval = 0; accumulated_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data_batch in data_loader_for_eval:\n",
    "            inputs_batch, labels_batch = [d.to(device_for_eval) for d in data_batch]\n",
    "            outputs_batch = model_to_eval(inputs_batch)\n",
    "            loss_batch = criterion_for_loss(outputs_batch, labels_batch)\n",
    "            accumulated_loss += loss_batch.item() * inputs_batch.size(0)\n",
    "            _, predicted_labels = torch.max(outputs_batch.data, 1)\n",
    "            total_samples_eval += labels_batch.size(0)\n",
    "            correct_preds += (predicted_labels == labels_batch).sum().item()\n",
    "    avg_loss_val = accumulated_loss / total_samples_eval if total_samples_eval > 0 else float('nan')\n",
    "    accuracy_val = 100 * correct_preds / total_samples_eval if total_samples_eval > 0 else 0\n",
    "    return {'macs': macs_val, 'params': params_val, 'size_mib': size_mib_val, 'accuracy': accuracy_val, 'loss': avg_loss_val}"
   ],
   "id": "cd1ed68504e9a13f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Pruning Function (Adapted for explicit ignored_layers and torchvision BNs)",
   "id": "8a1a16cee2081d13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T09:41:39.423217Z",
     "start_time": "2025-05-23T09:41:39.409157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gr_prune_model_by_ratio(model_to_prune_input, example_input_dev_prune, strategy_config_prune,\n",
    "                            target_pruning_ratio_prune, explicit_ignored_layers_list=None,\n",
    "                            iterative_steps_general_pruner=5,\n",
    "                            iterative_steps_taylor_pruner=5):\n",
    "    pruned_model_state = copy.deepcopy(model_to_prune_input) # Work on a copy\n",
    "    pruned_model_state.to(example_input_dev_prune.device)\n",
    "    initial_macs_prune, _ = calculate_macs_params(pruned_model_state, example_input_dev_prune)\n",
    "    print(f\"Initial MACs for ratio {target_pruning_ratio_prune:.2f}: {initial_macs_prune / 1e9:.3f} G\")\n",
    "\n",
    "    current_ignored_layers = explicit_ignored_layers_list if explicit_ignored_layers_list is not None else []\n",
    "    if not current_ignored_layers: print(\"Warning: No explicit ignored_layers provided to pruner.\")\n",
    "\n",
    "\n",
    "    if target_pruning_ratio_prune == 0.0:\n",
    "        print(\"Target pruning ratio 0.0 - no pruning needed.\")\n",
    "        return pruned_model_state\n",
    "\n",
    "    if isinstance(strategy_config_prune['importance'], tp.importance.TaylorImportance):\n",
    "        print(f\"TaylorImportance: ratio {target_pruning_ratio_prune:.2f}, {iterative_steps_taylor_pruner} backward passes.\")\n",
    "\n",
    "        # BatchNorm layers in torchvision.models.mobilenet_v2.features sequence\n",
    "        # that might receive 1x1 spatial input with 32x32 overall input image size.\n",
    "        # Feature map sizes before these layers if input is 32x32:\n",
    "        # features[13] (ConvBNReLU): Output before this is block 160->960->160, 2x2 spatially.\n",
    "        #                      Inside InvertedResidual for features[13] (stride 2, 96->160): hidden=576\n",
    "        #                      bn after depthwise conv with stride 2 receives 1x1. This is features[13].conv[4] (BN)\n",
    "        # features[14,15,16] (ConvBNReLU): block 160->960->160, 1x1 spatially. (stride 1, 160->160)\n",
    "        #                      All BNs inside these will get 1x1. (conv[1], conv[4], conv[7])\n",
    "        # features[17] (ConvBNReLU): block 160->960->320, 1x1 spatially. (stride 1, 160->320)\n",
    "        #                      All BNs inside this will get 1x1.\n",
    "        # features[18] (ConvBN2d): This is the final 1x1 conv before pooling. its BN gets 1x1.\n",
    "\n",
    "        bn_layers_to_eval_during_taylor = []\n",
    "        # Helper to add BNs from an InvertedResidual-like block (ConvBNRelu -> DwConvBNRelu -> ConvBN)\n",
    "        def add_bns_from_inv_res_if_1x1(block_module, spatial_is_1x1_input):\n",
    "            if hasattr(block_module, 'conv') and isinstance(block_module.conv, nn.Sequential):\n",
    "                # conv[1] is bn after first 1x1 conv\n",
    "                if spatial_is_1x1_input and isinstance(block_module.conv[1], nn.BatchNorm2d):\n",
    "                     bn_layers_to_eval_during_taylor.append(block_module.conv[1])\n",
    "                # conv[4] is bn after depthwise conv\n",
    "                # Spatial becomes 1x1 if input was 2x2 and stride=2 OR if input was already 1x1\n",
    "                # For simplicity here, we assume the outer logic determines if this block *overall* sees 1x1 due to prior stages\n",
    "                if spatial_is_1x1_input and isinstance(block_module.conv[4], nn.BatchNorm2d):\n",
    "                     bn_layers_to_eval_during_taylor.append(block_module.conv[4])\n",
    "                # conv[6] (careful with indexing if last is just BN not ConvBN) / or last BN in the sequence\n",
    "                if hasattr(block_module.conv, '6') and isinstance(block_module.conv[6], nn.BatchNorm2d) and spatial_is_1x1_input: # if structure is C-BN-ACT-DW-BN-ACT-C-BN\n",
    "                     bn_layers_to_eval_during_taylor.append(block_module.conv[6])\n",
    "                elif isinstance(block_module.conv[-1], nn.BatchNorm2d) and spatial_is_1x1_input: # More robust: last layer of seq is BN\n",
    "                    bn_layers_to_eval_during_taylor.append(block_module.conv[-1])\n",
    "\n",
    "\n",
    "        # Based on typical MobileNetV2 structure from torchvision.features:\n",
    "        # features are a nn.Sequential. Strides affect subsequent blocks.\n",
    "        # 32x32 -> [0] Conv s2 -> 16x16\n",
    "        # -> [1] InvRes s1 -> 16x16\n",
    "        # -> [2,3] InvRes s2 (block 3) -> 8x8\n",
    "        # -> [4,5,6] InvRes s2 (block 6) -> 4x4\n",
    "        # -> [7,8,9,10] InvRes s2 (block 10) -> 2x2\n",
    "        # -> [11,12,13] InvRes s1 (block 13, last has stride 2, features[13]) -> features[13] makes it 1x1\n",
    "        # The following blocks [14] through [17] and the final conv[18] will operate on 1x1.\n",
    "\n",
    "        if hasattr(pruned_model_state, 'features') and isinstance(pruned_model_state.features, nn.Sequential):\n",
    "            # Block starting from features[13] output leads to 1x1, so its bn_after_dw may get 1x1 if input to block was 2x2 and stride=2.\n",
    "            # It's safer to assume blocks from where spatial dim is consistently 1x1.\n",
    "            # features[13] is type InvertedResidual.\n",
    "            # Inside models.mobilenet_v2, the blocks are directly in features.\n",
    "            # features[13].conv[4] (BN after DW conv s=2 on 2x2 input)\n",
    "            if len(pruned_model_state.features) > 13 and hasattr(pruned_model_state.features[13], 'conv') and isinstance(pruned_model_state.features[13].conv, nn.Sequential):\n",
    "                 if isinstance(pruned_model_state.features[13].conv[4], nn.BatchNorm2d): bn_layers_to_eval_during_taylor.append(pruned_model_state.features[13].conv[4])\n",
    "                 if isinstance(pruned_model_state.features[13].conv[-1], nn.BatchNorm2d): bn_layers_to_eval_during_taylor.append(pruned_model_state.features[13].conv[-1])\n",
    "\n",
    "\n",
    "            # Blocks features[14] through features[17] operate on 1x1 spatial feature maps\n",
    "            for k in range(14, 18): # Indices for blocks that get 1x1 input\n",
    "                if len(pruned_model_state.features) > k and hasattr(pruned_model_state.features[k], 'conv') and isinstance(pruned_model_state.features[k].conv, nn.Sequential):\n",
    "                    # Add all BNs from these blocks: conv[1], conv[4], conv[-1] (final BN)\n",
    "                    if isinstance(pruned_model_state.features[k].conv[1], nn.BatchNorm2d): bn_layers_to_eval_during_taylor.append(pruned_model_state.features[k].conv[1])\n",
    "                    if isinstance(pruned_model_state.features[k].conv[4], nn.BatchNorm2d): bn_layers_to_eval_during_taylor.append(pruned_model_state.features[k].conv[4])\n",
    "                    if isinstance(pruned_model_state.features[k].conv[-1], nn.BatchNorm2d): bn_layers_to_eval_during_taylor.append(pruned_model_state.features[k].conv[-1])\n",
    "\n",
    "            # The BN in the final 1x1 conv layer (features[18] is often Conv2dNormActivation)\n",
    "            if len(pruned_model_state.features) > 18 and hasattr(pruned_model_state.features[18], '1') and isinstance(pruned_model_state.features[18][1], nn.BatchNorm2d): # Conv2dNormActivation [0]=conv, [1]=bn\n",
    "                 bn_layers_to_eval_during_taylor.append(pruned_model_state.features[18][1])\n",
    "\n",
    "        if bn_layers_to_eval_during_taylor: print(f\"Temporarily setting {len(bn_layers_to_eval_during_taylor)} BNs to eval for Taylor.\")\n",
    "\n",
    "        pruner_for_taylor = strategy_config_prune['pruner'](\n",
    "            pruned_model_state, example_input_dev_prune, importance=strategy_config_prune['importance'],\n",
    "            iterative_steps=iterative_steps_taylor_pruner, ch_sparsity=target_pruning_ratio_prune,\n",
    "            root_module_types=[nn.Conv2d], ignored_layers=current_ignored_layers\n",
    "        )\n",
    "        original_bn_modes_taylor = {}\n",
    "        for i in range(iterative_steps_taylor_pruner):\n",
    "            pruned_model_state.train()\n",
    "            for bn_l in bn_layers_to_eval_during_taylor: original_bn_modes_taylor[bn_l] = bn_l.training; bn_l.eval()\n",
    "            pruned_model_state.zero_grad()\n",
    "            loss_taylor = pruned_model_state(example_input_dev_prune.clone()).mean()\n",
    "            try: loss_taylor.backward()\n",
    "            except Exception as e_bw:\n",
    "                print(f\"Backward error in Taylor step {i+1}: {e_bw}\"); raise\n",
    "            finally: # Ensure BN modes are restored\n",
    "                 for bn_l, mode in original_bn_modes_taylor.items(): bn_l.training = mode\n",
    "                 original_bn_modes_taylor.clear()\n",
    "            pruned_model_state.eval()\n",
    "            pruner_for_taylor.step()\n",
    "            macs_after_taylor_step, _ = calculate_macs_params(pruned_model_state, example_input_dev_prune)\n",
    "            print(f\"  Taylor Step {i+1}/{iterative_steps_taylor_pruner} (Pruner {pruner_for_taylor.current_step}/{pruner_for_taylor.iterative_steps}): MACs {macs_after_taylor_step / 1e9:.3f} G\")\n",
    "            if pruner_for_taylor.current_step == pruner_for_taylor.iterative_steps: break\n",
    "    else:\n",
    "        print(f\"Applying {strategy_config_prune['importance'].__class__.__name__} @ ratio {target_pruning_ratio_prune:.2f} \"\n",
    "              f\"with {iterative_steps_general_pruner} pruner steps.\")\n",
    "        pruned_model_state.eval() # Non-Taylor pruners generally expect eval mode\n",
    "        pruner_general = strategy_config_prune['pruner'](\n",
    "            pruned_model_state, example_input_dev_prune, importance=strategy_config_prune['importance'],\n",
    "            iterative_steps=iterative_steps_general_pruner, ch_sparsity=target_pruning_ratio_prune,\n",
    "            root_module_types=[nn.Conv2d], ignored_layers=current_ignored_layers\n",
    "        )\n",
    "        pruner_general.step()\n",
    "\n",
    "    final_macs_prune, _ = calculate_macs_params(pruned_model_state, example_input_dev_prune)\n",
    "    reduction_prune = (initial_macs_prune - final_macs_prune) / initial_macs_prune * 100 if initial_macs_prune > 0 else 0\n",
    "    print(f\"Pruning for ratio {target_pruning_ratio_prune:.2f} done: MACs {final_macs_prune / 1e9:.3f} G (Reduced by {reduction_prune:.2f}%)\")\n",
    "    return pruned_model_state\n"
   ],
   "id": "4c602f3cba0dac9f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train the model (with Early Stopping)",
   "id": "7f476049d9772bf3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T09:41:39.478452Z",
     "start_time": "2025-05-23T09:41:39.469380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model_train, train_dl, criterion_train, optimizer_train, device_train, num_epochs_train,\n",
    "                val_loader=None, log_prefix_train=\"\", es_patience=None, es_metric='val_loss', load_best_es=True):\n",
    "    model_train.to(device_train)\n",
    "    history_train = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    best_metric_val = float('inf') if es_metric == 'val_loss' else float('-inf')\n",
    "    epochs_no_improve_train = 0; best_model_state_train = None\n",
    "\n",
    "    for epoch_train in range(num_epochs_train):\n",
    "        model_train.train(); running_loss_train = 0.0; correct_train = 0; total_train = 0\n",
    "        for _, data_train in enumerate(train_dl):\n",
    "            inputs_train, labels_train = [d.to(device_train) for d in data_train]\n",
    "            optimizer_train.zero_grad(); outputs_train = model_train(inputs_train); loss_train = criterion_train(outputs_train, labels_train)\n",
    "            loss_train.backward(); optimizer_train.step()\n",
    "            running_loss_train += loss_train.item(); _, predicted_train = torch.max(outputs_train.data, 1)\n",
    "            total_train += labels_train.size(0); correct_train += (predicted_train == labels_train).sum().item()\n",
    "\n",
    "        epoch_loss_train = running_loss_train/len(train_dl) if len(train_dl)>0 else 0\n",
    "        epoch_acc_train = 100*correct_train/total_train if total_train >0 else 0\n",
    "        history_train['train_loss'].append(epoch_loss_train); history_train['train_acc'].append(epoch_acc_train)\n",
    "        log_line = f\"E {epoch_train+1}/{num_epochs_train} ({log_prefix_train}): Tr L={epoch_loss_train:.4f}, Acc={epoch_acc_train:.2f}%\"\n",
    "\n",
    "        if val_loader:\n",
    "            model_train.eval(); running_val_loss_train = 0.0; correct_val_train = 0; total_val_train = 0\n",
    "            with torch.no_grad():\n",
    "                for data_val_train in val_loader:\n",
    "                    inputs_val_train, labels_val_train = [d.to(device_train) for d in data_val_train]\n",
    "                    outputs_val_train = model_train(inputs_val_train); val_loss_item_train = criterion_train(outputs_val_train, labels_val_train)\n",
    "                    running_val_loss_train += val_loss_item_train.item(); _, predicted_val_train = torch.max(outputs_val_train.data, 1)\n",
    "                    total_val_train += labels_val_train.size(0); correct_val_train += (predicted_val_train == labels_val_train).sum().item()\n",
    "            epoch_val_loss_train = running_val_loss_train/len(val_loader) if len(val_loader)>0 else 0\n",
    "            epoch_val_acc_train = 100*correct_val_train/total_val_train if total_val_train >0 else 0\n",
    "            history_train['val_loss'].append(epoch_val_loss_train); history_train['val_acc'].append(epoch_val_acc_train)\n",
    "            log_line += f\", Val L={epoch_val_loss_train:.4f}, Acc={epoch_val_acc_train:.2f}%\"\n",
    "\n",
    "            if es_patience is not None:\n",
    "                metric_val_check = epoch_val_loss_train if es_metric == 'val_loss' else epoch_val_acc_train\n",
    "                improved_es = (metric_val_check < best_metric_val) if es_metric == 'val_loss' else (metric_val_check > best_metric_val)\n",
    "                if improved_es:\n",
    "                    best_metric_val = metric_val_check; epochs_no_improve_train = 0\n",
    "                    if load_best_es: best_model_state_train = copy.deepcopy(model_train.state_dict())\n",
    "                    log_line += \" (New best)\"\n",
    "                else: epochs_no_improve_train += 1\n",
    "        else: # No val_loader\n",
    "            history_train['val_loss'].append(None); history_train['val_acc'].append(None)\n",
    "            if es_patience: print(f\"ES Warning ({log_prefix_train}): No val_loader provided.\")\n",
    "        print(log_line)\n",
    "        if es_patience and val_loader and epochs_no_improve_train >= es_patience:\n",
    "            print(f\"EarlyStopping for '{log_prefix_train}' at epoch {epoch_train+1}.\")\n",
    "            if load_best_es and best_model_state_train: model_train.load_state_dict(best_model_state_train)\n",
    "            break\n",
    "\n",
    "    if load_best_es and best_model_state_train:\n",
    "        final_val_metric_train = history_train[es_metric][-1] if history_train[es_metric] and history_train[es_metric][-1] is not None else None\n",
    "        load_final_best = True\n",
    "        if final_val_metric_train is not None: # Check if current state is already best\n",
    "            if es_metric == 'val_loss': load_final_best = best_metric_val < final_val_metric_train\n",
    "            else: load_final_best = best_metric_val > final_val_metric_train\n",
    "        if load_final_best :\n",
    "            print(f\"End of Train ({log_prefix_train}): Loading best state ({es_metric}={best_metric_val:.4f}).\")\n",
    "            model_train.load_state_dict(best_model_state_train)\n",
    "    return model_train, history_train\n"
   ],
   "id": "2b3351f6cd229313",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Plotting Utilities",
   "id": "91a2f8d03820278"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T09:41:39.538336Z",
     "start_time": "2025-05-23T09:41:39.524130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchinfo import summary as torchinfo_summary # For model architecture summary\n",
    "def summarize_loaded_initial_model(model_instance, model_name, eval_metrics, example_input_for_summary, output_dir):\n",
    "    \"\"\"\n",
    "    Saves architecture summary and evaluation metrics for a loaded initial model.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 1. Save Architecture Summary\n",
    "    summary_filename = os.path.join(output_dir, f\"{model_name}_architecture_summary.txt\")\n",
    "    try:\n",
    "        # Ensure model and example_input are on the same device or example_input is on CPU for summary\n",
    "        # torchinfo typically handles moving model to input_data's device or CPU.\n",
    "        # Let's make sure example_input_for_summary is on CPU for robust summary generation.\n",
    "        example_input_summary_cpu = example_input_for_summary.to('cpu')\n",
    "        model_for_summary_cpu = model_instance.to('cpu')\n",
    "\n",
    "        model_summary_str = str(torchinfo_summary(model_for_summary_cpu,\n",
    "                                                  input_data=example_input_summary_cpu, # Use CPU version of input\n",
    "                                                  verbose=0,\n",
    "                                                  col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"], # Added kernel_size\n",
    "                                                  row_settings=[\"var_names\"]))\n",
    "        with open(summary_filename, 'w') as f:\n",
    "            f.write(f\"Architecture Summary for: {model_name}\\n\\n\")\n",
    "            f.write(model_summary_str)\n",
    "        print(f\"✅ Architecture summary saved to: {summary_filename}\")\n",
    "        # Move model back to original device if needed, though this function gets a copy typically\n",
    "        model_instance.to(example_input_for_summary.device) # If model_instance was modified\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"torchinfo not installed. Skipping detailed architecture summary. Saving basic print(model).\")\n",
    "        print(\"Install torchinfo for detailed summaries: pip install torchinfo\")\n",
    "        with open(summary_filename, 'w') as f:\n",
    "            f.write(f\"Basic Model Structure for: {model_name}\\n\\n\")\n",
    "            f.write(str(model_instance)) # Fallback to basic model string representation\n",
    "        print(f\"✅ Basic model structure saved to: {summary_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate torchinfo summary: {e}. Saving basic print(model).\")\n",
    "        with open(summary_filename, 'w') as f:\n",
    "            f.write(f\"Basic Model Structure for: {model_name}\\n\\n\")\n",
    "            f.write(str(model_instance))\n",
    "        print(f\"✅ Basic model structure saved to: {summary_filename}\")\n",
    "\n",
    "\n",
    "    # 2. Save Evaluation Metrics as Text\n",
    "    metrics_filename = os.path.join(output_dir, f\"{model_name}_evaluation_metrics.txt\")\n",
    "    with open(metrics_filename, 'w') as f:\n",
    "        f.write(f\"Evaluation Metrics for: {model_name} (on Test Set)\\n\\n\")\n",
    "        if eval_metrics: # Ensure eval_metrics is not None\n",
    "            for key, value in eval_metrics.items():\n",
    "                if isinstance(value, float) and key not in ['macs', 'params']:\n",
    "                    f.write(f\"{key.capitalize()}: {value:.4f}\\n\")\n",
    "                elif isinstance(value, (float, int)) and key in ['macs', 'params']: # int for params if not float\n",
    "                     f.write(f\"{key.capitalize()}: {value:,.0f} (or {value:.3e})\\n\") # Formatted with comma and scientific\n",
    "                else:\n",
    "                    f.write(f\"{key.capitalize()}: {value}\\n\")\n",
    "        else:\n",
    "            f.write(\"No evaluation metrics provided.\\n\")\n",
    "    print(f\"✅ Evaluation metrics saved to: {metrics_filename}\")\n",
    "\n",
    "    # 3. Simple Bar Chart for key metrics (if metrics are available)\n",
    "    if eval_metrics:\n",
    "        metrics_to_plot_init = {\n",
    "            'MACs': eval_metrics.get('macs'), # Use .get for safety\n",
    "            'Params': eval_metrics.get('params'),\n",
    "            'Test Loss': eval_metrics.get('loss'),\n",
    "            'Test Acc (%)': eval_metrics.get('accuracy')\n",
    "        }\n",
    "\n",
    "        # Filter out None values before plotting\n",
    "        valid_metrics_for_plot = {k: v for k, v in metrics_to_plot_init.items() if v is not None and not (isinstance(v, float) and np.isnan(v))}\n",
    "\n",
    "        if len(valid_metrics_for_plot) > 0:\n",
    "            num_valid_metrics = len(valid_metrics_for_plot)\n",
    "            # Adjust subplot layout based on number of metrics\n",
    "            ncols = 2 if num_valid_metrics > 1 else 1\n",
    "            nrows = (num_valid_metrics + ncols - 1) // ncols # Calculate rows needed\n",
    "\n",
    "            fig, ax_subplots = plt.subplots(nrows, ncols, figsize=(6*ncols, 5*nrows), squeeze=False) # squeeze=False always returns 2D array\n",
    "            ax_flat = ax_subplots.ravel() # Flatten to easily iterate\n",
    "\n",
    "            plot_colors = ['skyblue', 'lightgreen', 'salmon', 'gold', 'lightcoral', 'lightskyblue'] # More colors if needed\n",
    "\n",
    "            for i, (metric_name_plot, metric_value_plot) in enumerate(valid_metrics_for_plot.items()):\n",
    "                ax_curr = ax_flat[i]\n",
    "                ax_curr.bar([metric_name_plot.replace(\" (%)\",\"\\n(%)\")], [metric_value_plot], color=plot_colors[i % len(plot_colors)])\n",
    "                ax_curr.set_title(metric_name_plot)\n",
    "\n",
    "                text_val = \"\"\n",
    "                if \"MACs\" in metric_name_plot or \"Params\" in metric_name_plot:\n",
    "                    text_val = f\"{metric_value_plot:,.0f}\\n({metric_value_plot:.2e})\"\n",
    "                    ax_curr.set_yscale('log') # Log scale for MACs/Params makes sense\n",
    "                elif \"Loss\" in metric_name_plot:\n",
    "                     text_val = f\"{metric_value_plot:.4f}\"\n",
    "                elif \"Acc\" in metric_name_plot: # Accuracy\n",
    "                    text_val = f\"{metric_value_plot:.2f}%\"\n",
    "                    ax_curr.set_ylim(min(0, metric_value_plot-10 if metric_value_plot else 0), max(100, metric_value_plot+5 if metric_value_plot else 100) + (5 if metric_value_plot==100 else 0) ) # Dynamic Y lim for acc\n",
    "\n",
    "                ax_curr.text(0, metric_value_plot, text_val, ha='center', va='bottom' if metric_value_plot > 0 else 'top')\n",
    "                ax_curr.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "\n",
    "            # Hide any unused subplots\n",
    "            for j in range(num_valid_metrics, nrows * ncols):\n",
    "                fig.delaxes(ax_flat[j])\n",
    "\n",
    "            fig.suptitle(f\"Initial Loaded Model Metrics: {model_name}\", fontsize=16)\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "            plot_path = os.path.join(output_dir, f\"{model_name}_metrics_summary_chart.png\")\n",
    "            plt.savefig(plot_path)\n",
    "            plt.close(fig)\n",
    "            print(f\"✅ Metrics summary chart saved to: {plot_path}\")\n",
    "        else:\n",
    "            print(\"No valid metrics to plot for initial model summary chart.\")\n",
    "    else:\n",
    "        print(\"No evaluation metrics provided for initial model summary chart.\")"
   ],
   "id": "60efe83c2d455e95",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T09:41:39.594932Z",
     "start_time": "2025-05-23T09:41:39.584145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_initial_model_training_summary(training_history_plot, model_name_str, output_dir_init_plots):\n",
    "    # (Identical to previous full script version)\n",
    "    os.makedirs(output_dir_init_plots, exist_ok=True); epochs_run_plot = len(training_history_plot['train_loss'])\n",
    "    if epochs_run_plot == 0: print(f\"No history for {model_name_str}. Skip plots.\"); return\n",
    "    epochs_axis_plot = range(1, epochs_run_plot + 1); plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs_axis_plot, training_history_plot['train_loss'], 'bo-', label='Train Loss')\n",
    "    if training_history_plot.get('val_loss') and any(v is not None for v in training_history_plot['val_loss']):\n",
    "        v_loss_epochs = [ep for ep, lo in zip(epochs_axis_plot, training_history_plot['val_loss']) if lo is not None]; v_loss_vals = [lo for lo in training_history_plot['val_loss'] if lo is not None]\n",
    "        if v_loss_vals: plt.plot(v_loss_epochs, v_loss_vals, 'ro-', label='Val Loss')\n",
    "    plt.title(f'{model_name_str}: Loss vs. Epochs'); plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_dir_init_plots, f'{model_name_str}_loss_vs_epochs.png')); plt.close(); print(f\"✅ {model_name_str} Loss/Ep saved to {output_dir_init_plots}\")\n",
    "    fig_la, axs_la = plt.subplots(1, 2 if (training_history_plot.get('val_loss') and any(v is not None for v in training_history_plot['val_loss'])) else 1, figsize=(12, 5), squeeze=False)\n",
    "    ax1_la = axs_la[0,0]; ax1_la.plot(training_history_plot['train_acc'], training_history_plot['train_loss'], 'bo-', label='Tr L/Acc')\n",
    "    if epochs_run_plot > 0: ax1_la.scatter(training_history_plot['train_acc'][0], training_history_plot['train_loss'][0], c='g', s=50, zorder=5, label='Start Tr'); ax1_la.scatter(training_history_plot['train_acc'][-1], training_history_plot['train_loss'][-1], c='r', s=50, zorder=5, label='End Tr')\n",
    "    ax1_la.set_title('Train Loss vs. Acc'); ax1_la.set_xlabel('Tr Acc %'); ax1_la.set_ylabel('Tr Loss'); ax1_la.legend(); ax1_la.grid(True)\n",
    "    if training_history_plot.get('val_loss') and any(v is not None for v in training_history_plot['val_loss']) and axs_la.shape[1] > 1:\n",
    "        ax2_la = axs_la[0,1]; v_indices = [i for i, (lo, ac) in enumerate(zip(training_history_plot['val_loss'], training_history_plot['val_acc'])) if lo is not None and ac is not None]\n",
    "        if v_indices:\n",
    "            v_acc_p = [training_history_plot['val_acc'][i] for i in v_indices]; v_loss_p = [training_history_plot['val_loss'][i] for i in v_indices]; ax2_la.plot(v_acc_p, v_loss_p, 'ro-', label='Val L/Acc')\n",
    "            if v_acc_p: ax2_la.scatter(v_acc_p[0], v_loss_p[0], c='lime', s=50, zorder=5, label='Start Val'); ax2_la.scatter(v_acc_p[-1], v_loss_p[-1], c='darkred', s=50, zorder=5, label='End Val')\n",
    "            ax2_la.set_title('Val Loss vs. Acc'); ax2_la.set_xlabel('Val Acc %'); ax2_la.set_ylabel('Val Loss'); ax2_la.legend(); ax2_la.grid(True)\n",
    "    fig_la.suptitle(f'{model_name_str}: Loss vs. Acc Dynamics', fontsize=16); plt.tight_layout(rect=[0,0,1,0.96]); plt.savefig(os.path.join(output_dir_init_plots, f'{model_name_str}_loss_vs_accuracy.png')); plt.close(); print(f\"✅ {model_name_str} Loss/Acc saved to {output_dir_init_plots}\")\n",
    "def plot_finetuning_curves(history_ft, plot_title_suffix_ft, output_dir_plots_ft, model_macs_val_ft):\n",
    "    os.makedirs(output_dir_plots_ft, exist_ok=True); actual_epochs = len(history_ft['train_loss']); epochs_range = range(1, actual_epochs + 1); plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1); plt.plot(epochs_range, history_ft['train_loss'][:actual_epochs], 'bo-', label='Tr L');\n",
    "    if history_ft.get('val_loss') and any(v is not None for v in history_ft['val_loss']): plt.plot(epochs_range, history_ft['val_loss'][:actual_epochs], 'ro-', label='Val L')\n",
    "    plt.title(f'L ({plot_title_suffix_ft})\\nMACs:{model_macs_val_ft:.2e}'); plt.xlabel('Ep'); plt.ylabel('L'); plt.legend(); plt.grid(True)\n",
    "    plt.subplot(1, 2, 2); plt.plot(epochs_range, history_ft['train_acc'][:actual_epochs], 'bo-', label='Tr Acc');\n",
    "    if history_ft.get('val_acc') and any(v is not None for v in history_ft['val_acc']): plt.plot(epochs_range, history_ft['val_acc'][:actual_epochs], 'ro-', label='Val Acc')\n",
    "    plt.title(f'Acc ({plot_title_suffix_ft})\\nMACs:{model_macs_val_ft:.2e}'); plt.xlabel('Ep'); plt.ylabel('Acc %'); plt.legend(); plt.grid(True)\n",
    "    plt.tight_layout(); safe_suffix_ft = plot_title_suffix_ft.replace(' ', '_').replace('/', '_').replace(':', '_'); plt.savefig(os.path.join(output_dir_plots_ft, f'ft_curves_{safe_suffix_ft}.png')); plt.close(); print(f\"✅ FT curves for {plot_title_suffix_ft} saved.\")\n",
    "def plot_metrics_vs_ratio_all_strategies(results_plot, ratios_plot, out_dir_compare):\n",
    "    os.makedirs(out_dir_compare, exist_ok=True); strat_plot = list(results_plot.keys())\n",
    "    if not strat_plot: print(\"No strats for ratio plots.\"); return\n",
    "    ratios_labels_plot = [f\"{r:.1f}\" for r in ratios_plot]; num_ratios_p = len(ratios_plot); num_strat_p = len(strat_plot); group_w = 0.8; bar_w = group_w / num_strat_p; idx_p = np.arange(num_ratios_p); cmap_p = plt.colormaps.get_cmap('tab10')\n",
    "    metrics_to_plot = [('macs', 'MACs (Log Scale)', True), ('loss', 'Avg. Test Loss', False), ('accuracy', 'Accuracy (%)', False)]\n",
    "    for metric_key, y_label, use_log_scale in metrics_to_plot:\n",
    "        plt.figure(figsize=(max(12, int(1.8*num_ratios_p)), 7));\n",
    "        for i, s_name_p in enumerate(strat_plot):\n",
    "            vals_p = [results_plot[s_name_p].get(r_val, {}).get(metric_key, np.nan) for r_val in ratios_plot]; bar_pos_p = idx_p - (group_w/2) + (i*bar_w) + (bar_w/2)\n",
    "            plt.bar(bar_pos_p, vals_p, bar_w, label=s_name_p, color=cmap_p(i % cmap_p.N))\n",
    "        plt.xlabel('Pruning Ratio (ch_sparsity)'); plt.ylabel(y_label); plt.title(f'{y_label.split(\"(\")[0].strip()} vs. Pruning Ratio (Fine-tuned)'); plt.xticks(idx_p, ratios_labels_plot)\n",
    "        if use_log_scale: plt.yscale('log'); plt.grid(True, which=\"both\", ls=\"-\", alpha=0.3)\n",
    "        else: plt.grid(True, alpha=0.3)\n",
    "        if metric_key == 'accuracy': plt.ylim(0,100)\n",
    "        plt.legend(title=\"Strategy\", bbox_to_anchor=(1.02,1), loc='upper left'); plt.tight_layout(rect=[0,0,0.85,1]); plt.savefig(os.path.join(out_dir_compare, f'{metric_key.upper()}_vs_Ratio_by_Strategy.png')); plt.close(); print(f\"✅ {metric_key.upper()} vs. Ratio plot saved to {out_dir_compare}\")\n"
   ],
   "id": "8fcf6940ff644acf",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Main Workflow",
   "id": "3739d47acf45ea9a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T09:41:39.646677Z",
     "start_time": "2025-05-23T09:41:39.635109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    cfg = {\n",
    "        'strategies': {\n",
    "            'MagnitudeL2': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.MagnitudeImportance(p=2)},\n",
    "            'BNScale': {'pruner': tp.pruner.BNScalePruner, 'importance': tp.importance.BNScaleImportance()},\n",
    "            'Random': {'pruner': tp.pruner.MagnitudePruner, 'importance': tp.importance.RandomImportance()},\n",
    "        },\n",
    "        'pruning_ratios_to_test': [0.0, 0.2, 0.5, 0.7],\n",
    "        'iterative_steps_pruner_general': 5,\n",
    "        'iterative_steps_taylor_pruning': 5,\n",
    "        'fine_tune_epochs': 30,\n",
    "        'early_stopping_patience': 7,\n",
    "        'early_stopping_metric': 'val_loss',\n",
    "        'load_best_model_on_early_stop': True,\n",
    "        'cifar10_data_root': './data',\n",
    "        'val_split_for_loader': 0.1, 'data_loader_seed': 42,\n",
    "        'local_mobilenetv2_pth_path': './base/mobilenet_v2-b0353104.pth',  # UPDATE THIS\n",
    "        'output_dir_base': f'./output_{MODEL_BASE_NAME}_local_pth_final',\n",
    "        'num_classes': 10, 'batch_size': 128,\n",
    "        'learning_rate_finetune': 0.0001,\n",
    "    }\n",
    "\n",
    "    # Setup directories and check .pth file\n",
    "    abs_local_pth_path = os.path.abspath(cfg['local_mobilenetv2_pth_path'])\n",
    "    if not os.path.exists(abs_local_pth_path):\n",
    "        print(f\"ERROR: PTH file not found: {abs_local_pth_path}\"); return\n",
    "    os.makedirs(cfg['output_dir_base'], exist_ok=True)\n",
    "    initial_model_info_dir = os.path.join(cfg['output_dir_base'], \"initial_model_info\")\n",
    "    os.makedirs(initial_model_info_dir, exist_ok=True)\n",
    "    overall_comparison_plots_out_dir = os.path.join(cfg['output_dir_base'], \"overall_comparison_plots\")\n",
    "    os.makedirs(overall_comparison_plots_out_dir, exist_ok=True)\n",
    "\n",
    "    # Data loaders\n",
    "    print(\"Initializing DataLoaders...\")\n",
    "    train_loader, val_loader, test_loader = get_data_loaders(\n",
    "        data_dir_path=cfg['cifar10_data_root'], batch_size=cfg['batch_size'],\n",
    "        val_split=cfg['val_split_for_loader'], seed=cfg['data_loader_seed']\n",
    "    )\n",
    "    example_input_cpu_onnx_main = torch.randn(1, 3, 32, 32)\n",
    "    example_input_dev_main = example_input_cpu_onnx_main.to(DEVICE)\n",
    "    criterion_main = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "    # Load and adapt the model\n",
    "    print(f\"--- Loading and Adapting Model from Local PTH: {abs_local_pth_path} ---\")\n",
    "    temp_model_for_loading = get_torchvision_mobilenetv2_structure(num_classes_local=1000, device_to_map=DEVICE)\n",
    "    try:\n",
    "        temp_model_for_loading.load_state_dict(torch.load(abs_local_pth_path, map_location=DEVICE))\n",
    "    except RuntimeError:\n",
    "        print(\"Attempting to load local .pth into a 10-class structure directly...\")\n",
    "        try:\n",
    "            temp_model_for_loading = get_torchvision_mobilenetv2_structure(num_classes_local=cfg['num_classes'], device_to_map=DEVICE)\n",
    "            temp_model_for_loading.load_state_dict(torch.load(abs_local_pth_path, map_location=DEVICE))\n",
    "        except Exception as e2:\n",
    "            print(f\"CRITICAL ERROR: Failed to load local .pth file. Error: {e2}\"); return\n",
    "    final_classifier_layer_tv = None\n",
    "    if isinstance(temp_model_for_loading.classifier, nn.Sequential) and len(temp_model_for_loading.classifier) > 0:\n",
    "        idx = -1\n",
    "        if isinstance(temp_model_for_loading.classifier[idx], nn.Linear): final_classifier_layer_tv = temp_model_for_loading.classifier[idx]\n",
    "        elif len(temp_model_for_loading.classifier) > 1 and isinstance(temp_model_for_loading.classifier[-2], nn.Linear): final_classifier_layer_tv = temp_model_for_loading.classifier[-2]; idx=-2\n",
    "    if final_classifier_layer_tv and final_classifier_layer_tv.out_features != cfg['num_classes']:\n",
    "        num_ftrs = final_classifier_layer_tv.in_features\n",
    "        if idx == -1: temp_model_for_loading.classifier[-1] = nn.Linear(num_ftrs, cfg['num_classes'])\n",
    "        else: temp_model_for_loading.classifier[-2] = nn.Linear(num_ftrs, cfg['num_classes'])\n",
    "    initial_dense_model_ready = temp_model_for_loading.to(DEVICE)\n",
    "\n",
    "    # **Added: Initial Training**\n",
    "    print(\"--- Training the initial model on CIFAR-10 ---\")\n",
    "    initial_dense_model_ready, initial_model_hist = train_model(\n",
    "        initial_dense_model_ready, train_loader, criterion_main,\n",
    "        optim.Adam(initial_dense_model_ready.parameters(), lr=cfg['learning_rate_finetune']),\n",
    "        DEVICE, cfg['fine_tune_epochs'], val_loader=val_loader,\n",
    "        log_prefix_train=\"Initial Training\", es_patience=cfg['early_stopping_patience'],\n",
    "        es_metric=cfg['early_stopping_metric'], load_best_es=cfg['load_best_model_on_early_stop']\n",
    "    )\n",
    "    trained_initial_model_path = os.path.join(cfg['output_dir_base'], f\"{MODEL_BASE_NAME}_trained_initial.pth\")\n",
    "    save_model(initial_dense_model_ready, trained_initial_model_path, example_input_cpu_onnx_main)\n",
    "    print(f\"Trained initial model saved to: {trained_initial_model_path}\")\n",
    "\n",
    "    # **Modified: Baseline Evaluation with Trained Model**\n",
    "    print(f\"\\n--- Evaluating Baseline {MODEL_BASE_NAME} (Trained Initial) on TEST SET ---\")\n",
    "    baseline_metrics = evaluate_model_with_loss(initial_dense_model_ready, test_loader, example_input_dev_main, criterion_main, DEVICE)\n",
    "    print(f\"Baseline Metrics (Trained Initial, on Test Set): {baseline_metrics}\")\n",
    "\n",
    "    # Summarize the trained model\n",
    "    summarize_loaded_initial_model(\n",
    "        model_instance=initial_dense_model_ready,\n",
    "        model_name=f\"{MODEL_BASE_NAME}_TrainedInitial\",\n",
    "        eval_metrics=baseline_metrics,\n",
    "        example_input_for_summary=example_input_dev_main,\n",
    "        output_dir=initial_model_info_dir\n",
    "    )\n",
    "    plot_initial_model_training_summary(initial_model_hist, f\"{MODEL_BASE_NAME}_InitialTraining\", initial_model_info_dir)\n",
    "\n",
    "    # Initialize results with baseline\n",
    "    results_all = {s_name: {0.0: baseline_metrics} for s_name in cfg['strategies'].keys()}\n",
    "\n",
    "    # Pruning and fine-tuning loop\n",
    "    for strat_name, strat_details in cfg['strategies'].items():\n",
    "        strat_out_dir = os.path.join(cfg['output_dir_base'], \"strategies_results\", strat_name)\n",
    "        os.makedirs(strat_out_dir, exist_ok=True)\n",
    "        strat_finetune_plots_out_dir = os.path.join(strat_out_dir, \"finetuning_plots\")\n",
    "        os.makedirs(strat_finetune_plots_out_dir, exist_ok=True)\n",
    "\n",
    "        for ratio_val in cfg['pruning_ratios_to_test']:\n",
    "            if ratio_val == 0.0: continue\n",
    "            ratio_fname_str = f\"R{ratio_val:.1f}\".replace('.', 'p')\n",
    "            print(f\"\\n--- Proc: Strat '{strat_name}', Ratio {ratio_val:.2f} ({ratio_fname_str}) ---\")\n",
    "\n",
    "            # **Modified: Load the trained model**\n",
    "            model_to_prune_strat_ratio = load_model_state(\n",
    "                get_torchvision_mobilenetv2_structure,\n",
    "                trained_initial_model_path,  # Use the trained model\n",
    "                DEVICE,\n",
    "                num_classes_for_model_struct=cfg['num_classes']\n",
    "            )\n",
    "            ignored_layers_tv = get_ignored_layer_for_torchvision_mobilenetv2(model_to_prune_strat_ratio)\n",
    "\n",
    "            print(f\"--- Pruning: {strat_name}, ratio {ratio_val:.2f} ---\")\n",
    "            pruned_model_strat_ratio = gr_prune_model_by_ratio(\n",
    "                model_to_prune_strat_ratio, example_input_dev_main, strat_details, ratio_val,\n",
    "                explicit_ignored_layers_list=ignored_layers_tv, iterative_steps_general_pruner=cfg['iterative_steps_pruner_general'],\n",
    "                iterative_steps_taylor_pruner=cfg['iterative_steps_taylor_pruning']\n",
    "            )\n",
    "            pruned_bf_ft_fname = f\"{MODEL_BASE_NAME}_{strat_name}_{ratio_fname_str}_pruned_bf_ft\"\n",
    "            save_model(pruned_model_strat_ratio, os.path.join(strat_out_dir, f\"{pruned_bf_ft_fname}.pth\"), example_input_cpu_onnx_main)\n",
    "            metrics_bf_ft = evaluate_model_with_loss(pruned_model_strat_ratio, test_loader, example_input_dev_main, criterion_main, DEVICE)\n",
    "            print(f\"Metrics '{strat_name}' @ {ratio_fname_str} (Pruned, Pre-FT, Test Set): {metrics_bf_ft}\")\n",
    "\n",
    "            print(f\"--- Fine-tuning: {strat_name}, ratio {ratio_fname_str} ---\")\n",
    "            fine_tuned_model_strat_ratio, ft_hist = train_model(\n",
    "                pruned_model_strat_ratio, train_loader, criterion_main,\n",
    "                optim.Adam(pruned_model_strat_ratio.parameters(), lr=cfg['learning_rate_finetune']),\n",
    "                DEVICE, cfg['fine_tune_epochs'], val_loader=val_loader,\n",
    "                log_prefix_train=f\"{strat_name}_{ratio_fname_str}\", es_patience=cfg['early_stopping_patience'],\n",
    "                es_metric=cfg['early_stopping_metric'], load_best_es=cfg['load_best_model_on_early_stop']\n",
    "            )\n",
    "            macs_ft_val, _ = calculate_macs_params(fine_tuned_model_strat_ratio, example_input_dev_main)\n",
    "            plot_finetuning_curves(ft_hist, f\"{MODEL_BASE_NAME}_{strat_name}_{ratio_fname_str}\", strat_finetune_plots_out_dir, macs_ft_val)\n",
    "            final_metrics_strat_ratio = evaluate_model_with_loss(fine_tuned_model_strat_ratio, test_loader, example_input_dev_main, criterion_main, DEVICE)\n",
    "            results_all[strat_name][ratio_val] = final_metrics_strat_ratio\n",
    "            print(f\"Metrics '{strat_name}' @ {ratio_fname_str} (Fine-tuned, Test Set): {final_metrics_strat_ratio}\")\n",
    "            final_model_fname = f\"{MODEL_BASE_NAME}_{strat_name}_{ratio_fname_str}_final\"\n",
    "            save_model(fine_tuned_model_strat_ratio, os.path.join(strat_out_dir, f\"{final_model_fname}.pth\"), example_input_cpu_onnx_main)\n",
    "\n",
    "    plot_metrics_vs_ratio_all_strategies(results_all, cfg['pruning_ratios_to_test'], overall_comparison_plots_out_dir)\n",
    "    print(f\"\\nAll experiments complete! Outputs in: {os.path.abspath(cfg['output_dir_base'])}\")"
   ],
   "id": "dcb33f43a82b0f2a",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T09:48:39.703793Z",
     "start_time": "2025-05-23T09:41:39.684863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "2052b52af051bb04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initializing DataLoaders...\n",
      "Attempting to load CIFAR-10 from pre-downloaded directory: /home/muis/thesis/github-repo/master-thesis/cnn/mobile_net_v2/data\n",
      "DataLoaders: Train 45000, Val 5000, Test 10000\n",
      "--- Loading and Adapting Model from Local PTH: /home/muis/thesis/github-repo/master-thesis/cnn/mobile_net_v2/base/mobilenet_v2-b0353104.pth ---\n",
      "--- Training the initial model on CIFAR-10 ---\n",
      "E 1/30 (Initial Training): Tr L=1.2105, Acc=57.37%, Val L=0.9065, Acc=68.20% (New best)\n",
      "E 2/30 (Initial Training): Tr L=0.7921, Acc=72.04%, Val L=0.7653, Acc=72.92% (New best)\n",
      "E 3/30 (Initial Training): Tr L=0.6308, Acc=77.93%, Val L=0.7290, Acc=74.86% (New best)\n",
      "E 4/30 (Initial Training): Tr L=0.5157, Acc=82.02%, Val L=0.6947, Acc=75.80% (New best)\n",
      "E 5/30 (Initial Training): Tr L=0.4289, Acc=84.89%, Val L=0.6902, Acc=76.60% (New best)\n",
      "E 6/30 (Initial Training): Tr L=0.3407, Acc=88.08%, Val L=0.6908, Acc=77.58%\n",
      "E 7/30 (Initial Training): Tr L=0.2723, Acc=90.63%, Val L=0.7793, Acc=75.72%\n",
      "E 8/30 (Initial Training): Tr L=0.2323, Acc=91.81%, Val L=0.8147, Acc=76.52%\n",
      "E 9/30 (Initial Training): Tr L=0.1711, Acc=94.02%, Val L=0.8632, Acc=76.30%\n",
      "E 10/30 (Initial Training): Tr L=0.1440, Acc=94.92%, Val L=0.8990, Acc=76.54%\n",
      "E 11/30 (Initial Training): Tr L=0.1317, Acc=95.43%, Val L=0.9050, Acc=77.20%\n",
      "E 12/30 (Initial Training): Tr L=0.1141, Acc=96.06%, Val L=0.9367, Acc=76.86%\n",
      "EarlyStopping for 'Initial Training' at epoch 12.\n",
      "End of Train (Initial Training): Loading best state (val_loss=0.6902).\n",
      "✅ Model state_dict saved to ./output_mobilenet_v2-b0353104_local_pth_final/mobilenet_v2-b0353104_trained_initial.pth\n",
      "✅ Model saved as ONNX to ./output_mobilenet_v2-b0353104_local_pth_final/mobilenet_v2-b0353104_trained_initial.onnx\n",
      "Trained initial model saved to: ./output_mobilenet_v2-b0353104_local_pth_final/mobilenet_v2-b0353104_trained_initial.pth\n",
      "\n",
      "--- Evaluating Baseline mobilenet_v2-b0353104 (Trained Initial) on TEST SET ---\n",
      "Baseline Metrics (Trained Initial, on Test Set): {'macs': 6522122.0, 'params': 2236682, 'size_mib': 8.532264709472656, 'accuracy': 76.05, 'loss': 0.7181533943176269}\n",
      "✅ Architecture summary saved to: ./output_mobilenet_v2-b0353104_local_pth_final/initial_model_info/mobilenet_v2-b0353104_TrainedInitial_architecture_summary.txt\n",
      "✅ Evaluation metrics saved to: ./output_mobilenet_v2-b0353104_local_pth_final/initial_model_info/mobilenet_v2-b0353104_TrainedInitial_evaluation_metrics.txt\n",
      "✅ Metrics summary chart saved to: ./output_mobilenet_v2-b0353104_local_pth_final/initial_model_info/mobilenet_v2-b0353104_TrainedInitial_metrics_summary_chart.png\n",
      "✅ mobilenet_v2-b0353104_InitialTraining Loss/Ep saved to ./output_mobilenet_v2-b0353104_local_pth_final/initial_model_info\n",
      "✅ mobilenet_v2-b0353104_InitialTraining Loss/Acc saved to ./output_mobilenet_v2-b0353104_local_pth_final/initial_model_info\n",
      "\n",
      "--- Proc: Strat 'MagnitudeL2', Ratio 0.20 (R0p2) ---\n",
      "✅ State loaded into model structure from: ./output_mobilenet_v2-b0353104_local_pth_final/mobilenet_v2-b0353104_trained_initial.pth\n",
      "--- Pruning: MagnitudeL2, ratio 0.20 ---\n",
      "Initial MACs for ratio 0.20: 0.007 G\n",
      "Applying MagnitudeImportance @ ratio 0.20 with 5 pruner steps.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch_pruning/pruner/algorithms/base_pruner.py:87: UserWarning: ch_sparsity is deprecated in v1.3.0. Please use pruning_ratio.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning for ratio 0.20 done: MACs 0.006 G (Reduced by 8.05%)\n",
      "✅ Model state_dict saved to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/MagnitudeL2/mobilenet_v2-b0353104_MagnitudeL2_R0p2_pruned_bf_ft.pth\n",
      "✅ Model saved as ONNX to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/MagnitudeL2/mobilenet_v2-b0353104_MagnitudeL2_R0p2_pruned_bf_ft.onnx\n",
      "Metrics 'MagnitudeL2' @ R0p2 (Pruned, Pre-FT, Test Set): {'macs': 5997155.0, 'params': 2057471, 'size_mib': 7.848628997802734, 'accuracy': 61.54, 'loss': 1.1281787590026855}\n",
      "--- Fine-tuning: MagnitudeL2, ratio R0p2 ---\n",
      "E 1/30 (MagnitudeL2_R0p2): Tr L=0.5731, Acc=79.80%, Val L=0.6849, Acc=75.46% (New best)\n",
      "E 2/30 (MagnitudeL2_R0p2): Tr L=0.4242, Acc=84.80%, Val L=0.7062, Acc=76.78%\n",
      "E 3/30 (MagnitudeL2_R0p2): Tr L=0.3441, Acc=87.79%, Val L=0.6939, Acc=77.78%\n",
      "E 4/30 (MagnitudeL2_R0p2): Tr L=0.2697, Acc=90.56%, Val L=0.7427, Acc=77.20%\n",
      "E 5/30 (MagnitudeL2_R0p2): Tr L=0.2089, Acc=92.74%, Val L=0.7967, Acc=77.20%\n",
      "E 6/30 (MagnitudeL2_R0p2): Tr L=0.1749, Acc=93.76%, Val L=0.8386, Acc=77.60%\n",
      "E 7/30 (MagnitudeL2_R0p2): Tr L=0.1381, Acc=95.19%, Val L=0.8763, Acc=77.78%\n",
      "E 8/30 (MagnitudeL2_R0p2): Tr L=0.1237, Acc=95.67%, Val L=0.9314, Acc=77.42%\n",
      "EarlyStopping for 'MagnitudeL2_R0p2' at epoch 8.\n",
      "End of Train (MagnitudeL2_R0p2): Loading best state (val_loss=0.6849).\n",
      "✅ FT curves for mobilenet_v2-b0353104_MagnitudeL2_R0p2 saved.\n",
      "Metrics 'MagnitudeL2' @ R0p2 (Fine-tuned, Test Set): {'macs': 5997155.0, 'params': 2057471, 'size_mib': 7.848628997802734, 'accuracy': 76.36, 'loss': 0.7077880823135376}\n",
      "✅ Model state_dict saved to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/MagnitudeL2/mobilenet_v2-b0353104_MagnitudeL2_R0p2_final.pth\n",
      "✅ Model saved as ONNX to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/MagnitudeL2/mobilenet_v2-b0353104_MagnitudeL2_R0p2_final.onnx\n",
      "\n",
      "--- Proc: Strat 'MagnitudeL2', Ratio 0.50 (R0p5) ---\n",
      "✅ State loaded into model structure from: ./output_mobilenet_v2-b0353104_local_pth_final/mobilenet_v2-b0353104_trained_initial.pth\n",
      "--- Pruning: MagnitudeL2, ratio 0.50 ---\n",
      "Initial MACs for ratio 0.50: 0.007 G\n",
      "Applying MagnitudeImportance @ ratio 0.50 with 5 pruner steps.\n",
      "Pruning for ratio 0.50 done: MACs 0.005 G (Reduced by 18.71%)\n",
      "✅ Model state_dict saved to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/MagnitudeL2/mobilenet_v2-b0353104_MagnitudeL2_R0p5_pruned_bf_ft.pth\n",
      "✅ Model saved as ONNX to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/MagnitudeL2/mobilenet_v2-b0353104_MagnitudeL2_R0p5_pruned_bf_ft.onnx\n",
      "Metrics 'MagnitudeL2' @ R0p5 (Pruned, Pre-FT, Test Set): {'macs': 5301534.0, 'params': 1817034, 'size_mib': 6.931434631347656, 'accuracy': 37.21, 'loss': 1.8885358850479126}\n",
      "--- Fine-tuning: MagnitudeL2, ratio R0p5 ---\n",
      "E 1/30 (MagnitudeL2_R0p5): Tr L=0.8001, Acc=71.57%, Val L=0.7432, Acc=73.92% (New best)\n",
      "E 2/30 (MagnitudeL2_R0p5): Tr L=0.5932, Acc=79.19%, Val L=0.7072, Acc=74.94% (New best)\n",
      "E 3/30 (MagnitudeL2_R0p5): Tr L=0.4852, Acc=83.02%, Val L=0.6951, Acc=75.62% (New best)\n",
      "E 4/30 (MagnitudeL2_R0p5): Tr L=0.4026, Acc=85.69%, Val L=0.6985, Acc=76.80%\n",
      "E 5/30 (MagnitudeL2_R0p5): Tr L=0.3226, Acc=88.82%, Val L=0.7385, Acc=75.78%\n",
      "E 6/30 (MagnitudeL2_R0p5): Tr L=0.2635, Acc=90.89%, Val L=0.7668, Acc=76.28%\n",
      "E 7/30 (MagnitudeL2_R0p5): Tr L=0.2112, Acc=92.60%, Val L=0.8063, Acc=76.46%\n",
      "E 8/30 (MagnitudeL2_R0p5): Tr L=0.1764, Acc=93.96%, Val L=0.8690, Acc=76.06%\n",
      "E 9/30 (MagnitudeL2_R0p5): Tr L=0.1524, Acc=94.67%, Val L=0.9064, Acc=76.00%\n",
      "E 10/30 (MagnitudeL2_R0p5): Tr L=0.1301, Acc=95.46%, Val L=0.9156, Acc=77.06%\n",
      "EarlyStopping for 'MagnitudeL2_R0p5' at epoch 10.\n",
      "End of Train (MagnitudeL2_R0p5): Loading best state (val_loss=0.6951).\n",
      "✅ FT curves for mobilenet_v2-b0353104_MagnitudeL2_R0p5 saved.\n",
      "Metrics 'MagnitudeL2' @ R0p5 (Fine-tuned, Test Set): {'macs': 5301534.0, 'params': 1817034, 'size_mib': 6.931434631347656, 'accuracy': 75.98, 'loss': 0.7228256922721863}\n",
      "✅ Model state_dict saved to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/MagnitudeL2/mobilenet_v2-b0353104_MagnitudeL2_R0p5_final.pth\n",
      "✅ Model saved as ONNX to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/MagnitudeL2/mobilenet_v2-b0353104_MagnitudeL2_R0p5_final.onnx\n",
      "\n",
      "--- Proc: Strat 'MagnitudeL2', Ratio 0.70 (R0p7) ---\n",
      "✅ State loaded into model structure from: ./output_mobilenet_v2-b0353104_local_pth_final/mobilenet_v2-b0353104_trained_initial.pth\n",
      "--- Pruning: MagnitudeL2, ratio 0.70 ---\n",
      "Initial MACs for ratio 0.70: 0.007 G\n",
      "Applying MagnitudeImportance @ ratio 0.70 with 5 pruner steps.\n",
      "Pruning for ratio 0.70 done: MACs 0.005 G (Reduced by 25.26%)\n",
      "✅ Model state_dict saved to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/MagnitudeL2/mobilenet_v2-b0353104_MagnitudeL2_R0p7_pruned_bf_ft.pth\n",
      "✅ Model saved as ONNX to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/MagnitudeL2/mobilenet_v2-b0353104_MagnitudeL2_R0p7_pruned_bf_ft.onnx\n",
      "Metrics 'MagnitudeL2' @ R0p7 (Pruned, Pre-FT, Test Set): {'macs': 4874414.0, 'params': 1660684, 'size_mib': 6.3350067138671875, 'accuracy': 29.02, 'loss': 2.2481177589416506}\n",
      "--- Fine-tuning: MagnitudeL2, ratio R0p7 ---\n",
      "E 1/30 (MagnitudeL2_R0p7): Tr L=0.9434, Acc=66.88%, Val L=0.8344, Acc=70.68% (New best)\n",
      "E 2/30 (MagnitudeL2_R0p7): Tr L=0.6990, Acc=75.39%, Val L=0.7489, Acc=73.06% (New best)\n",
      "E 3/30 (MagnitudeL2_R0p7): Tr L=0.5764, Acc=79.43%, Val L=0.7238, Acc=75.06% (New best)\n",
      "E 4/30 (MagnitudeL2_R0p7): Tr L=0.4860, Acc=82.86%, Val L=0.7190, Acc=74.98% (New best)\n",
      "E 5/30 (MagnitudeL2_R0p7): Tr L=0.4014, Acc=85.92%, Val L=0.7499, Acc=75.42%\n",
      "E 6/30 (MagnitudeL2_R0p7): Tr L=0.3404, Acc=88.04%, Val L=0.7694, Acc=75.66%\n",
      "E 7/30 (MagnitudeL2_R0p7): Tr L=0.2812, Acc=90.26%, Val L=0.8061, Acc=75.68%\n",
      "E 8/30 (MagnitudeL2_R0p7): Tr L=0.2309, Acc=92.04%, Val L=0.8402, Acc=76.04%\n",
      "E 9/30 (MagnitudeL2_R0p7): Tr L=0.1979, Acc=92.98%, Val L=0.8942, Acc=75.64%\n",
      "E 10/30 (MagnitudeL2_R0p7): Tr L=0.1707, Acc=94.03%, Val L=0.9329, Acc=75.38%\n",
      "E 11/30 (MagnitudeL2_R0p7): Tr L=0.1459, Acc=94.89%, Val L=0.9777, Acc=76.10%\n",
      "EarlyStopping for 'MagnitudeL2_R0p7' at epoch 11.\n",
      "End of Train (MagnitudeL2_R0p7): Loading best state (val_loss=0.7190).\n",
      "✅ FT curves for mobilenet_v2-b0353104_MagnitudeL2_R0p7 saved.\n",
      "Metrics 'MagnitudeL2' @ R0p7 (Fine-tuned, Test Set): {'macs': 4874414.0, 'params': 1660684, 'size_mib': 6.3350067138671875, 'accuracy': 75.17, 'loss': 0.7512264091491699}\n",
      "✅ Model state_dict saved to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/MagnitudeL2/mobilenet_v2-b0353104_MagnitudeL2_R0p7_final.pth\n",
      "✅ Model saved as ONNX to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/MagnitudeL2/mobilenet_v2-b0353104_MagnitudeL2_R0p7_final.onnx\n",
      "\n",
      "--- Proc: Strat 'BNScale', Ratio 0.20 (R0p2) ---\n",
      "✅ State loaded into model structure from: ./output_mobilenet_v2-b0353104_local_pth_final/mobilenet_v2-b0353104_trained_initial.pth\n",
      "--- Pruning: BNScale, ratio 0.20 ---\n",
      "Initial MACs for ratio 0.20: 0.007 G\n",
      "Applying BNScaleImportance @ ratio 0.20 with 5 pruner steps.\n",
      "Pruning for ratio 0.20 done: MACs 0.006 G (Reduced by 8.05%)\n",
      "✅ Model state_dict saved to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/BNScale/mobilenet_v2-b0353104_BNScale_R0p2_pruned_bf_ft.pth\n",
      "✅ Model saved as ONNX to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/BNScale/mobilenet_v2-b0353104_BNScale_R0p2_pruned_bf_ft.onnx\n",
      "Metrics 'BNScale' @ R0p2 (Pruned, Pre-FT, Test Set): {'macs': 5997155.0, 'params': 2057471, 'size_mib': 7.848628997802734, 'accuracy': 25.65, 'loss': 3.2052071784973144}\n",
      "--- Fine-tuning: BNScale, ratio R0p2 ---\n",
      "E 1/30 (BNScale_R0p2): Tr L=0.7197, Acc=75.02%, Val L=0.7314, Acc=74.80% (New best)\n",
      "E 2/30 (BNScale_R0p2): Tr L=0.5259, Acc=81.55%, Val L=0.7140, Acc=75.04% (New best)\n",
      "E 3/30 (BNScale_R0p2): Tr L=0.4231, Acc=85.20%, Val L=0.7106, Acc=76.86% (New best)\n",
      "E 4/30 (BNScale_R0p2): Tr L=0.3337, Acc=88.38%, Val L=0.7341, Acc=76.62%\n",
      "E 5/30 (BNScale_R0p2): Tr L=0.2735, Acc=90.38%, Val L=0.7645, Acc=76.94%\n",
      "E 6/30 (BNScale_R0p2): Tr L=0.2211, Acc=92.15%, Val L=0.8203, Acc=76.66%\n",
      "E 7/30 (BNScale_R0p2): Tr L=0.1824, Acc=93.63%, Val L=0.8477, Acc=76.90%\n",
      "E 8/30 (BNScale_R0p2): Tr L=0.1475, Acc=94.92%, Val L=0.9000, Acc=77.04%\n",
      "E 9/30 (BNScale_R0p2): Tr L=0.1188, Acc=95.84%, Val L=0.9737, Acc=76.58%\n",
      "E 10/30 (BNScale_R0p2): Tr L=0.1125, Acc=96.08%, Val L=0.9958, Acc=76.70%\n",
      "EarlyStopping for 'BNScale_R0p2' at epoch 10.\n",
      "End of Train (BNScale_R0p2): Loading best state (val_loss=0.7106).\n",
      "✅ FT curves for mobilenet_v2-b0353104_BNScale_R0p2 saved.\n",
      "Metrics 'BNScale' @ R0p2 (Fine-tuned, Test Set): {'macs': 5997155.0, 'params': 2057471, 'size_mib': 7.848628997802734, 'accuracy': 76.25, 'loss': 0.737090769481659}\n",
      "✅ Model state_dict saved to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/BNScale/mobilenet_v2-b0353104_BNScale_R0p2_final.pth\n",
      "✅ Model saved as ONNX to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/BNScale/mobilenet_v2-b0353104_BNScale_R0p2_final.onnx\n",
      "\n",
      "--- Proc: Strat 'BNScale', Ratio 0.50 (R0p5) ---\n",
      "✅ State loaded into model structure from: ./output_mobilenet_v2-b0353104_local_pth_final/mobilenet_v2-b0353104_trained_initial.pth\n",
      "--- Pruning: BNScale, ratio 0.50 ---\n",
      "Initial MACs for ratio 0.50: 0.007 G\n",
      "Applying BNScaleImportance @ ratio 0.50 with 5 pruner steps.\n",
      "Pruning for ratio 0.50 done: MACs 0.005 G (Reduced by 18.71%)\n",
      "✅ Model state_dict saved to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/BNScale/mobilenet_v2-b0353104_BNScale_R0p5_pruned_bf_ft.pth\n",
      "✅ Model saved as ONNX to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/BNScale/mobilenet_v2-b0353104_BNScale_R0p5_pruned_bf_ft.onnx\n",
      "Metrics 'BNScale' @ R0p5 (Pruned, Pre-FT, Test Set): {'macs': 5301534.0, 'params': 1817034, 'size_mib': 6.931434631347656, 'accuracy': 13.2, 'loss': 3.354557749938965}\n",
      "--- Fine-tuning: BNScale, ratio R0p5 ---\n",
      "E 1/30 (BNScale_R0p5): Tr L=1.0033, Acc=65.28%, Val L=0.8733, Acc=68.80% (New best)\n",
      "E 2/30 (BNScale_R0p5): Tr L=0.7520, Acc=73.83%, Val L=0.7920, Acc=72.02% (New best)\n",
      "E 3/30 (BNScale_R0p5): Tr L=0.6190, Acc=78.40%, Val L=0.7676, Acc=73.60% (New best)\n",
      "E 4/30 (BNScale_R0p5): Tr L=0.5230, Acc=81.47%, Val L=0.7403, Acc=74.62% (New best)\n",
      "E 5/30 (BNScale_R0p5): Tr L=0.4469, Acc=84.37%, Val L=0.7754, Acc=74.08%\n",
      "E 6/30 (BNScale_R0p5): Tr L=0.3714, Acc=87.05%, Val L=0.7996, Acc=74.58%\n",
      "E 7/30 (BNScale_R0p5): Tr L=0.3112, Acc=89.08%, Val L=0.8562, Acc=74.74%\n",
      "E 8/30 (BNScale_R0p5): Tr L=0.2541, Acc=91.30%, Val L=0.8946, Acc=75.18%\n",
      "E 9/30 (BNScale_R0p5): Tr L=0.2237, Acc=92.12%, Val L=0.9334, Acc=74.38%\n",
      "E 10/30 (BNScale_R0p5): Tr L=0.1833, Acc=93.49%, Val L=0.9790, Acc=75.26%\n",
      "E 11/30 (BNScale_R0p5): Tr L=0.1639, Acc=94.25%, Val L=1.0048, Acc=74.78%\n",
      "EarlyStopping for 'BNScale_R0p5' at epoch 11.\n",
      "End of Train (BNScale_R0p5): Loading best state (val_loss=0.7403).\n",
      "✅ FT curves for mobilenet_v2-b0353104_BNScale_R0p5 saved.\n",
      "Metrics 'BNScale' @ R0p5 (Fine-tuned, Test Set): {'macs': 5301534.0, 'params': 1817034, 'size_mib': 6.931434631347656, 'accuracy': 74.32, 'loss': 0.7626246742248535}\n",
      "✅ Model state_dict saved to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/BNScale/mobilenet_v2-b0353104_BNScale_R0p5_final.pth\n",
      "✅ Model saved as ONNX to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/BNScale/mobilenet_v2-b0353104_BNScale_R0p5_final.onnx\n",
      "\n",
      "--- Proc: Strat 'BNScale', Ratio 0.70 (R0p7) ---\n",
      "✅ State loaded into model structure from: ./output_mobilenet_v2-b0353104_local_pth_final/mobilenet_v2-b0353104_trained_initial.pth\n",
      "--- Pruning: BNScale, ratio 0.70 ---\n",
      "Initial MACs for ratio 0.70: 0.007 G\n",
      "Applying BNScaleImportance @ ratio 0.70 with 5 pruner steps.\n",
      "Pruning for ratio 0.70 done: MACs 0.005 G (Reduced by 25.26%)\n",
      "✅ Model state_dict saved to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/BNScale/mobilenet_v2-b0353104_BNScale_R0p7_pruned_bf_ft.pth\n",
      "✅ Model saved as ONNX to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/BNScale/mobilenet_v2-b0353104_BNScale_R0p7_pruned_bf_ft.onnx\n",
      "Metrics 'BNScale' @ R0p7 (Pruned, Pre-FT, Test Set): {'macs': 4874414.0, 'params': 1660684, 'size_mib': 6.3350067138671875, 'accuracy': 12.09, 'loss': 2.8982230674743654}\n",
      "--- Fine-tuning: BNScale, ratio R0p7 ---\n",
      "E 1/30 (BNScale_R0p7): Tr L=1.1075, Acc=61.61%, Val L=0.9334, Acc=67.12% (New best)\n",
      "E 2/30 (BNScale_R0p7): Tr L=0.8352, Acc=70.59%, Val L=0.8413, Acc=69.96% (New best)\n",
      "E 3/30 (BNScale_R0p7): Tr L=0.7051, Acc=75.17%, Val L=0.8039, Acc=72.14% (New best)\n",
      "E 4/30 (BNScale_R0p7): Tr L=0.6128, Acc=78.17%, Val L=0.7879, Acc=72.84% (New best)\n",
      "E 5/30 (BNScale_R0p7): Tr L=0.5275, Acc=81.42%, Val L=0.7744, Acc=73.36% (New best)\n",
      "E 6/30 (BNScale_R0p7): Tr L=0.4519, Acc=84.05%, Val L=0.8317, Acc=72.56%\n",
      "E 7/30 (BNScale_R0p7): Tr L=0.3928, Acc=86.07%, Val L=0.8140, Acc=73.50%\n",
      "E 8/30 (BNScale_R0p7): Tr L=0.3423, Acc=87.84%, Val L=0.8511, Acc=74.12%\n",
      "E 9/30 (BNScale_R0p7): Tr L=0.2912, Acc=89.70%, Val L=0.9087, Acc=73.44%\n",
      "E 10/30 (BNScale_R0p7): Tr L=0.2481, Acc=91.30%, Val L=0.9448, Acc=74.34%\n",
      "E 11/30 (BNScale_R0p7): Tr L=0.2132, Acc=92.49%, Val L=0.9788, Acc=73.78%\n",
      "E 12/30 (BNScale_R0p7): Tr L=0.1853, Acc=93.61%, Val L=1.0386, Acc=73.40%\n",
      "EarlyStopping for 'BNScale_R0p7' at epoch 12.\n",
      "End of Train (BNScale_R0p7): Loading best state (val_loss=0.7744).\n",
      "✅ FT curves for mobilenet_v2-b0353104_BNScale_R0p7 saved.\n",
      "Metrics 'BNScale' @ R0p7 (Fine-tuned, Test Set): {'macs': 4874414.0, 'params': 1660684, 'size_mib': 6.3350067138671875, 'accuracy': 72.86, 'loss': 0.8055851736068725}\n",
      "✅ Model state_dict saved to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/BNScale/mobilenet_v2-b0353104_BNScale_R0p7_final.pth\n",
      "✅ Model saved as ONNX to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/BNScale/mobilenet_v2-b0353104_BNScale_R0p7_final.onnx\n",
      "\n",
      "--- Proc: Strat 'Random', Ratio 0.20 (R0p2) ---\n",
      "✅ State loaded into model structure from: ./output_mobilenet_v2-b0353104_local_pth_final/mobilenet_v2-b0353104_trained_initial.pth\n",
      "--- Pruning: Random, ratio 0.20 ---\n",
      "Initial MACs for ratio 0.20: 0.007 G\n",
      "Applying RandomImportance @ ratio 0.20 with 5 pruner steps.\n",
      "Pruning for ratio 0.20 done: MACs 0.006 G (Reduced by 8.05%)\n",
      "✅ Model state_dict saved to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/Random/mobilenet_v2-b0353104_Random_R0p2_pruned_bf_ft.pth\n",
      "✅ Model saved as ONNX to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/Random/mobilenet_v2-b0353104_Random_R0p2_pruned_bf_ft.onnx\n",
      "Metrics 'Random' @ R0p2 (Pruned, Pre-FT, Test Set): {'macs': 5997155.0, 'params': 2057471, 'size_mib': 7.848628997802734, 'accuracy': 24.71, 'loss': 2.6877522186279297}\n",
      "--- Fine-tuning: Random, ratio R0p2 ---\n",
      "E 1/30 (Random_R0p2): Tr L=0.7579, Acc=73.47%, Val L=0.7269, Acc=74.20% (New best)\n",
      "E 2/30 (Random_R0p2): Tr L=0.5509, Acc=80.52%, Val L=0.6819, Acc=75.94% (New best)\n",
      "E 3/30 (Random_R0p2): Tr L=0.4438, Acc=84.42%, Val L=0.6997, Acc=76.08%\n",
      "E 4/30 (Random_R0p2): Tr L=0.3603, Acc=87.25%, Val L=0.6976, Acc=77.38%\n",
      "E 5/30 (Random_R0p2): Tr L=0.2814, Acc=90.15%, Val L=0.7552, Acc=77.02%\n",
      "E 6/30 (Random_R0p2): Tr L=0.2286, Acc=91.95%, Val L=0.7772, Acc=77.34%\n",
      "E 7/30 (Random_R0p2): Tr L=0.1837, Acc=93.57%, Val L=0.8436, Acc=77.44%\n",
      "E 8/30 (Random_R0p2): Tr L=0.1556, Acc=94.56%, Val L=0.8873, Acc=77.58%\n",
      "E 9/30 (Random_R0p2): Tr L=0.1333, Acc=95.32%, Val L=0.9098, Acc=77.46%\n",
      "EarlyStopping for 'Random_R0p2' at epoch 9.\n",
      "End of Train (Random_R0p2): Loading best state (val_loss=0.6819).\n",
      "✅ FT curves for mobilenet_v2-b0353104_Random_R0p2 saved.\n",
      "Metrics 'Random' @ R0p2 (Fine-tuned, Test Set): {'macs': 5997155.0, 'params': 2057471, 'size_mib': 7.848628997802734, 'accuracy': 76.58, 'loss': 0.7050074172973633}\n",
      "✅ Model state_dict saved to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/Random/mobilenet_v2-b0353104_Random_R0p2_final.pth\n",
      "✅ Model saved as ONNX to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/Random/mobilenet_v2-b0353104_Random_R0p2_final.onnx\n",
      "\n",
      "--- Proc: Strat 'Random', Ratio 0.50 (R0p5) ---\n",
      "✅ State loaded into model structure from: ./output_mobilenet_v2-b0353104_local_pth_final/mobilenet_v2-b0353104_trained_initial.pth\n",
      "--- Pruning: Random, ratio 0.50 ---\n",
      "Initial MACs for ratio 0.50: 0.007 G\n",
      "Applying RandomImportance @ ratio 0.50 with 5 pruner steps.\n",
      "Pruning for ratio 0.50 done: MACs 0.005 G (Reduced by 18.71%)\n",
      "✅ Model state_dict saved to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/Random/mobilenet_v2-b0353104_Random_R0p5_pruned_bf_ft.pth\n",
      "✅ Model saved as ONNX to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/Random/mobilenet_v2-b0353104_Random_R0p5_pruned_bf_ft.onnx\n",
      "Metrics 'Random' @ R0p5 (Pruned, Pre-FT, Test Set): {'macs': 5301534.0, 'params': 1817034, 'size_mib': 6.931434631347656, 'accuracy': 14.04, 'loss': 3.807326854324341}\n",
      "--- Fine-tuning: Random, ratio R0p5 ---\n",
      "E 1/30 (Random_R0p5): Tr L=1.0632, Acc=63.14%, Val L=0.9008, Acc=68.22% (New best)\n",
      "E 2/30 (Random_R0p5): Tr L=0.7807, Acc=72.68%, Val L=0.8232, Acc=71.02% (New best)\n",
      "E 3/30 (Random_R0p5): Tr L=0.6539, Acc=76.93%, Val L=0.7682, Acc=73.36% (New best)\n",
      "E 4/30 (Random_R0p5): Tr L=0.5556, Acc=80.30%, Val L=0.7658, Acc=73.58% (New best)\n",
      "E 5/30 (Random_R0p5): Tr L=0.4685, Acc=83.56%, Val L=0.7957, Acc=73.04%\n",
      "E 6/30 (Random_R0p5): Tr L=0.3951, Acc=86.15%, Val L=0.8019, Acc=74.44%\n",
      "E 7/30 (Random_R0p5): Tr L=0.3273, Acc=88.50%, Val L=0.8478, Acc=73.72%\n",
      "E 8/30 (Random_R0p5): Tr L=0.2753, Acc=90.45%, Val L=0.8945, Acc=74.32%\n",
      "E 9/30 (Random_R0p5): Tr L=0.2291, Acc=92.02%, Val L=0.9627, Acc=73.44%\n",
      "E 10/30 (Random_R0p5): Tr L=0.1946, Acc=93.22%, Val L=0.9813, Acc=74.20%\n",
      "E 11/30 (Random_R0p5): Tr L=0.1576, Acc=94.60%, Val L=1.0306, Acc=73.96%\n",
      "EarlyStopping for 'Random_R0p5' at epoch 11.\n",
      "End of Train (Random_R0p5): Loading best state (val_loss=0.7658).\n",
      "✅ FT curves for mobilenet_v2-b0353104_Random_R0p5 saved.\n",
      "Metrics 'Random' @ R0p5 (Fine-tuned, Test Set): {'macs': 5301534.0, 'params': 1817034, 'size_mib': 6.931434631347656, 'accuracy': 73.44, 'loss': 0.7857154356002808}\n",
      "✅ Model state_dict saved to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/Random/mobilenet_v2-b0353104_Random_R0p5_final.pth\n",
      "✅ Model saved as ONNX to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/Random/mobilenet_v2-b0353104_Random_R0p5_final.onnx\n",
      "\n",
      "--- Proc: Strat 'Random', Ratio 0.70 (R0p7) ---\n",
      "✅ State loaded into model structure from: ./output_mobilenet_v2-b0353104_local_pth_final/mobilenet_v2-b0353104_trained_initial.pth\n",
      "--- Pruning: Random, ratio 0.70 ---\n",
      "Initial MACs for ratio 0.70: 0.007 G\n",
      "Applying RandomImportance @ ratio 0.70 with 5 pruner steps.\n",
      "Pruning for ratio 0.70 done: MACs 0.005 G (Reduced by 25.26%)\n",
      "✅ Model state_dict saved to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/Random/mobilenet_v2-b0353104_Random_R0p7_pruned_bf_ft.pth\n",
      "✅ Model saved as ONNX to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/Random/mobilenet_v2-b0353104_Random_R0p7_pruned_bf_ft.onnx\n",
      "Metrics 'Random' @ R0p7 (Pruned, Pre-FT, Test Set): {'macs': 4874414.0, 'params': 1660684, 'size_mib': 6.3350067138671875, 'accuracy': 11.08, 'loss': 3.709833403778076}\n",
      "--- Fine-tuning: Random, ratio R0p7 ---\n",
      "E 1/30 (Random_R0p7): Tr L=1.3796, Acc=51.83%, Val L=1.1415, Acc=59.34% (New best)\n",
      "E 2/30 (Random_R0p7): Tr L=1.0217, Acc=64.12%, Val L=0.9983, Acc=64.74% (New best)\n",
      "E 3/30 (Random_R0p7): Tr L=0.8619, Acc=69.49%, Val L=0.9382, Acc=66.48% (New best)\n",
      "E 4/30 (Random_R0p7): Tr L=0.7525, Acc=73.38%, Val L=0.8952, Acc=69.36% (New best)\n",
      "E 5/30 (Random_R0p7): Tr L=0.6515, Acc=76.91%, Val L=0.8895, Acc=70.12% (New best)\n",
      "E 6/30 (Random_R0p7): Tr L=0.5687, Acc=80.02%, Val L=0.9054, Acc=69.64%\n",
      "E 7/30 (Random_R0p7): Tr L=0.4939, Acc=82.51%, Val L=0.9184, Acc=70.50%\n",
      "E 8/30 (Random_R0p7): Tr L=0.4219, Acc=84.96%, Val L=0.9452, Acc=70.98%\n",
      "E 9/30 (Random_R0p7): Tr L=0.3635, Acc=87.08%, Val L=1.0152, Acc=69.06%\n",
      "E 10/30 (Random_R0p7): Tr L=0.3109, Acc=89.14%, Val L=1.0511, Acc=70.12%\n",
      "E 11/30 (Random_R0p7): Tr L=0.2645, Acc=90.76%, Val L=1.1190, Acc=69.80%\n",
      "E 12/30 (Random_R0p7): Tr L=0.2313, Acc=91.78%, Val L=1.1458, Acc=69.82%\n",
      "EarlyStopping for 'Random_R0p7' at epoch 12.\n",
      "End of Train (Random_R0p7): Loading best state (val_loss=0.8895).\n",
      "✅ FT curves for mobilenet_v2-b0353104_Random_R0p7 saved.\n",
      "Metrics 'Random' @ R0p7 (Fine-tuned, Test Set): {'macs': 4874414.0, 'params': 1660684, 'size_mib': 6.3350067138671875, 'accuracy': 69.96, 'loss': 0.8880257802963257}\n",
      "✅ Model state_dict saved to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/Random/mobilenet_v2-b0353104_Random_R0p7_final.pth\n",
      "✅ Model saved as ONNX to ./output_mobilenet_v2-b0353104_local_pth_final/strategies_results/Random/mobilenet_v2-b0353104_Random_R0p7_final.onnx\n",
      "✅ MACS vs. Ratio plot saved to ./output_mobilenet_v2-b0353104_local_pth_final/overall_comparison_plots\n",
      "✅ LOSS vs. Ratio plot saved to ./output_mobilenet_v2-b0353104_local_pth_final/overall_comparison_plots\n",
      "✅ ACCURACY vs. Ratio plot saved to ./output_mobilenet_v2-b0353104_local_pth_final/overall_comparison_plots\n",
      "\n",
      "All experiments complete! Outputs in: /home/muis/thesis/github-repo/master-thesis/cnn/mobile_net_v2/output_mobilenet_v2-b0353104_local_pth_final\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
