{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T23:42:06.735607Z",
     "start_time": "2025-05-28T23:35:37.557646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_pruning as tp\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_BASE_NAME = \"mobilenet_v2\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "def get_data_loaders(data_dir_path='./data', batch_size=128, val_split=0.1, seed=42):\n",
    "    \"\"\"Load CIFAR-10 dataset with train/val/test splits\"\"\"\n",
    "    abs_data_dir = os.path.abspath(data_dir_path)\n",
    "    print(f\"Loading CIFAR-10 from: {abs_data_dir}\")\n",
    "\n",
    "    transform_cifar = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "\n",
    "    # Load CIFAR-10 from local directory (assuming it's already downloaded)\n",
    "    full_train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=abs_data_dir, train=True, download=False, transform=transform_cifar\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=abs_data_dir, train=False, download=False, transform=transform_cifar\n",
    "    )\n",
    "\n",
    "    # Create train/validation split\n",
    "    val_size = int(len(full_train_dataset) * val_split)\n",
    "    train_size = len(full_train_dataset) - val_size\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_train_dataset, [train_size, val_size], generator=generator\n",
    "    )\n",
    "\n",
    "    # Create data loaders\n",
    "    num_workers = min(4, os.cpu_count() or 2)\n",
    "    pin_memory = True if DEVICE.type == 'cuda' else False\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=num_workers, pin_memory=pin_memory)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                            num_workers=num_workers, pin_memory=pin_memory)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                             num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    print(f\"DataLoaders created - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "def get_mobilenetv2_model(num_classes=10, use_pretrained=True, pretrained_path='./base/mobilenet_v2-b0353104.pth'):\n",
    "    \"\"\"Get MobileNetV2 model adapted for CIFAR-10\"\"\"\n",
    "    # Always create model without weights first\n",
    "    model = models.mobilenet_v2(weights=None)\n",
    "\n",
    "    if use_pretrained and os.path.exists(pretrained_path):\n",
    "        # Load pre-downloaded weights from local file\n",
    "        print(f\"Loading pre-trained weights from: {pretrained_path}\")\n",
    "        pretrained_state_dict = torch.load(pretrained_path, map_location=DEVICE)\n",
    "\n",
    "        # Load the weights, ignoring the classifier layer if it doesn't match\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_state_dict.items()\n",
    "                           if k in model_dict and model_dict[k].shape == v.shape}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict, strict=False)\n",
    "        print(\"✅ Loaded MobileNetV2 with pre-downloaded ImageNet weights\")\n",
    "    else:\n",
    "        if use_pretrained:\n",
    "            print(f\"Warning: Pre-trained weights not found at {pretrained_path}\")\n",
    "        print(\"✅ Created MobileNetV2 without pretrained weights\")\n",
    "\n",
    "    # Adapt classifier for CIFAR-10\n",
    "    if isinstance(model.classifier, nn.Sequential):\n",
    "        # Find the Linear layer in classifier\n",
    "        for i, layer in enumerate(model.classifier):\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                in_features = layer.in_features\n",
    "                model.classifier[i] = nn.Linear(in_features, num_classes)\n",
    "                break\n",
    "    elif isinstance(model.classifier, nn.Linear):\n",
    "        model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
    "\n",
    "    print(f\"✅ Adapted classifier for {num_classes} classes\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_ignored_layers(model):\n",
    "    \"\"\"Get layers to ignore during pruning (typically final classifier)\"\"\"\n",
    "    ignored_layers = []\n",
    "    if hasattr(model, 'classifier'):\n",
    "        if isinstance(model.classifier, nn.Sequential):\n",
    "            for layer in model.classifier:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    ignored_layers.append(layer)\n",
    "        elif isinstance(model.classifier, nn.Linear):\n",
    "            ignored_layers.append(model.classifier)\n",
    "    return ignored_layers\n",
    "\n",
    "\n",
    "def calculate_macs_params(model, example_input):\n",
    "    \"\"\"Calculate MACs and parameters using torch_pruning\"\"\"\n",
    "    model.eval()\n",
    "    target_device = example_input.device\n",
    "    model_on_device = model.to(target_device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        macs, params = tp.utils.count_ops_and_params(model_on_device, example_input)\n",
    "\n",
    "    return macs, params\n",
    "\n",
    "\n",
    "def save_model(model, save_path, example_input_cpu=None):\n",
    "    \"\"\"Save model state dict and optionally ONNX\"\"\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"✅ Model saved to {save_path}\")\n",
    "\n",
    "    if example_input_cpu is not None:\n",
    "        onnx_path = save_path.replace('.pth', '.onnx')\n",
    "        try:\n",
    "            model_cpu = model.to('cpu')\n",
    "            torch.onnx.export(\n",
    "                model_cpu, example_input_cpu, onnx_path,\n",
    "                export_params=True, opset_version=13,\n",
    "                input_names=['input'], output_names=['output'],\n",
    "                dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "            )\n",
    "            print(f\"✅ ONNX model saved to {onnx_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: ONNX export failed: {e}\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, example_input, criterion, device):\n",
    "    \"\"\"Evaluate model and return comprehensive metrics\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Calculate efficiency metrics\n",
    "    macs, params = calculate_macs_params(model, example_input.to(device))\n",
    "    model_size_mb = params * 4 / (1024 * 1024)  # Assuming float32\n",
    "\n",
    "    # Calculate accuracy and loss\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / total if total > 0 else float('nan')\n",
    "    accuracy = 100.0 * correct / total if total > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'loss': avg_loss,\n",
    "        'macs': macs,\n",
    "        'params': params,\n",
    "        'size_mb': model_size_mb\n",
    "    }\n",
    "\n",
    "\n",
    "def prune_model(model, strategy_config, sparsity_ratio, example_input, ignored_layers=None):\n",
    "    \"\"\"Apply structured pruning to model\"\"\"\n",
    "    if sparsity_ratio == 0.0:\n",
    "        print(\"No pruning needed (sparsity = 0.0)\")\n",
    "        return model\n",
    "\n",
    "    model.eval()\n",
    "    pruned_model = copy.deepcopy(model)\n",
    "    pruned_model.to(example_input.device)\n",
    "\n",
    "    # Calculate initial MACs\n",
    "    initial_macs, _ = calculate_macs_params(pruned_model, example_input)\n",
    "    print(f\"Initial MACs: {initial_macs / 1e6:.2f}M\")\n",
    "\n",
    "    ignored_layers = ignored_layers or []\n",
    "\n",
    "    # Create pruner based on strategy\n",
    "    pruner = strategy_config['pruner'](\n",
    "        pruned_model,\n",
    "        example_input,\n",
    "        importance=strategy_config['importance'],\n",
    "        iterative_steps=5,  # Use 5 iterative steps\n",
    "        ch_sparsity=sparsity_ratio,\n",
    "        root_module_types=[nn.Conv2d],\n",
    "        ignored_layers=ignored_layers\n",
    "    )\n",
    "\n",
    "    print(f\"Applying {strategy_config['importance'].__class__.__name__} pruning at {sparsity_ratio:.1%} sparsity...\")\n",
    "\n",
    "    # Apply pruning\n",
    "    pruner.step()\n",
    "\n",
    "    # Calculate final MACs\n",
    "    final_macs, _ = calculate_macs_params(pruned_model, example_input)\n",
    "    reduction = (initial_macs - final_macs) / initial_macs * 100 if initial_macs > 0 else 0\n",
    "    print(f\"Final MACs: {final_macs / 1e6:.2f}M (Reduction: {reduction:.1f}%)\")\n",
    "\n",
    "    return pruned_model\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs,\n",
    "                val_loader=None, patience=7, log_prefix=\"\"):\n",
    "    \"\"\"Train model with early stopping\"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total += target.size(0)\n",
    "            train_correct += (predicted == target).sum().item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_acc = 100.0 * train_correct / train_total\n",
    "\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "\n",
    "        log_msg = f\"Epoch {epoch + 1}/{num_epochs} ({log_prefix}): Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%\"\n",
    "\n",
    "        # Validation phase\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for data, target in val_loader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = torch.max(output.data, 1)\n",
    "                    val_total += target.size(0)\n",
    "                    val_correct += (predicted == target).sum().item()\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_acc = 100.0 * val_correct / val_total\n",
    "\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            history['val_acc'].append(val_acc)\n",
    "\n",
    "            log_msg += f\", Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%\"\n",
    "\n",
    "            # Early stopping check\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                epochs_no_improve = 0\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "                log_msg += \" (Best)\"\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"{log_msg}\")\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "                break\n",
    "        else:\n",
    "            history['val_loss'].append(None)\n",
    "            history['val_acc'].append(None)\n",
    "\n",
    "        print(log_msg)\n",
    "\n",
    "    # Load best model state if available\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"Loaded best model state\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def save_results_to_files(all_results, output_dir):\n",
    "    \"\"\"Save experimental results to JSON and CSV files\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save complete results as JSON\n",
    "    results_json_path = os.path.join(output_dir, 'complete_results.json')\n",
    "    with open(results_json_path, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2, default=str)\n",
    "    print(f\"✅ Complete results saved to {results_json_path}\")\n",
    "\n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for strategy, strategy_results in all_results.items():\n",
    "        for sparsity, metrics in strategy_results.items():\n",
    "            row = {\n",
    "                'strategy': strategy,\n",
    "                'sparsity_ratio': sparsity,\n",
    "                'accuracy': metrics['accuracy'],\n",
    "                'loss': metrics['loss'],\n",
    "                'macs_millions': metrics['macs'] / 1e6,\n",
    "                'params_millions': metrics['params'] / 1e6,\n",
    "                'size_mb': metrics['size_mb']\n",
    "            }\n",
    "            summary_data.append(row)\n",
    "\n",
    "    # Save summary as CSV\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_csv_path = os.path.join(output_dir, 'summary_results.csv')\n",
    "    summary_df.to_csv(summary_csv_path, index=False)\n",
    "    print(f\"✅ Summary results saved to {summary_csv_path}\")\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "def create_results_plots(summary_df, output_dir):\n",
    "    \"\"\"Create visualization plots\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    strategies = summary_df['strategy'].unique()\n",
    "    sparsity_levels = sorted(summary_df['sparsity_ratio'].unique())\n",
    "\n",
    "    # Plot 1: Accuracy vs Sparsity\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for strategy in strategies:\n",
    "        strategy_data = summary_df[summary_df['strategy'] == strategy].sort_values('sparsity_ratio')\n",
    "        plt.plot(strategy_data['sparsity_ratio'] * 100, strategy_data['accuracy'],\n",
    "                 'o-', linewidth=2, markersize=8, label=strategy)\n",
    "\n",
    "    plt.xlabel('Sparsity (%)', fontsize=12)\n",
    "    plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "    plt.title('MobileNetV2: Accuracy vs Sparsity', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(output_dir, 'accuracy_vs_sparsity.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✅ Accuracy plot saved to {plot_path}\")\n",
    "\n",
    "    # Plot 2: Efficiency frontier (Accuracy vs MACs)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for strategy in strategies:\n",
    "        strategy_data = summary_df[summary_df['strategy'] == strategy].sort_values('sparsity_ratio')\n",
    "        plt.scatter(strategy_data['macs_millions'], strategy_data['accuracy'],\n",
    "                    s=100, label=strategy, alpha=0.8)\n",
    "        plt.plot(strategy_data['macs_millions'], strategy_data['accuracy'],\n",
    "                 '--', alpha=0.6)\n",
    "\n",
    "    plt.xlabel('MACs (Millions)', fontsize=12)\n",
    "    plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "    plt.title('MobileNetV2: Efficiency Frontier (Accuracy vs MACs)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(output_dir, 'efficiency_frontier.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✅ Efficiency frontier plot saved to {plot_path}\")\n",
    "\n",
    "\n",
    "def print_results_table(summary_df):\n",
    "    \"\"\"Print formatted results table\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXPERIMENTAL RESULTS SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Baseline results\n",
    "    baseline_results = summary_df[summary_df['sparsity_ratio'] == 0.0].iloc[0]\n",
    "    print(f\"\\nBaseline Performance:\")\n",
    "    print(f\"  Accuracy: {baseline_results['accuracy']:.2f}%\")\n",
    "    print(f\"  MACs: {baseline_results['macs_millions']:.2f}M\")\n",
    "    print(f\"  Parameters: {baseline_results['params_millions']:.2f}M\")\n",
    "    print(f\"  Model Size: {baseline_results['size_mb']:.2f}MB\")\n",
    "\n",
    "    # Strategy comparison at 50% sparsity\n",
    "    print(f\"\\nStrategy Comparison at 50% Sparsity:\")\n",
    "    sparsity_50_data = summary_df[summary_df['sparsity_ratio'] == 0.5]\n",
    "    for _, row in sparsity_50_data.iterrows():\n",
    "        degradation = baseline_results['accuracy'] - row['accuracy']\n",
    "        retention = (row['accuracy'] / baseline_results['accuracy']) * 100\n",
    "        print(\n",
    "            f\"  {row['strategy']:>12}: {row['accuracy']:>6.2f}% accuracy ({degradation:>+5.2f}%, {retention:>5.1f}% retention)\")\n",
    "\n",
    "    # Complete results table\n",
    "    print(f\"\\nComplete Results Table:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Strategy':<12} {'Sparsity':<8} {'Accuracy':<8} {'MACs(M)':<8} {'Params(M)':<9} {'Size(MB)':<8}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for _, row in summary_df.sort_values(['strategy', 'sparsity_ratio']).iterrows():\n",
    "        print(f\"{row['strategy']:<12} {row['sparsity_ratio'] * 100:>6.0f}% \"\n",
    "              f\"{row['accuracy']:>7.2f}% {row['macs_millions']:>7.2f} \"\n",
    "              f\"{row['params_millions']:>8.2f} {row['size_mb']:>7.2f}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main experimental workflow\"\"\"\n",
    "    print(\"Starting MobileNetV2 CIFAR-10 Pruning Experiments\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'strategies': {\n",
    "            'BNScale': {\n",
    "                'pruner': tp.pruner.BNScalePruner,\n",
    "                'importance': tp.importance.BNScaleImportance()\n",
    "            },\n",
    "            'MagnitudeL2': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.MagnitudeImportance(p=2)\n",
    "            },\n",
    "            'Random': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.RandomImportance()\n",
    "            },\n",
    "        },\n",
    "        'pruning_ratios': [0.0, 0.2, 0.5, 0.7],\n",
    "        'num_classes': 10,\n",
    "        'batch_size': 128,\n",
    "        'learning_rate': 0.0001,\n",
    "        'epochs': 30,\n",
    "        'patience': 7,\n",
    "        'output_dir': './results_mobilenetv2_cifar10',\n",
    "        'models_dir': './base',\n",
    "        'pretrained_path': './base/mobilenet_v2-b0353104.pth'  # Path to pre-downloaded weights\n",
    "    }\n",
    "\n",
    "    # Create output directories\n",
    "    os.makedirs(config['output_dir'], exist_ok=True)\n",
    "    os.makedirs(config['models_dir'], exist_ok=True)\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading CIFAR-10 dataset...\")\n",
    "    train_loader, val_loader, test_loader = get_data_loaders(\n",
    "        batch_size=config['batch_size']\n",
    "    )\n",
    "\n",
    "    # Prepare inputs and criterion\n",
    "    example_input_cpu = torch.randn(1, 3, 32, 32)\n",
    "    example_input_device = example_input_cpu.to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Get baseline model and train it\n",
    "    print(\"\\nCreating and training baseline model...\")\n",
    "    model = get_mobilenetv2_model(\n",
    "        num_classes=config['num_classes'],\n",
    "        use_pretrained=True,\n",
    "        pretrained_path=config['pretrained_path']\n",
    "    )\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Train baseline model\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    trained_model, training_history = train_model(\n",
    "        model, train_loader, criterion, optimizer, DEVICE,\n",
    "        config['epochs'], val_loader, config['patience'], \"Baseline Training\"\n",
    "    )\n",
    "\n",
    "    # Save baseline model\n",
    "    baseline_model_path = os.path.join(config['models_dir'], 'baseline_model.pth')\n",
    "    save_model(trained_model, baseline_model_path, example_input_cpu)\n",
    "\n",
    "    # Evaluate baseline\n",
    "    print(\"\\nEvaluating baseline model...\")\n",
    "    baseline_metrics = evaluate_model(trained_model, test_loader, example_input_device, criterion, DEVICE)\n",
    "    print(f\"Baseline Results: Accuracy={baseline_metrics['accuracy']:.2f}%, \"\n",
    "          f\"MACs={baseline_metrics['macs'] / 1e6:.2f}M, \"\n",
    "          f\"Params={baseline_metrics['params'] / 1e6:.2f}M\")\n",
    "\n",
    "    # Initialize results storage\n",
    "    all_results = {}\n",
    "    for strategy_name in config['strategies'].keys():\n",
    "        all_results[strategy_name] = {0.0: baseline_metrics}\n",
    "\n",
    "    # Get ignored layers\n",
    "    ignored_layers = get_ignored_layers(trained_model)\n",
    "\n",
    "    # Run pruning experiments\n",
    "    print(\"\\nStarting pruning experiments...\")\n",
    "    for strategy_name, strategy_config in config['strategies'].items():\n",
    "        print(f\"\\n--- Strategy: {strategy_name} ---\")\n",
    "\n",
    "        for sparsity_ratio in config['pruning_ratios']:\n",
    "            if sparsity_ratio == 0.0:\n",
    "                continue  # Skip baseline (already done)\n",
    "\n",
    "            print(f\"\\nProcessing {strategy_name} at {sparsity_ratio:.1%} sparsity...\")\n",
    "\n",
    "            # Load fresh copy of trained baseline\n",
    "            model_copy = get_mobilenetv2_model(\n",
    "                num_classes=config['num_classes'],\n",
    "                use_pretrained=False\n",
    "            )\n",
    "            model_copy.load_state_dict(torch.load(baseline_model_path, map_location=DEVICE))\n",
    "            model_copy.to(DEVICE)\n",
    "\n",
    "            # Apply pruning\n",
    "            pruned_model = prune_model(\n",
    "                model_copy, strategy_config, sparsity_ratio,\n",
    "                example_input_device, ignored_layers\n",
    "            )\n",
    "\n",
    "            # Fine-tune pruned model\n",
    "            print(\"Fine-tuning pruned model...\")\n",
    "            optimizer_ft = optim.Adam(pruned_model.parameters(), lr=config['learning_rate'])\n",
    "            fine_tuned_model, ft_history = train_model(\n",
    "                pruned_model, train_loader, criterion, optimizer_ft, DEVICE,\n",
    "                config['epochs'], val_loader, config['patience'],\n",
    "                f\"{strategy_name}-{sparsity_ratio:.1%}\"\n",
    "            )\n",
    "\n",
    "            # Evaluate fine-tuned model\n",
    "            final_metrics = evaluate_model(fine_tuned_model, test_loader, example_input_device, criterion, DEVICE)\n",
    "            all_results[strategy_name][sparsity_ratio] = final_metrics\n",
    "\n",
    "            print(f\"Results: Accuracy={final_metrics['accuracy']:.2f}%, \"\n",
    "                  f\"MACs={final_metrics['macs'] / 1e6:.2f}M\")\n",
    "\n",
    "            # Save fine-tuned model\n",
    "            model_filename = f\"{strategy_name.lower()}_sparsity_{sparsity_ratio:.1f}.pth\"\n",
    "            model_path = os.path.join(config['models_dir'], model_filename)\n",
    "            save_model(fine_tuned_model, model_path, example_input_cpu)\n",
    "\n",
    "    # Save and analyze results\n",
    "    print(\"\\nSaving results...\")\n",
    "    summary_df = save_results_to_files(all_results, config['output_dir'])\n",
    "\n",
    "    # Create plots\n",
    "    print(\"Creating plots...\")\n",
    "    create_results_plots(summary_df, config['output_dir'])\n",
    "\n",
    "    # Print summary\n",
    "    print_results_table(summary_df)\n",
    "\n",
    "    print(f\"\\n🎉 All experiments completed!\")\n",
    "    print(f\"📁 Results saved to: {os.path.abspath(config['output_dir'])}\")\n",
    "    print(f\"📁 Models saved to: {os.path.abspath(config['models_dir'])}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "10ba42bbb036a1fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting MobileNetV2 CIFAR-10 Pruning Experiments\n",
      "============================================================\n",
      "Loading CIFAR-10 dataset...\n",
      "Loading CIFAR-10 from: /home/muis/thesis/github-repo/master-thesis/cnn/mobile_net_v2/data\n",
      "DataLoaders created - Train: 45000, Val: 5000, Test: 10000\n",
      "\n",
      "Creating and training baseline model...\n",
      "Loading pre-trained weights from: ./base/mobilenet_v2-b0353104.pth\n",
      "✅ Loaded MobileNetV2 with pre-downloaded ImageNet weights\n",
      "✅ Adapted classifier for 10 classes\n",
      "Epoch 1/30 (Baseline Training): Train Loss: 1.1789, Train Acc: 58.31%, Val Loss: 0.8614, Val Acc: 68.76% (Best)\n",
      "Epoch 2/30 (Baseline Training): Train Loss: 0.7497, Train Acc: 73.55%, Val Loss: 0.7360, Val Acc: 73.56% (Best)\n",
      "Epoch 3/30 (Baseline Training): Train Loss: 0.5960, Train Acc: 79.02%, Val Loss: 0.6817, Val Acc: 76.28% (Best)\n",
      "Epoch 4/30 (Baseline Training): Train Loss: 0.4822, Train Acc: 83.01%, Val Loss: 0.6636, Val Acc: 77.80% (Best)\n",
      "Epoch 5/30 (Baseline Training): Train Loss: 0.3971, Train Acc: 85.97%, Val Loss: 0.6673, Val Acc: 78.16%\n",
      "Epoch 6/30 (Baseline Training): Train Loss: 0.3176, Train Acc: 88.80%, Val Loss: 0.6949, Val Acc: 78.54%\n",
      "Epoch 7/30 (Baseline Training): Train Loss: 0.2521, Train Acc: 91.15%, Val Loss: 0.7332, Val Acc: 77.90%\n",
      "Epoch 8/30 (Baseline Training): Train Loss: 0.2062, Train Acc: 92.84%, Val Loss: 0.7676, Val Acc: 78.22%\n",
      "Epoch 9/30 (Baseline Training): Train Loss: 0.1712, Train Acc: 94.04%, Val Loss: 0.7983, Val Acc: 78.14%\n",
      "Epoch 10/30 (Baseline Training): Train Loss: 0.1318, Train Acc: 95.42%, Val Loss: 0.8520, Val Acc: 78.12%\n",
      "Epoch 11/30 (Baseline Training): Train Loss: 0.1148, Train Acc: 95.98%, Val Loss: 0.9142, Val Acc: 77.40%\n",
      "Early stopping triggered after 11 epochs\n",
      "Loaded best model state\n",
      "✅ Model saved to ./base/baseline_model.pth\n",
      "✅ ONNX model saved to ./base/baseline_model.onnx\n",
      "\n",
      "Evaluating baseline model...\n",
      "Baseline Results: Accuracy=77.14%, MACs=6.52M, Params=2.24M\n",
      "\n",
      "Starting pruning experiments...\n",
      "\n",
      "--- Strategy: BNScale ---\n",
      "\n",
      "Processing BNScale at 20.0% sparsity...\n",
      "✅ Created MobileNetV2 without pretrained weights\n",
      "✅ Adapted classifier for 10 classes\n",
      "Initial MACs: 6.52M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muis/.virtualenvs/master-thesis/lib/python3.12/site-packages/torch_pruning/pruner/algorithms/base_pruner.py:87: UserWarning: ch_sparsity is deprecated in v1.3.0. Please use pruning_ratio.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying BNScaleImportance pruning at 20.0% sparsity...\n",
      "Final MACs: 6.00M (Reduction: 8.0%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/30 (BNScale-20.0%): Train Loss: 0.6941, Train Acc: 75.75%, Val Loss: 0.7017, Val Acc: 75.46% (Best)\n",
      "Epoch 2/30 (BNScale-20.0%): Train Loss: 0.5219, Train Acc: 81.51%, Val Loss: 0.6853, Val Acc: 76.52% (Best)\n",
      "Epoch 3/30 (BNScale-20.0%): Train Loss: 0.4200, Train Acc: 85.12%, Val Loss: 0.6878, Val Acc: 76.60%\n",
      "Epoch 4/30 (BNScale-20.0%): Train Loss: 0.3386, Train Acc: 88.18%, Val Loss: 0.7055, Val Acc: 77.20%\n",
      "Epoch 5/30 (BNScale-20.0%): Train Loss: 0.2716, Train Acc: 90.48%, Val Loss: 0.7655, Val Acc: 77.48%\n",
      "Epoch 6/30 (BNScale-20.0%): Train Loss: 0.2199, Train Acc: 92.26%, Val Loss: 0.7792, Val Acc: 77.42%\n",
      "Epoch 7/30 (BNScale-20.0%): Train Loss: 0.1769, Train Acc: 93.74%, Val Loss: 0.8502, Val Acc: 76.98%\n",
      "Epoch 8/30 (BNScale-20.0%): Train Loss: 0.1463, Train Acc: 94.87%, Val Loss: 0.8843, Val Acc: 76.34%\n",
      "Epoch 9/30 (BNScale-20.0%): Train Loss: 0.1282, Train Acc: 95.56%, Val Loss: 0.9289, Val Acc: 77.52%\n",
      "Early stopping triggered after 9 epochs\n",
      "Loaded best model state\n",
      "Results: Accuracy=76.67%, MACs=6.00M\n",
      "✅ Model saved to ./base/bnscale_sparsity_0.2.pth\n",
      "✅ ONNX model saved to ./base/bnscale_sparsity_0.2.onnx\n",
      "\n",
      "Processing BNScale at 50.0% sparsity...\n",
      "✅ Created MobileNetV2 without pretrained weights\n",
      "✅ Adapted classifier for 10 classes\n",
      "Initial MACs: 6.52M\n",
      "Applying BNScaleImportance pruning at 50.0% sparsity...\n",
      "Final MACs: 5.30M (Reduction: 18.7%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/30 (BNScale-50.0%): Train Loss: 0.9886, Train Acc: 65.68%, Val Loss: 0.8552, Val Acc: 69.44% (Best)\n",
      "Epoch 2/30 (BNScale-50.0%): Train Loss: 0.7496, Train Acc: 73.95%, Val Loss: 0.7966, Val Acc: 72.26% (Best)\n",
      "Epoch 3/30 (BNScale-50.0%): Train Loss: 0.6297, Train Acc: 77.88%, Val Loss: 0.7802, Val Acc: 72.30% (Best)\n",
      "Epoch 4/30 (BNScale-50.0%): Train Loss: 0.5331, Train Acc: 81.18%, Val Loss: 0.7592, Val Acc: 73.96% (Best)\n",
      "Epoch 5/30 (BNScale-50.0%): Train Loss: 0.4725, Train Acc: 83.41%, Val Loss: 0.7599, Val Acc: 73.78%\n",
      "Epoch 6/30 (BNScale-50.0%): Train Loss: 0.3909, Train Acc: 86.30%, Val Loss: 0.7974, Val Acc: 74.32%\n",
      "Epoch 7/30 (BNScale-50.0%): Train Loss: 0.3185, Train Acc: 88.79%, Val Loss: 0.8298, Val Acc: 74.44%\n",
      "Epoch 8/30 (BNScale-50.0%): Train Loss: 0.2609, Train Acc: 90.77%, Val Loss: 0.9030, Val Acc: 74.86%\n",
      "Epoch 9/30 (BNScale-50.0%): Train Loss: 0.2296, Train Acc: 91.80%, Val Loss: 0.8829, Val Acc: 74.76%\n",
      "Epoch 10/30 (BNScale-50.0%): Train Loss: 0.1889, Train Acc: 93.36%, Val Loss: 0.9573, Val Acc: 74.42%\n",
      "Epoch 11/30 (BNScale-50.0%): Train Loss: 0.1804, Train Acc: 93.65%, Val Loss: 0.9794, Val Acc: 75.22%\n",
      "Early stopping triggered after 11 epochs\n",
      "Loaded best model state\n",
      "Results: Accuracy=74.44%, MACs=5.30M\n",
      "✅ Model saved to ./base/bnscale_sparsity_0.5.pth\n",
      "✅ ONNX model saved to ./base/bnscale_sparsity_0.5.onnx\n",
      "\n",
      "Processing BNScale at 70.0% sparsity...\n",
      "✅ Created MobileNetV2 without pretrained weights\n",
      "✅ Adapted classifier for 10 classes\n",
      "Initial MACs: 6.52M\n",
      "Applying BNScaleImportance pruning at 70.0% sparsity...\n",
      "Final MACs: 4.87M (Reduction: 25.3%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/30 (BNScale-70.0%): Train Loss: 1.0646, Train Acc: 62.92%, Val Loss: 0.9069, Val Acc: 68.40% (Best)\n",
      "Epoch 2/30 (BNScale-70.0%): Train Loss: 0.8122, Train Acc: 71.47%, Val Loss: 0.8356, Val Acc: 70.52% (Best)\n",
      "Epoch 3/30 (BNScale-70.0%): Train Loss: 0.6928, Train Acc: 75.48%, Val Loss: 0.7775, Val Acc: 73.10% (Best)\n",
      "Epoch 4/30 (BNScale-70.0%): Train Loss: 0.5988, Train Acc: 78.88%, Val Loss: 0.7611, Val Acc: 74.02% (Best)\n",
      "Epoch 5/30 (BNScale-70.0%): Train Loss: 0.5204, Train Acc: 81.67%, Val Loss: 0.7893, Val Acc: 73.22%\n",
      "Epoch 6/30 (BNScale-70.0%): Train Loss: 0.4573, Train Acc: 83.62%, Val Loss: 0.8057, Val Acc: 73.80%\n",
      "Epoch 7/30 (BNScale-70.0%): Train Loss: 0.3872, Train Acc: 86.42%, Val Loss: 0.8238, Val Acc: 73.74%\n",
      "Epoch 8/30 (BNScale-70.0%): Train Loss: 0.3568, Train Acc: 87.32%, Val Loss: 0.8447, Val Acc: 73.76%\n",
      "Epoch 9/30 (BNScale-70.0%): Train Loss: 0.2875, Train Acc: 89.82%, Val Loss: 0.9126, Val Acc: 73.16%\n",
      "Epoch 10/30 (BNScale-70.0%): Train Loss: 0.2456, Train Acc: 91.34%, Val Loss: 0.9502, Val Acc: 73.06%\n",
      "Epoch 11/30 (BNScale-70.0%): Train Loss: 0.2285, Train Acc: 91.93%, Val Loss: 1.0055, Val Acc: 73.24%\n",
      "Early stopping triggered after 11 epochs\n",
      "Loaded best model state\n",
      "Results: Accuracy=73.07%, MACs=4.87M\n",
      "✅ Model saved to ./base/bnscale_sparsity_0.7.pth\n",
      "✅ ONNX model saved to ./base/bnscale_sparsity_0.7.onnx\n",
      "\n",
      "--- Strategy: MagnitudeL2 ---\n",
      "\n",
      "Processing MagnitudeL2 at 20.0% sparsity...\n",
      "✅ Created MobileNetV2 without pretrained weights\n",
      "✅ Adapted classifier for 10 classes\n",
      "Initial MACs: 6.52M\n",
      "Applying MagnitudeImportance pruning at 20.0% sparsity...\n",
      "Final MACs: 6.00M (Reduction: 8.0%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/30 (MagnitudeL2-20.0%): Train Loss: 0.5840, Train Acc: 79.39%, Val Loss: 0.6568, Val Acc: 77.50% (Best)\n",
      "Epoch 2/30 (MagnitudeL2-20.0%): Train Loss: 0.4376, Train Acc: 84.50%, Val Loss: 0.6772, Val Acc: 77.38%\n",
      "Epoch 3/30 (MagnitudeL2-20.0%): Train Loss: 0.3511, Train Acc: 87.57%, Val Loss: 0.6808, Val Acc: 78.00%\n",
      "Epoch 4/30 (MagnitudeL2-20.0%): Train Loss: 0.2838, Train Acc: 89.98%, Val Loss: 0.7277, Val Acc: 77.46%\n",
      "Epoch 5/30 (MagnitudeL2-20.0%): Train Loss: 0.2158, Train Acc: 92.56%, Val Loss: 0.7634, Val Acc: 78.18%\n",
      "Epoch 6/30 (MagnitudeL2-20.0%): Train Loss: 0.1786, Train Acc: 93.78%, Val Loss: 0.8420, Val Acc: 77.90%\n",
      "Epoch 7/30 (MagnitudeL2-20.0%): Train Loss: 0.1462, Train Acc: 94.88%, Val Loss: 0.8869, Val Acc: 77.30%\n",
      "Epoch 8/30 (MagnitudeL2-20.0%): Train Loss: 0.1203, Train Acc: 95.81%, Val Loss: 0.9707, Val Acc: 77.16%\n",
      "Early stopping triggered after 8 epochs\n",
      "Loaded best model state\n",
      "Results: Accuracy=76.91%, MACs=6.00M\n",
      "✅ Model saved to ./base/magnitudel2_sparsity_0.2.pth\n",
      "✅ ONNX model saved to ./base/magnitudel2_sparsity_0.2.onnx\n",
      "\n",
      "Processing MagnitudeL2 at 50.0% sparsity...\n",
      "✅ Created MobileNetV2 without pretrained weights\n",
      "✅ Adapted classifier for 10 classes\n",
      "Initial MACs: 6.52M\n",
      "Applying MagnitudeImportance pruning at 50.0% sparsity...\n",
      "Final MACs: 5.30M (Reduction: 18.7%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/30 (MagnitudeL2-50.0%): Train Loss: 0.7800, Train Acc: 72.57%, Val Loss: 0.7403, Val Acc: 73.60% (Best)\n",
      "Epoch 2/30 (MagnitudeL2-50.0%): Train Loss: 0.5858, Train Acc: 79.30%, Val Loss: 0.6848, Val Acc: 76.10% (Best)\n",
      "Epoch 3/30 (MagnitudeL2-50.0%): Train Loss: 0.4757, Train Acc: 83.08%, Val Loss: 0.6773, Val Acc: 77.00% (Best)\n",
      "Epoch 4/30 (MagnitudeL2-50.0%): Train Loss: 0.3918, Train Acc: 86.07%, Val Loss: 0.6786, Val Acc: 77.10%\n",
      "Epoch 5/30 (MagnitudeL2-50.0%): Train Loss: 0.3182, Train Acc: 88.73%, Val Loss: 0.7105, Val Acc: 76.98%\n",
      "Epoch 6/30 (MagnitudeL2-50.0%): Train Loss: 0.2550, Train Acc: 91.09%, Val Loss: 0.7970, Val Acc: 76.10%\n",
      "Epoch 7/30 (MagnitudeL2-50.0%): Train Loss: 0.2121, Train Acc: 92.47%, Val Loss: 0.8109, Val Acc: 77.00%\n",
      "Epoch 8/30 (MagnitudeL2-50.0%): Train Loss: 0.1745, Train Acc: 93.87%, Val Loss: 0.8384, Val Acc: 76.90%\n",
      "Epoch 9/30 (MagnitudeL2-50.0%): Train Loss: 0.1438, Train Acc: 94.96%, Val Loss: 0.8879, Val Acc: 77.88%\n",
      "Epoch 10/30 (MagnitudeL2-50.0%): Train Loss: 0.1217, Train Acc: 95.80%, Val Loss: 0.9893, Val Acc: 76.06%\n",
      "Early stopping triggered after 10 epochs\n",
      "Loaded best model state\n",
      "Results: Accuracy=76.54%, MACs=5.30M\n",
      "✅ Model saved to ./base/magnitudel2_sparsity_0.5.pth\n",
      "✅ ONNX model saved to ./base/magnitudel2_sparsity_0.5.onnx\n",
      "\n",
      "Processing MagnitudeL2 at 70.0% sparsity...\n",
      "✅ Created MobileNetV2 without pretrained weights\n",
      "✅ Adapted classifier for 10 classes\n",
      "Initial MACs: 6.52M\n",
      "Applying MagnitudeImportance pruning at 70.0% sparsity...\n",
      "Final MACs: 4.87M (Reduction: 25.3%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/30 (MagnitudeL2-70.0%): Train Loss: 0.8982, Train Acc: 68.40%, Val Loss: 0.7916, Val Acc: 72.54% (Best)\n",
      "Epoch 2/30 (MagnitudeL2-70.0%): Train Loss: 0.6724, Train Acc: 76.37%, Val Loss: 0.7278, Val Acc: 74.34% (Best)\n",
      "Epoch 3/30 (MagnitudeL2-70.0%): Train Loss: 0.5561, Train Acc: 80.49%, Val Loss: 0.7059, Val Acc: 75.48% (Best)\n",
      "Epoch 4/30 (MagnitudeL2-70.0%): Train Loss: 0.4666, Train Acc: 83.54%, Val Loss: 0.7010, Val Acc: 75.56% (Best)\n",
      "Epoch 5/30 (MagnitudeL2-70.0%): Train Loss: 0.3968, Train Acc: 86.04%, Val Loss: 0.7206, Val Acc: 75.88%\n",
      "Epoch 6/30 (MagnitudeL2-70.0%): Train Loss: 0.3270, Train Acc: 88.58%, Val Loss: 0.7516, Val Acc: 76.76%\n",
      "Epoch 7/30 (MagnitudeL2-70.0%): Train Loss: 0.2737, Train Acc: 90.42%, Val Loss: 0.7665, Val Acc: 76.28%\n",
      "Epoch 8/30 (MagnitudeL2-70.0%): Train Loss: 0.2275, Train Acc: 92.06%, Val Loss: 0.8559, Val Acc: 75.56%\n",
      "Epoch 9/30 (MagnitudeL2-70.0%): Train Loss: 0.1927, Train Acc: 93.35%, Val Loss: 0.8749, Val Acc: 76.18%\n",
      "Epoch 10/30 (MagnitudeL2-70.0%): Train Loss: 0.1661, Train Acc: 94.10%, Val Loss: 0.9172, Val Acc: 76.38%\n",
      "Epoch 11/30 (MagnitudeL2-70.0%): Train Loss: 0.1392, Train Acc: 95.22%, Val Loss: 0.9696, Val Acc: 76.92%\n",
      "Early stopping triggered after 11 epochs\n",
      "Loaded best model state\n",
      "Results: Accuracy=75.58%, MACs=4.87M\n",
      "✅ Model saved to ./base/magnitudel2_sparsity_0.7.pth\n",
      "✅ ONNX model saved to ./base/magnitudel2_sparsity_0.7.onnx\n",
      "\n",
      "--- Strategy: Random ---\n",
      "\n",
      "Processing Random at 20.0% sparsity...\n",
      "✅ Created MobileNetV2 without pretrained weights\n",
      "✅ Adapted classifier for 10 classes\n",
      "Initial MACs: 6.52M\n",
      "Applying RandomImportance pruning at 20.0% sparsity...\n",
      "Final MACs: 6.00M (Reduction: 8.0%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/30 (Random-20.0%): Train Loss: 0.7371, Train Acc: 74.18%, Val Loss: 0.7315, Val Acc: 74.12% (Best)\n",
      "Epoch 2/30 (Random-20.0%): Train Loss: 0.5474, Train Acc: 80.78%, Val Loss: 0.7099, Val Acc: 75.06% (Best)\n",
      "Epoch 3/30 (Random-20.0%): Train Loss: 0.4393, Train Acc: 84.42%, Val Loss: 0.6884, Val Acc: 76.28% (Best)\n",
      "Epoch 4/30 (Random-20.0%): Train Loss: 0.3576, Train Acc: 87.36%, Val Loss: 0.7131, Val Acc: 76.34%\n",
      "Epoch 5/30 (Random-20.0%): Train Loss: 0.2800, Train Acc: 90.07%, Val Loss: 0.7881, Val Acc: 76.58%\n",
      "Epoch 6/30 (Random-20.0%): Train Loss: 0.2287, Train Acc: 91.98%, Val Loss: 0.8063, Val Acc: 77.12%\n",
      "Epoch 7/30 (Random-20.0%): Train Loss: 0.1888, Train Acc: 93.46%, Val Loss: 0.8802, Val Acc: 76.16%\n",
      "Epoch 8/30 (Random-20.0%): Train Loss: 0.1574, Train Acc: 94.40%, Val Loss: 0.9359, Val Acc: 76.62%\n",
      "Epoch 9/30 (Random-20.0%): Train Loss: 0.1272, Train Acc: 95.47%, Val Loss: 0.9651, Val Acc: 76.88%\n",
      "Epoch 10/30 (Random-20.0%): Train Loss: 0.1085, Train Acc: 96.22%, Val Loss: 0.9978, Val Acc: 76.50%\n",
      "Early stopping triggered after 10 epochs\n",
      "Loaded best model state\n",
      "Results: Accuracy=76.99%, MACs=6.00M\n",
      "✅ Model saved to ./base/random_sparsity_0.2.pth\n",
      "✅ ONNX model saved to ./base/random_sparsity_0.2.onnx\n",
      "\n",
      "Processing Random at 50.0% sparsity...\n",
      "✅ Created MobileNetV2 without pretrained weights\n",
      "✅ Adapted classifier for 10 classes\n",
      "Initial MACs: 6.52M\n",
      "Applying RandomImportance pruning at 50.0% sparsity...\n",
      "Final MACs: 5.30M (Reduction: 18.7%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/30 (Random-50.0%): Train Loss: 1.1326, Train Acc: 59.76%, Val Loss: 0.9481, Val Acc: 65.72% (Best)\n",
      "Epoch 2/30 (Random-50.0%): Train Loss: 0.8472, Train Acc: 70.04%, Val Loss: 0.8480, Val Acc: 69.10% (Best)\n",
      "Epoch 3/30 (Random-50.0%): Train Loss: 0.7114, Train Acc: 74.62%, Val Loss: 0.8227, Val Acc: 70.00% (Best)\n",
      "Epoch 4/30 (Random-50.0%): Train Loss: 0.6060, Train Acc: 78.67%, Val Loss: 0.8026, Val Acc: 71.44% (Best)\n",
      "Epoch 5/30 (Random-50.0%): Train Loss: 0.5177, Train Acc: 81.50%, Val Loss: 0.8204, Val Acc: 71.68%\n",
      "Epoch 6/30 (Random-50.0%): Train Loss: 0.4436, Train Acc: 84.15%, Val Loss: 0.8230, Val Acc: 72.28%\n",
      "Epoch 7/30 (Random-50.0%): Train Loss: 0.3729, Train Acc: 86.73%, Val Loss: 0.8553, Val Acc: 73.06%\n",
      "Epoch 8/30 (Random-50.0%): Train Loss: 0.3125, Train Acc: 88.79%, Val Loss: 0.9121, Val Acc: 72.74%\n",
      "Epoch 9/30 (Random-50.0%): Train Loss: 0.2553, Train Acc: 90.92%, Val Loss: 0.9450, Val Acc: 72.68%\n",
      "Epoch 10/30 (Random-50.0%): Train Loss: 0.2224, Train Acc: 92.26%, Val Loss: 1.0219, Val Acc: 71.82%\n",
      "Epoch 11/30 (Random-50.0%): Train Loss: 0.1793, Train Acc: 93.66%, Val Loss: 1.0769, Val Acc: 73.06%\n",
      "Early stopping triggered after 11 epochs\n",
      "Loaded best model state\n",
      "Results: Accuracy=71.65%, MACs=5.30M\n",
      "✅ Model saved to ./base/random_sparsity_0.5.pth\n",
      "✅ ONNX model saved to ./base/random_sparsity_0.5.onnx\n",
      "\n",
      "Processing Random at 70.0% sparsity...\n",
      "✅ Created MobileNetV2 without pretrained weights\n",
      "✅ Adapted classifier for 10 classes\n",
      "Initial MACs: 6.52M\n",
      "Applying RandomImportance pruning at 70.0% sparsity...\n",
      "Final MACs: 4.87M (Reduction: 25.3%)\n",
      "Fine-tuning pruned model...\n",
      "Epoch 1/30 (Random-70.0%): Train Loss: 1.2902, Train Acc: 54.62%, Val Loss: 1.1065, Val Acc: 60.56% (Best)\n",
      "Epoch 2/30 (Random-70.0%): Train Loss: 0.9994, Train Acc: 64.67%, Val Loss: 0.9763, Val Acc: 65.26% (Best)\n",
      "Epoch 3/30 (Random-70.0%): Train Loss: 0.8552, Train Acc: 69.84%, Val Loss: 0.9251, Val Acc: 66.86% (Best)\n",
      "Epoch 4/30 (Random-70.0%): Train Loss: 0.7468, Train Acc: 73.65%, Val Loss: 0.8717, Val Acc: 68.46% (Best)\n",
      "Epoch 5/30 (Random-70.0%): Train Loss: 0.6530, Train Acc: 77.01%, Val Loss: 0.8629, Val Acc: 69.84% (Best)\n",
      "Epoch 6/30 (Random-70.0%): Train Loss: 0.5671, Train Acc: 80.20%, Val Loss: 0.8818, Val Acc: 69.68%\n",
      "Epoch 7/30 (Random-70.0%): Train Loss: 0.4882, Train Acc: 82.84%, Val Loss: 0.8997, Val Acc: 69.44%\n",
      "Epoch 8/30 (Random-70.0%): Train Loss: 0.4187, Train Acc: 85.32%, Val Loss: 0.9518, Val Acc: 69.82%\n",
      "Epoch 9/30 (Random-70.0%): Train Loss: 0.3622, Train Acc: 87.11%, Val Loss: 0.9863, Val Acc: 70.08%\n",
      "Epoch 10/30 (Random-70.0%): Train Loss: 0.3059, Train Acc: 89.22%, Val Loss: 1.0474, Val Acc: 69.22%\n",
      "Epoch 11/30 (Random-70.0%): Train Loss: 0.2634, Train Acc: 90.85%, Val Loss: 1.1142, Val Acc: 69.76%\n",
      "Epoch 12/30 (Random-70.0%): Train Loss: 0.2288, Train Acc: 91.95%, Val Loss: 1.1305, Val Acc: 69.66%\n",
      "Early stopping triggered after 12 epochs\n",
      "Loaded best model state\n",
      "Results: Accuracy=69.80%, MACs=4.87M\n",
      "✅ Model saved to ./base/random_sparsity_0.7.pth\n",
      "✅ ONNX model saved to ./base/random_sparsity_0.7.onnx\n",
      "\n",
      "Saving results...\n",
      "✅ Complete results saved to ./results_mobilenetv2_cifar10/complete_results.json\n",
      "✅ Summary results saved to ./results_mobilenetv2_cifar10/summary_results.csv\n",
      "Creating plots...\n",
      "✅ Accuracy plot saved to ./results_mobilenetv2_cifar10/accuracy_vs_sparsity.png\n",
      "✅ Efficiency frontier plot saved to ./results_mobilenetv2_cifar10/efficiency_frontier.png\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENTAL RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Baseline Performance:\n",
      "  Accuracy: 77.14%\n",
      "  MACs: 6.52M\n",
      "  Parameters: 2.24M\n",
      "  Model Size: 8.53MB\n",
      "\n",
      "Strategy Comparison at 50% Sparsity:\n",
      "       BNScale:  74.44% accuracy (+2.70%,  96.5% retention)\n",
      "   MagnitudeL2:  76.54% accuracy (+0.60%,  99.2% retention)\n",
      "        Random:  71.65% accuracy (+5.49%,  92.9% retention)\n",
      "\n",
      "Complete Results Table:\n",
      "--------------------------------------------------------------------------------\n",
      "Strategy     Sparsity Accuracy MACs(M)  Params(M) Size(MB)\n",
      "--------------------------------------------------------------------------------\n",
      "BNScale           0%   77.14%    6.52     2.24    8.53\n",
      "BNScale          20%   76.67%    6.00     2.06    7.85\n",
      "BNScale          50%   74.44%    5.30     1.82    6.93\n",
      "BNScale          70%   73.07%    4.87     1.66    6.34\n",
      "MagnitudeL2       0%   77.14%    6.52     2.24    8.53\n",
      "MagnitudeL2      20%   76.91%    6.00     2.06    7.85\n",
      "MagnitudeL2      50%   76.54%    5.30     1.82    6.93\n",
      "MagnitudeL2      70%   75.58%    4.87     1.66    6.34\n",
      "Random            0%   77.14%    6.52     2.24    8.53\n",
      "Random           20%   76.99%    6.00     2.06    7.85\n",
      "Random           50%   71.65%    5.30     1.82    6.93\n",
      "Random           70%   69.80%    4.87     1.66    6.34\n",
      "\n",
      "🎉 All experiments completed!\n",
      "📁 Results saved to: /home/muis/thesis/github-repo/master-thesis/cnn/mobile_net_v2/results_mobilenetv2_cifar10\n",
      "📁 Models saved to: /home/muis/thesis/github-repo/master-thesis/cnn/mobile_net_v2/base\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
