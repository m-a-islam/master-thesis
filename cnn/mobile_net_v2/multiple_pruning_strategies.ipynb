{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "377de838",
   "metadata": {},
   "source": [
    "## import necessary libraries for pruning"
   ]
  },
  {
   "cell_type": "code",
   "id": "7b6381473b81b890",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T23:36:53.021197Z",
     "start_time": "2025-05-19T23:36:53.016417Z"
    }
   },
   "source": [
    "import torch, copy\n",
    "import torch.nn as nn\n",
    "import torch_pruning as tp\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "from torch_pruning.pruner.algorithms.scheduler import linear_scheduler\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "from cnn.resNet.resnet_example import get_data_loaders\n",
    "import torch\n",
    "from torch import nn"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "id": "631981174f5d8d8a",
   "metadata": {},
   "source": [
    "### Seed Network"
   ]
  },
  {
   "cell_type": "code",
   "id": "248e7c99c6815764",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T20:49:05.575772Z",
     "start_time": "2025-05-19T20:49:05.570145Z"
    }
   },
   "source": [
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, expansion=6):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.use_res_connect = self.stride == 1 and in_channels == out_channels\n",
    "\n",
    "        # Standard PyTorch layers (NO torch_pruning wrappers needed)\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels * expansion, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels * expansion)\n",
    "        self.relu = nn.ReLU6(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels * expansion, in_channels * expansion, kernel_size=3,\n",
    "            stride=stride, padding=1, groups=in_channels * expansion, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(in_channels * expansion)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels * expansion, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x if self.use_res_connect else None\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.use_res_connect:\n",
    "            return identity + out\n",
    "        else:\n",
    "            return out"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "6431a8ad17e1d446",
   "metadata": {},
   "source": [
    "### Mask Network"
   ]
  },
  {
   "cell_type": "code",
   "id": "4da304cf4cf9f871",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T20:49:07.890143Z",
     "start_time": "2025-05-19T20:49:07.883937Z"
    }
   },
   "source": [
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        # --- Remove mask-related parameters ---\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU6(inplace=True)\n",
    "\n",
    "        # Define blocks (no mask_index or mask)\n",
    "        self.block1 = InvertedResidual(32, 16, stride=1)\n",
    "        self.block2 = InvertedResidual(16, 24, stride=2)\n",
    "        self.block3 = InvertedResidual(24, 32, stride=2)\n",
    "        self.block4 = InvertedResidual(32, 64, stride=2)\n",
    "        self.block5 = InvertedResidual(64, 96, stride=1)\n",
    "        self.block6 = InvertedResidual(96, 160, stride=2)\n",
    "        self.block7 = InvertedResidual(160, 320, stride=1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(1280)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(1280, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # --- Remove mask-based block skipping ---\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        x = self.block7(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "4ad82bf3520f8612",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T20:49:10.351924Z",
     "start_time": "2025-05-19T20:49:10.347656Z"
    }
   },
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "7ebb1e087d2f0cd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T20:49:11.365316Z",
     "start_time": "2025-05-19T20:49:11.360285Z"
    }
   },
   "source": [
    "def save_model_as_onnx(model, example_input, output_path):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    # Export to ONNX\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        example_input,\n",
    "        output_path,\n",
    "        export_params=True,\n",
    "        opset_version=13,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "    )\n",
    "    print(f\"✅ Model saved as ONNX to {output_path}\")"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "3e9a817a37b770da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T20:49:13.162596Z",
     "start_time": "2025-05-19T20:49:13.157983Z"
    }
   },
   "source": [
    "def calculate_macs(model, example_input):\n",
    "    macs, params = tp.utils.count_ops_and_params(model, example_input)\n",
    "    return macs, params"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "5b29391c83349f69",
   "metadata": {},
   "source": [
    "### compare results of different pruning strategies"
   ]
  },
  {
   "cell_type": "code",
   "id": "becad7e59744f0dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T20:49:14.246352Z",
     "start_time": "2025-05-19T20:49:14.240898Z"
    }
   },
   "source": [
    "def compare_results(results):\n",
    "    print(\"\\n=== Pruning Strategy Comparison ===\")\n",
    "    print(f\"{'Strategy':<12} | {'MACs':<12} | {'Size (MB)':<10} | {'Accuracy (%)':<12}\")\n",
    "    print(\"-\" * 55)\n",
    "    for strategy, metrics in results.items():\n",
    "        print(f\"{strategy:<12} | {metrics['macs']:.2e} | {metrics['size_mb']:>9.2f} | {metrics['accuracy']:>12.2f}\")"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "df0e1911de55ade6",
   "metadata": {},
   "source": [
    "### compare and plot results"
   ]
  },
  {
   "cell_type": "code",
   "id": "5feb4387794b8311",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T20:49:15.949650Z",
     "start_time": "2025-05-19T20:49:15.935262Z"
    }
   },
   "source": [
    "def compare_results_and_plot(results_dict, strategies_config, output_dir='output'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    metrics_data = results_dict\n",
    "\n",
    "    print(\"\\n=== Pruning Strategy Comparison ===\")\n",
    "    header = f\"{'Strategy':<35} | {'MACs':<12} | {'Params':<12} | {'Size (MiB)':<10} | {'Accuracy (%)':<12}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    if 'initial' in metrics_data:\n",
    "        strat_name = 'initial'\n",
    "        metrics = metrics_data[strat_name]\n",
    "        print(f\"{strat_name:<35} | {metrics['macs']:.2e} | {metrics['params']:.2e} | {metrics['size_mib']:>9.2f} | {metrics['accuracy']:>12.2f}\")\n",
    "\n",
    "    sorted_strategy_keys = sorted(strategies_config.keys())\n",
    "\n",
    "    for strategy_key_orig in sorted_strategy_keys:\n",
    "        strat_name_pruned = f\"{strategy_key_orig}_pruned_not_finetuned\"\n",
    "        if strat_name_pruned in metrics_data:\n",
    "            metrics = metrics_data[strat_name_pruned]\n",
    "            print(f\"{strat_name_pruned:<35} | {metrics['macs']:.2e} | {metrics['params']:.2e} | {metrics['size_mib']:>9.2f} | {metrics['accuracy']:>12.2f}\")\n",
    "\n",
    "        strat_name_final = strategy_key_orig\n",
    "        if strat_name_final in metrics_data:\n",
    "            metrics = metrics_data[strat_name_final]\n",
    "            print(f\"{strat_name_final:<35} | {metrics['macs']:.2e} | {metrics['params']:.2e} | {metrics['size_mib']:>9.2f} | {metrics['accuracy']:>12.2f}\")\n",
    "\n",
    "    plot_strategies_final_names = ['initial'] + sorted_strategy_keys\n",
    "\n",
    "    metric_keys_to_plot = ['macs', 'params', 'size_mib', 'accuracy']\n",
    "    titles = {\n",
    "        'macs': 'MACs Comparison (Final Models)',\n",
    "        'params': 'Parameters Comparison (Final Models)',\n",
    "        'size_mib': 'Model Size (MiB) Comparison (Final Models)',\n",
    "        'accuracy': 'Accuracy (%) Comparison (Final Models)'\n",
    "    }\n",
    "    y_labels = {\n",
    "        'macs': 'MACs',\n",
    "        'params': 'Parameters',\n",
    "        'size_mib': 'Size (MiB)',\n",
    "        'accuracy': 'Accuracy (%)'\n",
    "    }\n",
    "\n",
    "    num_strategies_for_plot = len(plot_strategies_final_names)\n",
    "\n",
    "    # Fix for MatplotlibDeprecationWarning and color generation\n",
    "    colors_cmap = plt.colormaps.get_cmap('tab10') # Get the colormap object\n",
    "\n",
    "    for metric_key in metric_keys_to_plot:\n",
    "        values = []\n",
    "        valid_strategies_for_plot = []\n",
    "        for strategy_name_for_plot in plot_strategies_final_names:\n",
    "            if strategy_name_for_plot in metrics_data:\n",
    "                actual_key_in_results = strategy_name_for_plot\n",
    "                if actual_key_in_results in metrics_data and metric_key in metrics_data[actual_key_in_results]:\n",
    "                    values.append(metrics_data[actual_key_in_results][metric_key])\n",
    "                    valid_strategies_for_plot.append(strategy_name_for_plot)\n",
    "\n",
    "        if not values:\n",
    "            print(f\"Skipping plot for {metric_key} as no data was found.\")\n",
    "            continue\n",
    "\n",
    "        plt.figure(figsize=(max(12, int(1.5 * len(valid_strategies_for_plot))), 7)) # Dynamic width, ensure int\n",
    "\n",
    "        # Generate colors for the valid strategies being plotted\n",
    "        bar_colors = [colors_cmap(i % colors_cmap.N) for i in range(len(valid_strategies_for_plot))]\n",
    "        bars = plt.bar(valid_strategies_for_plot, values, color=bar_colors)\n",
    "\n",
    "        plt.xlabel('Strategy')\n",
    "        plt.ylabel(y_labels[metric_key])\n",
    "        plt.title(titles[metric_key])\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "        for bar in bars:\n",
    "            yval = bar.get_height()\n",
    "            if metric_key in ['macs', 'params']:\n",
    "                 plt.text(bar.get_x() + bar.get_width()/2., yval, f'{yval:.2e}', ha='center', va='bottom')\n",
    "            else:\n",
    "                 plt.text(bar.get_x() + bar.get_width()/2., yval, f'{yval:.2f}', ha='center', va='bottom')\n",
    "\n",
    "        if 'initial' in metrics_data and metric_key in metrics_data['initial']:\n",
    "            initial_value = metrics_data['initial'][metric_key]\n",
    "            # Fix for ValueError: Invalid format specifier\n",
    "            if metric_key in [\"macs\", \"params\"]:\n",
    "                label_text = f'Initial ({initial_value:.2e})'\n",
    "            else:\n",
    "                label_text = f'Initial ({initial_value:.2f})'\n",
    "            plt.axhline(y=initial_value, color='r', linestyle='--', label=label_text)\n",
    "\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f'{metric_key}_comparison_final.png'))\n",
    "        plt.close()\n",
    "    print(f\"✅ Comparison plots saved to {output_dir}\")"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "23869493d2f36a72",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "615130e04992f7ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T20:49:19.836351Z",
     "start_time": "2025-05-19T20:49:19.833144Z"
    }
   },
   "source": [
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "bbd2e296738ea559",
   "metadata": {},
   "source": [
    "### Utility function to save the model"
   ]
  },
  {
   "cell_type": "code",
   "id": "b7f8aa766a2fa06a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T20:49:21.048906Z",
     "start_time": "2025-05-19T20:49:21.043517Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "def save_model(model, path, example_input=None):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), path)\n",
    "    if example_input is not None:\n",
    "        onnx_path = path.replace('.pth', '.onnx')\n",
    "        save_model_as_onnx(model, example_input, onnx_path)\n",
    "\n",
    "def load_model_state(model_class, path, device, *args, **kwargs):\n",
    "    # *args, **kwargs are for model_class constructor (e.g. num_classes)\n",
    "    model = model_class(*args, **kwargs)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    print(f\"✅ Model loaded from {path} to {device}\")\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "89740bc4e512ecde",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "id": "5484dc87c1b63c28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T20:49:22.903200Z",
     "start_time": "2025-05-19T20:49:22.898272Z"
    }
   },
   "source": [
    "def evaluate_model(model, test_loader, example_input_device, criterion_eval, device_eval):\n",
    "    model.eval()\n",
    "    model.to(device_eval)\n",
    "    macs, num_params = calculate_macs(model, example_input_device)\n",
    "    size_mib = num_params * 4 / (1024 * 1024)\n",
    "    correct = 0; total = 0; running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = [d.to(device_eval) for d in data]\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion_eval(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    avg_loss = running_loss / total if total > 0 else float('nan')\n",
    "    accuracy = 100 * correct / total if total > 0 else 0\n",
    "    return {'macs': macs, 'params': num_params, 'size_mib': size_mib, 'accuracy': accuracy, 'loss': avg_loss}\n"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "78f7147c66b6d346",
   "metadata": {},
   "source": [
    "### Prune the model"
   ]
  },
  {
   "cell_type": "code",
   "id": "6532a7494fc3fcbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T20:49:24.659149Z",
     "start_time": "2025-05-19T20:49:24.653902Z"
    }
   },
   "source": [
    "def prune_model(model, example_input, target_macs, strategy, iterative_steps=5):\n",
    "    #, iterative_pruning_ratio_scheduler=linear_scheduler()):\n",
    "    if isinstance(strategy['importance'], tp.importance.TaylorImportance):\n",
    "        pruning_ratio = 0.1\n",
    "    else:\n",
    "        pruning_ratio = 0.5\n",
    "\n",
    "    pruner = strategy['pruner'](\n",
    "        model,\n",
    "        example_input,\n",
    "        importance=strategy['importance'],\n",
    "        iterative_steps=iterative_steps,\n",
    "        ch_sparsity=pruning_ratio,  # Initial sparsity\n",
    "        #iterative_pruning_ratio_scheduler=iterative_pruning_ratio_scheduler,\n",
    "        root_module_types=[nn.Conv2d],\n",
    "        ignored_layers=[model.fc],\n",
    "    )\n",
    "\n",
    "    current_macs, base_nparams = calculate_macs(model, example_input)\n",
    "    # while current_macs > target_macs:\n",
    "    #     pruner.step()\n",
    "    #     current_macs = calculate_macs(model, example_input)\n",
    "\n",
    "\n",
    "    for i in range(iterative_steps):\n",
    "            if isinstance(strategy['importance'], tp.importance.TaylorImportance):\n",
    "                loss = model(example_input).sum() # a dummy loss for TaylorImportance\n",
    "                loss.backward()\n",
    "            for g in pruner.step(interactive=True):\n",
    "                g.prune()\n",
    "            macs, nparams = tp.utils.count_ops_and_params(model, example_input)\n",
    "            #print(model(example_input).shape)\n",
    "            print(\n",
    "                \"  Iter %d/%d, Params: %.2f M => %.2f M\"\n",
    "                % (i + 1, iterative_steps, base_nparams / 1e6, nparams / 1e6)\n",
    "            )\n",
    "            print(\n",
    "                \"  Iter %d/%d, MACs: %.2f G => %.2f G\"\n",
    "                % (i + 1, iterative_steps, current_macs / 1e9, macs / 1e9)\n",
    "            )\n",
    "\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "6adb22651c74e5f0",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "id": "a6e1edc8a36e5260",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T20:49:27.481278Z",
     "start_time": "2025-05-19T20:49:27.466768Z"
    }
   },
   "source": [
    "def train_model(model,\n",
    "                train_loader,\n",
    "                criterion,\n",
    "                optimizer,\n",
    "                device,\n",
    "                num_epochs,\n",
    "                val_loader=None, strategy_name=\"\",\n",
    "                early_stopping_patience=None,\n",
    "                early_stopping_metric='val_loss',\n",
    "                load_best_model_on_stop=True\n",
    "                ): # <<<< ADDED EARLY STOPPING PARAMETERS\n",
    "\n",
    "    model.to(device)\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "    # Early stopping specific variables\n",
    "    best_metric_score = float('inf') if early_stopping_metric == 'val_loss' else float('-inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state_dict = None # To store the state_dict of the best model\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = [d.to(device) for d in data]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader) if len(train_loader) > 0 else 0\n",
    "        epoch_acc = 100 * correct_train / total_train if total_train > 0 else 0\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc)\n",
    "\n",
    "        log_msg = f\"Strategy: {strategy_name} - Epoch {epoch+1}/{num_epochs}: Train Loss={epoch_loss:.4f}, Train Acc={epoch_acc:.2f}%\"\n",
    "\n",
    "        current_val_metric = None # To store the metric value for the current epoch\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            running_val_loss = 0.0\n",
    "            correct_val = 0\n",
    "            total_val = 0\n",
    "            with torch.no_grad():\n",
    "                for data_val in val_loader:\n",
    "                    inputs_val, labels_val = [d.to(device) for d in data_val]\n",
    "                    outputs_val = model(inputs_val)\n",
    "                    val_loss_item = criterion(outputs_val, labels_val)\n",
    "                    running_val_loss += val_loss_item.item()\n",
    "                    _, predicted_val = torch.max(outputs_val.data, 1)\n",
    "                    total_val += labels_val.size(0)\n",
    "                    correct_val += (predicted_val == labels_val).sum().item()\n",
    "\n",
    "            epoch_val_loss = running_val_loss / len(val_loader) if len(val_loader) > 0 else 0\n",
    "            epoch_val_acc = 100 * correct_val / total_val if total_val > 0 else 0\n",
    "            history['val_loss'].append(epoch_val_loss)\n",
    "            history['val_acc'].append(epoch_val_acc)\n",
    "            log_msg += f\", Val Loss={epoch_val_loss:.4f}, Val Acc={epoch_val_acc:.2f}%\"\n",
    "\n",
    "            # Early stopping logic\n",
    "            if early_stopping_patience is not None:\n",
    "                if early_stopping_metric == 'val_loss':\n",
    "                    current_val_metric = epoch_val_loss\n",
    "                    if current_val_metric < best_metric_score:\n",
    "                        best_metric_score = current_val_metric\n",
    "                        epochs_no_improve = 0\n",
    "                        if load_best_model_on_stop:\n",
    "                            best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "                        log_msg += \" (New best val_loss)\"\n",
    "                    else:\n",
    "                        epochs_no_improve += 1\n",
    "                elif early_stopping_metric == 'val_acc':\n",
    "                    current_val_metric = epoch_val_acc\n",
    "                    if current_val_metric > best_metric_score:\n",
    "                        best_metric_score = current_val_metric\n",
    "                        epochs_no_improve = 0\n",
    "                        if load_best_model_on_stop:\n",
    "                            best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "                        log_msg += \" (New best val_acc)\"\n",
    "                    else:\n",
    "                        epochs_no_improve += 1\n",
    "                else:\n",
    "                    # This case should ideally not be hit if parameters are validated upstream\n",
    "                    # or have defaults, but good for robustness.\n",
    "                    print(f\"Warning: Unsupported early_stopping_metric: {early_stopping_metric}. Defaulting to 'val_loss'.\")\n",
    "                    early_stopping_metric = 'val_loss' # Fallback\n",
    "                    # Re-evaluate for the current epoch with the fallback metric (could be complex, simpler to just warn and continue)\n",
    "\n",
    "        else: # No validation loader\n",
    "            history['val_loss'].append(None)\n",
    "            history['val_acc'].append(None)\n",
    "            if early_stopping_patience is not None: # Only warn if ES was intended\n",
    "                print(f\"Warning: Early stopping for '{strategy_name}' configured (patience: {early_stopping_patience}), but no validation loader provided. Early stopping will be inactive.\")\n",
    "\n",
    "        print(log_msg)\n",
    "\n",
    "        # Check for early stopping condition\n",
    "        if early_stopping_patience is not None and val_loader and epochs_no_improve >= early_stopping_patience:\n",
    "            print(f\"Early stopping triggered for '{strategy_name}' after {epoch+1} epochs. No improvement in '{early_stopping_metric}' for {early_stopping_patience} epochs.\")\n",
    "            if load_best_model_on_stop and best_model_state_dict is not None:\n",
    "                print(f\"Loading best model weights from epoch {epoch + 1 - epochs_no_improve} with {early_stopping_metric}: {best_metric_score:.4f}\")\n",
    "                model.load_state_dict(best_model_state_dict)\n",
    "            break # Stop training loop\n",
    "\n",
    "    # After the loop, if training completed fully (not early stopped) OR early stopped but we want the best model\n",
    "    if load_best_model_on_stop and best_model_state_dict is not None:\n",
    "        # This check ensures we load the best model if the last epoch wasn't the best one,\n",
    "        # even if early stopping didn't trigger (e.g., training ran for all num_epochs)\n",
    "        # Or if it did trigger, this ensures the best model is loaded.\n",
    "        # Need to get the last recorded validation metric if val_loader was present\n",
    "        last_recorded_val_metric = None\n",
    "        if val_loader and history[early_stopping_metric]: # Check if history has entries for the metric\n",
    "             # Get the last non-None value for the metric\n",
    "            valid_metrics = [m for m in history[early_stopping_metric] if m is not None]\n",
    "            if valid_metrics:\n",
    "                last_recorded_val_metric = valid_metrics[-1]\n",
    "\n",
    "        should_load_best = True # Default to loading if we have a best_model_state_dict\n",
    "        if last_recorded_val_metric is not None: # If we have a final metric to compare\n",
    "            if early_stopping_metric == 'val_loss':\n",
    "                if last_recorded_val_metric <= best_metric_score: # Current/last model is as good or better\n",
    "                    should_load_best = False\n",
    "            elif early_stopping_metric == 'val_acc':\n",
    "                if last_recorded_val_metric >= best_metric_score: # Current/last model is as good or better\n",
    "                    should_load_best = False\n",
    "\n",
    "        if should_load_best:\n",
    "            print(f\"Training for '{strategy_name}' finished. Loading best recorded model state with {early_stopping_metric}: {best_metric_score:.4f}\")\n",
    "            model.load_state_dict(best_model_state_dict)\n",
    "\n",
    "    return model, history"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Plotting fine-tuning curves",
   "id": "336281ff4136c31"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T23:38:40.299627Z",
     "start_time": "2025-05-19T23:38:40.288206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "def plot_finetuning_curves(history, plot_title_suffix, output_dir_plots, model_macs_val):\n",
    "    os.makedirs(output_dir_plots, exist_ok=True)\n",
    "    actual_epochs = len(history['train_loss'])\n",
    "    epochs_range = range(1, actual_epochs + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1); plt.plot(epochs_range, history['train_loss'][:actual_epochs], 'bo-', label='Train Loss')\n",
    "    if history.get('val_loss') and any(v is not None for v in history['val_loss']):\n",
    "        plt.plot(epochs_range, history['val_loss'][:actual_epochs], 'ro-', label='Val Loss')\n",
    "    plt.title(f'Loss ({plot_title_suffix})\\nMACs: {model_macs_val:.2e}'); plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)\n",
    "    plt.subplot(1, 2, 2); plt.plot(epochs_range, history['train_acc'][:actual_epochs], 'bo-', label='Train Acc')\n",
    "    if history.get('val_acc') and any(v is not None for v in history['val_acc']):\n",
    "        plt.plot(epochs_range, history['val_acc'][:actual_epochs], 'ro-', label='Val Acc')\n",
    "    plt.title(f'Accuracy ({plot_title_suffix})\\nMACs: {model_macs_val:.2e}'); plt.xlabel('Epochs'); plt.ylabel('Acc %'); plt.legend(); plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    safe_suffix = plot_title_suffix.replace(' ', '_').replace('/', '_').replace(':', '_')\n",
    "    plt.savefig(os.path.join(output_dir_plots, f'finetune_curves_{safe_suffix}.png')); plt.close()\n",
    "    print(f\"✅ Fine-tuning curves for {plot_title_suffix} saved.\")\n",
    "\n",
    "\n",
    "def plot_metrics_vs_ratio_all_strategies(results_data_plot, ratios_tested_plot, output_dir_main_plots):\n",
    "    os.makedirs(output_dir_main_plots, exist_ok=True)\n",
    "    strategies_plot = list(results_data_plot.keys())\n",
    "    if not strategies_plot: print(\"No strategies in results for ratio plots.\"); return\n",
    "    ratios_str_plot = [f\"{r:.1f}\" for r in ratios_tested_plot]; num_ratios_plot = len(ratios_tested_plot); num_strategies_plot = len(strategies_plot)\n",
    "    bar_width_plot = 0.8 / num_strategies_plot; index_plot = np.arange(num_ratios_plot)\n",
    "    colors_map_plot = plt.colormaps.get_cmap('tab10')\n",
    "\n",
    "    # MACs Plot\n",
    "    plt.figure(figsize=(max(12, int(1.8 * num_ratios_plot * num_strategies_plot / 3)), 7))\n",
    "    for i, s_name in enumerate(strategies_plot):\n",
    "        macs_vals = [results_data_plot[s_name].get(r_val, {}).get('macs', np.nan) for r_val in ratios_tested_plot]\n",
    "        plt.bar(index_plot + i * bar_width_plot, macs_vals, bar_width_plot, label=s_name, color=colors_map_plot(i % colors_map_plot.N))\n",
    "    plt.xlabel('Pruning Ratio (ch_sparsity)'); plt.ylabel('MACs (Log Scale)'); plt.title('MACs vs. Pruning Ratio (Fine-tuned Models)')\n",
    "    plt.xticks(index_plot + bar_width_plot * (num_strategies_plot - 1) / 2, ratios_str_plot); plt.yscale('log')\n",
    "    plt.legend(title=\"Strategy\", bbox_to_anchor=(1.02, 1), loc='upper left'); plt.grid(True, which=\"both\", ls=\"-\", alpha=0.3)\n",
    "    plt.tight_layout(rect=[0, 0, 0.88, 1]); plt.savefig(os.path.join(output_dir_main_plots, 'MACs_vs_Ratio_by_Strategy.png')); plt.close()\n",
    "    print(f\"✅ MACs vs. Ratio by Strategy plot saved to {output_dir_main_plots}\")\n",
    "\n",
    "    # Loss Plot\n",
    "    plt.figure(figsize=(max(12, int(1.8 * num_ratios_plot * num_strategies_plot / 3)), 7))\n",
    "    for i, s_name in enumerate(strategies_plot):\n",
    "        loss_vals = [results_data_plot[s_name].get(r_val, {}).get('loss', np.nan) for r_val in ratios_tested_plot]\n",
    "        plt.bar(index_plot + i * bar_width_plot, loss_vals, bar_width_plot, label=s_name, color=colors_map_plot(i % colors_map_plot.N))\n",
    "    plt.xlabel('Pruning Ratio (ch_sparsity)'); plt.ylabel('Avg. Test Loss'); plt.title('Avg. Test Loss vs. Pruning Ratio (Fine-tuned Models)')\n",
    "    plt.xticks(index_plot + bar_width_plot * (num_strategies_plot - 1) / 2, ratios_str_plot)\n",
    "    plt.legend(title=\"Strategy\", bbox_to_anchor=(1.02, 1), loc='upper left'); plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout(rect=[0, 0, 0.88, 1]); plt.savefig(os.path.join(output_dir_main_plots, 'Loss_vs_Ratio_by_Strategy.png')); plt.close()\n",
    "    print(f\"✅ Loss vs. Ratio by Strategy plot saved to {output_dir_main_plots}\")"
   ],
   "id": "b4a43f15e22c98b2",
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "id": "b26416e6ff76416a",
   "metadata": {},
   "source": [
    "### Main workflow"
   ]
  },
  {
   "cell_type": "code",
   "id": "bdf93b3196076098",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T22:07:36.499885Z",
     "start_time": "2025-05-19T22:07:36.485165Z"
    }
   },
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Configuration\n",
    "    cfg = {\n",
    "        'strategies': {\n",
    "            'magnitude': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.MagnitudeImportance(p=2),\n",
    "            },\n",
    "            'bn_scale': {\n",
    "                'pruner': tp.pruner.BNScalePruner,\n",
    "                'importance': tp.importance.BNScaleImportance(),\n",
    "            },\n",
    "            # todo: check the examples for the following strategies, why it is giving error\n",
    "            'group_norm': {\n",
    "                'pruner': tp.pruner.GroupNormPruner,\n",
    "                'importance': tp.importance.GroupMagnitudeImportance(p=1),\n",
    "            },\n",
    "            'random': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.RandomImportance(),\n",
    "            },\n",
    "            'Taylor': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.TaylorImportance()\n",
    "            },\n",
    "            'Hessian': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.GroupHessianImportance()\n",
    "            },\n",
    "            'lamp': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.LAMPImportance(p=2)\n",
    "            },\n",
    "            'geometry': {\n",
    "                'pruner': tp.pruner.MagnitudePruner,\n",
    "                'importance': tp.importance.FPGMImportance()\n",
    "            },\n",
    "            #todo: implement growing reg pruning\n",
    "        },\n",
    "        #todo: different types of schedulers can be added\n",
    "        'pruning_ratios_to_test': [0.0, 0.2, 0.5, 0.7], # 0.0 for baseline\n",
    "        'iterative_steps_pruner_general': 5, # For non-Taylor pruners inside gr_prune_model_by_ratio\n",
    "        'iterative_steps_taylor_pruning': 5, # Number of backward passes for Taylor method in gr_prune_model_by_ratio\n",
    "        'val_split_for_loader': 0.1, # Corresponds to your get_data_loaders val_split\n",
    "        #'iterative_pruning_ratio_scheduler': linear_scheduler(),\n",
    "         'initial_train_epochs': 30,\n",
    "        'fine_tune_epochs': 50,\n",
    "        'early_stopping_patience': 15,\n",
    "        'early_stopping_metric': 'val_loss', # 'val_loss' or 'val_acc'\n",
    "        'load_best_model_on_early_stop': True,\n",
    "\n",
    "        'data_dir': './data',\n",
    "        'output_dir_base': './output_final_ratio_experiment',\n",
    "        'num_classes': 10, 'batch_size': 128,\n",
    "        'learning_rate_initial': 0.001, 'learning_rate_finetune': 0.0001,\n",
    "    }\n",
    "\n",
    "    os.makedirs(cfg['output_dir_base'], exist_ok=True)\n",
    "\n",
    "    # --- Updated Data Loader Call ---\n",
    "    print(\"Initializing DataLoaders...\")\n",
    "    # Now get_data_loaders returns train, val, test\n",
    "    train_loader, val_loader, test_loader = get_data_loaders(\n",
    "        data_dir=cfg['data_dir'],\n",
    "        batch_size=cfg['batch_size'],\n",
    "        val_split=cfg['val_split_for_loader'] # Pass the val_split\n",
    "    )\n",
    "    print(f\"Train loader: {len(train_loader.dataset)} samples, Val loader: {len(val_loader.dataset)} samples, Test loader: {len(test_loader.dataset)} samples\")\n",
    "    # --- End of Updated Data Loader Call ---\n",
    "\n",
    "    example_input_cpu_onnx = torch.randn(1, 3, 32, 32)\n",
    "    example_input_dev = example_input_cpu_onnx.to(device)\n",
    "\n",
    "    initial_model_trained_path = os.path.join(cfg['output_dir_base'], \"mobilenetv2_initial_dense_trained.pth\")\n",
    "    criterion_train_eval = nn.CrossEntropyLoss().to(device) # Define criterion once\n",
    "\n",
    "    if not os.path.exists(initial_model_trained_path):\n",
    "        print(\"--- Training Initial Dense MobileNetV2 ---\")\n",
    "        dense_model_instance = MobileNetV2(num_classes=cfg['num_classes']).to(device)\n",
    "        trained_dense_model, _ = train_model(\n",
    "            model=dense_model_instance,\n",
    "            train_loader=train_loader, # Use train_loader from get_data_loaders\n",
    "            criterion=criterion_train_eval,\n",
    "            optimizer=optim.Adam(dense_model_instance.parameters(), lr=cfg['learning_rate_initial']),\n",
    "            device=device,\n",
    "            num_epochs=cfg['initial_train_epochs'],\n",
    "            val_loader=val_loader, # <<<< PASS THE DEDICATED val_loader HERE\n",
    "            strategy_name=\"InitialDenseModel\",\n",
    "            early_stopping_patience=cfg['early_stopping_patience'],\n",
    "            early_stopping_metric=cfg['early_stopping_metric'],\n",
    "            load_best_model_on_stop=cfg['load_best_model_on_early_stop']\n",
    "        )\n",
    "        save_model(trained_dense_model, initial_model_trained_path, example_input=example_input_cpu_onnx.to(device))\n",
    "    else:\n",
    "        print(f\"--- Using Pre-trained Dense Model from {initial_model_trained_path} ---\")\n",
    "\n",
    "    results_all_ratios_strategies = {s_name: {} for s_name in cfg['strategies'].keys()}\n",
    "\n",
    "    print(\"\\n--- Evaluating Baseline Model (Ratio 0.0) on TEST SET---\")\n",
    "    baseline_model_eval = load_model_state(MobileNetV2, initial_model_trained_path, device, num_classes=cfg['num_classes'])\n",
    "    # Final evaluation of baseline model on the TEST set\n",
    "    baseline_metrics_eval = evaluate_model(baseline_model_eval, test_loader, example_input_dev, criterion_train_eval, device)\n",
    "    print(f\"Baseline Metrics (Ratio 0.0, Evaluated on Test Set): {baseline_metrics_eval}\")\n",
    "    for s_name_key in cfg['strategies'].keys():\n",
    "        results_all_ratios_strategies[s_name_key][0.0] = baseline_metrics_eval # Store test set metrics\n",
    "\n",
    "    for strategy_name_key, strategy_details_dict in cfg['strategies'].items():\n",
    "        strategy_specific_output_dir = os.path.join(cfg['output_dir_base'], strategy_name_key)\n",
    "        os.makedirs(strategy_specific_output_dir, exist_ok=True)\n",
    "\n",
    "        for current_ratio in cfg['pruning_ratios_to_test']:\n",
    "            if current_ratio == 0.0: continue\n",
    "\n",
    "            print(f\"\\n\\n--- Processing: Strategy '{strategy_name_key}', Ratio: {current_ratio:.2f} ---\")\n",
    "            model_for_pruning_current = load_model_state(MobileNetV2, initial_model_trained_path, device, num_classes=cfg['num_classes'])\n",
    "\n",
    "            print(f\"--- Pruning model with {strategy_name_key} to ratio {current_ratio:.2f} ---\")\n",
    "            pruned_model_current = gr_prune_model_by_ratio( # Your adapted pruning function\n",
    "                model_for_pruning_current, example_input_dev, strategy_details_dict, current_ratio,\n",
    "                iterative_steps_config=cfg['iterative_steps_pruner_general'],\n",
    "                iterative_steps_taylor=cfg['iterative_steps_taylor_pruning']\n",
    "            )\n",
    "\n",
    "            # Optional: Evaluate on test set *before* fine-tuning\n",
    "            metrics_pruned_bf_ft_current = evaluate_model(pruned_model_current, test_loader, example_input_dev, criterion_train_eval, device)\n",
    "            print(f\"Metrics for '{strategy_name_key}' @ Ratio {current_ratio:.2f} (Pruned, Before FT, on Test Set): {metrics_pruned_bf_ft_current}\")\n",
    "\n",
    "            print(f\"--- Fine-tuning pruned model ({strategy_name_key} @ Ratio {current_ratio:.2f}) ---\")\n",
    "            fine_tuned_model_current, ft_history_current = train_model(\n",
    "                model=pruned_model_current,\n",
    "                train_loader=train_loader, # Use train_loader\n",
    "                criterion=criterion_train_eval,\n",
    "                optimizer=optim.Adam(pruned_model_current.parameters(), lr=cfg['learning_rate_finetune']),\n",
    "                device=device,\n",
    "                num_epochs=cfg['fine_tune_epochs'],\n",
    "                val_loader=val_loader, # <<<< PASS THE DEDICATED val_loader HERE\n",
    "                strategy_name=f\"{strategy_name_key}_R{current_ratio:.1f}\",\n",
    "                early_stopping_patience=cfg['early_stopping_patience'],\n",
    "                early_stopping_metric=cfg['early_stopping_metric'],\n",
    "                load_best_model_on_stop=cfg['load_best_model_on_early_stop']\n",
    "            )\n",
    "\n",
    "            macs_ft_current, _ = calculate_macs(fine_tuned_model_current, example_input_dev)\n",
    "            plot_finetuning_curves(ft_history_current, f\"{strategy_name_key}_R{current_ratio:.1f}\",\n",
    "                                   strategy_specific_output_dir, macs_ft_current)\n",
    "\n",
    "            # Final evaluation of fine-tuned model on the TEST set\n",
    "            print(f\"--- Evaluating Fine-tuned Model ({strategy_name_key} @ Ratio {current_ratio:.2f}) on TEST SET ---\")\n",
    "            final_metrics_current = evaluate_model(fine_tuned_model_current, test_loader, example_input_dev, criterion_train_eval, device)\n",
    "            results_all_ratios_strategies[strategy_name_key][current_ratio] = final_metrics_current\n",
    "            print(f\"Metrics for '{strategy_name_key}' @ Ratio {current_ratio:.2f} (Fine-tuned, on Test Set): {final_metrics_current}\")\n",
    "            save_model(fine_tuned_model_current,\n",
    "                       os.path.join(strategy_specific_output_dir, f\"model_R{current_ratio:.1f}_final.pth\"),\n",
    "                       example_input=example_input_cpu_onnx.to(device))\n",
    "\n",
    "    plot_metrics_vs_ratio_all_strategies(results_all_ratios_strategies, cfg['pruning_ratios_to_test'], cfg['output_dir_base'])\n",
    "    print(\"\\nAll ratio-based pruning experiments completed successfully!\")"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "id": "6d51f760-3726-4028-98a7-02e0105ebd82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T23:53:21.084522Z",
     "start_time": "2025-05-19T23:38:47.685467Z"
    }
   },
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing DataLoaders...\n",
      "Using dataset directory: /home/muis/thesis/github-repo/master-thesis/cnn/mobile_net_v2/data\n",
      "Train loader: 45000 samples, Val loader: 5000 samples, Test loader: 10000 samples\n",
      "--- Using Pre-trained Dense Model from ./output_final_ratio_experiment/mobilenetv2_initial_dense_trained.pth ---\n",
      "\n",
      "--- Evaluating Baseline Model (Ratio 0.0) on TEST SET---\n",
      "✅ Model loaded from ./output_final_ratio_experiment/mobilenetv2_initial_dense_trained.pth to cuda\n",
      "Baseline Metrics (Ratio 0.0, Evaluated on Test Set): {'macs': 6059786.0, 'params': 1169642, 'size_mib': 4.461830139160156, 'accuracy': 63.29, 'loss': 1.032341127204895}\n",
      "\n",
      "\n",
      "--- Processing: Strategy 'magnitude', Ratio: 0.20 ---\n",
      "✅ Model loaded from ./output_final_ratio_experiment/mobilenetv2_initial_dense_trained.pth to cuda\n",
      "--- Pruning model with magnitude to ratio 0.20 ---\n",
      "Initial MACs before pruning for ratio 0.20: 0.006 G\n",
      "Applying MagnitudeImportance with target ratio: 0.20 using pruner's 5 iterative_steps.\n",
      "After pruning for ratio 0.20: MACs 0.006 G (Reduction: 8.36%)\n",
      "Metrics for 'magnitude' @ Ratio 0.20 (Pruned, Before FT, on Test Set): {'macs': 5553441.0, 'params': 1076719, 'size_mib': 4.107357025146484, 'accuracy': 57.9, 'loss': 1.2013130826950074}\n",
      "--- Fine-tuning pruned model (magnitude @ Ratio 0.20) ---\n",
      "Strategy: magnitude_R0.2 - Epoch 1/10: Train Loss=0.8249, Train Acc=70.91%, Val Loss=0.7385, Val Acc=73.86% (New best val_loss)\n",
      "Strategy: magnitude_R0.2 - Epoch 2/10: Train Loss=0.7504, Train Acc=73.47%, Val Loss=0.7244, Val Acc=73.90% (New best val_loss)\n",
      "Strategy: magnitude_R0.2 - Epoch 3/10: Train Loss=0.6980, Train Acc=75.32%, Val Loss=0.7205, Val Acc=73.90% (New best val_loss)\n",
      "Strategy: magnitude_R0.2 - Epoch 4/10: Train Loss=0.6594, Train Acc=76.89%, Val Loss=0.7191, Val Acc=74.22% (New best val_loss)\n",
      "Strategy: magnitude_R0.2 - Epoch 5/10: Train Loss=0.6154, Train Acc=78.35%, Val Loss=0.7289, Val Acc=74.22%\n",
      "Strategy: magnitude_R0.2 - Epoch 6/10: Train Loss=0.5723, Train Acc=79.82%, Val Loss=0.7449, Val Acc=73.86%\n",
      "Strategy: magnitude_R0.2 - Epoch 7/10: Train Loss=0.5343, Train Acc=81.28%, Val Loss=0.7525, Val Acc=74.26%\n",
      "Early stopping triggered for 'magnitude_R0.2' after 7 epochs. No improvement in 'val_loss' for 3 epochs.\n",
      "Loading best model weights from epoch 4 with val_loss: 0.7191\n",
      "Training for 'magnitude_R0.2' finished. Loading best recorded model state with val_loss: 0.7191\n",
      "✅ Fine-tuning curves for magnitude_R0.2 saved.\n",
      "--- Evaluating Fine-tuned Model (magnitude @ Ratio 0.20) on TEST SET ---\n",
      "Metrics for 'magnitude' @ Ratio 0.20 (Fine-tuned, on Test Set): {'macs': 5553441.0, 'params': 1076719, 'size_mib': 4.107357025146484, 'accuracy': 67.28, 'loss': 0.9493350761413574}\n",
      "✅ Model saved as ONNX to ./output_final_ratio_experiment/magnitude/model_R0.2_final.onnx\n",
      "\n",
      "\n",
      "--- Processing: Strategy 'magnitude', Ratio: 0.50 ---\n",
      "✅ Model loaded from ./output_final_ratio_experiment/mobilenetv2_initial_dense_trained.pth to cuda\n",
      "--- Pruning model with magnitude to ratio 0.50 ---\n",
      "Initial MACs before pruning for ratio 0.50: 0.006 G\n",
      "Applying MagnitudeImportance with target ratio: 0.50 using pruner's 5 iterative_steps.\n",
      "After pruning for ratio 0.50: MACs 0.005 G (Reduction: 18.74%)\n",
      "Metrics for 'magnitude' @ Ratio 0.50 (Pruned, Before FT, on Test Set): {'macs': 4924394.0, 'params': 950314, 'size_mib': 3.6251602172851562, 'accuracy': 39.09, 'loss': 1.716886913871765}\n",
      "--- Fine-tuning pruned model (magnitude @ Ratio 0.50) ---\n",
      "Strategy: magnitude_R0.5 - Epoch 1/10: Train Loss=0.9321, Train Acc=66.73%, Val Loss=0.8341, Val Acc=70.10% (New best val_loss)\n",
      "Strategy: magnitude_R0.5 - Epoch 2/10: Train Loss=0.8301, Train Acc=70.25%, Val Loss=0.8064, Val Acc=71.00% (New best val_loss)\n",
      "Strategy: magnitude_R0.5 - Epoch 3/10: Train Loss=0.7787, Train Acc=72.26%, Val Loss=0.7958, Val Acc=71.22% (New best val_loss)\n",
      "Strategy: magnitude_R0.5 - Epoch 4/10: Train Loss=0.7303, Train Acc=73.98%, Val Loss=0.7885, Val Acc=71.30% (New best val_loss)\n",
      "Strategy: magnitude_R0.5 - Epoch 5/10: Train Loss=0.6883, Train Acc=75.54%, Val Loss=0.7927, Val Acc=71.78%\n",
      "Strategy: magnitude_R0.5 - Epoch 6/10: Train Loss=0.6479, Train Acc=76.93%, Val Loss=0.7978, Val Acc=71.68%\n",
      "Strategy: magnitude_R0.5 - Epoch 7/10: Train Loss=0.6114, Train Acc=78.40%, Val Loss=0.8146, Val Acc=71.18%\n",
      "Early stopping triggered for 'magnitude_R0.5' after 7 epochs. No improvement in 'val_loss' for 3 epochs.\n",
      "Loading best model weights from epoch 4 with val_loss: 0.7885\n",
      "Training for 'magnitude_R0.5' finished. Loading best recorded model state with val_loss: 0.7885\n",
      "✅ Fine-tuning curves for magnitude_R0.5 saved.\n",
      "--- Evaluating Fine-tuned Model (magnitude @ Ratio 0.50) on TEST SET ---\n",
      "Metrics for 'magnitude' @ Ratio 0.50 (Fine-tuned, on Test Set): {'macs': 4924394.0, 'params': 950314, 'size_mib': 3.6251602172851562, 'accuracy': 66.89, 'loss': 0.9644097650527954}\n",
      "✅ Model saved as ONNX to ./output_final_ratio_experiment/magnitude/model_R0.5_final.onnx\n",
      "\n",
      "\n",
      "--- Processing: Strategy 'magnitude', Ratio: 0.70 ---\n",
      "✅ Model loaded from ./output_final_ratio_experiment/mobilenetv2_initial_dense_trained.pth to cuda\n",
      "--- Pruning model with magnitude to ratio 0.70 ---\n",
      "Initial MACs before pruning for ratio 0.70: 0.006 G\n",
      "Applying MagnitudeImportance with target ratio: 0.70 using pruner's 5 iterative_steps.\n",
      "After pruning for ratio 0.70: MACs 0.005 G (Reduction: 25.07%)\n",
      "Metrics for 'magnitude' @ Ratio 0.70 (Pruned, Before FT, on Test Set): {'macs': 4540476.0, 'params': 868248, 'size_mib': 3.312103271484375, 'accuracy': 19.56, 'loss': 2.2257247673034666}\n",
      "--- Fine-tuning pruned model (magnitude @ Ratio 0.70) ---\n",
      "Strategy: magnitude_R0.7 - Epoch 1/10: Train Loss=1.0020, Train Acc=64.23%, Val Loss=0.8885, Val Acc=68.20% (New best val_loss)\n",
      "Strategy: magnitude_R0.7 - Epoch 2/10: Train Loss=0.8853, Train Acc=68.37%, Val Loss=0.8504, Val Acc=69.00% (New best val_loss)\n",
      "Strategy: magnitude_R0.7 - Epoch 3/10: Train Loss=0.8199, Train Acc=70.66%, Val Loss=0.8377, Val Acc=69.44% (New best val_loss)\n",
      "Strategy: magnitude_R0.7 - Epoch 4/10: Train Loss=0.7755, Train Acc=72.22%, Val Loss=0.8300, Val Acc=69.96% (New best val_loss)\n",
      "Strategy: magnitude_R0.7 - Epoch 5/10: Train Loss=0.7335, Train Acc=73.75%, Val Loss=0.8303, Val Acc=69.94%\n",
      "Strategy: magnitude_R0.7 - Epoch 6/10: Train Loss=0.6950, Train Acc=75.03%, Val Loss=0.8331, Val Acc=69.60%\n",
      "Strategy: magnitude_R0.7 - Epoch 7/10: Train Loss=0.6588, Train Acc=76.48%, Val Loss=0.8396, Val Acc=70.04%\n",
      "Early stopping triggered for 'magnitude_R0.7' after 7 epochs. No improvement in 'val_loss' for 3 epochs.\n",
      "Loading best model weights from epoch 4 with val_loss: 0.8300\n",
      "Training for 'magnitude_R0.7' finished. Loading best recorded model state with val_loss: 0.8300\n",
      "✅ Fine-tuning curves for magnitude_R0.7 saved.\n",
      "--- Evaluating Fine-tuned Model (magnitude @ Ratio 0.70) on TEST SET ---\n",
      "Metrics for 'magnitude' @ Ratio 0.70 (Fine-tuned, on Test Set): {'macs': 4540476.0, 'params': 868248, 'size_mib': 3.312103271484375, 'accuracy': 65.67, 'loss': 0.9734017040252686}\n",
      "✅ Model saved as ONNX to ./output_final_ratio_experiment/magnitude/model_R0.7_final.onnx\n",
      "\n",
      "\n",
      "--- Processing: Strategy 'bn_scale', Ratio: 0.20 ---\n",
      "✅ Model loaded from ./output_final_ratio_experiment/mobilenetv2_initial_dense_trained.pth to cuda\n",
      "--- Pruning model with bn_scale to ratio 0.20 ---\n",
      "Initial MACs before pruning for ratio 0.20: 0.006 G\n",
      "Applying BNScaleImportance with target ratio: 0.20 using pruner's 5 iterative_steps.\n",
      "After pruning for ratio 0.20: MACs 0.006 G (Reduction: 8.36%)\n",
      "Metrics for 'bn_scale' @ Ratio 0.20 (Pruned, Before FT, on Test Set): {'macs': 5553441.0, 'params': 1076719, 'size_mib': 4.107357025146484, 'accuracy': 58.41, 'loss': 1.2011870141983032}\n",
      "--- Fine-tuning pruned model (bn_scale @ Ratio 0.20) ---\n",
      "Strategy: bn_scale_R0.2 - Epoch 1/10: Train Loss=0.8287, Train Acc=70.51%, Val Loss=0.7425, Val Acc=73.84% (New best val_loss)\n",
      "Strategy: bn_scale_R0.2 - Epoch 2/10: Train Loss=0.7482, Train Acc=73.48%, Val Loss=0.7178, Val Acc=74.56% (New best val_loss)\n",
      "Strategy: bn_scale_R0.2 - Epoch 3/10: Train Loss=0.6989, Train Acc=75.25%, Val Loss=0.7161, Val Acc=74.56% (New best val_loss)\n",
      "Strategy: bn_scale_R0.2 - Epoch 4/10: Train Loss=0.6534, Train Acc=76.96%, Val Loss=0.7197, Val Acc=74.52%\n",
      "Strategy: bn_scale_R0.2 - Epoch 5/10: Train Loss=0.6107, Train Acc=78.40%, Val Loss=0.7307, Val Acc=74.50%\n",
      "Strategy: bn_scale_R0.2 - Epoch 6/10: Train Loss=0.5726, Train Acc=79.81%, Val Loss=0.7457, Val Acc=74.68%\n",
      "Early stopping triggered for 'bn_scale_R0.2' after 6 epochs. No improvement in 'val_loss' for 3 epochs.\n",
      "Loading best model weights from epoch 3 with val_loss: 0.7161\n",
      "Training for 'bn_scale_R0.2' finished. Loading best recorded model state with val_loss: 0.7161\n",
      "✅ Fine-tuning curves for bn_scale_R0.2 saved.\n",
      "--- Evaluating Fine-tuned Model (bn_scale @ Ratio 0.20) on TEST SET ---\n",
      "Metrics for 'bn_scale' @ Ratio 0.20 (Fine-tuned, on Test Set): {'macs': 5553441.0, 'params': 1076719, 'size_mib': 4.107357025146484, 'accuracy': 67.51, 'loss': 0.9420500101089477}\n",
      "✅ Model saved as ONNX to ./output_final_ratio_experiment/bn_scale/model_R0.2_final.onnx\n",
      "\n",
      "\n",
      "--- Processing: Strategy 'bn_scale', Ratio: 0.50 ---\n",
      "✅ Model loaded from ./output_final_ratio_experiment/mobilenetv2_initial_dense_trained.pth to cuda\n",
      "--- Pruning model with bn_scale to ratio 0.50 ---\n",
      "Initial MACs before pruning for ratio 0.50: 0.006 G\n",
      "Applying BNScaleImportance with target ratio: 0.50 using pruner's 5 iterative_steps.\n",
      "After pruning for ratio 0.50: MACs 0.005 G (Reduction: 18.74%)\n",
      "Metrics for 'bn_scale' @ Ratio 0.50 (Pruned, Before FT, on Test Set): {'macs': 4924394.0, 'params': 950314, 'size_mib': 3.6251602172851562, 'accuracy': 39.62, 'loss': 1.7024258895874023}\n",
      "--- Fine-tuning pruned model (bn_scale @ Ratio 0.50) ---\n",
      "Strategy: bn_scale_R0.5 - Epoch 1/10: Train Loss=0.9239, Train Acc=67.14%, Val Loss=0.8268, Val Acc=70.54% (New best val_loss)\n",
      "Strategy: bn_scale_R0.5 - Epoch 2/10: Train Loss=0.8292, Train Acc=70.42%, Val Loss=0.8010, Val Acc=71.24% (New best val_loss)\n",
      "Strategy: bn_scale_R0.5 - Epoch 3/10: Train Loss=0.7750, Train Acc=72.50%, Val Loss=0.7818, Val Acc=72.06% (New best val_loss)\n",
      "Strategy: bn_scale_R0.5 - Epoch 4/10: Train Loss=0.7284, Train Acc=73.98%, Val Loss=0.7765, Val Acc=71.98% (New best val_loss)\n",
      "Strategy: bn_scale_R0.5 - Epoch 5/10: Train Loss=0.6875, Train Acc=75.47%, Val Loss=0.7773, Val Acc=72.06%\n",
      "Strategy: bn_scale_R0.5 - Epoch 6/10: Train Loss=0.6482, Train Acc=77.04%, Val Loss=0.7881, Val Acc=71.80%\n",
      "Strategy: bn_scale_R0.5 - Epoch 7/10: Train Loss=0.6071, Train Acc=78.52%, Val Loss=0.8145, Val Acc=72.18%\n",
      "Early stopping triggered for 'bn_scale_R0.5' after 7 epochs. No improvement in 'val_loss' for 3 epochs.\n",
      "Loading best model weights from epoch 4 with val_loss: 0.7765\n",
      "Training for 'bn_scale_R0.5' finished. Loading best recorded model state with val_loss: 0.7765\n",
      "✅ Fine-tuning curves for bn_scale_R0.5 saved.\n",
      "--- Evaluating Fine-tuned Model (bn_scale @ Ratio 0.50) on TEST SET ---\n",
      "Metrics for 'bn_scale' @ Ratio 0.50 (Fine-tuned, on Test Set): {'macs': 4924394.0, 'params': 950314, 'size_mib': 3.6251602172851562, 'accuracy': 66.69, 'loss': 0.9552896131515503}\n",
      "✅ Model saved as ONNX to ./output_final_ratio_experiment/bn_scale/model_R0.5_final.onnx\n",
      "\n",
      "\n",
      "--- Processing: Strategy 'bn_scale', Ratio: 0.70 ---\n",
      "✅ Model loaded from ./output_final_ratio_experiment/mobilenetv2_initial_dense_trained.pth to cuda\n",
      "--- Pruning model with bn_scale to ratio 0.70 ---\n",
      "Initial MACs before pruning for ratio 0.70: 0.006 G\n",
      "Applying BNScaleImportance with target ratio: 0.70 using pruner's 5 iterative_steps.\n",
      "After pruning for ratio 0.70: MACs 0.005 G (Reduction: 25.07%)\n",
      "Metrics for 'bn_scale' @ Ratio 0.70 (Pruned, Before FT, on Test Set): {'macs': 4540476.0, 'params': 868248, 'size_mib': 3.312103271484375, 'accuracy': 24.5, 'loss': 2.013347299194336}\n",
      "--- Fine-tuning pruned model (bn_scale @ Ratio 0.70) ---\n",
      "Strategy: bn_scale_R0.7 - Epoch 1/10: Train Loss=0.9862, Train Acc=64.72%, Val Loss=0.8835, Val Acc=69.12% (New best val_loss)\n",
      "Strategy: bn_scale_R0.7 - Epoch 2/10: Train Loss=0.8810, Train Acc=68.30%, Val Loss=0.8432, Val Acc=69.78% (New best val_loss)\n",
      "Strategy: bn_scale_R0.7 - Epoch 3/10: Train Loss=0.8215, Train Acc=70.61%, Val Loss=0.8280, Val Acc=70.68% (New best val_loss)\n",
      "Strategy: bn_scale_R0.7 - Epoch 4/10: Train Loss=0.7745, Train Acc=72.43%, Val Loss=0.8221, Val Acc=70.76% (New best val_loss)\n",
      "Strategy: bn_scale_R0.7 - Epoch 5/10: Train Loss=0.7295, Train Acc=73.79%, Val Loss=0.8211, Val Acc=70.66% (New best val_loss)\n",
      "Strategy: bn_scale_R0.7 - Epoch 6/10: Train Loss=0.6936, Train Acc=75.30%, Val Loss=0.8302, Val Acc=70.96%\n",
      "Strategy: bn_scale_R0.7 - Epoch 7/10: Train Loss=0.6571, Train Acc=76.46%, Val Loss=0.8349, Val Acc=70.60%\n",
      "Strategy: bn_scale_R0.7 - Epoch 8/10: Train Loss=0.6188, Train Acc=78.03%, Val Loss=0.8546, Val Acc=70.88%\n",
      "Early stopping triggered for 'bn_scale_R0.7' after 8 epochs. No improvement in 'val_loss' for 3 epochs.\n",
      "Loading best model weights from epoch 5 with val_loss: 0.8211\n",
      "Training for 'bn_scale_R0.7' finished. Loading best recorded model state with val_loss: 0.8211\n",
      "✅ Fine-tuning curves for bn_scale_R0.7 saved.\n",
      "--- Evaluating Fine-tuned Model (bn_scale @ Ratio 0.70) on TEST SET ---\n",
      "Metrics for 'bn_scale' @ Ratio 0.70 (Fine-tuned, on Test Set): {'macs': 4540476.0, 'params': 868248, 'size_mib': 3.312103271484375, 'accuracy': 66.48, 'loss': 0.9644965002059936}\n",
      "✅ Model saved as ONNX to ./output_final_ratio_experiment/bn_scale/model_R0.7_final.onnx\n",
      "\n",
      "\n",
      "--- Processing: Strategy 'group_norm', Ratio: 0.20 ---\n",
      "✅ Model loaded from ./output_final_ratio_experiment/mobilenetv2_initial_dense_trained.pth to cuda\n",
      "--- Pruning model with group_norm to ratio 0.20 ---\n",
      "Initial MACs before pruning for ratio 0.20: 0.006 G\n",
      "Applying GroupMagnitudeImportance with target ratio: 0.20 using pruner's 5 iterative_steps.\n",
      "After pruning for ratio 0.20: MACs 0.006 G (Reduction: 8.36%)\n",
      "Metrics for 'group_norm' @ Ratio 0.20 (Pruned, Before FT, on Test Set): {'macs': 5553441.0, 'params': 1076719, 'size_mib': 4.107357025146484, 'accuracy': 56.87, 'loss': 1.2491445064544677}\n",
      "--- Fine-tuning pruned model (group_norm @ Ratio 0.20) ---\n",
      "Strategy: group_norm_R0.2 - Epoch 1/10: Train Loss=0.8205, Train Acc=71.04%, Val Loss=0.7309, Val Acc=73.88% (New best val_loss)\n",
      "Strategy: group_norm_R0.2 - Epoch 2/10: Train Loss=0.7413, Train Acc=73.64%, Val Loss=0.7205, Val Acc=74.16% (New best val_loss)\n",
      "Strategy: group_norm_R0.2 - Epoch 3/10: Train Loss=0.6897, Train Acc=75.66%, Val Loss=0.7072, Val Acc=74.40% (New best val_loss)\n",
      "Strategy: group_norm_R0.2 - Epoch 4/10: Train Loss=0.6456, Train Acc=77.30%, Val Loss=0.7128, Val Acc=74.24%\n",
      "Strategy: group_norm_R0.2 - Epoch 5/10: Train Loss=0.6091, Train Acc=78.41%, Val Loss=0.7160, Val Acc=74.52%\n",
      "Strategy: group_norm_R0.2 - Epoch 6/10: Train Loss=0.5697, Train Acc=79.95%, Val Loss=0.7290, Val Acc=74.18%\n",
      "Early stopping triggered for 'group_norm_R0.2' after 6 epochs. No improvement in 'val_loss' for 3 epochs.\n",
      "Loading best model weights from epoch 3 with val_loss: 0.7072\n",
      "Training for 'group_norm_R0.2' finished. Loading best recorded model state with val_loss: 0.7072\n",
      "✅ Fine-tuning curves for group_norm_R0.2 saved.\n",
      "--- Evaluating Fine-tuned Model (group_norm @ Ratio 0.20) on TEST SET ---\n",
      "Metrics for 'group_norm' @ Ratio 0.20 (Fine-tuned, on Test Set): {'macs': 5553441.0, 'params': 1076719, 'size_mib': 4.107357025146484, 'accuracy': 67.23, 'loss': 0.9468494297981263}\n",
      "✅ Model saved as ONNX to ./output_final_ratio_experiment/group_norm/model_R0.2_final.onnx\n",
      "\n",
      "\n",
      "--- Processing: Strategy 'group_norm', Ratio: 0.50 ---\n",
      "✅ Model loaded from ./output_final_ratio_experiment/mobilenetv2_initial_dense_trained.pth to cuda\n",
      "--- Pruning model with group_norm to ratio 0.50 ---\n",
      "Initial MACs before pruning for ratio 0.50: 0.006 G\n",
      "Applying GroupMagnitudeImportance with target ratio: 0.50 using pruner's 5 iterative_steps.\n",
      "After pruning for ratio 0.50: MACs 0.005 G (Reduction: 18.74%)\n",
      "Metrics for 'group_norm' @ Ratio 0.50 (Pruned, Before FT, on Test Set): {'macs': 4924394.0, 'params': 950314, 'size_mib': 3.6251602172851562, 'accuracy': 35.68, 'loss': 1.8798140466690063}\n",
      "--- Fine-tuning pruned model (group_norm @ Ratio 0.50) ---\n",
      "Strategy: group_norm_R0.5 - Epoch 1/10: Train Loss=0.9269, Train Acc=67.08%, Val Loss=0.8311, Val Acc=70.22% (New best val_loss)\n",
      "Strategy: group_norm_R0.5 - Epoch 2/10: Train Loss=0.8234, Train Acc=70.69%, Val Loss=0.7991, Val Acc=70.50% (New best val_loss)\n",
      "Strategy: group_norm_R0.5 - Epoch 3/10: Train Loss=0.7704, Train Acc=72.76%, Val Loss=0.7968, Val Acc=71.38% (New best val_loss)\n",
      "Strategy: group_norm_R0.5 - Epoch 4/10: Train Loss=0.7254, Train Acc=74.29%, Val Loss=0.7832, Val Acc=72.10% (New best val_loss)\n",
      "Strategy: group_norm_R0.5 - Epoch 5/10: Train Loss=0.6825, Train Acc=75.81%, Val Loss=0.7875, Val Acc=72.24%\n",
      "Strategy: group_norm_R0.5 - Epoch 6/10: Train Loss=0.6469, Train Acc=77.14%, Val Loss=0.7913, Val Acc=72.12%\n",
      "Strategy: group_norm_R0.5 - Epoch 7/10: Train Loss=0.6111, Train Acc=78.33%, Val Loss=0.8175, Val Acc=71.74%\n",
      "Early stopping triggered for 'group_norm_R0.5' after 7 epochs. No improvement in 'val_loss' for 3 epochs.\n",
      "Loading best model weights from epoch 4 with val_loss: 0.7832\n",
      "Training for 'group_norm_R0.5' finished. Loading best recorded model state with val_loss: 0.7832\n",
      "✅ Fine-tuning curves for group_norm_R0.5 saved.\n",
      "--- Evaluating Fine-tuned Model (group_norm @ Ratio 0.50) on TEST SET ---\n",
      "Metrics for 'group_norm' @ Ratio 0.50 (Fine-tuned, on Test Set): {'macs': 4924394.0, 'params': 950314, 'size_mib': 3.6251602172851562, 'accuracy': 66.83, 'loss': 0.9662658648490906}\n",
      "✅ Model saved as ONNX to ./output_final_ratio_experiment/group_norm/model_R0.5_final.onnx\n",
      "\n",
      "\n",
      "--- Processing: Strategy 'group_norm', Ratio: 0.70 ---\n",
      "✅ Model loaded from ./output_final_ratio_experiment/mobilenetv2_initial_dense_trained.pth to cuda\n",
      "--- Pruning model with group_norm to ratio 0.70 ---\n",
      "Initial MACs before pruning for ratio 0.70: 0.006 G\n",
      "Applying GroupMagnitudeImportance with target ratio: 0.70 using pruner's 5 iterative_steps.\n",
      "After pruning for ratio 0.70: MACs 0.005 G (Reduction: 25.07%)\n",
      "Metrics for 'group_norm' @ Ratio 0.70 (Pruned, Before FT, on Test Set): {'macs': 4540476.0, 'params': 868248, 'size_mib': 3.312103271484375, 'accuracy': 22.19, 'loss': 2.159414351463318}\n",
      "--- Fine-tuning pruned model (group_norm @ Ratio 0.70) ---\n",
      "Strategy: group_norm_R0.7 - Epoch 1/10: Train Loss=0.9970, Train Acc=64.65%, Val Loss=0.8795, Val Acc=68.40% (New best val_loss)\n",
      "Strategy: group_norm_R0.7 - Epoch 2/10: Train Loss=0.8791, Train Acc=68.86%, Val Loss=0.8483, Val Acc=69.56% (New best val_loss)\n",
      "Strategy: group_norm_R0.7 - Epoch 3/10: Train Loss=0.8171, Train Acc=71.07%, Val Loss=0.8334, Val Acc=70.12% (New best val_loss)\n",
      "Strategy: group_norm_R0.7 - Epoch 4/10: Train Loss=0.7732, Train Acc=72.63%, Val Loss=0.8311, Val Acc=70.22% (New best val_loss)\n",
      "Strategy: group_norm_R0.7 - Epoch 5/10: Train Loss=0.7316, Train Acc=74.06%, Val Loss=0.8275, Val Acc=70.72% (New best val_loss)\n",
      "Strategy: group_norm_R0.7 - Epoch 6/10: Train Loss=0.6927, Train Acc=75.32%, Val Loss=0.8333, Val Acc=70.32%\n",
      "Strategy: group_norm_R0.7 - Epoch 7/10: Train Loss=0.6534, Train Acc=76.79%, Val Loss=0.8358, Val Acc=70.42%\n",
      "Strategy: group_norm_R0.7 - Epoch 8/10: Train Loss=0.6165, Train Acc=78.04%, Val Loss=0.8522, Val Acc=69.92%\n",
      "Early stopping triggered for 'group_norm_R0.7' after 8 epochs. No improvement in 'val_loss' for 3 epochs.\n",
      "Loading best model weights from epoch 5 with val_loss: 0.8275\n",
      "Training for 'group_norm_R0.7' finished. Loading best recorded model state with val_loss: 0.8275\n",
      "✅ Fine-tuning curves for group_norm_R0.7 saved.\n",
      "--- Evaluating Fine-tuned Model (group_norm @ Ratio 0.70) on TEST SET ---\n",
      "Metrics for 'group_norm' @ Ratio 0.70 (Fine-tuned, on Test Set): {'macs': 4540476.0, 'params': 868248, 'size_mib': 3.312103271484375, 'accuracy': 66.05, 'loss': 0.9747017654418946}\n",
      "✅ Model saved as ONNX to ./output_final_ratio_experiment/group_norm/model_R0.7_final.onnx\n",
      "\n",
      "\n",
      "--- Processing: Strategy 'random', Ratio: 0.20 ---\n",
      "✅ Model loaded from ./output_final_ratio_experiment/mobilenetv2_initial_dense_trained.pth to cuda\n",
      "--- Pruning model with random to ratio 0.20 ---\n",
      "Initial MACs before pruning for ratio 0.20: 0.006 G\n",
      "Applying RandomImportance with target ratio: 0.20 using pruner's 5 iterative_steps.\n",
      "After pruning for ratio 0.20: MACs 0.006 G (Reduction: 8.36%)\n",
      "Metrics for 'random' @ Ratio 0.20 (Pruned, Before FT, on Test Set): {'macs': 5553441.0, 'params': 1076719, 'size_mib': 4.107357025146484, 'accuracy': 50.18, 'loss': 1.400829422569275}\n",
      "--- Fine-tuning pruned model (random @ Ratio 0.20) ---\n",
      "Strategy: random_R0.2 - Epoch 1/10: Train Loss=0.8577, Train Acc=69.52%, Val Loss=0.7740, Val Acc=72.00% (New best val_loss)\n",
      "Strategy: random_R0.2 - Epoch 2/10: Train Loss=0.7673, Train Acc=72.73%, Val Loss=0.7542, Val Acc=72.98% (New best val_loss)\n",
      "Strategy: random_R0.2 - Epoch 3/10: Train Loss=0.7128, Train Acc=74.68%, Val Loss=0.7539, Val Acc=73.30% (New best val_loss)\n",
      "Strategy: random_R0.2 - Epoch 4/10: Train Loss=0.6748, Train Acc=76.18%, Val Loss=0.7494, Val Acc=73.64% (New best val_loss)\n",
      "Strategy: random_R0.2 - Epoch 5/10: Train Loss=0.6319, Train Acc=77.62%, Val Loss=0.7549, Val Acc=73.60%\n",
      "Strategy: random_R0.2 - Epoch 6/10: Train Loss=0.5918, Train Acc=78.96%, Val Loss=0.7626, Val Acc=73.38%\n",
      "Strategy: random_R0.2 - Epoch 7/10: Train Loss=0.5533, Train Acc=80.44%, Val Loss=0.7767, Val Acc=73.38%\n",
      "Early stopping triggered for 'random_R0.2' after 7 epochs. No improvement in 'val_loss' for 3 epochs.\n",
      "Loading best model weights from epoch 4 with val_loss: 0.7494\n",
      "Training for 'random_R0.2' finished. Loading best recorded model state with val_loss: 0.7494\n",
      "✅ Fine-tuning curves for random_R0.2 saved.\n",
      "--- Evaluating Fine-tuned Model (random @ Ratio 0.20) on TEST SET ---\n",
      "Metrics for 'random' @ Ratio 0.20 (Fine-tuned, on Test Set): {'macs': 5553441.0, 'params': 1076719, 'size_mib': 4.107357025146484, 'accuracy': 66.67, 'loss': 0.9684271211624146}\n",
      "✅ Model saved as ONNX to ./output_final_ratio_experiment/random/model_R0.2_final.onnx\n",
      "\n",
      "\n",
      "--- Processing: Strategy 'random', Ratio: 0.50 ---\n",
      "✅ Model loaded from ./output_final_ratio_experiment/mobilenetv2_initial_dense_trained.pth to cuda\n",
      "--- Pruning model with random to ratio 0.50 ---\n",
      "Initial MACs before pruning for ratio 0.50: 0.006 G\n",
      "Applying RandomImportance with target ratio: 0.50 using pruner's 5 iterative_steps.\n",
      "After pruning for ratio 0.50: MACs 0.005 G (Reduction: 18.74%)\n",
      "Metrics for 'random' @ Ratio 0.50 (Pruned, Before FT, on Test Set): {'macs': 4924394.0, 'params': 950314, 'size_mib': 3.6251602172851562, 'accuracy': 24.58, 'loss': 2.115874075126648}\n",
      "--- Fine-tuning pruned model (random @ Ratio 0.50) ---\n",
      "Strategy: random_R0.5 - Epoch 1/10: Train Loss=0.9847, Train Acc=64.97%, Val Loss=0.8823, Val Acc=69.56% (New best val_loss)\n",
      "Strategy: random_R0.5 - Epoch 2/10: Train Loss=0.8663, Train Acc=69.11%, Val Loss=0.8496, Val Acc=70.42% (New best val_loss)\n",
      "Strategy: random_R0.5 - Epoch 3/10: Train Loss=0.8061, Train Acc=71.52%, Val Loss=0.8316, Val Acc=70.78% (New best val_loss)\n",
      "Strategy: random_R0.5 - Epoch 4/10: Train Loss=0.7556, Train Acc=73.04%, Val Loss=0.8175, Val Acc=71.54% (New best val_loss)\n",
      "Strategy: random_R0.5 - Epoch 5/10: Train Loss=0.7150, Train Acc=74.66%, Val Loss=0.8244, Val Acc=71.50%\n",
      "Strategy: random_R0.5 - Epoch 6/10: Train Loss=0.6746, Train Acc=76.22%, Val Loss=0.8325, Val Acc=71.58%\n",
      "Strategy: random_R0.5 - Epoch 7/10: Train Loss=0.6337, Train Acc=77.52%, Val Loss=0.8356, Val Acc=71.58%\n",
      "Early stopping triggered for 'random_R0.5' after 7 epochs. No improvement in 'val_loss' for 3 epochs.\n",
      "Loading best model weights from epoch 4 with val_loss: 0.8175\n",
      "Training for 'random_R0.5' finished. Loading best recorded model state with val_loss: 0.8175\n",
      "✅ Fine-tuning curves for random_R0.5 saved.\n",
      "--- Evaluating Fine-tuned Model (random @ Ratio 0.50) on TEST SET ---\n",
      "Metrics for 'random' @ Ratio 0.50 (Fine-tuned, on Test Set): {'macs': 4924394.0, 'params': 950314, 'size_mib': 3.6251602172851562, 'accuracy': 65.94, 'loss': 0.9781766780853272}\n",
      "✅ Model saved as ONNX to ./output_final_ratio_experiment/random/model_R0.5_final.onnx\n",
      "\n",
      "\n",
      "--- Processing: Strategy 'random', Ratio: 0.70 ---\n",
      "✅ Model loaded from ./output_final_ratio_experiment/mobilenetv2_initial_dense_trained.pth to cuda\n",
      "--- Pruning model with random to ratio 0.70 ---\n",
      "Initial MACs before pruning for ratio 0.70: 0.006 G\n",
      "Applying RandomImportance with target ratio: 0.70 using pruner's 5 iterative_steps.\n",
      "After pruning for ratio 0.70: MACs 0.005 G (Reduction: 25.07%)\n",
      "Metrics for 'random' @ Ratio 0.70 (Pruned, Before FT, on Test Set): {'macs': 4540476.0, 'params': 868248, 'size_mib': 3.312103271484375, 'accuracy': 16.98, 'loss': 2.1885966522216798}\n",
      "--- Fine-tuning pruned model (random @ Ratio 0.70) ---\n",
      "Strategy: random_R0.7 - Epoch 1/10: Train Loss=1.0721, Train Acc=61.81%, Val Loss=0.9589, Val Acc=66.06% (New best val_loss)\n",
      "Strategy: random_R0.7 - Epoch 2/10: Train Loss=0.9322, Train Acc=66.73%, Val Loss=0.9134, Val Acc=67.60% (New best val_loss)\n",
      "Strategy: random_R0.7 - Epoch 3/10: Train Loss=0.8656, Train Acc=69.39%, Val Loss=0.8907, Val Acc=68.50% (New best val_loss)\n",
      "Strategy: random_R0.7 - Epoch 4/10: Train Loss=0.8144, Train Acc=71.08%, Val Loss=0.8863, Val Acc=68.96% (New best val_loss)\n",
      "Strategy: random_R0.7 - Epoch 5/10: Train Loss=0.7713, Train Acc=72.77%, Val Loss=0.8804, Val Acc=69.06% (New best val_loss)\n",
      "Strategy: random_R0.7 - Epoch 6/10: Train Loss=0.7248, Train Acc=74.10%, Val Loss=0.8889, Val Acc=68.64%\n",
      "Strategy: random_R0.7 - Epoch 7/10: Train Loss=0.6904, Train Acc=75.28%, Val Loss=0.8923, Val Acc=69.06%\n",
      "Strategy: random_R0.7 - Epoch 8/10: Train Loss=0.6516, Train Acc=76.75%, Val Loss=0.9018, Val Acc=68.60%\n",
      "Early stopping triggered for 'random_R0.7' after 8 epochs. No improvement in 'val_loss' for 3 epochs.\n",
      "Loading best model weights from epoch 5 with val_loss: 0.8804\n",
      "Training for 'random_R0.7' finished. Loading best recorded model state with val_loss: 0.8804\n",
      "✅ Fine-tuning curves for random_R0.7 saved.\n",
      "--- Evaluating Fine-tuned Model (random @ Ratio 0.70) on TEST SET ---\n",
      "Metrics for 'random' @ Ratio 0.70 (Fine-tuned, on Test Set): {'macs': 4540476.0, 'params': 868248, 'size_mib': 3.312103271484375, 'accuracy': 64.92, 'loss': 0.9943559801101685}\n",
      "✅ Model saved as ONNX to ./output_final_ratio_experiment/random/model_R0.7_final.onnx\n",
      "\n",
      "\n",
      "--- Processing: Strategy 'Taylor', Ratio: 0.20 ---\n",
      "✅ Model loaded from ./output_final_ratio_experiment/mobilenetv2_initial_dense_trained.pth to cuda\n",
      "--- Pruning model with Taylor to ratio 0.20 ---\n",
      "Initial MACs before pruning for ratio 0.20: 0.006 G\n",
      "Applying TaylorImportance with target ratio: 0.20 using manual backward passes in eval mode.\n",
      "  Taylor Manual Step 1/5 (target overall ratio: 0.20)\n",
      "  MACs after Taylor step 1: 0.006 G\n",
      "  Taylor Manual Step 2/5 (target overall ratio: 0.20)\n",
      "  No more groups to prune at Taylor step 2 based on ch_sparsity=0.040.\n",
      "  MACs after Taylor step 2: 0.006 G\n",
      "  Taylor Manual Step 3/5 (target overall ratio: 0.20)\n",
      "  No more groups to prune at Taylor step 3 based on ch_sparsity=0.040.\n",
      "  MACs after Taylor step 3: 0.006 G\n",
      "  Taylor Manual Step 4/5 (target overall ratio: 0.20)\n",
      "  No more groups to prune at Taylor step 4 based on ch_sparsity=0.040.\n",
      "  MACs after Taylor step 4: 0.006 G\n",
      "  Taylor Manual Step 5/5 (target overall ratio: 0.20)\n",
      "  No more groups to prune at Taylor step 5 based on ch_sparsity=0.040.\n",
      "  MACs after Taylor step 5: 0.006 G\n",
      "After pruning for ratio 0.20: MACs 0.006 G (Reduction: 8.36%)\n",
      "Metrics for 'Taylor' @ Ratio 0.20 (Pruned, Before FT, on Test Set): {'macs': 5553441.0, 'params': 1076719, 'size_mib': 4.107357025146484, 'accuracy': 53.11, 'loss': 1.3721149129867554}\n",
      "--- Fine-tuning pruned model (Taylor @ Ratio 0.20) ---\n",
      "Strategy: Taylor_R0.2 - Epoch 1/10: Train Loss=0.8358, Train Acc=70.61%, Val Loss=0.7515, Val Acc=73.70% (New best val_loss)\n",
      "Strategy: Taylor_R0.2 - Epoch 2/10: Train Loss=0.7527, Train Acc=73.38%, Val Loss=0.7333, Val Acc=74.26% (New best val_loss)\n",
      "Strategy: Taylor_R0.2 - Epoch 3/10: Train Loss=0.7021, Train Acc=75.23%, Val Loss=0.7283, Val Acc=74.74% (New best val_loss)\n",
      "Strategy: Taylor_R0.2 - Epoch 4/10: Train Loss=0.6548, Train Acc=76.91%, Val Loss=0.7323, Val Acc=74.34%\n",
      "Strategy: Taylor_R0.2 - Epoch 5/10: Train Loss=0.6100, Train Acc=78.39%, Val Loss=0.7400, Val Acc=74.20%\n",
      "Strategy: Taylor_R0.2 - Epoch 6/10: Train Loss=0.5737, Train Acc=79.84%, Val Loss=0.7467, Val Acc=74.62%\n",
      "Early stopping triggered for 'Taylor_R0.2' after 6 epochs. No improvement in 'val_loss' for 3 epochs.\n",
      "Loading best model weights from epoch 3 with val_loss: 0.7283\n",
      "Training for 'Taylor_R0.2' finished. Loading best recorded model state with val_loss: 0.7283\n",
      "✅ Fine-tuning curves for Taylor_R0.2 saved.\n",
      "--- Evaluating Fine-tuned Model (Taylor @ Ratio 0.20) on TEST SET ---\n",
      "Metrics for 'Taylor' @ Ratio 0.20 (Fine-tuned, on Test Set): {'macs': 5553441.0, 'params': 1076719, 'size_mib': 4.107357025146484, 'accuracy': 67.46, 'loss': 0.9516651949882508}\n",
      "✅ Model saved as ONNX to ./output_final_ratio_experiment/Taylor/model_R0.2_final.onnx\n",
      "\n",
      "\n",
      "--- Processing: Strategy 'Taylor', Ratio: 0.50 ---\n",
      "✅ Model loaded from ./output_final_ratio_experiment/mobilenetv2_initial_dense_trained.pth to cuda\n",
      "--- Pruning model with Taylor to ratio 0.50 ---\n",
      "Initial MACs before pruning for ratio 0.50: 0.006 G\n",
      "Applying TaylorImportance with target ratio: 0.50 using manual backward passes in eval mode.\n",
      "  Taylor Manual Step 1/5 (target overall ratio: 0.50)\n",
      "  MACs after Taylor step 1: 0.005 G\n",
      "  Taylor Manual Step 2/5 (target overall ratio: 0.50)\n",
      "  No more groups to prune at Taylor step 2 based on ch_sparsity=0.100.\n",
      "  MACs after Taylor step 2: 0.005 G\n",
      "  Taylor Manual Step 3/5 (target overall ratio: 0.50)\n",
      "  No more groups to prune at Taylor step 3 based on ch_sparsity=0.100.\n",
      "  MACs after Taylor step 3: 0.005 G\n",
      "  Taylor Manual Step 4/5 (target overall ratio: 0.50)\n",
      "  No more groups to prune at Taylor step 4 based on ch_sparsity=0.100.\n",
      "  MACs after Taylor step 4: 0.005 G\n",
      "  Taylor Manual Step 5/5 (target overall ratio: 0.50)\n",
      "  No more groups to prune at Taylor step 5 based on ch_sparsity=0.100.\n",
      "  MACs after Taylor step 5: 0.005 G\n",
      "After pruning for ratio 0.50: MACs 0.005 G (Reduction: 18.74%)\n",
      "Metrics for 'Taylor' @ Ratio 0.50 (Pruned, Before FT, on Test Set): {'macs': 4924394.0, 'params': 950314, 'size_mib': 3.6251602172851562, 'accuracy': 39.56, 'loss': 1.785759843635559}\n",
      "--- Fine-tuning pruned model (Taylor @ Ratio 0.50) ---\n",
      "Strategy: Taylor_R0.5 - Epoch 1/10: Train Loss=0.9673, Train Acc=65.69%, Val Loss=0.8596, Val Acc=69.24% (New best val_loss)\n",
      "Strategy: Taylor_R0.5 - Epoch 2/10: Train Loss=0.8502, Train Acc=69.86%, Val Loss=0.8389, Val Acc=70.20% (New best val_loss)\n",
      "Strategy: Taylor_R0.5 - Epoch 3/10: Train Loss=0.7911, Train Acc=71.91%, Val Loss=0.8132, Val Acc=70.94% (New best val_loss)\n",
      "Strategy: Taylor_R0.5 - Epoch 4/10: Train Loss=0.7428, Train Acc=73.64%, Val Loss=0.8088, Val Acc=71.24% (New best val_loss)\n",
      "Strategy: Taylor_R0.5 - Epoch 5/10: Train Loss=0.6976, Train Acc=75.39%, Val Loss=0.8167, Val Acc=71.66%\n",
      "Strategy: Taylor_R0.5 - Epoch 6/10: Train Loss=0.6563, Train Acc=76.89%, Val Loss=0.8185, Val Acc=71.24%\n",
      "Strategy: Taylor_R0.5 - Epoch 7/10: Train Loss=0.6174, Train Acc=78.01%, Val Loss=0.8293, Val Acc=71.06%\n",
      "Early stopping triggered for 'Taylor_R0.5' after 7 epochs. No improvement in 'val_loss' for 3 epochs.\n",
      "Loading best model weights from epoch 4 with val_loss: 0.8088\n",
      "Training for 'Taylor_R0.5' finished. Loading best recorded model state with val_loss: 0.8088\n",
      "✅ Fine-tuning curves for Taylor_R0.5 saved.\n",
      "--- Evaluating Fine-tuned Model (Taylor @ Ratio 0.50) on TEST SET ---\n",
      "Metrics for 'Taylor' @ Ratio 0.50 (Fine-tuned, on Test Set): {'macs': 4924394.0, 'params': 950314, 'size_mib': 3.6251602172851562, 'accuracy': 66.12, 'loss': 0.9785566274642944}\n",
      "✅ Model saved as ONNX to ./output_final_ratio_experiment/Taylor/model_R0.5_final.onnx\n",
      "\n",
      "\n",
      "--- Processing: Strategy 'Taylor', Ratio: 0.70 ---\n",
      "✅ Model loaded from ./output_final_ratio_experiment/mobilenetv2_initial_dense_trained.pth to cuda\n",
      "--- Pruning model with Taylor to ratio 0.70 ---\n",
      "Initial MACs before pruning for ratio 0.70: 0.006 G\n",
      "Applying TaylorImportance with target ratio: 0.70 using manual backward passes in eval mode.\n",
      "  Taylor Manual Step 1/5 (target overall ratio: 0.70)\n",
      "  MACs after Taylor step 1: 0.005 G\n",
      "  Taylor Manual Step 2/5 (target overall ratio: 0.70)\n",
      "  No more groups to prune at Taylor step 2 based on ch_sparsity=0.140.\n",
      "  MACs after Taylor step 2: 0.005 G\n",
      "  Taylor Manual Step 3/5 (target overall ratio: 0.70)\n",
      "  No more groups to prune at Taylor step 3 based on ch_sparsity=0.140.\n",
      "  MACs after Taylor step 3: 0.005 G\n",
      "  Taylor Manual Step 4/5 (target overall ratio: 0.70)\n",
      "  No more groups to prune at Taylor step 4 based on ch_sparsity=0.140.\n",
      "  MACs after Taylor step 4: 0.005 G\n",
      "  Taylor Manual Step 5/5 (target overall ratio: 0.70)\n",
      "  No more groups to prune at Taylor step 5 based on ch_sparsity=0.140.\n",
      "  MACs after Taylor step 5: 0.005 G\n",
      "After pruning for ratio 0.70: MACs 0.005 G (Reduction: 25.07%)\n",
      "Metrics for 'Taylor' @ Ratio 0.70 (Pruned, Before FT, on Test Set): {'macs': 4540476.0, 'params': 868248, 'size_mib': 3.312103271484375, 'accuracy': 32.65, 'loss': 1.9685843803405763}\n",
      "--- Fine-tuning pruned model (Taylor @ Ratio 0.70) ---\n",
      "Strategy: Taylor_R0.7 - Epoch 1/10: Train Loss=1.0596, Train Acc=62.62%, Val Loss=0.9238, Val Acc=66.94% (New best val_loss)\n",
      "Strategy: Taylor_R0.7 - Epoch 2/10: Train Loss=0.9145, Train Acc=67.47%, Val Loss=0.8802, Val Acc=68.66% (New best val_loss)\n",
      "Strategy: Taylor_R0.7 - Epoch 3/10: Train Loss=0.8481, Train Acc=69.89%, Val Loss=0.8578, Val Acc=69.52% (New best val_loss)\n",
      "Strategy: Taylor_R0.7 - Epoch 4/10: Train Loss=0.7970, Train Acc=71.57%, Val Loss=0.8494, Val Acc=69.72% (New best val_loss)\n",
      "Strategy: Taylor_R0.7 - Epoch 5/10: Train Loss=0.7498, Train Acc=73.52%, Val Loss=0.8445, Val Acc=70.60% (New best val_loss)\n",
      "Strategy: Taylor_R0.7 - Epoch 6/10: Train Loss=0.7105, Train Acc=74.65%, Val Loss=0.8542, Val Acc=70.02%\n",
      "Strategy: Taylor_R0.7 - Epoch 7/10: Train Loss=0.6731, Train Acc=76.21%, Val Loss=0.8564, Val Acc=70.12%\n",
      "Strategy: Taylor_R0.7 - Epoch 8/10: Train Loss=0.6351, Train Acc=77.48%, Val Loss=0.8697, Val Acc=70.20%\n",
      "Early stopping triggered for 'Taylor_R0.7' after 8 epochs. No improvement in 'val_loss' for 3 epochs.\n",
      "Loading best model weights from epoch 5 with val_loss: 0.8445\n",
      "Training for 'Taylor_R0.7' finished. Loading best recorded model state with val_loss: 0.8445\n",
      "✅ Fine-tuning curves for Taylor_R0.7 saved.\n",
      "--- Evaluating Fine-tuned Model (Taylor @ Ratio 0.70) on TEST SET ---\n",
      "Metrics for 'Taylor' @ Ratio 0.70 (Fine-tuned, on Test Set): {'macs': 4540476.0, 'params': 868248, 'size_mib': 3.312103271484375, 'accuracy': 64.98, 'loss': 1.0014290266036987}\n",
      "✅ Model saved as ONNX to ./output_final_ratio_experiment/Taylor/model_R0.7_final.onnx\n",
      "\n",
      "\n",
      "--- Processing: Strategy 'Hessian', Ratio: 0.20 ---\n",
      "✅ Model loaded from ./output_final_ratio_experiment/mobilenetv2_initial_dense_trained.pth to cuda\n",
      "--- Pruning model with Hessian to ratio 0.20 ---\n",
      "Initial MACs before pruning for ratio 0.20: 0.006 G\n",
      "Applying GroupHessianImportance with target ratio: 0.20 using pruner's 5 iterative_steps.\n",
      "After pruning for ratio 0.20: MACs 0.006 G (Reduction: 0.00%)\n",
      "Metrics for 'Hessian' @ Ratio 0.20 (Pruned, Before FT, on Test Set): {'macs': 6059786.0, 'params': 1169642, 'size_mib': 4.461830139160156, 'accuracy': 63.29, 'loss': 1.032341127204895}\n",
      "--- Fine-tuning pruned model (Hessian @ Ratio 0.20) ---\n",
      "Strategy: Hessian_R0.2 - Epoch 1/10: Train Loss=0.7493, Train Acc=73.67%, Val Loss=0.6693, Val Acc=76.56% (New best val_loss)\n",
      "Strategy: Hessian_R0.2 - Epoch 2/10: Train Loss=0.6764, Train Acc=76.31%, Val Loss=0.6590, Val Acc=76.96% (New best val_loss)\n",
      "Strategy: Hessian_R0.2 - Epoch 3/10: Train Loss=0.6311, Train Acc=78.02%, Val Loss=0.6599, Val Acc=76.98%\n",
      "Strategy: Hessian_R0.2 - Epoch 4/10: Train Loss=0.5899, Train Acc=79.43%, Val Loss=0.6607, Val Acc=76.74%\n",
      "Strategy: Hessian_R0.2 - Epoch 5/10: Train Loss=0.5506, Train Acc=80.66%, Val Loss=0.6762, Val Acc=76.78%\n",
      "Early stopping triggered for 'Hessian_R0.2' after 5 epochs. No improvement in 'val_loss' for 3 epochs.\n",
      "Loading best model weights from epoch 2 with val_loss: 0.6590\n",
      "Training for 'Hessian_R0.2' finished. Loading best recorded model state with val_loss: 0.6590\n",
      "✅ Fine-tuning curves for Hessian_R0.2 saved.\n",
      "--- Evaluating Fine-tuned Model (Hessian @ Ratio 0.20) on TEST SET ---\n",
      "Metrics for 'Hessian' @ Ratio 0.20 (Fine-tuned, on Test Set): {'macs': 6059786.0, 'params': 1169642, 'size_mib': 4.461830139160156, 'accuracy': 67.97, 'loss': 0.9267688980102539}\n",
      "✅ Model saved as ONNX to ./output_final_ratio_experiment/Hessian/model_R0.2_final.onnx\n",
      "\n",
      "\n",
      "--- Processing: Strategy 'Hessian', Ratio: 0.50 ---\n",
      "✅ Model loaded from ./output_final_ratio_experiment/mobilenetv2_initial_dense_trained.pth to cuda\n",
      "--- Pruning model with Hessian to ratio 0.50 ---\n",
      "Initial MACs before pruning for ratio 0.50: 0.006 G\n",
      "Applying GroupHessianImportance with target ratio: 0.50 using pruner's 5 iterative_steps.\n",
      "After pruning for ratio 0.50: MACs 0.006 G (Reduction: 0.00%)\n",
      "Metrics for 'Hessian' @ Ratio 0.50 (Pruned, Before FT, on Test Set): {'macs': 6059786.0, 'params': 1169642, 'size_mib': 4.461830139160156, 'accuracy': 63.29, 'loss': 1.032341127204895}\n",
      "--- Fine-tuning pruned model (Hessian @ Ratio 0.50) ---\n",
      "Strategy: Hessian_R0.5 - Epoch 1/10: Train Loss=0.7474, Train Acc=73.85%, Val Loss=0.6823, Val Acc=76.26% (New best val_loss)\n",
      "Strategy: Hessian_R0.5 - Epoch 2/10: Train Loss=0.6789, Train Acc=76.07%, Val Loss=0.6621, Val Acc=76.68% (New best val_loss)\n",
      "Strategy: Hessian_R0.5 - Epoch 3/10: Train Loss=0.6325, Train Acc=77.80%, Val Loss=0.6612, Val Acc=77.16% (New best val_loss)\n",
      "Strategy: Hessian_R0.5 - Epoch 4/10: Train Loss=0.5886, Train Acc=79.33%, Val Loss=0.6626, Val Acc=76.90%\n",
      "Strategy: Hessian_R0.5 - Epoch 5/10: Train Loss=0.5461, Train Acc=80.87%, Val Loss=0.6803, Val Acc=76.80%\n",
      "Strategy: Hessian_R0.5 - Epoch 6/10: Train Loss=0.5079, Train Acc=82.12%, Val Loss=0.6839, Val Acc=76.80%\n",
      "Early stopping triggered for 'Hessian_R0.5' after 6 epochs. No improvement in 'val_loss' for 3 epochs.\n",
      "Loading best model weights from epoch 3 with val_loss: 0.6612\n",
      "Training for 'Hessian_R0.5' finished. Loading best recorded model state with val_loss: 0.6612\n",
      "✅ Fine-tuning curves for Hessian_R0.5 saved.\n",
      "--- Evaluating Fine-tuned Model (Hessian @ Ratio 0.50) on TEST SET ---\n",
      "Metrics for 'Hessian' @ Ratio 0.50 (Fine-tuned, on Test Set): {'macs': 6059786.0, 'params': 1169642, 'size_mib': 4.461830139160156, 'accuracy': 68.0, 'loss': 0.9446450121879577}\n",
      "✅ Model saved as ONNX to ./output_final_ratio_experiment/Hessian/model_R0.5_final.onnx\n",
      "\n",
      "\n",
      "--- Processing: Strategy 'Hessian', Ratio: 0.70 ---\n",
      "✅ Model loaded from ./output_final_ratio_experiment/mobilenetv2_initial_dense_trained.pth to cuda\n",
      "--- Pruning model with Hessian to ratio 0.70 ---\n",
      "Initial MACs before pruning for ratio 0.70: 0.006 G\n",
      "Applying GroupHessianImportance with target ratio: 0.70 using pruner's 5 iterative_steps.\n",
      "After pruning for ratio 0.70: MACs 0.006 G (Reduction: 0.00%)\n",
      "Metrics for 'Hessian' @ Ratio 0.70 (Pruned, Before FT, on Test Set): {'macs': 6059786.0, 'params': 1169642, 'size_mib': 4.461830139160156, 'accuracy': 63.29, 'loss': 1.032341127204895}\n",
      "--- Fine-tuning pruned model (Hessian @ Ratio 0.70) ---\n",
      "Strategy: Hessian_R0.7 - Epoch 1/10: Train Loss=0.7479, Train Acc=73.72%, Val Loss=0.6808, Val Acc=76.14% (New best val_loss)\n",
      "Strategy: Hessian_R0.7 - Epoch 2/10: Train Loss=0.6793, Train Acc=75.97%, Val Loss=0.6625, Val Acc=76.82% (New best val_loss)\n",
      "Strategy: Hessian_R0.7 - Epoch 3/10: Train Loss=0.6337, Train Acc=77.72%, Val Loss=0.6611, Val Acc=76.58% (New best val_loss)\n",
      "Strategy: Hessian_R0.7 - Epoch 4/10: Train Loss=0.5900, Train Acc=79.31%, Val Loss=0.6683, Val Acc=76.78%\n",
      "Strategy: Hessian_R0.7 - Epoch 5/10: Train Loss=0.5490, Train Acc=80.70%, Val Loss=0.6772, Val Acc=76.26%\n",
      "Strategy: Hessian_R0.7 - Epoch 6/10: Train Loss=0.5082, Train Acc=82.19%, Val Loss=0.6934, Val Acc=76.36%\n",
      "Early stopping triggered for 'Hessian_R0.7' after 6 epochs. No improvement in 'val_loss' for 3 epochs.\n",
      "Loading best model weights from epoch 3 with val_loss: 0.6611\n",
      "Training for 'Hessian_R0.7' finished. Loading best recorded model state with val_loss: 0.6611\n",
      "✅ Fine-tuning curves for Hessian_R0.7 saved.\n",
      "--- Evaluating Fine-tuned Model (Hessian @ Ratio 0.70) on TEST SET ---\n",
      "Metrics for 'Hessian' @ Ratio 0.70 (Fine-tuned, on Test Set): {'macs': 6059786.0, 'params': 1169642, 'size_mib': 4.461830139160156, 'accuracy': 68.02, 'loss': 0.941760112285614}\n",
      "✅ Model saved as ONNX to ./output_final_ratio_experiment/Hessian/model_R0.7_final.onnx\n",
      "\n",
      "\n",
      "--- Processing: Strategy 'lamp', Ratio: 0.20 ---\n",
      "✅ Model loaded from ./output_final_ratio_experiment/mobilenetv2_initial_dense_trained.pth to cuda\n",
      "--- Pruning model with lamp to ratio 0.20 ---\n",
      "Initial MACs before pruning for ratio 0.20: 0.006 G\n",
      "Applying LAMPImportance with target ratio: 0.20 using pruner's 5 iterative_steps.\n",
      "After pruning for ratio 0.20: MACs 0.006 G (Reduction: 8.36%)\n",
      "Metrics for 'lamp' @ Ratio 0.20 (Pruned, Before FT, on Test Set): {'macs': 5553441.0, 'params': 1076719, 'size_mib': 4.107357025146484, 'accuracy': 57.9, 'loss': 1.2013130826950074}\n",
      "--- Fine-tuning pruned model (lamp @ Ratio 0.20) ---\n",
      "Strategy: lamp_R0.2 - Epoch 1/10: Train Loss=0.8296, Train Acc=70.60%, Val Loss=0.7427, Val Acc=74.26% (New best val_loss)\n",
      "Strategy: lamp_R0.2 - Epoch 2/10: Train Loss=0.7460, Train Acc=73.57%, Val Loss=0.7312, Val Acc=73.50% (New best val_loss)\n",
      "Strategy: lamp_R0.2 - Epoch 3/10: Train Loss=0.6990, Train Acc=75.21%, Val Loss=0.7288, Val Acc=74.02% (New best val_loss)\n",
      "Strategy: lamp_R0.2 - Epoch 4/10: Train Loss=0.6528, Train Acc=76.82%, Val Loss=0.7221, Val Acc=73.82% (New best val_loss)\n",
      "Strategy: lamp_R0.2 - Epoch 5/10: Train Loss=0.6109, Train Acc=78.28%, Val Loss=0.7308, Val Acc=73.86%\n",
      "Strategy: lamp_R0.2 - Epoch 6/10: Train Loss=0.5749, Train Acc=79.86%, Val Loss=0.7496, Val Acc=73.94%\n",
      "Strategy: lamp_R0.2 - Epoch 7/10: Train Loss=0.5379, Train Acc=81.09%, Val Loss=0.7582, Val Acc=73.48%\n",
      "Early stopping triggered for 'lamp_R0.2' after 7 epochs. No improvement in 'val_loss' for 3 epochs.\n",
      "Loading best model weights from epoch 4 with val_loss: 0.7221\n",
      "Training for 'lamp_R0.2' finished. Loading best recorded model state with val_loss: 0.7221\n",
      "✅ Fine-tuning curves for lamp_R0.2 saved.\n",
      "--- Evaluating Fine-tuned Model (lamp @ Ratio 0.20) on TEST SET ---\n",
      "Metrics for 'lamp' @ Ratio 0.20 (Fine-tuned, on Test Set): {'macs': 5553441.0, 'params': 1076719, 'size_mib': 4.107357025146484, 'accuracy': 67.16, 'loss': 0.9491597379684448}\n",
      "✅ Model saved as ONNX to ./output_final_ratio_experiment/lamp/model_R0.2_final.onnx\n",
      "\n",
      "\n",
      "--- Processing: Strategy 'lamp', Ratio: 0.50 ---\n",
      "✅ Model loaded from ./output_final_ratio_experiment/mobilenetv2_initial_dense_trained.pth to cuda\n",
      "--- Pruning model with lamp to ratio 0.50 ---\n",
      "Initial MACs before pruning for ratio 0.50: 0.006 G\n",
      "Applying LAMPImportance with target ratio: 0.50 using pruner's 5 iterative_steps.\n",
      "After pruning for ratio 0.50: MACs 0.005 G (Reduction: 18.74%)\n",
      "Metrics for 'lamp' @ Ratio 0.50 (Pruned, Before FT, on Test Set): {'macs': 4924394.0, 'params': 950314, 'size_mib': 3.6251602172851562, 'accuracy': 39.09, 'loss': 1.716886913871765}\n",
      "--- Fine-tuning pruned model (lamp @ Ratio 0.50) ---\n",
      "Strategy: lamp_R0.5 - Epoch 1/10: Train Loss=0.9361, Train Acc=66.64%, Val Loss=0.8356, Val Acc=69.62% (New best val_loss)\n",
      "Strategy: lamp_R0.5 - Epoch 2/10: Train Loss=0.8327, Train Acc=70.24%, Val Loss=0.8096, Val Acc=70.52% (New best val_loss)\n",
      "Strategy: lamp_R0.5 - Epoch 3/10: Train Loss=0.7765, Train Acc=72.36%, Val Loss=0.7943, Val Acc=71.28% (New best val_loss)\n",
      "Strategy: lamp_R0.5 - Epoch 4/10: Train Loss=0.7299, Train Acc=73.91%, Val Loss=0.7878, Val Acc=71.50% (New best val_loss)\n",
      "Strategy: lamp_R0.5 - Epoch 5/10: Train Loss=0.6900, Train Acc=75.45%, Val Loss=0.7933, Val Acc=71.60%\n",
      "Strategy: lamp_R0.5 - Epoch 6/10: Train Loss=0.6506, Train Acc=76.87%, Val Loss=0.8030, Val Acc=70.92%\n",
      "Strategy: lamp_R0.5 - Epoch 7/10: Train Loss=0.6142, Train Acc=78.13%, Val Loss=0.8147, Val Acc=71.48%\n",
      "Early stopping triggered for 'lamp_R0.5' after 7 epochs. No improvement in 'val_loss' for 3 epochs.\n",
      "Loading best model weights from epoch 4 with val_loss: 0.7878\n",
      "Training for 'lamp_R0.5' finished. Loading best recorded model state with val_loss: 0.7878\n",
      "✅ Fine-tuning curves for lamp_R0.5 saved.\n",
      "--- Evaluating Fine-tuned Model (lamp @ Ratio 0.50) on TEST SET ---\n",
      "Metrics for 'lamp' @ Ratio 0.50 (Fine-tuned, on Test Set): {'macs': 4924394.0, 'params': 950314, 'size_mib': 3.6251602172851562, 'accuracy': 66.69, 'loss': 0.9608036447525025}\n",
      "✅ Model saved as ONNX to ./output_final_ratio_experiment/lamp/model_R0.5_final.onnx\n",
      "\n",
      "\n",
      "--- Processing: Strategy 'lamp', Ratio: 0.70 ---\n",
      "✅ Model loaded from ./output_final_ratio_experiment/mobilenetv2_initial_dense_trained.pth to cuda\n",
      "--- Pruning model with lamp to ratio 0.70 ---\n",
      "Initial MACs before pruning for ratio 0.70: 0.006 G\n",
      "Applying LAMPImportance with target ratio: 0.70 using pruner's 5 iterative_steps.\n",
      "After pruning for ratio 0.70: MACs 0.005 G (Reduction: 25.07%)\n",
      "Metrics for 'lamp' @ Ratio 0.70 (Pruned, Before FT, on Test Set): {'macs': 4540476.0, 'params': 868248, 'size_mib': 3.312103271484375, 'accuracy': 19.56, 'loss': 2.2257247673034666}\n",
      "--- Fine-tuning pruned model (lamp @ Ratio 0.70) ---\n",
      "Strategy: lamp_R0.7 - Epoch 1/10: Train Loss=0.9993, Train Acc=64.32%, Val Loss=0.8914, Val Acc=67.66% (New best val_loss)\n",
      "Strategy: lamp_R0.7 - Epoch 2/10: Train Loss=0.8817, Train Acc=68.29%, Val Loss=0.8457, Val Acc=69.14% (New best val_loss)\n",
      "Strategy: lamp_R0.7 - Epoch 3/10: Train Loss=0.8239, Train Acc=70.61%, Val Loss=0.8341, Val Acc=69.50% (New best val_loss)\n",
      "Strategy: lamp_R0.7 - Epoch 4/10: Train Loss=0.7796, Train Acc=72.04%, Val Loss=0.8221, Val Acc=70.10% (New best val_loss)\n",
      "Strategy: lamp_R0.7 - Epoch 5/10: Train Loss=0.7303, Train Acc=73.71%, Val Loss=0.8302, Val Acc=69.98%\n",
      "Strategy: lamp_R0.7 - Epoch 6/10: Train Loss=0.6951, Train Acc=75.21%, Val Loss=0.8358, Val Acc=69.68%\n",
      "Strategy: lamp_R0.7 - Epoch 7/10: Train Loss=0.6579, Train Acc=76.34%, Val Loss=0.8402, Val Acc=70.28%\n",
      "Early stopping triggered for 'lamp_R0.7' after 7 epochs. No improvement in 'val_loss' for 3 epochs.\n",
      "Loading best model weights from epoch 4 with val_loss: 0.8221\n",
      "Training for 'lamp_R0.7' finished. Loading best recorded model state with val_loss: 0.8221\n",
      "✅ Fine-tuning curves for lamp_R0.7 saved.\n",
      "--- Evaluating Fine-tuned Model (lamp @ Ratio 0.70) on TEST SET ---\n",
      "Metrics for 'lamp' @ Ratio 0.70 (Fine-tuned, on Test Set): {'macs': 4540476.0, 'params': 868248, 'size_mib': 3.312103271484375, 'accuracy': 65.74, 'loss': 0.9686968242645264}\n",
      "✅ Model saved as ONNX to ./output_final_ratio_experiment/lamp/model_R0.7_final.onnx\n",
      "\n",
      "\n",
      "--- Processing: Strategy 'geometry', Ratio: 0.20 ---\n",
      "✅ Model loaded from ./output_final_ratio_experiment/mobilenetv2_initial_dense_trained.pth to cuda\n",
      "--- Pruning model with geometry to ratio 0.20 ---\n",
      "Initial MACs before pruning for ratio 0.20: 0.006 G\n",
      "Applying FPGMImportance with target ratio: 0.20 using pruner's 5 iterative_steps.\n",
      "After pruning for ratio 0.20: MACs 0.006 G (Reduction: 8.36%)\n",
      "Metrics for 'geometry' @ Ratio 0.20 (Pruned, Before FT, on Test Set): {'macs': 5553441.0, 'params': 1076719, 'size_mib': 4.107357025146484, 'accuracy': 55.74, 'loss': 1.2712053163528443}\n",
      "--- Fine-tuning pruned model (geometry @ Ratio 0.20) ---\n",
      "Strategy: geometry_R0.2 - Epoch 1/10: Train Loss=0.8205, Train Acc=70.89%, Val Loss=0.7434, Val Acc=73.52% (New best val_loss)\n",
      "Strategy: geometry_R0.2 - Epoch 2/10: Train Loss=0.7436, Train Acc=73.55%, Val Loss=0.7301, Val Acc=73.94% (New best val_loss)\n",
      "Strategy: geometry_R0.2 - Epoch 3/10: Train Loss=0.6958, Train Acc=75.31%, Val Loss=0.7258, Val Acc=74.40% (New best val_loss)\n",
      "Strategy: geometry_R0.2 - Epoch 4/10: Train Loss=0.6511, Train Acc=76.96%, Val Loss=0.7224, Val Acc=74.22% (New best val_loss)\n",
      "Strategy: geometry_R0.2 - Epoch 5/10: Train Loss=0.6058, Train Acc=78.57%, Val Loss=0.7268, Val Acc=74.60%\n",
      "Strategy: geometry_R0.2 - Epoch 6/10: Train Loss=0.5735, Train Acc=79.77%, Val Loss=0.7342, Val Acc=74.44%\n",
      "Strategy: geometry_R0.2 - Epoch 7/10: Train Loss=0.5360, Train Acc=81.11%, Val Loss=0.7594, Val Acc=74.18%\n",
      "Early stopping triggered for 'geometry_R0.2' after 7 epochs. No improvement in 'val_loss' for 3 epochs.\n",
      "Loading best model weights from epoch 4 with val_loss: 0.7224\n",
      "Training for 'geometry_R0.2' finished. Loading best recorded model state with val_loss: 0.7224\n",
      "✅ Fine-tuning curves for geometry_R0.2 saved.\n",
      "--- Evaluating Fine-tuned Model (geometry @ Ratio 0.20) on TEST SET ---\n",
      "Metrics for 'geometry' @ Ratio 0.20 (Fine-tuned, on Test Set): {'macs': 5553441.0, 'params': 1076719, 'size_mib': 4.107357025146484, 'accuracy': 67.25, 'loss': 0.9611073186874389}\n",
      "✅ Model saved as ONNX to ./output_final_ratio_experiment/geometry/model_R0.2_final.onnx\n",
      "\n",
      "\n",
      "--- Processing: Strategy 'geometry', Ratio: 0.50 ---\n",
      "✅ Model loaded from ./output_final_ratio_experiment/mobilenetv2_initial_dense_trained.pth to cuda\n",
      "--- Pruning model with geometry to ratio 0.50 ---\n",
      "Initial MACs before pruning for ratio 0.50: 0.006 G\n",
      "Applying FPGMImportance with target ratio: 0.50 using pruner's 5 iterative_steps.\n",
      "After pruning for ratio 0.50: MACs 0.005 G (Reduction: 18.74%)\n",
      "Metrics for 'geometry' @ Ratio 0.50 (Pruned, Before FT, on Test Set): {'macs': 4924394.0, 'params': 950314, 'size_mib': 3.6251602172851562, 'accuracy': 33.42, 'loss': 1.8461406827926636}\n",
      "--- Fine-tuning pruned model (geometry @ Ratio 0.50) ---\n",
      "Strategy: geometry_R0.5 - Epoch 1/10: Train Loss=0.9275, Train Acc=67.18%, Val Loss=0.8234, Val Acc=69.98% (New best val_loss)\n",
      "Strategy: geometry_R0.5 - Epoch 2/10: Train Loss=0.8267, Train Acc=70.45%, Val Loss=0.7968, Val Acc=71.02% (New best val_loss)\n",
      "Strategy: geometry_R0.5 - Epoch 3/10: Train Loss=0.7712, Train Acc=72.66%, Val Loss=0.7844, Val Acc=71.30% (New best val_loss)\n",
      "Strategy: geometry_R0.5 - Epoch 4/10: Train Loss=0.7298, Train Acc=74.14%, Val Loss=0.7787, Val Acc=71.68% (New best val_loss)\n",
      "Strategy: geometry_R0.5 - Epoch 5/10: Train Loss=0.6894, Train Acc=75.50%, Val Loss=0.7832, Val Acc=71.80%\n",
      "Strategy: geometry_R0.5 - Epoch 6/10: Train Loss=0.6491, Train Acc=76.88%, Val Loss=0.7856, Val Acc=71.66%\n",
      "Strategy: geometry_R0.5 - Epoch 7/10: Train Loss=0.6131, Train Acc=78.35%, Val Loss=0.7984, Val Acc=71.50%\n",
      "Early stopping triggered for 'geometry_R0.5' after 7 epochs. No improvement in 'val_loss' for 3 epochs.\n",
      "Loading best model weights from epoch 4 with val_loss: 0.7787\n",
      "Training for 'geometry_R0.5' finished. Loading best recorded model state with val_loss: 0.7787\n",
      "✅ Fine-tuning curves for geometry_R0.5 saved.\n",
      "--- Evaluating Fine-tuned Model (geometry @ Ratio 0.50) on TEST SET ---\n",
      "Metrics for 'geometry' @ Ratio 0.50 (Fine-tuned, on Test Set): {'macs': 4924394.0, 'params': 950314, 'size_mib': 3.6251602172851562, 'accuracy': 66.51, 'loss': 0.9561022503852844}\n",
      "✅ Model saved as ONNX to ./output_final_ratio_experiment/geometry/model_R0.5_final.onnx\n",
      "\n",
      "\n",
      "--- Processing: Strategy 'geometry', Ratio: 0.70 ---\n",
      "✅ Model loaded from ./output_final_ratio_experiment/mobilenetv2_initial_dense_trained.pth to cuda\n",
      "--- Pruning model with geometry to ratio 0.70 ---\n",
      "Initial MACs before pruning for ratio 0.70: 0.006 G\n",
      "Applying FPGMImportance with target ratio: 0.70 using pruner's 5 iterative_steps.\n",
      "After pruning for ratio 0.70: MACs 0.005 G (Reduction: 25.07%)\n",
      "Metrics for 'geometry' @ Ratio 0.70 (Pruned, Before FT, on Test Set): {'macs': 4540476.0, 'params': 868248, 'size_mib': 3.312103271484375, 'accuracy': 19.69, 'loss': 2.091989059448242}\n",
      "--- Fine-tuning pruned model (geometry @ Ratio 0.70) ---\n",
      "Strategy: geometry_R0.7 - Epoch 1/10: Train Loss=1.0071, Train Acc=64.20%, Val Loss=0.8885, Val Acc=68.04% (New best val_loss)\n",
      "Strategy: geometry_R0.7 - Epoch 2/10: Train Loss=0.8853, Train Acc=68.52%, Val Loss=0.8524, Val Acc=69.38% (New best val_loss)\n",
      "Strategy: geometry_R0.7 - Epoch 3/10: Train Loss=0.8267, Train Acc=70.61%, Val Loss=0.8333, Val Acc=69.86% (New best val_loss)\n",
      "Strategy: geometry_R0.7 - Epoch 4/10: Train Loss=0.7797, Train Acc=72.28%, Val Loss=0.8230, Val Acc=70.04% (New best val_loss)\n",
      "Strategy: geometry_R0.7 - Epoch 5/10: Train Loss=0.7372, Train Acc=73.76%, Val Loss=0.8275, Val Acc=70.22%\n",
      "Strategy: geometry_R0.7 - Epoch 6/10: Train Loss=0.7018, Train Acc=75.13%, Val Loss=0.8380, Val Acc=70.38%\n",
      "Strategy: geometry_R0.7 - Epoch 7/10: Train Loss=0.6629, Train Acc=76.55%, Val Loss=0.8448, Val Acc=69.60%\n",
      "Early stopping triggered for 'geometry_R0.7' after 7 epochs. No improvement in 'val_loss' for 3 epochs.\n",
      "Loading best model weights from epoch 4 with val_loss: 0.8230\n",
      "Training for 'geometry_R0.7' finished. Loading best recorded model state with val_loss: 0.8230\n",
      "✅ Fine-tuning curves for geometry_R0.7 saved.\n",
      "--- Evaluating Fine-tuned Model (geometry @ Ratio 0.70) on TEST SET ---\n",
      "Metrics for 'geometry' @ Ratio 0.70 (Fine-tuned, on Test Set): {'macs': 4540476.0, 'params': 868248, 'size_mib': 3.312103271484375, 'accuracy': 66.26, 'loss': 0.9666220963478088}\n",
      "✅ Model saved as ONNX to ./output_final_ratio_experiment/geometry/model_R0.7_final.onnx\n",
      "✅ MACs vs. Ratio by Strategy plot saved to ./output_final_ratio_experiment\n",
      "✅ Loss vs. Ratio by Strategy plot saved to ./output_final_ratio_experiment\n",
      "\n",
      "All ratio-based pruning experiments completed successfully!\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "id": "238badca29395cae",
   "metadata": {},
   "source": [
    "### Load the saved Onnx model and convert to Pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "229d26f48f444268",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T12:24:13.022499Z",
     "start_time": "2025-04-10T12:24:12.993796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ONNX model loaded and verified.\n",
      "✅ ONNX model converted to PyTorch.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import onnx\n",
    "import onnxruntime\n",
    "from onnx2torch import convert\n",
    "\n",
    "# Step 1: Load the ONNX model\n",
    "onnx_model_path = './output/strategies/mobilenetv2_bn_scale_final.onnx'\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "onnx.checker.check_model(onnx_model)  # Verify the ONNX model\n",
    "print(\"✅ ONNX model loaded and verified.\")\n",
    "\n",
    "# Step 2: Convert ONNX to PyTorch\n",
    "torch_model = convert(onnx_model)\n",
    "print(\"✅ ONNX model converted to PyTorch.\")\n",
    "tp.utils.print_tool.before_pruning(torch_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "38ba67685c8f9c69",
   "metadata": {},
   "source": [
    "from torchinfo import summary\n",
    "print(summary(torch_model))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4d1dd20bc238d90a",
   "metadata": {},
   "source": [
    "\n",
    "from torchinfo import summary\n",
    "print(summary(torch_model))\n",
    "#tp.utils.print_tool.after_pruning(torch_model)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8de5ca7f1456a3d2",
   "metadata": {},
   "source": [
    "### Load the converted Pytorch model and fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "id": "e1ece7efcc1c6179",
   "metadata": {},
   "source": [
    "train_loader, test_loader = get_data_loaders('./data')\n",
    "# Example usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch_model = torch_model.to(device)\n",
    "\n",
    "tp.utils.print_tool.before_pruning(torch_model)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(torch_model.parameters(), lr=0.001)\n",
    "num_epochs = 50\n",
    "\n",
    "fine_tuned_model = train_model(torch_model, train_loader, criterion, optimizer, device, num_epochs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4a2269b7d3b40915",
   "metadata": {},
   "source": [
    "### Plotting params results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e570a6196c46943",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T13:05:22.667967Z",
     "start_time": "2025-04-10T13:05:22.317180Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data: Total parameters for each model\n",
    "strategies = ['Initial', 'Magnitude', 'BN Scale', 'Group Norm', 'Random', 'Taylor', 'Hessian', 'Lamp']\n",
    "params = [\n",
    "    1162530,  # Initial model (from your query)\n",
    "    305350,   # Magnitude (from output: 0.31M after pruning, assuming final tuned model)\n",
    "    301558,   # BN Scale (from your query and output: 0.31M after pruning)\n",
    "    305350,   # Group Norm (from output: 0.31M after pruning, assuming similar to magnitude)\n",
    "    305350,   # Random (from output: 0.31M after pruning, assuming similar to magnitude)\n",
    "    949650,   # Taylor (from output: 0.95M after pruning)\n",
    "    1162530,  # Hessian (from output: 1.17M, no reduction observed)\n",
    "    305350    # Lamp (from output: 0.31M after pruning)\n",
    "]\n",
    "initial_params = params[0]\n",
    "\n",
    "# Calculate percentage reduction for each strategy\n",
    "reductions = [((initial_params - p) / initial_params * 100) for p in params]\n",
    "\n",
    "# Colors for each strategy\n",
    "colors = ['gray', 'blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink']\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(14, 8))\n",
    "bars = plt.bar(strategies, params, color=colors)\n",
    "\n",
    "# Add value labels and percentage reduction\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:,}\\n({reductions[i]:.1f}%)' if i > 0 else f'{height:,}',\n",
    "             ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Add horizontal line for initial parameters\n",
    "plt.axhline(y=initial_params, color='gray', linestyle='--', label='Initial Parameters')\n",
    "\n",
    "# Customize the chart\n",
    "plt.xlabel('Pruning Strategy')\n",
    "plt.ylabel('Total Parameters')\n",
    "plt.title('Comparison of Total Parameters Across Pruning Strategies')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the chart to a file (since we're not displaying it interactively)\n",
    "plt.savefig('output/pruning_comparison.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3546bf41b88f86",
   "metadata": {},
   "source": [
    "### comparison for MACs, params, size_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24effc323e5da8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:12:58.906564Z",
     "start_time": "2025-04-10T14:12:58.499793Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data: Metrics for each model\n",
    "strategies = ['Initial', 'Magnitude', 'BN Scale', 'Group Norm', 'Random', 'Taylor', 'Hessian', 'Lamp']\n",
    "metrics = {\n",
    "    'params': [1162530, 305350, 301558, 305350, 305350, 949650, 1162530, 305350],  # Total parameters\n",
    "    'macs': [6060000, 1840000, 1840000, 1840000, 1840000, 4920000, 6060000, 1840000],  # MACs\n",
    "    'size_mb': [4.68, 1.22, 1.22, 1.22, 1.22, 3.80, 4.68, 1.22]  # Model size in MB\n",
    "}\n",
    "initial_metrics = {k: v[0] for k, v in metrics.items()}\n",
    "\n",
    "# Calculate percentage reductions\n",
    "reductions = {k: [((initial_metrics[k] - v) / initial_metrics[k] * 100) for v in values] for k, values in metrics.items()}\n",
    "\n",
    "# Colors for each strategy\n",
    "colors = ['gray', 'blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink']\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 18))\n",
    "\n",
    "# Plot each metric\n",
    "for i, (metric_name, values) in enumerate(metrics.items()):\n",
    "    ax = axes[i]\n",
    "    bars = ax.bar(strategies, values, color=colors)\n",
    "\n",
    "    # Add value labels and percentage reduction\n",
    "    for j, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        label = f'{height:,}' if metric_name != 'size_mb' else f'{height:.2f}'\n",
    "        if j > 0:\n",
    "            label += f'\\n({reductions[metric_name][j]:.1f}%)'\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height, label, ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    # Add horizontal line for initial metric\n",
    "    ax.axhline(y=initial_metrics[metric_name], color='gray', linestyle='--', label='Initial')\n",
    "\n",
    "    # Customize subplot\n",
    "    ax.set_xlabel('Pruning Strategy')\n",
    "    ax.set_ylabel(metric_name.replace('_', ' ').title())\n",
    "    ax.set_title(f'Comparison of {metric_name.replace(\"_\", \" \").title()} Across Pruning Strategies')\n",
    "    ax.set_xticks(range(len(strategies)))\n",
    "    ax.set_xticklabels(strategies, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "\n",
    "# Adjust layout and save\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/pruning_metrics_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "id": "6946f1a5fde43aaf",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "epochs = list(range(1, 11))\n",
    "train_acc = [18.48, 21.53, 21.21, 23.94, 26.32, 29.75, 31.91, 34.05, 36.34, 36.77]\n",
    "valid_acc = [21.93, 20.94, 22.23, 25.33, 27.88, 30.41, 33.33, 35.23, 37.02, 38.57]\n",
    "test_acc = [21.90, 21.01, 22.10, 24.86, 27.84, 31.08, 33.47, 35.38, 37.53, 38.45]\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "bar_width = 0.25\n",
    "x = np.arange(len(epochs))\n",
    "\n",
    "# Create bars\n",
    "plt.bar(x - bar_width, train_acc, bar_width, label='Train Accuracy', color='skyblue')\n",
    "plt.bar(x, valid_acc, bar_width, label='Validation Accuracy', color='lightgreen')\n",
    "plt.bar(x + bar_width, test_acc, bar_width, label='Test Accuracy', color='salmon')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Model Accuracy per Epoch')\n",
    "plt.xticks(x, epochs)\n",
    "plt.legend()\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('darts_accuracy_bar_plot.png')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "77dea4c672dce55",
   "metadata": {},
   "source": [
    "### Gr: prune with threshold"
   ]
  },
  {
   "cell_type": "code",
   "id": "59ea3f26d421e293",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T23:18:29.124138Z",
     "start_time": "2025-05-19T23:18:29.116520Z"
    }
   },
   "source": [
    "#def gr_prune_model_with_threshold(model, example_input, target_macs, target_size_mb, strategy, max_iterative_steps=20, max_sparsity=0.9):\n",
    "def gr_prune_model_by_ratio(model_to_prune, example_input_device, strategy_config, target_pruning_ratio,\n",
    "                            iterative_steps_config=5,  # For non-Taylor pruners or overall iterations\n",
    "                            iterative_steps_taylor=5 # For the outer loop of backward passes for Taylor\n",
    "                           ):\n",
    "    model = copy.deepcopy(model_to_prune)\n",
    "    model.to(example_input_device.device)\n",
    "    initial_macs, _ = calculate_macs(model, example_input_device)\n",
    "    print(f\"Initial MACs before pruning for ratio {target_pruning_ratio:.2f}: {initial_macs / 1e9:.3f} G\")\n",
    "\n",
    "    ignored_layers_list = []\n",
    "    if hasattr(model, 'fc'):\n",
    "        ignored_layers_list = [model.fc]\n",
    "    else:\n",
    "        print(\"Warning: model.fc not found. Pruning might affect classifier.\")\n",
    "\n",
    "    if target_pruning_ratio == 0.0:\n",
    "        print(\"Target pruning ratio is 0.0. No pruning applied.\")\n",
    "        return model\n",
    "\n",
    "    if isinstance(strategy_config['importance'], tp.importance.TaylorImportance):\n",
    "        print(f\"Applying TaylorImportance with target ratio: {target_pruning_ratio:.2f} using manual backward passes in eval mode.\")\n",
    "\n",
    "        # For this approach, the pruner is initialized to apply a portion of the pruning\n",
    "        # in each step of our manual loop, similar to your original working snippet.\n",
    "        # The pruner's own `iterative_steps` should be 1 because we are controlling the iteration.\n",
    "        sparsity_per_manual_step = target_pruning_ratio / iterative_steps_taylor\n",
    "        # Ensure some sparsity if target > 0 and steps are too many\n",
    "        if sparsity_per_manual_step == 0 and target_pruning_ratio > 0 :\n",
    "             sparsity_per_manual_step = target_pruning_ratio\n",
    "\n",
    "        # This pruner instance is for ONE application within our manual loop\n",
    "        pruner_for_taylor_step = strategy_config['pruner'](\n",
    "            model,\n",
    "            example_input_device,\n",
    "            importance=strategy_config['importance'],\n",
    "            iterative_steps=1,  # Pruner itself does one application based on current grads\n",
    "            ch_sparsity=sparsity_per_manual_step, # Target for THIS single pruner.step() call\n",
    "            root_module_types=[nn.Conv2d],\n",
    "            ignored_layers=ignored_layers_list\n",
    "        )\n",
    "\n",
    "        # Keep model in EVAL mode for forward/backward to try and avoid BN error\n",
    "        # This relies on requires_grad=True for parameters.\n",
    "        model.eval() # Explicitly set to eval mode\n",
    "\n",
    "        for i in range(iterative_steps_taylor):\n",
    "            # Ensure gradients are zeroed for this step's computation\n",
    "            model.zero_grad()\n",
    "            # Parameters should retain requires_grad=True unless explicitly turned off\n",
    "            for param in model.parameters(): # Ensure gradients are enabled if they were somehow turned off\n",
    "                 if not param.requires_grad:\n",
    "                      param.requires_grad_(True)\n",
    "\n",
    "            # Forward pass in eval mode\n",
    "            loss = model(example_input_device.clone()).mean()\n",
    "\n",
    "            try:\n",
    "                # Backward pass (still in eval mode for BNs)\n",
    "                loss.backward()\n",
    "            except Exception as e:\n",
    "                print(f\"Error during loss.backward() at Taylor step {i+1} (model in eval mode): {e}\")\n",
    "                raise\n",
    "\n",
    "            # Pruner step using the gradients computed above\n",
    "            # The pruner might internally switch parts of the model to train if absolutely necessary,\n",
    "            # but typically it just consumes the gradients.\n",
    "            print(f\"  Taylor Manual Step {i+1}/{iterative_steps_taylor} (target overall ratio: {target_pruning_ratio:.2f})\")\n",
    "            groups_to_prune = list(pruner_for_taylor_step.step(interactive=True)) # Get groups based on current step's sparsity target\n",
    "            if not groups_to_prune and target_pruning_ratio > 0: # If positive target but no groups\n",
    "                 print(f\"  No more groups to prune at Taylor step {i+1} based on ch_sparsity={sparsity_per_manual_step:.3f}.\")\n",
    "                 # We might want to break if we expect more pruning to happen for the overall target\n",
    "                 # This depends on how pruner_for_taylor_step.step() handles reaching its *own* iterative_steps=1.\n",
    "                 # If it can be called multiple times meaningfully to achieve more pruning, then continue.\n",
    "                 # If it only effectively prunes once based on initial sparsity, this loop structure needs rethink.\n",
    "                 # Let's assume for now that calling it multiple times could achieve more if candidates exist.\n",
    "\n",
    "            for g in groups_to_prune:\n",
    "                g.prune()\n",
    "\n",
    "            macs_after_step, _ = calculate_macs(model, example_input_device)\n",
    "            print(f\"  MACs after Taylor step {i+1}: {macs_after_step / 1e9:.3f} G\")\n",
    "\n",
    "            # Check if overall target MACs or effective sparsity is reached (optional premature exit)\n",
    "            # This requires defining what the MACs target would be for `target_pruning_ratio`.\n",
    "            # For simplicity, we run all `iterative_steps_taylor_manual_loop`\n",
    "\n",
    "            # Zero gradients again IF parameters are shared across pruner re-initialization\n",
    "            # But here pruner_for_taylor_step is initialized once. Grads should be cleared by model.zero_grad() at loop start.\n",
    "\n",
    "        # After the loop, model remains in eval() mode. This is usually fine.\n",
    "\n",
    "    else: # For non-Taylor strategies\n",
    "        print(f\"Applying {strategy_config['importance'].__class__.__name__} with target ratio: {target_pruning_ratio:.2f} \"\n",
    "              f\"using pruner's {iterative_steps_config} iterative_steps.\")\n",
    "\n",
    "        # Set model to eval for non-Taylor pruning as well for consistency,\n",
    "        # as most pruners expect eval mode for graph analysis.\n",
    "        model.eval()\n",
    "\n",
    "        pruner = strategy_config['pruner'](\n",
    "            model,\n",
    "            example_input_device,\n",
    "            importance=strategy_config['importance'],\n",
    "            iterative_steps=iterative_steps_config,\n",
    "            ch_sparsity=target_pruning_ratio,\n",
    "            root_module_types=[nn.Conv2d],\n",
    "            ignored_layers=ignored_layers_list\n",
    "        )\n",
    "        pruner.step()\n",
    "\n",
    "    final_macs, _ = calculate_macs(model, example_input_device)\n",
    "    reduction = (initial_macs - final_macs) / initial_macs * 100 if initial_macs > 0 else 0\n",
    "    print(f\"After pruning for ratio {target_pruning_ratio:.2f}: MACs {final_macs / 1e9:.3f} G (Reduction: {reduction:.2f}%)\")\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "id": "ed9159eb03e0d570",
   "metadata": {},
   "source": [
    "### Gem: Prune with threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "572d5d540d81ce45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T03:15:38.633900Z",
     "start_time": "2025-04-29T03:15:38.623094Z"
    }
   },
   "outputs": [],
   "source": [
    "import time # Optional: for adding time limits or tracking\n",
    "\n",
    "# Assume calculate_macs is defined as before:\n",
    "# def calculate_macs(model, example_input):\n",
    "#     macs, params = tp.utils.count_ops_and_params(model, example_input)\n",
    "#     return macs, params\n",
    "\n",
    "def gem_prune_model_by_threshold(model, example_input, target_macs, target_params, strategy, max_iterations=100, step_ch_sparsity=0.1):\n",
    "    \"\"\"\n",
    "    Prunes the model iteratively until both MACs and parameter count are below\n",
    "    the specified thresholds, or max_iterations is reached.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model to prune.\n",
    "        example_input: Example input tensor for MACs calculation and pruner.\n",
    "        target_macs: The desired maximum MAC count.\n",
    "        target_params: The desired maximum parameter count.\n",
    "        strategy: Dictionary containing 'pruner' and 'importance'.\n",
    "        max_iterations (int): Safety limit for the number of pruning steps.\n",
    "        step_ch_sparsity (float): Channel sparsity target for each individual pruning step.\n",
    "                                  Influences how many candidates `pruner.step` proposes.\n",
    "                                  Smaller values lead to potentially finer steps.\n",
    "\n",
    "    Returns:\n",
    "        The pruned model.\n",
    "    \"\"\"\n",
    "    device = example_input.device\n",
    "    model.to(device) # Ensure model is on the correct device\n",
    "\n",
    "    print(f\"--- Starting Pruning (Strategy: {strategy['importance'].__class__.__name__}) ---\")\n",
    "    print(f\"Target MACs: {target_macs:,.0f}, Target Params: {target_params:,.0f}\")\n",
    "\n",
    "    # Instantiate the pruner\n",
    "    # Note: 'iterative_steps' in init is less critical here as the while loop controls iteration.\n",
    "    pruner = strategy['pruner'](\n",
    "        model,\n",
    "        example_input,\n",
    "        importance=strategy['importance'],\n",
    "        ch_sparsity=step_ch_sparsity, # Target sparsity *per step*\n",
    "        root_module_types=[nn.Conv2d], # Focus pruning on Conv layers\n",
    "        ignored_layers=[model.fc], # Don't prune the final classifier\n",
    "        # Optional: other pruner args like round_to might be useful depending on strategy/model\n",
    "        # round_to=8, # Example: commonly used for hardware efficiency\n",
    "    )\n",
    "\n",
    "    # Get initial state\n",
    "    current_macs, current_params = calculate_macs(model, example_input)\n",
    "    initial_macs, initial_params = current_macs, current_params # Keep for logging\n",
    "    print(f\"Initial | MACs: {current_macs:,.0f}, Params: {current_params:,.0f}\")\n",
    "\n",
    "    iteration = 0\n",
    "    model.eval() # Ensure model is in eval mode for pruning logic unless Taylor\n",
    "\n",
    "    while (current_macs > target_macs or current_params > target_params) and iteration < max_iterations:\n",
    "        iteration += 1\n",
    "        macs_before_step = current_macs\n",
    "        params_before_step = current_params\n",
    "\n",
    "        # --- Special handling for Importance methods requiring gradients ---\n",
    "        if isinstance(strategy['importance'], (tp.importance.TaylorImportance, tp.importance.GroupHessianImportance)):\n",
    "             model.train() # Need gradients\n",
    "             # Ensure requires_grad is True if it was turned off\n",
    "             for param in model.parameters():\n",
    "                 param.requires_grad_(True)\n",
    "\n",
    "             loss = model(example_input).mean() # Use mean or sum as dummy loss\n",
    "             try:\n",
    "                 loss.backward() # Calculate gradients needed for importance\n",
    "             except Exception as e:\n",
    "                 print(f\"Error during backward pass for importance calc (Iter {iteration}): {e}\")\n",
    "                 # Decide how to handle: break, skip step, etc.\n",
    "                 break # Safer to stop if backward fails\n",
    "\n",
    "        # --- Perform one step of interactive pruning ---\n",
    "        try:\n",
    "            # Get the next set of pruning candidates based on current importance\n",
    "            pruning_groups = list(pruner.step(interactive=True))\n",
    "        except Exception as e:\n",
    "             print(f\"Error during pruner.step() (Iter {iteration}): {e}\")\n",
    "             # Handle potential errors during dependency analysis or importance scoring\n",
    "             break # Stop if pruner step fails\n",
    "\n",
    "        if not pruning_groups:\n",
    "            print(f\"Iteration {iteration}: Pruner found no more candidates. Stopping.\")\n",
    "            break # No more structures can be pruned according to the strategy/dependencies\n",
    "\n",
    "        # --- Apply the pruning ---\n",
    "        for group in pruning_groups:\n",
    "            group.prune()\n",
    "\n",
    "        # --- Clean up gradients if calculated ---\n",
    "        if isinstance(strategy['importance'], (tp.importance.TaylorImportance, tp.importance.GroupHessianImportance)):\n",
    "            # Zero gradients to prevent interference with potential future training/fine-tuning\n",
    "            model.zero_grad()\n",
    "            model.eval() # Switch back to eval mode after grad calculation\n",
    "\n",
    "        # --- Recalculate metrics ---\n",
    "        # It's crucial to recalculate AFTER pruning is applied\n",
    "        current_macs, current_params = calculate_macs(model, example_input)\n",
    "\n",
    "        # --- Log progress ---\n",
    "        print(\n",
    "            f\"Iter {iteration: >3}/{max_iterations} | \"\n",
    "            f\"MACs: {macs_before_step:,.0f} -> {current_macs:,.0f} \"\n",
    "            f\"({(macs_before_step-current_macs)/macs_before_step*100:+.1f}%) | \"\n",
    "            f\"Params: {params_before_step:,.0f} -> {current_params:,.0f} \"\n",
    "            f\"({(params_before_step-current_params)/params_before_step*100:+.1f}%)\"\n",
    "        )\n",
    "\n",
    "        # --- Check for Stagnation (optional but recommended) ---\n",
    "        if current_macs >= macs_before_step and current_params >= params_before_step:\n",
    "            print(f\"Iteration {iteration}: No reduction in MACs or Params this step. Stopping to prevent loop.\")\n",
    "            # This might happen if the only prunable groups left have negligible impact\n",
    "            # or if there's an issue with the importance/pruning logic.\n",
    "            break\n",
    "\n",
    "    # --- Final Status Report ---\n",
    "    print(f\"--- Finished Pruning (Strategy: {strategy['importance'].__class__.__name__}) ---\")\n",
    "    if iteration >= max_iterations:\n",
    "        print(f\"Warning: Reached maximum pruning iterations ({max_iterations}).\")\n",
    "\n",
    "    final_macs, final_params = calculate_macs(model, example_input)\n",
    "    print(f\"Initial | MACs: {initial_macs:,.0f}, Params: {initial_params:,.0f}\")\n",
    "    print(f\"Final   | MACs: {final_macs:,.0f}, Params: {final_params:,.0f}\")\n",
    "    print(f\"Target  | MACs: {target_macs:,.0f}, Params: {target_params:,.0f}\")\n",
    "\n",
    "    macs_reduction = (initial_macs - final_macs) / initial_macs * 100 if initial_macs > 0 else 0\n",
    "    params_reduction = (initial_params - final_params) / initial_params * 100 if initial_params > 0 else 0\n",
    "    print(f\"Reduction | MACs: {macs_reduction:.2f}%, Params: {params_reduction:.2f}%\")\n",
    "\n",
    "    if final_macs > target_macs or final_params > target_params:\n",
    "         print(\"Warning: Pruning finished, but target threshold(s) were not fully met.\")\n",
    "\n",
    "    model.eval() # Ensure model is in eval mode finally\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
